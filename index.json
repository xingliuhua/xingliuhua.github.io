[{"categories":["项目总结"],"content":"项目中mysql date_time四舍五入问题","date":"2023-10-29","objectID":"/posts/%E9%A1%B9%E7%9B%AE_mysql_datetime%E5%9B%9B%E8%88%8D%E4%BA%94%E5%85%A5%E9%97%AE%E9%A2%98/","tags":["项目"],"title":"项目中mysql date_time四舍五入问题","uri":"/posts/%E9%A1%B9%E7%9B%AE_mysql_datetime%E5%9B%9B%E8%88%8D%E4%BA%94%E5%85%A5%E9%97%AE%E9%A2%98/"},{"categories":["项目总结"],"content":"背景 项目中向插入消息中心服务发送数据，消息字符串中有时间，格式是2006-01-02 15:04:05,于此同时向mysql数据库写数据，其中有字段create_time字段类型为date_time。发现有偶然消息中的时间和数据库的时间不一致的情况，数据库会慢一秒。 抽象一下场景，作为复现的步骤： 新建一个数据表 CREATE TABLE `user` ( `id` int(11) NOT NULL AUTO_INCREMENT, `create_time` datetime NOT NULL DEFAULT '1970-01-01 00:00:00', PRIMARY KEY (`id`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4; 代码示例 func main() { err := mysqlconn.InitDb() if err != nil { fmt.Println(err) return } db, err := mysqlconn.GetDb() if err != nil { fmt.Println(err) return } fmt.Println(db) now := time.Now() // 打印2023-10-29 16:40:45.759 fmt.Println(now.Format(\"2006-01-02 15:04:05.000\")) //打印2023-10-29 16:40:45 fmt.Println(now.Format(\"2006-01-02 15:04:05\")) u := User{ CreateTime: now, } // 打印INSERT INTO `user` (`create_time`) VALUES (\"2023-10-29 16:40:45.759\") err = db.Create(\u0026u).Error if err != nil { fmt.Println(err) return } } 四舍五入 经观察上方代码打印日志，并没有什么不妥，但是查询数据却发现 数据库里面是2023-10-29 08:40:46，慢了一秒。 原来是毫秒大于等于500就会自动四舍五入。 解决方案 第一种解决方案就是设置精度 alter table user change create_time create_time datetime(3) NOT NULL DEFAULT '1970-01-01 00:00:00'; 这样插入的数据是2023-10-29 08:52:51.510 第二种解决方案就是golang的时间去掉毫秒 now := time.Now().Truncate(time.Second) 第三种比较直接，但是个人最喜欢，就是项目中存储时间都用bigint存时间戳 ","date":"2023-10-29","objectID":"/posts/%E9%A1%B9%E7%9B%AE_mysql_datetime%E5%9B%9B%E8%88%8D%E4%BA%94%E5%85%A5%E9%97%AE%E9%A2%98/:0:0","tags":["项目"],"title":"项目中mysql date_time四舍五入问题","uri":"/posts/%E9%A1%B9%E7%9B%AE_mysql_datetime%E5%9B%9B%E8%88%8D%E4%BA%94%E5%85%A5%E9%97%AE%E9%A2%98/"},{"categories":["笔记"],"content":"项目中gin封装","date":"2022-06-06","objectID":"/posts/%E9%A1%B9%E7%9B%AE_gin%E5%B0%81%E8%A3%85/","tags":["项目"],"title":"项目中gin封装","uri":"/posts/%E9%A1%B9%E7%9B%AE_gin%E5%B0%81%E8%A3%85/"},{"categories":["笔记"],"content":"背景 项目中使用gin框架，一般的注册路由和函数都这样写 func main() { engine := gin.New() engine.GET(\"/test\",testHandler) engine.Run(\"localhost:8080\") } type TestReq struct { Name string } func testHandler(ctx *gin.Context) { // 解析参数 name:=ctx.Query(\"name\") testReq:=TestReq{Name: name} // 检查参数 checkTestReq(testReq) ... // 获取session sessionId:=ctx.GetHeader(\"Session-Id\") ... } 在这里我们只写一些核心伪代码。 我们经常要再hanler里去解析参数，校验参数，获取登录者的session，这样才能校验登录拿到userId等操作。 每一个hanler里面基本都要写一套这样的东西，好烦！ 当然我们可以把这一块封装一下，把解析参数、检查参数、获取session封装个函数，然后在各个handler调用一下不就好了吗？ 可以，但是每次都调用一下也够的。 如果我们每次都只处理这样的函数就好了： func MyTestHandler(ctx *gin.Context,userId int,req *TestReq) { // req是已经过校验合法的 // 安心处理自己的逻辑 } 修改注册路由 我们首先要修改路由注册原始的方法。 func main() { engine := gin.New() engine.GET(\"/test\",GenHandlerFunc(MyTestHandler)) engine.Run(\"localhost:8080\") } func GenHandlerFunc(myHandlerFunc interface{}) gin.HandlerFunc { return func(context *gin.Context) { // 代码1处。调用myHandlerFunc } } func MyTestHandler(ctx *gin.Context,userId int,req *TestReq) { // req是已经过校验合法的 // 安心处理自己的逻辑 } MyTestHandler函数是用着方便了，可是GenHandlerFunc可麻烦了。代码1处调用myHandlerFunc谈何容易。 以后有越来越来多的MyTestHandler，每个接口都有自己的req类型，怎么能动态填充千变万化的req参数呢? 封装 首先为了校验req参数是否合法，我们定义一个参数，每个req结构体都要实现该方法 type ValidateAble interface { Valid() error } type TestReq struct { Name string `form:\"name\"` } func (req *TestReq) Valid() error { if req.Name==\"\"{ return errors.New(\"name is empty\") } return nil } func GenHandlerFunc(myHandlerFunc interface{}) gin.HandlerFunc { return func(context *gin.Context) { // 反射函数 controllerFuncType := reflect.TypeOf(myHandlerFunc) if controllerFuncType.Kind() != reflect.Func { panic( \"controllerFunc not a func\") } // 调用函数所需要的参数 args := make([]reflect.Value, 0) args = append(args, reflect.ValueOf(context)) // 为了方便我们假设userId是10,开发中根据自己业务去取 args = append(args, reflect.ValueOf(10)) for i := 0; i \u003c controllerFuncType.NumIn(); i++ { inParam := controllerFuncType.In(i) // 判断req是否实现了ValidateAble接口 if !inParam.Implements(reflect.TypeOf((*ValidateAble)(nil)).Elem()) { continue } inParamVale := reflect.New(inParam.Elem()) req, ok := inParamVale.Interface().(ValidateAble) if !ok { panic(context.Request.URL.Path + \" req convert requestutil.ValidateAble err\") } err:=context.ShouldBind(req) fmt.Println(req) if err != nil { fmt.Println(err) } err = req.Valid() if err != nil { context.JSON(404,gin.H{ \"message\":\"参数错误\", }) return } args = append(args, reflect.ValueOf(req)) } reflect.ValueOf(myHandlerFunc).Call(args) } } 其实核心代码是inParamVale := reflect.New(inParam.Elem()) 我们看下reflect.New()方法 // New returns a Value representing a pointer to a new zero value // for the specified type. That is, the returned Value's Type is PtrTo(typ). func New(typ Type) Value { if typ == nil { panic(\"reflect: New(nil)\") } t := typ.(*rtype) ptr := unsafe_New(t) fl := flag(Ptr) return Value{t.ptrTo(), ptr, fl} } 我们只要看注释就行了，写的很明白， 返回一个零值的结构体的指针。虽然我们不知道到底需要哪个类型的req,没关系。 ","date":"2022-06-06","objectID":"/posts/%E9%A1%B9%E7%9B%AE_gin%E5%B0%81%E8%A3%85/:0:0","tags":["项目"],"title":"项目中gin封装","uri":"/posts/%E9%A1%B9%E7%9B%AE_gin%E5%B0%81%E8%A3%85/"},{"categories":["笔记"],"content":"项目中高并发秒杀处理","date":"2022-06-06","objectID":"/posts/%E9%A1%B9%E7%9B%AE_%E9%AB%98%E5%B9%B6%E5%8F%91%E7%A7%92%E6%9D%80/","tags":["项目"],"title":"项目中高并发秒杀处理","uri":"/posts/%E9%A1%B9%E7%9B%AE_%E9%AB%98%E5%B9%B6%E5%8F%91%E7%A7%92%E6%9D%80/"},{"categories":["笔记"],"content":"高并发秒杀问题 流程： 先来看秒杀活动流程： 活动入口 客户端秒杀列表页 商品秒杀详情页 点击购买 携带活动ID，商品ID提交服务端 服务端判断活动是否结束、商品是否在此活动中、修改数据库商品状态、下单、给用户发短信、给运营发邮件 问题： 问题1： 不到秒杀时间，用户疯狂点击秒杀按钮，给服务端带来额外不必要的压力。 问题2： 秒杀详情用户疯狂的下拉刷新页面。mysql压力大，采用redis缓存活动到时短时期内大量请求到redis没有数据，缓存击穿。 问题3： 同时提交购买，超卖问题 问题4： 秒杀成功后期操作多耗时问题 优化： 未到秒杀时间客户端先本地拦截部分请求，减轻服务端压力。 商品详情缓存redis，活动快开始时预放置缓存中，商品详情秒杀活动时商品信息几乎不变动。 避免缓存击穿，热点商品缓存失效时，分布式锁实现单一协程读mysql。 服务端把商品状态和参加活动的信息放置在缓存中，避免大量请求mysql。 商品在售状态，抢夺分布式锁，抢夺成功后，修改商品redis商品状态，发送mq，将后续任务异步处理，删除分布式锁。未抢到分布式锁返回客户端排队中，客户端进行短轮询。查询到商品redis状态已修改，提示商品已售出。 mq读取消息，下单，并用乐观锁检查，防止超卖。再处理后续任务。 防止黑客直接请求接口，接口验签。 令牌桶流量控制。 这里为啥还要乐观锁呢？不是每次只有一个用户会拿到商品对应的分布式锁吗？ 其实，我们想，如果A拿到分布式锁，分布式锁的过期时间我们设置为2秒，那两秒过去了，订单处理还没完成，那么B可能就拿到分布式锁了，就会出现并发修改mysql的情况。 ","date":"2022-06-06","objectID":"/posts/%E9%A1%B9%E7%9B%AE_%E9%AB%98%E5%B9%B6%E5%8F%91%E7%A7%92%E6%9D%80/:0:0","tags":["项目"],"title":"项目中高并发秒杀处理","uri":"/posts/%E9%A1%B9%E7%9B%AE_%E9%AB%98%E5%B9%B6%E5%8F%91%E7%A7%92%E6%9D%80/"},{"categories":["笔记"],"content":"项目中高并发抢红包","date":"2022-06-06","objectID":"/posts/%E9%A1%B9%E7%9B%AE_%E7%BA%A2%E5%8C%85%E9%97%AE%E9%A2%98/","tags":["项目"],"title":"项目中高并发抢红包","uri":"/posts/%E9%A1%B9%E7%9B%AE_%E7%BA%A2%E5%8C%85%E9%97%AE%E9%A2%98/"},{"categories":["笔记"],"content":"高并发抢红包问题 ","date":"2022-06-06","objectID":"/posts/%E9%A1%B9%E7%9B%AE_%E7%BA%A2%E5%8C%85%E9%97%AE%E9%A2%98/:0:0","tags":["项目"],"title":"项目中高并发抢红包","uri":"/posts/%E9%A1%B9%E7%9B%AE_%E7%BA%A2%E5%8C%85%E9%97%AE%E9%A2%98/"},{"categories":["笔记"],"content":"红包随机值 为了避免抢红包时实时计算效率慢，采用创建红包活动时就提前计算入红包库。 分配红包尽可能平均，假如现剩60块钱，还要分成5个红包，平均是12，那么应该在0-24之间随机 简单示例代码： // 分红包 func GenerateReadPackage(total int, num int) []int { rand.Seed(time.Now().UnixNano()) packages := make([]int, num) left := num for i := 0; i \u003c num; i++ { if i == num-1 { // 最后一个不用分 packages[i] = total break } randInt := rand.Intn(total / left * 2) fmt.Println(total, left, total/left, randInt) total = total - randInt left-- packages[i] = randInt } return packages } ","date":"2022-06-06","objectID":"/posts/%E9%A1%B9%E7%9B%AE_%E7%BA%A2%E5%8C%85%E9%97%AE%E9%A2%98/:1:0","tags":["项目"],"title":"项目中高并发抢红包","uri":"/posts/%E9%A1%B9%E7%9B%AE_%E7%BA%A2%E5%8C%85%E9%97%AE%E9%A2%98/"},{"categories":["笔记"],"content":"并发抢 红包表中有used字段表明是否已经被抢。 如果并发抢都查mysql，采用悲观锁造成大量用户等待，不可取。如果采用乐观锁，势必会造成大量用户更新红包used字段时失败，mysql数据库压力也大。 采用redis分布式锁可以减轻mysql压力，但是也会造成每次只有一个人拿到分布式锁后，才能去更新红包表中一个数据。 采用分段分布式锁可以减少对同一锁的压力。 我们在redis分成5个锁，lock1-5。另外还有5个list，list1-5，list元素为红包ID。 用户抢红包时根据用户ID的最后一位来判断要抢那个锁，[0-1]抢lock1，[2-3]抢lock2，以此类推。 当用户1拿到lock1后，取出list1中一个红包ID，去更新mysql红包状态处理后续工作。如果list1中元素没有了去数据库中一次取50个。红包表还有个pre_used字段表明已经被分配到redis list中 有种极端情况，redis中list1-5同时没有了，都去mysql取50个，并更新这50个pre_used为1，如果使用乐观锁，会造成只有一个成功，所以这里要用乐观锁再进行二次判断，更新状态时发现已经被别人取走，再取50条。 ","date":"2022-06-06","objectID":"/posts/%E9%A1%B9%E7%9B%AE_%E7%BA%A2%E5%8C%85%E9%97%AE%E9%A2%98/:2:0","tags":["项目"],"title":"项目中高并发抢红包","uri":"/posts/%E9%A1%B9%E7%9B%AE_%E7%BA%A2%E5%8C%85%E9%97%AE%E9%A2%98/"},{"categories":["笔记"],"content":"项目中链路追踪","date":"2022-06-06","objectID":"/posts/%E9%A1%B9%E7%9B%AE_%E9%93%BE%E8%B7%AF%E8%BF%BD%E8%B8%AA/","tags":["项目"],"title":"项目中链路追踪","uri":"/posts/%E9%A1%B9%E7%9B%AE_%E9%93%BE%E8%B7%AF%E8%BF%BD%E8%B8%AA/"},{"categories":["笔记"],"content":"背景 为了测试环境调试方便和线上环境快速定位问题，客户端请求接口我们要给他们返回一个traceID来标识本次请求。这样拿着traceId来找我们，我们就能分析日志方便了。我们要在中间件中生成唯一traceId,然后后面的请求处理中要拿到这个ID，接口日志打印啊，sql日期打印啊，上传报错平台啊。 生成traceId 我们一般在日志中间件中生成traceID,接口日志中间件执行比较早的中间件。 func LogRequestMiddleware(c *gin.Context) { blw := \u0026bodyLogWriter{body: bytes.NewBufferString(\"\"), ResponseWriter: c.Writer} c.Writer = blw startTime := time.Now().UnixNano() requestId := idutil.GetSnowFlakeId() c.Set(constconfig.GIN_CONTEXT_REQUEST_ID_KEY, requestId) c.Writer.Header().Add(\"Request-Id\", requestId) c.Next() // 用于获取接口大概处理时间 endTime := time.Now().UnixNano() // get request headers ... // get response headers ... // set response body if c.Writer.Status() == http.StatusOK { fmt.Println(string(blw.body.Bytes())) err := json.Unmarshal(blw.body.Bytes(), \u0026response) if err != nil { logutil.LogError(c, err, \"打印请求日志错误，response body 反序列化错误\", nil) return } ... } // 拿到请求信息和响应信息，带上traceID打印 logutil.LogRequest(\u0026httpLog) } 我们c.Set(constconfig.GIN_CONTEXT_REQUEST_ID_KEY, requestId)来给context中加入了traceId,后面的中间件都可以取到了。 gorm中打印traceId gorm默认的logger是 var Default = New(log.New(os.Stdout, \"\\r\\n\", log.LstdFlags), Config{ SlowThreshold: 100 * time.Millisecond, LogLevel: Warn, Colorful: true, }) 慢查询是100毫秒，最主要是标准输出。更别想自定义打印我们的traceId了。 type MySqlLogger struct { logger.Interface } func (m MySqlLogger) Trace(ctx context.Context, begin time.Time, fc func() (string, int64), err error) { sql, rows := fc() _, file, line, _ := runtime.Caller(3) debug := configutil.GoFrameWorkConfigInfo.App.Debug if debug { fmt.Printf(\"\\n%s\\n%d行受影响\\n%s:%d\\nerr:%+v\\n\", sql, rows, file, line, err) } logutil.LogSql(map[string]interface{}{ \"sql\": sql, \"rows\": rows, \"caller\": fmt.Sprintf(\"%s:%d\", file, line), \"message\": ctx.Value(\"sqlMsg\"), \"requestId\": ctx.Value(\"requestId\"), \"userId\": ctx.Value(\"userId\"), \"err\": err, }) } newLogger := logger.New( log.New(os.Stdout, \"\\r\\n\", log.LstdFlags), // io writer logger.Config{ SlowThreshold: time.Second, // 慢 SQL 阈值 LogLevel: logger.Info, // Log level Colorful: true, // 禁用彩色打印 }, ) mySqlLogger := MySqlLogger{newLogger} gormDb.Logger = mySqlLogger 我们重写默认的logger的Trace方法,参数中这个context并不是gin.context，是我们要调用gorm的db.WithContext方法传进来的context.context。这个gorm v2版本的新增方法，这样我们就能轻松拿到上下文了。 ctx:=context.WithValue(context.Background(),\"requestId\",10) db.WithContext(ctx).Raw(\"select * from user\").Scan(\u0026users) ","date":"2022-06-06","objectID":"/posts/%E9%A1%B9%E7%9B%AE_%E9%93%BE%E8%B7%AF%E8%BF%BD%E8%B8%AA/:0:0","tags":["项目"],"title":"项目中链路追踪","uri":"/posts/%E9%A1%B9%E7%9B%AE_%E9%93%BE%E8%B7%AF%E8%BF%BD%E8%B8%AA/"},{"categories":["项目总结"],"content":"项目中错误处理","date":"2022-05-23","objectID":"/posts/%E9%A1%B9%E7%9B%AE_%E9%94%99%E8%AF%AF%E5%A4%84%E7%90%86/","tags":["项目"],"title":"项目中错误处理","uri":"/posts/%E9%A1%B9%E7%9B%AE_%E9%94%99%E8%AF%AF%E5%A4%84%E7%90%86/"},{"categories":["项目总结"],"content":"项目中底层函数错误上报不能带上文件名和行号，后面查找很难，有些函数甚至同样的错误信息，比如 func AAA() error { if ...{ return errors.New(\"file write err\") } ... if ...{ return errors.New(\"file write err\") } } pkg/errors 使用实例： import ( \"errors\" \"fmt\" pkgError \"github.com/pkg/errors\" ) func main() { err := test3() fmt.Printf(\"%+v\\n\",err) // 打印原始err及调用栈信息 fmt.Println(pkgError.Cause(err)) // 获取原始err fmt.Println(pkgError.Is(err,TestErr)) // 对比原始err } func test3() error { return pkgError.WithMessage(test2(),\"this is test3\") } func test2() error { return pkgError.WithMessage(test1(),\"this is test2\") } var TestErr =errors.New(\"test err\") func test1() error { return pkgError.Wrap(TestErr,\"this is test1\") } 我们只需要在第一次出现err的地方wrap即可，不然会打印重复的调用栈信息，没有啥用。 go1.3error 1.3对参考pkg/errors引入As,Is,Wrap,Unwrap func main() { err := test3() fmt.Printf(\"%+v\\n\",err) // 打印原始err及调用栈信息 fmt.Println(errors.Unwrap(err)) // 和%w是反向操作,一次只能剥一层 fmt.Println(errors.Is(err,TestErr)) // 对比原始err } func test3() error { return fmt.Errorf(\"%w,this is test3\",test2()) } func test2() error { return fmt.Errorf(\"%w,this is test2\",test1()) } var TestErr =errors.New(\"test err\") func test1() error { return fmt.Errorf(\"%w,this is test1\",TestErr) // 通过%w来wrap error } 打印： test err,this is test1,this is test2,this is test3 test err,this is test1,this is test2 true 其实还是没有打印调用栈，并不能快速定位到文件和行 引发的打印思考 pkg/errors，要想打印出调用栈必须使用%+v，%v和%#v都不行,而且他这打印出非常规格式的属于定制，是怎么做到的？ 看了代码发现他有函数： func (f *fundamental) Format(s fmt.State, verb rune) { switch verb { case 'v': if s.Flag('+') { io.WriteString(s, f.msg) f.stack.Format(s, verb) return } fallthrough case 's': io.WriteString(s, f.msg) case 'q': fmt.Fprintf(s, \"%q\", f.msg) } } 在这里判断了+v符号，所以只有这种情形才能打印,如果定义了一个结构体，想自定义打印样式，可以实现这个方法。 ","date":"2022-05-23","objectID":"/posts/%E9%A1%B9%E7%9B%AE_%E9%94%99%E8%AF%AF%E5%A4%84%E7%90%86/:0:0","tags":["项目"],"title":"项目中错误处理","uri":"/posts/%E9%A1%B9%E7%9B%AE_%E9%94%99%E8%AF%AF%E5%A4%84%E7%90%86/"},{"categories":["笔记"],"content":"websocket协议","date":"2022-05-18","objectID":"/posts/network_websocket/","tags":["网络"],"title":"websocket协议","uri":"/posts/network_websocket/"},{"categories":["笔记"],"content":"概述 HTML5 开始提供的一种浏览器与服务器进行全双工通讯的网络技术，属于应用层协议。它基于 TCP 传输协议，并复用 HTTP 的握手通道。 说到优点，这里的对比参照物是 HTTP 协议，概括地说就是：支持双向通信，更灵活，更高效，可扩展性更好。 1）支持双向通信，实时性更强。WebSocket 的默认端口也选择了 80 和 443，因为现在互联网上的防火墙屏蔽了绝大多数的端口，只对 HTTP 的 80、443 端口“放行”。 2）更好的二进制支持。WebSocket 采用了二进制帧结构，语法、语义与 HTTP 完全不兼容，但因为它的主要运行环境是浏览器，为了便于推广和应用，就不得不“搭便车”，在使用习惯上尽量向 HTTP 靠拢，这就是它名字里“Web”的含义。 3）较少的控制开销。连接创建后，ws 客户端、服务端进行数据交换时，协议控制的数据包头部较小。在不包含头部的情况下，服务端到客户端的包头只有 2~10 字节（取决于数据包长度），客户端到服务端的的话，需要加上额外的 4 字节的掩码。而 HTTP 协议每次通信都需要携带完整的头部； 4）支持扩展。ws 协议定义了扩展，用户可以扩展协议，或者实现自定义的子协议。（比如支持自定义压缩算法等）。WebSocket 没有使用 TCP 的“IP 地址 + 端口号”，而是延用了 HTTP 的 URI 格式，但开头的协议名不是“http”，引入的是两个新的名字：“ws”和“wss”，分别表示明文和加密的 WebSocket 协议。 如何建立连接 前面提到，WebSocket 复用了 HTTP 的握手通道。具体指的是，客户端通过 HTTP 请求与 WebSocket 服务端协商升级协议。协议升级完成后，后续的数据交换则遵照 WebSocket 的协议。 ","date":"2022-05-18","objectID":"/posts/network_websocket/:0:0","tags":["网络"],"title":"websocket协议","uri":"/posts/network_websocket/"},{"categories":["笔记"],"content":"客户端申请协议升级 首先，客户端发起协议升级请求。可以看到，采用的是标准的 HTTP 报文格式，且只支持 GET 方法。 GET / HTTP/1.1 Host: localhost:8080 Origin:http: //127.0.0.1:3000 Connection: Upgrade Upgrade: websocket Sec-WebSocket-Version: 13 Sec-WebSocket-Key: w4v7O6xFTi36lq3RNcgctw== 重点请求首部意义如下： 1）Connection: Upgrade：表示要升级协议； 2）Upgrade: websocket：表示要升级到 websocket 协议； 3）Sec-WebSocket-Version: 13：表示 websocket 的版本。如果服务端不支持该版本，需要返回一个 Sec-WebSocket-Versionheader，里面包含服务端支持的版本号； 4）Sec-WebSocket-Key：与后面服务端响应首部的 Sec-WebSocket-Accept 是配套的，提供基本的防护，比如恶意的连接，或者无意的连接。 注意：上面请求省略了部分非重点请求首部。由于是标准的 HTTP 请求，类似 Host、Origin、Cookie 等请求首部会照常发送。在握手阶段，可以通过相关请求首部进行 安全限制、权限校验等 ","date":"2022-05-18","objectID":"/posts/network_websocket/:1:0","tags":["网络"],"title":"websocket协议","uri":"/posts/network_websocket/"},{"categories":["笔记"],"content":"服务端响应协议升级 服务端返回内容如下，状态代码 101 表示协议切换。到此完成协议升级，后续的数据交互都按照新的协议来。 HTTP/1.1 101 Switching Protocols Connection:Upgrade Upgrade: websocket Sec-WebSocket-Accept: Oy4NRAQ13jhfONC7bP8dTKb4PTU= Sec-WebSocket-Accept 的计算 Sec-WebSocket-Accept 根据客户端请求首部的 Sec-WebSocket-Key 计算出来。 计算公式为： 1）将 Sec-WebSocket-Key 跟 258EAFA5-E914-47DA-95CA-C5AB0DC85B11 拼接； 2）通过 SHA1 计算出摘要，并转成 base64 字符串。 伪代码如下： toBase64( sha1( Sec-WebSocket-Key + 258EAFA5-E914-47DA-95CA-C5AB0DC85B11 ) ) websocket帧格式 客户端、服务端数据的交换，离不开数据帧格式的定义。因此，在实际讲解数据交换之前，我们先来看下 WebSocket 的数据帧格式。 WebSocket 客户端、服务端通信的最小单位是帧（frame），由 1 个或多个帧组成一条完整的消息（message）。 1）发送端：将消息切割成多个帧，并发送给服务端； 2）接收端：接收消息帧，并将关联的帧重新组装成完整的消息。 1）FIN：1 个比特。 如果是 1，表示这是消息（message）的最后一个分片（fragment），如果是 0，表示不是是消息（message）的最后一个分片（fragment）。 2）RSV1, RSV2, RSV3：各占 1 个比特。 一般情况下全为 0。当客户端、服务端协商采用 WebSocket 扩展时，这三个标志位可以非 0，且值的含义由扩展进行定义。如果出现非零的值，且并没有采用 WebSocket 扩展，连接出错。 3）Opcode：4 个比特。 操作代码，Opcode 的值决定了应该如何解析后续的数据载荷（data payload）。如果操作代码是不认识的，那么接收端应该断开连接（fail the connection）。 可选的操作代码如下： %x0：表示一个延续帧。当 Opcode 为 0 时，表示本次数据传输采用了数据分片，当前收到的数据帧为其中一个数据分片。 %x1：表示这是一个文本帧（frame） %x2：表示这是一个二进制帧（frame） %x3-7：保留的操作代码，用于后续定义的非控制帧。 %x8：表示连接断开。 %x9：表示这是一个 ping 操作。 %xA：表示这是一个 pong 操作。 %xB-F：保留的操作代码，用于后续定义的控制帧。 4）Mask：1 个比特。 表示是否要对数据载荷进行掩码操作。从客户端向服务端发送数据时，需要对数据进行掩码操作；从服务端向客户端发送数据时，不需要对数据进行掩码操作。 如果服务端接收到的数据没有进行过掩码操作，服务端需要断开连接。 如果 Mask 是 1，那么在 Masking-key 中会定义一个掩码键（masking key），并用这个掩码键来对数据载荷进行反掩码。所有客户端发送到服务端的数据帧，Mask 都是 1。 掩码的算法、用途在下一小节讲解。 5）Payload length：数据载荷的长度，单位是字节。为 7 位，或 7+16 位，或 1+64 位。 假设数 Payload length === x，如果： x 为 0~126：数据的长度为 x 字节。 x 为 126：后续 2 个字节代表一个 16 位的无符号整数，该无符号整数的值为数据的长度。 x 为 127：后续 8 个字节代表一个 64 位的无符号整数（最高位为 0），该无符号整数的值为数据的长度。 此外，如果 payload length 占用了多个字节的话，payload length 的二进制表达采用网络序（big endian，重要的位在前）。 6）Masking-key：0 或 4 字节（32 位） 所有从客户端传送到服务端的数据帧，数据载荷都进行了掩码操作，Mask 为 1，且携带了 4 字节的 Masking-key。如果 Mask 为 0，则没有 Masking-key。 备注：载荷数据的长度，不包括 mask key 的长度。 7）Payload data：(x+y) 字节 载荷数据：包括了扩展数据、应用数据。其中，扩展数据 x 字节，应用数据 y 字节。 扩展数据：如果没有协商使用扩展的话，扩展数据数据为 0 字节。所有的扩展都必须声明扩展数据的长度，或者可以如何计算出扩展数据的长度。此外，扩展如何使用必须在握手阶段就协商好。如果扩展数据存在，那么载荷数据长度必须将扩展数据的长度包含在内。 应用数据：任意的应用数据，在扩展数据之后（如果存在扩展数据），占据了数据帧剩余的位置。载荷数据长度 减去 扩展数据长度，就得到应用数据的长度。 数据传输 一旦 WebSocket 客户端、服务端建立连接后，后续的操作都是基于数据帧的传递。 WebSocket 根据 opcode 来区分操作的类型。比如 0x8 表示断开连接，0x0-0x2 表示数据交互。 数据分片 WebSocket 的每条消息可能被切分成多个数据帧。当 WebSocket 的接收方收到一个数据帧时，会根据 FIN 的值来判断，是否已经收到消息的最后一个数据帧。 FIN=1 表示当前数据帧为消息的最后一个数据帧，此时接收方已经收到完整的消息，可以对消息进行处理。FIN=0，则接收方还需要继续监听接收其余的数据帧。 此外，opcode 在数据交换的场景下，表示的是数据的类型。0x01 表示文本，0x02 表示二进制。而 0x00 比较特殊，表示延续帧（continuation frame），要和前面的合一块。 来个例子： 第一条消息： FIN=1, 表示是当前消息的最后一个数据帧。服务端收到当前数据帧后，可以处理消息。opcode=0x1，表示客户端发送的是文本类型。 第二条消息： 1）FIN=0，opcode=0x1，表示发送的是文本类型，且消息还没发送完成，还有后续的数据帧； 2）FIN=0，opcode=0x0，表示消息还没发送完成，还有后续的数据帧，当前的数据帧需要接在上一条数据帧之后； 3）FIN=1，opcode=0x0，表示消息已经发送完成，没有后续的数据帧，当前的数据帧需要接在上一条数据帧之后。服务端可以将关联的数据帧组装成完整的消息。 Client: FIN=1, opcode=0x1, msg=“hello” Server: (process complete message immediately) Hi. Client: FIN=0, opcode=0x1, msg=“and a” Server: (listening, new message containing text started) Client: FIN=0, opcode=0x0, msg=“happy new” Server: (listening, payload concatenated to previous message) Client: FIN=1, opcode=0x0, msg=“year!” Server: (process complete message) Happy new year to you too! ","date":"2022-05-18","objectID":"/posts/network_websocket/:2:0","tags":["网络"],"title":"websocket协议","uri":"/posts/network_websocket/"},{"categories":["笔记"],"content":"mysql慢查询分析","date":"2022-04-20","objectID":"/posts/mysql_%E6%85%A2%E6%9F%A5%E8%AF%A2%E5%88%86%E6%9E%90/","tags":["mysql"],"title":"mysql慢查询分析","uri":"/posts/mysql_%E6%85%A2%E6%9F%A5%E8%AF%A2%E5%88%86%E6%9E%90/"},{"categories":["笔记"],"content":"InnoDB中，因为直接操作磁盘会比较慢，所以加了一层内存提提速，叫buffer pool，这里面，放了很多内存页，每一页16KB，有些内存页放的是数据库表里看到的那种一行行的数据，有些则是放的索引信息。 查询SQL到了InnoDB中。会根据前面优化器里计算得到的索引，去查询相应的索引页，如果不在buffer pool里则从磁盘里加载索引页。再通过索引页加速查询，得到数据页的具体位置。如果这些数据页不在buffer pool中，则从磁盘里加载进来。 最后将得到的数据结果返回给客户端。 profiling 如果上面的流程比较慢的话，我们可以通过开启profiling看到流程慢在哪。 mysql\u003e set profiling=ON; Query OK, 0 rows affected, 1 warning (0.00 sec) mysql\u003e show variables like 'profiling'; +---------------+-------+ | Variable_name | Value | +---------------+-------+ | profiling | ON | +---------------+-------+ 1 row in set (0.00 sec) 然后正常执行sql语句。 这些SQL语句的执行时间都会被记录下来，此时你想查看有哪些语句被记录下来了，可以执行 show profiles; mysql\u003e show profiles; +----------+------------+---------------------------------------------------+ | Query_ID | Duration | Query | +----------+------------+---------------------------------------------------+ | 1 | 0.06811025 | select * from user where age\u003e=60 | | 2 | 0.00151375 | select * from user where gender = 2 and age = 80 | | 3 | 0.00230425 | select * from user where gender = 2 and age = 60 | | 4 | 0.00070400 | select * from user where gender = 2 and age = 100 | | 5 | 0.07797650 | select * from user where age!=60 | +----------+------------+---------------------------------------------------+ 5 rows in set, 1 warning (0.00 sec) 关注下上面的query_id，比如select * from user where age\u003e=60对应的query_id是1，如果你想查看这条SQL语句的具体耗时，那么可以执行以下的命令。 mysql\u003e show profile for query 1; +----------------------+----------+ | Status | Duration | +----------------------+----------+ | starting | 0.000074 | | checking permissions | 0.000010 | | Opening tables | 0.000034 | | init | 0.000032 | | System lock | 0.000027 | | optimizing | 0.000020 | | statistics | 0.000058 | | preparing | 0.000018 | | executing | 0.000013 | | Sending data | 0.067701 | | end | 0.000021 | | query end | 0.000015 | | closing tables | 0.000014 | | freeing items | 0.000047 | | cleaning up | 0.000027 | +----------------------+----------+ 15 rows in set, 1 warning (0.00 sec) 通过上面的各个项，大家就可以看到具体耗时在哪。比如从上面可以看出Sending data的耗时最大. 连接数过小 索引相关的原因我们聊完了，我们来聊聊，除了索引之外，还有哪些因素会限制我们的查询速度的。 我们可以看到，mysql的server层里有个连接管理，它的作用是管理客户端和mysql之间的长连接。 正常情况下，客户端与server层如果只有一条连接，那么在执行sql查询之后，只能阻塞等待结果返回，如果有大量查询同时并发请求，那么后面的请求都需要等待前面的请求执行完成后，才能开始执行。 而连接数过小的问题，受数据库和客户端两侧同时限制。 ","date":"2022-04-20","objectID":"/posts/mysql_%E6%85%A2%E6%9F%A5%E8%AF%A2%E5%88%86%E6%9E%90/:0:0","tags":["mysql"],"title":"mysql慢查询分析","uri":"/posts/mysql_%E6%85%A2%E6%9F%A5%E8%AF%A2%E5%88%86%E6%9E%90/"},{"categories":["笔记"],"content":"数据库连接数过小 Mysql的最大连接数默认是100, 最大可以达到16384。 可以通过设置mysql的max_connections参数，更改数据库的最大连接数。 ","date":"2022-04-20","objectID":"/posts/mysql_%E6%85%A2%E6%9F%A5%E8%AF%A2%E5%88%86%E6%9E%90/:1:0","tags":["mysql"],"title":"mysql慢查询分析","uri":"/posts/mysql_%E6%85%A2%E6%9F%A5%E8%AF%A2%E5%88%86%E6%9E%90/"},{"categories":["笔记"],"content":"应用侧连接数过小 应用侧与mysql底层的连接，是基于TCP协议的长链接，而TCP协议，需要经过三次握手和四次挥手来实现建连和释放。如果我每次执行sql都重新建立一个新的连接的话，那就要不断握手和挥手，这很耗时。所以一般会建立一个长连接池，连接用完之后，塞到连接池里，下次要执行sql的时候，再从里面捞一条连接出来用。 连接池，一般会有个大小。这个大小就控制了你的连接数最大值 buffer pool太小 我们在前面的数据库查询流程里，提到了进了innodb之后，会有一层内存buffer pool，用于将磁盘数据页加载到内存页中，只要查询到buffer pool里有，就可以直接返回，否则就要走磁盘IO，那就慢了。 下面的命令查询到buffer pool的大小，单位是Byte。 mysql\u003e show global variables like 'innodb_buffer_pool_size'; +-------------------------+-----------+ | Variable_name | Value | +-------------------------+-----------+ | innodb_buffer_pool_size | 134217728 | +-------------------------+-----------+ 1 row in set (0.01 sec) 怎么知道buffer pool是不是太小了？ 这个我们可以看buffer pool的缓存命中率。 通过 show status like ‘Innodb_buffer_pool_%’;可以看到跟buffer pool有关的一些信息。 Innodb_buffer_pool_read_requests表示读请求的次数。 Innodb_buffer_pool_reads 表示从物理磁盘中读取数据的请求次数。 一般情况下buffer pool命中率都在99%以上，如果低于这个值，才需要考虑加大innodb buffer pool的大小。 ","date":"2022-04-20","objectID":"/posts/mysql_%E6%85%A2%E6%9F%A5%E8%AF%A2%E5%88%86%E6%9E%90/:2:0","tags":["mysql"],"title":"mysql慢查询分析","uri":"/posts/mysql_%E6%85%A2%E6%9F%A5%E8%AF%A2%E5%88%86%E6%9E%90/"},{"categories":["笔记"],"content":"go运行目录","date":"2022-03-21","objectID":"/posts/go_%E8%BF%90%E8%A1%8C%E7%9B%AE%E5%BD%95/","tags":["golang"],"title":"go运行目录","uri":"/posts/go_%E8%BF%90%E8%A1%8C%E7%9B%AE%E5%BD%95/"},{"categories":["笔记"],"content":"获取运行目录和工作目录 二进制hello文件所在文件为/a/b/c/hello 如果在目录/c/d/e目录下执行文件hello /a/b/c/hello f.txt fmt.Println(getCurrentAbPathByExecutable()) // 获取当前执行程序所在的绝对路径 func getCurrentAbPathByExecutable() string { exePath, err := os.Executable() if err != nil { log.Fatal(err) } res, _ := filepath.EvalSymlinks(filepath.Dir(exePath)) return res } 上面的代码会打印/a/b/c/hello getwd, err := os.Getwd() fmt.Println(getwd, err) abs, err := filepath.Abs(os.Args[1]) fmt.Println(abs, err) os.Getwd()和命令行中pwd命令效果一样，返回当前工作目录，此时对应的是/c/d/e。 os.Args[1]是f.txt，filepath.Abs方法会自动判断f.txt是相对目录，就会加上/c/d/e，所以会打印/c/d/e/f.txt ","date":"2022-03-21","objectID":"/posts/go_%E8%BF%90%E8%A1%8C%E7%9B%AE%E5%BD%95/:0:0","tags":["golang"],"title":"go运行目录","uri":"/posts/go_%E8%BF%90%E8%A1%8C%E7%9B%AE%E5%BD%95/"},{"categories":["笔记"],"content":"go 平滑重启endless方案","date":"2021-12-02","objectID":"/posts/go_%E5%B9%B3%E6%BB%91%E9%87%8D%E5%90%AF_endless/","tags":["golang"],"title":"go 平滑重启endless方案","uri":"/posts/go_%E5%B9%B3%E6%BB%91%E9%87%8D%E5%90%AF_endless/"},{"categories":["笔记"],"content":"信号处理 Go 信号通知通过在 Channel 上发送 os.Signal 值来工作。如我们如果使用 Ctrl+C，那么会触发 SIGINT 信号，操作系统会中断该进程的正常流程，并进入相应的信号处理函数执行操作，完成后再回到中断的地方继续执行。 func main() { sigs := make(chan os.Signal, 1) done := make(chan bool, 1) // 监听信号 signal.Notify(sigs, syscall.SIGINT, syscall.SIGTERM) go func() { // 接收到信号返回 sig := \u003c-sigs fmt.Println() fmt.Println(sig) done \u003c- true }() fmt.Println(\"awaiting signal\") // 等待信号的接收 \u003c-done fmt.Println(\"exiting\") } 我们就可以监听 SIGINT 和 SIGTERM 信号。当 Go 接收到操作系统发送过来的信号，那么会将信号值放入到 sigs 管道中进行处理。 fork子进程 file := netListener.File() // this returns a Dup() path := \"/path/to/executable\" args := []string{ \"-graceful\"} // 产生 Cmd 实例 cmd := exec.Command(path, args...) // 标准输出 cmd.Stdout = os.Stdout // 标准错误输出 cmd.Stderr = os.Stderr cmd.ExtraFiles = []*os.File{file} // 启动命令 err := cmd.Start() if err != nil { log.Fatalf(\"gracefulRestart: Failed to launch, error: %v\", err) } 通过调用 exec 包的 Command 命令传入 path（将要执行的命令路径）、args （命令的参数）即可返回 Cmd 实例，通过 ExtraFiles 字段指定额外被新进程继承的已打开文件，最后调用 Start 方法创建子进程。 这里的 netListener.File会通过系统调用 dup 复制一份 file descriptor 文件描述符。 func Dup(oldfd int) (fd int, err error) { r0, _, e1 := Syscall(SYS_DUP, uintptr(oldfd), 0, 0) fd = int(r0) if e1 != 0 { err = errnoErr(e1) } return } endless 不停机重启示例 func initRouter2() { engine := gin.Default() engine.GET(\"/bbb/aa\", func(context *gin.Context) { time.Sleep(time.Second * 40) context.JSON(http.StatusOK, \"11\") }) err := endless.ListenAndServe(\"localhost:8080\", engine) if err != nil { log.Println(err) } log.Println(\"Server on 8080 stopped\") } go build -o hello ./hello 修改文件，context.JSON(http.StatusOK, \"22\") go build -o hello curl http://localhost:8080/bbb/aa kill -1 hello_pid curl http://localhost:8080/bbb/aa ... 请求返回11 请求返回22 我们要做的不停机重启，监听 SIGHUP 信号； 收到信号时 fork 子进程（使用相同的启动命令），将服务监听的 socket 文件描述符传递给子进程； 子进程监听父进程的 socket，这个时候父进程和子进程都可以接收请求； 子进程启动成功之后发送 SIGTERM 信号给父进程，父进程停止接收新的连接，等待旧连接处理完成（或超时）； 父进程退出，升级完成； endless实现 func (srv *endlessServer) ListenAndServe() (err error) { addr := srv.Addr if addr == \"\" { addr = \":http\" } // 异步处理信号量 go srv.handleSignals() // 获取端口监听 l, err := srv.getListener(addr) if err != nil { log.Println(err) return } // 将监听转为 endlessListener srv.EndlessListener = newEndlessListener(l, srv) // 如果是子进程，那么发送 SIGTERM 信号给父进程 if srv.isChild { syscall.Kill(syscall.Getppid(), syscall.SIGTERM) } srv.BeforeBegin(srv.Addr) // 响应Listener监听，执行对应请求逻辑 return srv.Serve() } 信号处理主要是信号的一个监听，然后根据不同的信号循环处理。 func (srv *endlessServer) handleSignals() { var sig os.Signal // 注册信号监听 signal.Notify( srv.sigChan, hookableSignals..., ) // 获取pid pid := syscall.Getpid() for { sig = \u003c-srv.sigChan // 在处理信号之前触发hook srv.signalHooks(PRE_SIGNAL, sig) switch sig { // 接收到平滑重启信号 case syscall.SIGHUP: log.Println(pid, \"Received SIGHUP. forking.\") err := srv.fork() if err != nil { log.Println(\"Fork err:\", err) } // 停机信号 case syscall.SIGINT: log.Println(pid, \"Received SIGINT.\") srv.shutdown() // 停机信号 case syscall.SIGTERM: log.Println(pid, \"Received SIGTERM.\") srv.shutdown() ... // 在处理信号之后触发hook srv.signalHooks(POST_SIGNAL, sig) } } 当我们用kill -1 $pid 的时候这里 srv.sigChan 就会接收到相应的信号，并进入到 case syscall.SIGHUP 这块逻辑代码中。 参考链接：https://www.luozhiyun.com/archives/584 ","date":"2021-12-02","objectID":"/posts/go_%E5%B9%B3%E6%BB%91%E9%87%8D%E5%90%AF_endless/:0:0","tags":["golang"],"title":"go 平滑重启endless方案","uri":"/posts/go_%E5%B9%B3%E6%BB%91%E9%87%8D%E5%90%AF_endless/"},{"categories":["笔记"],"content":"gin-http流程","date":"2021-10-27","objectID":"/posts/go_gin_http%E6%B5%81%E7%A8%8B/","tags":["golang"],"title":"gin-http流程","uri":"/posts/go_gin_http%E6%B5%81%E7%A8%8B/"},{"categories":["笔记"],"content":"go http流程 go中使用web非常简单，因为api封装很完美，一般我们会写下面代码： http.Handle(\"/\",handler) http.HandleFunc(\"/user\",HandlerFunc) http.ListenAndServe(\":8080\",nil) 还有下面的： mux := http.ServeMux{} mux.Handle(\"\",handler) http.ListenAndServe(\"\",\u0026mux) 前者没有自动路由器，那么会使用默认的serveMux。 还会有疑问，为什么有的是handler有的是方法？ 其实方法也是实现了handler接口，传进去后被强转为handler即可。 证据如下： type HandlerFunc func(ResponseWriter, *Request) // ServeHTTP calls f(w, r). func (f HandlerFunc) ServeHTTP(w ResponseWriter, r *Request) { f(w, r) } // HandleFunc registers the handler function for the given pattern. func (mux *ServeMux) HandleFunc(pattern string, handler func(ResponseWriter, *Request)) { if handler == nil { panic(\"http: nil handler\") } mux.Handle(pattern, HandlerFunc(handler)) } Go代码的执行流程 通过对http包的分析之后，现在让我们来梳理一下整个的代码执行过程。 首先调用Http.HandleFunc 按顺序做了几件事： 1 调用了DefaultServeMux的HandleFunc 2 调用了DefaultServeMux的Handle 3 往DefaultServeMux的map[string]muxEntry中增加对应的handler和路由规则 其次调用http.ListenAndServe(\":9090\", nil) 按顺序做了几件事情： 实例化Server 调用Server的ListenAndServe() 调用net.Listen(“tcp”, addr)监听端口 启动一个for循环，在循环体中Accept请求 对每个请求实例化一个Conn，并且开启一个goroutine为这个请求进行服务go c.serve() 读取每个请求的内容w, err := c.readRequest() 判断handler是否为空，如果没有设置handler（这个例子就没有设置handler），handler就设置为DefaultServeMux 调用handler的ServeHttp 在这个例子中，下面就进入到DefaultServeMux.ServeHttp 根据request选择handler，并且进入到这个handler的ServeHTTP mux.handler(r).ServeHTTP(w, r) 选择handler： A 判断是否有路由能满足这个request（循环遍历ServerMux的muxEntry） B 如果有路由满足，调用这个路由handler的ServeHttp C 如果没有路由满足，调用NotFoundHandler的ServeHttp net/http 流程 不管server代码如何封装, 都离不开bind,listen,accept这些函数. 就从上面这个简单的demo入手查看源码. ","date":"2021-10-27","objectID":"/posts/go_gin_http%E6%B5%81%E7%A8%8B/:0:0","tags":["golang"],"title":"gin-http流程","uri":"/posts/go_gin_http%E6%B5%81%E7%A8%8B/"},{"categories":["笔记"],"content":"注册路由 func main() { http.HandleFunc(\"/\", func(w http.ResponseWriter, r *http.Request) { w.Write([]byte(\"Hello World\")) }) if err := http.ListenAndServe(\":8000\", nil); err != nil { fmt.Println(\"start http server fail:\", err) } } 这段代码是在注册一个路由及这个路由的handler到DefaultServeMux中 // server.go:L2366-2388 func (mux *ServeMux) Handle(pattern string, handler Handler) { mux.mu.Lock() defer mux.mu.Unlock() if pattern == \"\" { panic(\"http: invalid pattern\") } if handler == nil { panic(\"http: nil handler\") } if _, exist := mux.m[pattern]; exist { panic(\"http: multiple registrations for \" + pattern) } if mux.m == nil { mux.m = make(map[string]muxEntry) } mux.m[pattern] = muxEntry{h: handler, pattern: pattern} if pattern[0] != '/' { mux.hosts = true } } 可以看到这个路由注册太过简单了, 也就给gin, iris, echo等框架留下了扩展的空间 ","date":"2021-10-27","objectID":"/posts/go_gin_http%E6%B5%81%E7%A8%8B/:1:0","tags":["golang"],"title":"gin-http流程","uri":"/posts/go_gin_http%E6%B5%81%E7%A8%8B/"},{"categories":["笔记"],"content":"等待客户端连接 // net/http/server.go:L2752-2765 func (srv *Server) ListenAndServe() error { // ... 省略代码 ln, err := net.Listen(\"tcp\", addr) // \u003c-----看这里listen if err != nil { return err } return srv.Serve(tcpKeepAliveListener{ln.(*net.TCPListener)}) } // net/http/server.go:L2805-2853 func (srv *Server) Serve(l net.Listener) error { // ... 省略代码 for { rw, e := l.Accept() // \u003c----- 看这里accept if e != nil { select { case \u003c-srv.getDoneChan(): return ErrServerClosed default: } if ne, ok := e.(net.Error); ok \u0026\u0026 ne.Temporary() { if tempDelay == 0 { tempDelay = 5 * time.Millisecond } else { tempDelay *= 2 } if max := 1 * time.Second; tempDelay \u003e max { tempDelay = max } srv.logf(\"http: Accept error: %v; retrying in %v\", e, tempDelay) time.Sleep(tempDelay) continue } return e } tempDelay = 0 c := srv.newConn(rw) c.setState(c.rwc, StateNew) // before Serve can return go c.serve(ctx) // \u003c--- 看这里 } } // net/http/server.go:L1739-1878 func (c *conn) serve(ctx context.Context) { // ... 省略代码 serverHandler{c.server}.ServeHTTP(w, w.req) w.cancelCtx() if c.hijacked() { return } w.finishRequest() // ... 省略代码 } // net/http/server.go:L2733-2742 func (sh serverHandler) ServeHTTP(rw ResponseWriter, req *Request) { handler := sh.srv.Handler if handler == nil { handler = DefaultServeMux } if req.RequestURI == \"*\" \u0026\u0026 req.Method == \"OPTIONS\" { handler = globalOptionsHandler{} } handler.ServeHTTP(rw, req) } // net/http/server.go:L2352-2362 func (mux *ServeMux) ServeHTTP(w ResponseWriter, r *Request) { if r.RequestURI == \"*\" { if r.ProtoAtLeast(1, 1) { w.Header().Set(\"Connection\", \"close\") } w.WriteHeader(StatusBadRequest) return } h, _ := mux.Handler(r) // \u003c--- 看这里 h.ServeHTTP(w, r) } // net/http/server.go:L1963-1965 func (f HandlerFunc) ServeHTTP(w ResponseWriter, r *Request) { f(w, r) } 这基本是整个过程的代码了. ln, err := net.Listen(“tcp”, addr)做了初试化了socket, bind, listen的操作. rw, e := l.Accept()进行accept, 等待客户端进行连接. go c.serve(ctx) 启动新的goroutine来处理本次请求. 同时主goroutine继续等待客户端连接, 进行高并发操作. h, _ := mux.Handler(r) 获取注册的路由, 然后拿到这个路由的handler, 然后将处理结果返回给客户端 ","date":"2021-10-27","objectID":"/posts/go_gin_http%E6%B5%81%E7%A8%8B/:2:0","tags":["golang"],"title":"gin-http流程","uri":"/posts/go_gin_http%E6%B5%81%E7%A8%8B/"},{"categories":["笔记"],"content":"gin-http请求流入gin","date":"2021-10-27","objectID":"/posts/go_gin_http%E8%AF%B7%E6%B1%82%E6%B5%81%E5%85%A5gin/","tags":["golang"],"title":"gin-http请求流入gin","uri":"/posts/go_gin_http%E8%AF%B7%E6%B1%82%E6%B5%81%E5%85%A5gin/"},{"categories":["笔记"],"content":"默认的路由器引流 func main() { http.HandleFunc(\"/\", func(w http.ResponseWriter, r *http.Request) { w.Write([]byte(\"Hello World\")) }) if err := http.ListenAndServe(\":8000\", nil); err != nil { fmt.Println(\"start http server fail:\", err) } } 例子中 http.HandleFunc 通过看源码，可以看到 URI “/” 被注册到了 DefaultServeMux 上。 net/http 里面有个非常重要的 Handler interface。只有实现了这个方法才能请求的处理逻辑引入自己的处理流程中。 type Handler interface { ServeHTTP(ResponseWriter, *Request) } 默认的 DefaultServeMux 就实现了这个 ServeHTTP 这个 request 的流转过程： socket.accept 接收到客户端请求后，启动 go c.serve(connCtx) [net/http server.go:L3013]行，专门处理这次请求，server 继续等待客户端连接 获取能处理这次请求的 handler -\u003e serverHandler{c.server}.ServeHTTP(w, w.req) [net/http server.go:L1952] 跳转到真正的 ServeHTTP 去匹配路由，获取 handler 由于并没有自定义路由，于是使用的是 net/http 默认路由 [net/http server.go:L2880-2887] 所以最终调用去 DefaultServeMux 匹配路由，输出返回对应的结果 gin ServeHTTP 的调用链路 package main import \"github.com/gin-gonic/gin\" func main() { r := gin.Default() r.GET(\"/ping\", func(c *gin.Context) { c.JSON(200, gin.H{ \"message\": \"pong\", }) }) r.Run() // listen and serve on 0.0.0.0:8080 } 这段代码的大概流程: r := gin.Default() 初始化了相关的参数 将路由 /ping 以及对应的 handler 注册到路由树中 使用 r.Run() 启动 server r.Run 的底层依然是 http.ListenAndServe func (engine *Engine) Run(addr ...string) (err error) { defer func() { debugPrintError(err) }() trustedCIDRs, err := engine.prepareTrustedCIDRs() if err != nil { return err } engine.trustedCIDRs = trustedCIDRs address := resolveAddress(addr) debugPrint(\"Listening and serving HTTP on %s\\n\", address) err = http.ListenAndServe(address, engine) return } 所以 gin 建立 socket 的过程，accept 客户端请求的过程与 net/http 没有差别，会同样重复上面的过程。唯一有差别的位置就是在于获取 ServeHTTP 的位置 func (sh serverHandler) ServeHTTP(rw ResponseWriter, req *Request) { handler := sh.srv.Handler // 这里就是engine if handler == nil { handler = DefaultServeMux } if req.RequestURI == \"*\" \u0026\u0026 req.Method == \"OPTIONS\" { handler = globalOptionsHandler{} } handler.ServeHTTP(rw, req) } 由于 sh.srv.Handler 是 interface 类型，但是其真正的类型是 gin.Engine，根据 interace 的动态转发特性，最终会跳转到 gin.Engine.ServeHTTP 函数中。 func (engine *Engine) ServeHTTP(w http.ResponseWriter, req *http.Request) { c := engine.pool.Get().(*Context) c.writermem.reset(w) c.Request = req c.reset() engine.handleHTTPRequest(c) engine.pool.Put(c) } 至此，终于我们看到了 gin.ServeHTTP 的全貌了 从 sync.pool 里面拿去一块内存 对这块内存做初始化工作，防止数据污染 处理请求 handleHTTPRequest 请求处理完成后，把这块内存归还到 sync.pool 中 ","date":"2021-10-27","objectID":"/posts/go_gin_http%E8%AF%B7%E6%B1%82%E6%B5%81%E5%85%A5gin/:0:0","tags":["golang"],"title":"gin-http请求流入gin","uri":"/posts/go_gin_http%E8%AF%B7%E6%B1%82%E6%B5%81%E5%85%A5gin/"},{"categories":["笔记"],"content":"gin-前缀树","date":"2021-10-27","objectID":"/posts/go_gin_%E5%89%8D%E7%BC%80%E6%A0%91/","tags":["golang"],"title":"gin-前缀树","uri":"/posts/go_gin_%E5%89%8D%E7%BC%80%E6%A0%91/"},{"categories":["笔记"],"content":"什么是“Trie 树”？ Trie 树，也叫“字典树”。顾名思义，它是一个树形结构。它是一种专门处理字符串匹配的数据结构，用来解决在一组字符串集合中快速查找某个字符串的问题。 我们有 6 个字符串，它们分别是：how，hi，her，hello，so，see。我们希望在里面多次查找某个字符串是否存在。如果每次查找，都是拿要查找的字符串跟这 6 个字符串依次进行字符串匹配，那效率就比较低。 可以先对这 6 个字符串做一下预处理，组织成 Trie 树的结构，之后每次查找，都是在 Trie 树中进行匹配查找。Trie 树的本质，就是利用字符串之间的公共前缀，将重复的前缀合并在一起。最后构造出来的就是下面这个图中的样子。 当我们在 Trie 树中查找一个字符串的时候，比如查找字符串“her”，那我们将要查找的字符串分割成单个的字符 h，e，r，然后从 Trie 树的根节点开始匹配 Trie 树与散列表、红黑树的比较 一组字符串中查找字符串，Trie 树实际上表现得并不好。它对要处理的字符串有及其严苛的要求。 第一，字符串中包含的字符集不能太大。我们前面讲到，如果字符集太大，那存储空间可能就会浪费很多。即便可以优化，但也要付出牺牲查询、插入效率的代价。 第二，要求字符串的前缀重合比较多，不然空间消耗会变大很多。 第三，如果要用 Trie 树解决问题，那我们就要自己从零开始实现一个 Trie 树，还要保证没有 bug，这个在工程上是将简单问题复杂化，除非必须，一般不建议这样做。 第四，我们知道，通过指针串起来的数据块是不连续的，而 Trie 树中用到了指针，所以，对缓存并不友好，性能上会打个折扣。 综合这几点，针对在一组字符串中查找字符串的问题，我们在工程中，更倾向于用散列表或者红黑树。因为这两种数据结构，我们都不需要自己去实现，直接利用编程语言中提供的现成类库就行了。 实际上，Trie 树只是不适合精确匹配查找，这种问题更适合用散列表或者红黑树来解决。Trie 树比较适合的是查找前缀匹配的字符串，也就是类似开篇问题的那种场景。 gin数据结构 Engine 是 gin 框架的实例，在 Engine 结构中，trees 是一个数组，针对框架支持的每一种方法，都会创建一个节点。例如 GET、POST 是 trees 的两个元素。 //框架实例包含一个方法数数组 type Engine struct { trees methodTrees } type methodTrees []methodTree //方法树的定义 type methodTree struct { method string root *node } 我们看到，每一种请求方式对应一个树。 ","date":"2021-10-27","objectID":"/posts/go_gin_%E5%89%8D%E7%BC%80%E6%A0%91/:0:0","tags":["golang"],"title":"gin-前缀树","uri":"/posts/go_gin_%E5%89%8D%E7%BC%80%E6%A0%91/"},{"categories":["笔记"],"content":"前缀树节点的定义 //path树的节点结构 type node struct { path string indices string children []*node handlers HandlersChain priority uint32 nType nodeType maxParams uint8 wildChild bool fullPath string } path：表示当前节点的 path； indices：通常情况下维护了 children 列表的 path 的各首字符组成的 string，之所以是通常情况，是在处理包含通配符的 path 处理中会有一些例外情况； priority：代表了有几条路由会经过此节点，用于在节点进行排序时使用； nType：是节点的类型，默认是 static 类型，还包括了 root 类型，对于 path 包含冒号通配符的情况，nType 是 param 类型，对于包含 * 通配符的情况，nType 类型是 catchAll 类型； wildChild：默认是 false，当 children 是 通配符类型时，wildChild 为 true； fullPath：是从 root 节点到当前节点的全部 path 部分；如果此节点为终结节点 handlers 为对应的处理链，否则为 nil；maxParams 是当前节点到各个叶子节点的包含的通配符的最大数量。 节点创建程 ","date":"2021-10-27","objectID":"/posts/go_gin_%E5%89%8D%E7%BC%80%E6%A0%91/:1:0","tags":["golang"],"title":"gin-前缀树","uri":"/posts/go_gin_%E5%89%8D%E7%BC%80%E6%A0%91/"},{"categories":["笔记"],"content":"案例1-普通的路径 engine := gin.Default() helloGroup := engine.Group(\"/hello\") { helloGroup.Use(func(context *gin.Context) { }) helloGroup.GET(\"/aaa/aa\", func(context *gin.Context) { context.JSON(http.StatusOK, \"ok\") }) // 1 helloGroup.GET(\"/aaa2/bb\", func(context *gin.Context) { context.JSON(http.StatusOK, \"ok\") }) // 2 helloGroup.GET(\"/bbb/aa\", func(context *gin.Context) { context.JSON(http.StatusOK, \"ok\") }) // 3 helloGroup.GET(\"/bbb/a/ccc\", func(context *gin.Context) { context.JSON(http.StatusOK, \"ok\") }) // 4 } 代码1处： 代码2处： 代码3处： 代码4处： ","date":"2021-10-27","objectID":"/posts/go_gin_%E5%89%8D%E7%BC%80%E6%A0%91/:2:0","tags":["golang"],"title":"gin-前缀树","uri":"/posts/go_gin_%E5%89%8D%E7%BC%80%E6%A0%91/"},{"categories":["笔记"],"content":"案例2-通配符 单条包含两个冒号通配 path 的一颗前缀树 路由信息： “/user/:name/:age” name 和 age 是通配符的名字；这条路径形成的前缀树如下。 ","date":"2021-10-27","objectID":"/posts/go_gin_%E5%89%8D%E7%BC%80%E6%A0%91/:3:0","tags":["golang"],"title":"gin-前缀树","uri":"/posts/go_gin_%E5%89%8D%E7%BC%80%E6%A0%91/"},{"categories":["笔记"],"content":"单条包含星号通配 path 的一颗前缀树 路由信息：\"/user/:name/*age\" ，其中包含一个冒号通配符和一个星号通配符；这条路径形成的前缀树如下： ","date":"2021-10-27","objectID":"/posts/go_gin_%E5%89%8D%E7%BC%80%E6%A0%91/:4:0","tags":["golang"],"title":"gin-前缀树","uri":"/posts/go_gin_%E5%89%8D%E7%BC%80%E6%A0%91/"},{"categories":["笔记"],"content":"golang进程线程协程","date":"2021-10-27","objectID":"/posts/go_%E8%BF%9B%E7%A8%8B%E7%BA%BF%E7%A8%8B%E5%8D%8F%E7%A8%8B/","tags":["golang"],"title":"golang进程线程协程","uri":"/posts/go_%E8%BF%9B%E7%A8%8B%E7%BA%BF%E7%A8%8B%E5%8D%8F%E7%A8%8B/"},{"categories":["笔记"],"content":"进程 通常我们把一个程序的执行成为一个进程。它是操作系统进行资源分配的最小单位。 进程使用fork（这个一个系统调用）来创建若干的新的进程。前者是后者的父进程，后者是前者的子进程。unix系统每一个进程都有父进程。 每一个进程都有唯一的ID，就pid,除此之外还有个ppid也就是parent pid表示父进程ID。 第一个进程是内核启动进程，它的pid是1。 进程的切换 进程是由内核来管理和调度的，进程的切换只能发生在内核态。所以，进程的上下文不仅包括了虚拟内存、栈、全局变量等用户空间的资源，还包括了内核堆栈、寄存器等内核空间的状态。 因此，进程的上下文切换就比系统调用时多了一步：在保存当前进程的内核状态和 CPU 寄存器之前，需要先把该进程的虚拟内存、栈等保存下来；而加载了下一进程的内核态后，还需要刷新进程的虚拟内存和用户栈。 在进程上下文切换次数较多的情况下，很容易导致 CPU 将大量时间耗费在寄存器、内核栈以及虚拟内存等资源的保存和恢复上，进而大大缩短了真正运行进程的时间。这也正是导致平均负载升高的一个重要因素。 ","date":"2021-10-27","objectID":"/posts/go_%E8%BF%9B%E7%A8%8B%E7%BA%BF%E7%A8%8B%E5%8D%8F%E7%A8%8B/:0:0","tags":["golang"],"title":"golang进程线程协程","uri":"/posts/go_%E8%BF%9B%E7%A8%8B%E7%BA%BF%E7%A8%8B%E5%8D%8F%E7%A8%8B/"},{"categories":["笔记"],"content":"线程 一个进程在红色会包含一个线程。 任何线程都可以使用pthread_create（系统调用）来创建新的线程。 线程与进程最大的区别在于，线程是调度的基本单位，而进程则是资源拥有的基本单位。说白了，所谓内核中的任务调度，实际上的调度对象是线程；而进程只是给线程提供了虚拟内存、全局变量等资源。所以，对于线程和进程，我们可以这么理解： 当进程只有一个线程时，可以认为进程就等于线程。 当进程拥有多个线程时，这些线程会共享相同的虚拟内存和全局变量等资源。这些资源在上下文切换时是不需要修改的。 另外，线程也有自己的私有数据，比如栈和寄存器等，这些在上下文切换时也是需要保存的。 线程的上下文切换其实就可以分为两种情况： 第一种， 前后两个线程属于不同进程。此时，因为资源不共享，所以切换过程就跟进程上下文切换是一样。 第二种，前后两个线程属于同一个进程。此时，因为虚拟内存是共享的，所以在切换时，虚拟内存这些资源就保持不动，只需要切换线程的私有数据、寄存器等不共享的数据。 到这里你应该也发现了，虽然同为上下文切换，但同进程内的线程切换，要比多进程间的切换消耗更少的资源，而这，也正是多线程代替多进程的一个优势。 协程 协程: 协程是一种用户态的轻量级线程，协程的调度完全是由用户来控制的。协程拥有自己的寄存器上下文和栈。协程调度切换时，将寄存器上下文和栈保存到其他地方，在切回来的时候，恢复先前保存的寄存器上下文和栈，直接操作栈则基本没有内核切换的开销，可以不加锁的访问全局变量，所以上下文的切换非常快。 ","date":"2021-10-27","objectID":"/posts/go_%E8%BF%9B%E7%A8%8B%E7%BA%BF%E7%A8%8B%E5%8D%8F%E7%A8%8B/:1:0","tags":["golang"],"title":"golang进程线程协程","uri":"/posts/go_%E8%BF%9B%E7%A8%8B%E7%BA%BF%E7%A8%8B%E5%8D%8F%E7%A8%8B/"},{"categories":["笔记"],"content":"golang内存逃逸","date":"2021-10-27","objectID":"/posts/go_%E5%86%85%E5%AD%98%E9%80%83%E9%80%B8/","tags":["golang"],"title":"golang内存逃逸","uri":"/posts/go_%E5%86%85%E5%AD%98%E9%80%83%E9%80%B8/"},{"categories":["笔记"],"content":"栈和堆 在编译时，一切无法确定大小或大小可以改变的数据，最好放到堆上，堆适合不可预知大小的内存分配。但是为此付出的代价是分配速度较慢，而且会形成内存碎片。 函数中申请一个新的对象： 如果分配在栈中，则函数执行结束可自动将内存回收； 如果分配在堆中，则函数执行结束可交给GC（垃圾回收）处理。 逃逸分析基本原则 编译器会根据变量是否被外部引用来决定是否逃逸： 如果函数外部没有引用，则优先放到栈中； 如果函数外部存在引用，则必定放到堆中; 如果栈上放不开，则必定放到堆上； 指针逃逸 我们知道Go可以返回局部变量指针，这种情况下，函数虽然退出了，但是因为指针的存在，对象的内存不能随着函数结束而回收，因此只能分配在堆上。 package main type Person struct { Name string Age int } func PersonRegister(name string, age int) *Person { p := new(Person) //局部变量s逃逸到堆 p.Name = name p.Age = age return p } func main() { PersonRegister(\"微客鸟窝\", 18) } 函数 PersonRegister() 内部 p 为局部变量，其值通过函数返回值返回， p 本身为一指针，其指向的内存地址不会是栈而是堆，这就是典型的逃逸案例。 通过编译参数-gcflag=-m可以查看编译过程中的逃逸分析：go build -gcflags=-m 栈空间不足逃逸 package main func Slice() { s := make([]int, 1000, 1000) for index, _ := range s { s[index] = index } } func main() { Slice() } 发现并没有发生逃逸。我们把切片长度扩大10倍再试试： s := make([]int, 10000, 10000) 发现当切片长度扩大到10000时就会逃逸。当栈空间不足以存放当前对象时或无法判断当前切片长度时会将对象分配到堆中。 动态类型逃逸 在 Go 中，空接口 interface{} 可以表示任意的类型，如果函数参数为 interface{}，编译期间很难确定其参数的具体类型，也会发生逃逸。 因为 fmt.Println() 的参数类型定义为 interface{}，因此也发生了逃逸 闭包引用对象逃逸 package main func main() { f := fibonacci() for i := 0; i \u003c 10; i++ { f() } } func fibonacci() func() int { a, b := 0, 1 return func() int { a, b = b, a+b return a } } Fibonacci()函数中原本属于局部变量的a和b由于闭包的引用，不得不将二者放到堆上，以致产生逃逸。 ","date":"2021-10-27","objectID":"/posts/go_%E5%86%85%E5%AD%98%E9%80%83%E9%80%B8/:0:0","tags":["golang"],"title":"golang内存逃逸","uri":"/posts/go_%E5%86%85%E5%AD%98%E9%80%83%E9%80%B8/"},{"categories":["笔记"],"content":"golang语法50问","date":"2021-10-27","objectID":"/posts/go_%E8%AF%AD%E6%B3%95%E9%97%AE/","tags":["golang"],"title":"golang语法问题","uri":"/posts/go_%E8%AF%AD%E6%B3%95%E9%97%AE/"},{"categories":["笔记"],"content":"使用值为 nil 的 slice、map会发生啥 允许对值为 nil 的 slice 添加元素，但对值为 nil 的 map 添加元素，则会造成运行时 panic。 func main() { var m map[string]int v, ok := m[\"one\"] // nil map读没有问题 delete(m, \"one\") // nil map删除元素没有问题 m[\"one\"] = 1 // error: panic: assignment to entry in nil map var s []int s = append(s, 1) // nil slice追加元素没有问题 s[0] = 3 // nil slice写元素报错 index out of range [0] with length 0 fmt.Println(s[0]) // nil slice读元素报错 index out of range [0] with length 0 } nil channel注意事项 func main() { var ch chan int close(ch) // 关闭nil通道，引发panic: close of nil channel go func() { ch \u003c- 3 // nil channel会阻塞对该channel的所有读、写。但不报错 }() ch := make(chan int, 0) close(ch) close(ch) // 重复关闭引发panic: close of closed channel ch := make(chan int, 0) close(ch) \u003c-ch // 关闭后读数据没问题 ch \u003c- 2 // 关闭后发数据 panic: send on closed channel } 主动关闭过http连接，为啥要这样做 不关闭会程序可能会消耗完 socket 描述符。 Golang的net包中client.go, transport.go, response.go和request.go这几个文件中实现了HTTP Client。当应用层调用client.Do()函数后，transport层会首先找与该请求相关的已经缓存的连接（这个缓存是一个map，map的key是请求方法、请求地址和proxy地址，value是一个叫persistConn的连接描述结构），如果已经有可以复用的旧连接，就会在这个旧连接上发送和接受该HTTP请求，否则会新建一个TCP连接，然后在这个连接上读写数据。当client接受到整个响应后，如果应用层没有 调用response.Body.Close()函数，刚刚传输数据的persistConn就不会被加入到连接缓存中，这样如果您在下次发起HTTP请求的时候，就会重新建立TCP连接，重新分配persistConn结构，这是不调用response.Body.Close()的一个副作用。 如果不调用response.Body.Close()还存在一个问题。如果请求完成后，对端关闭了连接（对端的HTTP服务器向我发送了FIN），如果这边不调用response.Body.Close()，那么可以看到与这个请求相关的TCP连接的状态一直处于CLOSE_WAIT状态（还记得么？CLOSE_WAIT是连接的半开半闭状态，它是收到对方的FIN并且我们也发送了ACK，但是本端还没有发送FIN到对端，如果本段不调用close关闭连接，那么连接将一直处于CLOSE_WAIT状态，不会被系统回收）。 调用了response.Body.Close()就万无一失了么？上面代码中也调用了body.Close()为什么还会有很多ESTABLISHED状态的连接呢？因为在函数DoRequest()的每次调用中，我们都会新创建transport和client结构，当HTTP请求完成并且接收到响应后，如果对端的HTTP服务器没有关闭连接，那么这个连接会一直处于ESTABLISHED状态。如何解呢？ 有两个方法： 第一个方法是用一个全局的client，函数DoRequest()中每次都只在这个全局client上发送数据。但是如果我就想用短连接呢？用方法二。 第二个方法是在transport分配时将它的DisableKeepAlives参数置为false 解析 JSON 数据时，默认将数值当做哪种类型 在 encode/decode JSON 数据时，Go 默认会将数值当做 float64 处理。 func main() { var data = []byte(`{\"status\": 200}`) var result map[string]interface{} if err := json.Unmarshal(data, \u0026result); err != nil { log.Fatalln(err) } 解析出来的 200 是 float 类型。 defer 调用时多层嵌套依然无效。 func main() { defer func() { func() { recover() }() }() panic(1) } 在循环内部执行defer语句会发生啥 defer 在函数退出时才能执行，在 for 执行 defer 会导致资源延迟释放。func 是一个局部函数，在局部函数里面执行 defer 将不会有问题。可以这样： func main() { for i := 0; i \u003c 5; i++ { func() { f, err := os.Open(\"/path/to/file\") if err != nil { log.Fatal(err) } defer f.Close() }() } } 如何跳出for select 循环 通常在for循环中，使用break可以跳出循环，但是注意在go语言中，for select配合时，break 并不能跳出循环。可以使用标签。 ","date":"2021-10-27","objectID":"/posts/go_%E8%AF%AD%E6%B3%95%E9%97%AE/:0:0","tags":["golang"],"title":"golang语法问题","uri":"/posts/go_%E8%AF%AD%E6%B3%95%E9%97%AE/"},{"categories":["笔记"],"content":"golang 结构体内存对齐","date":"2021-09-30","objectID":"/posts/go_%E7%BB%93%E6%9E%84%E4%BD%93%E5%86%85%E5%AD%98%E5%AF%B9%E9%BD%90/","tags":["golang"],"title":"golang 结构体内存对齐","uri":"/posts/go_%E7%BB%93%E6%9E%84%E4%BD%93%E5%86%85%E5%AD%98%E5%AF%B9%E9%BD%90/"},{"categories":["笔记"],"content":"为何需要内存对齐 CPU和内存数据交互的过程。CPU和内存是通过总线进行数据交互的。其中地址总线用来传递CPU需要的数据地址，内存将数据通过数据总线传递给CPU， 或者CPU将数据通过数据总线回传给内存。 ","date":"2021-09-30","objectID":"/posts/go_%E7%BB%93%E6%9E%84%E4%BD%93%E5%86%85%E5%AD%98%E5%AF%B9%E9%BD%90/:0:0","tags":["golang"],"title":"golang 结构体内存对齐","uri":"/posts/go_%E7%BB%93%E6%9E%84%E4%BD%93%E5%86%85%E5%AD%98%E5%AF%B9%E9%BD%90/"},{"categories":["笔记"],"content":"地址总线 专门用来传送地址的，由于地址只能从CPU传向外部存储器或I／O端口，所以地址总线总是单向的。地址总线的位数决定了CPU可直接寻址的内存空间大小，比如8位微型机的地址总线为16位，则其最大可寻址空间为2^16＝64KB，16位微型机的地址总线为20位，其可寻址空间为2^20＝1MB。 ","date":"2021-09-30","objectID":"/posts/go_%E7%BB%93%E6%9E%84%E4%BD%93%E5%86%85%E5%AD%98%E5%AF%B9%E9%BD%90/:1:0","tags":["golang"],"title":"golang 结构体内存对齐","uri":"/posts/go_%E7%BB%93%E6%9E%84%E4%BD%93%E5%86%85%E5%AD%98%E5%AF%B9%E9%BD%90/"},{"categories":["笔记"],"content":"数据总线 是CPU与内存或其他器件之间的数据传送的通道。每条传输线一次只能传输1位二进制数据, 数据总线每次可以传输的字节总数就称为机器字长或者数据总线的宽度。 它决定了CPU和外界的数据传送速度。我们现在日常使用的基本上是32位(每次可以传输4字节)或者64位(每次可以传输8字节)机器字长的机器。 由于数据是通过总线进行传输，若数据未经一定规则的对齐，CPU的访址操作与总线的传输操作将会异常的复杂，所以编译器在程序编译期间会对各种类型的数据按照一定的规则进行对齐, 对齐过程会按一定规则对内存的数据段进行的字节填充， 这就是字节对齐。 例如: 现在要存储变量A（int32）和B（int64）那么不做任何字节对齐优化的情况下，内存布局是这样的: 字节对齐优化后是这样子的： 字节对齐后浪费了内存， 但是当我们去读取内存中的数据给CPU时，64位的机器（一次可以原子读取8字节）在内存对齐和不对齐的情况下A变量都只需要原子读取一次就行， 但是对齐后B变量的读取只需一次， 而不对齐的情况下，B需要读取2次，且需要额外的处理牺牲性能来保证2次读取的原子性。所以本质上，内存填充是一种以空间换时间， 通过额外的内存填充来提高内存读取的效率的手段。 总的来说，内存对齐主要解决以下两个问题： 【1】跨平台问题：如果数据不对齐，那么在64位字长机器存储的数据可能在32位字长的机器可能就无法正常的读取。 【2】性能问题：如果不对齐，那么每个数据要通过多少次总线传输是未知的，如果每次都要处理这些复杂的情况，那么数据的读/写性能将会收到很大的影响。之所以有些CPU支持访问任意地址，是因为处理器在后面多做了很多额外处理。 内存对齐的规则 内存对齐主要是为了保证数据的原子读取， 因此内存对齐的最大边界只可能为当前机器的字长。当然如果每种类型都使用最大的对齐边界，那么对内存将是一种浪费，实际上我们只要保证同一个数据不要分开在多次总线事务中便可。 Go也提供了unsafe.Alignof(x)来返回一个类型的对齐值，并且作出了如下约定： 对于任意类型的变量 x ，unsafe.Alignof(x) 至少为 1。 对于 struct 结构体类型的变量 x，计算 x 每一个字段 f 的 unsafe.Alignof(x.f)，unsafe.Alignof(x) 等于其中的最大值。 对于 array 数组类型的变量 x，unsafe.Alignof(x) 等于构成数组的元素类型的对齐倍数。 没有任何字段的空 struct{} 和没有任何元素的 array 占据的内存空间大小为 0，不同的大小为 0 的变量可能指向同一块地址。 总结来说，分为基本类型对齐和结构体类型对齐. ","date":"2021-09-30","objectID":"/posts/go_%E7%BB%93%E6%9E%84%E4%BD%93%E5%86%85%E5%AD%98%E5%AF%B9%E9%BD%90/:2:0","tags":["golang"],"title":"golang 结构体内存对齐","uri":"/posts/go_%E7%BB%93%E6%9E%84%E4%BD%93%E5%86%85%E5%AD%98%E5%AF%B9%E9%BD%90/"},{"categories":["笔记"],"content":"基本类型对齐 go语言的基本类型的内存对齐是按照基本类型的大小和机器字长中最小值进行对齐 ","date":"2021-09-30","objectID":"/posts/go_%E7%BB%93%E6%9E%84%E4%BD%93%E5%86%85%E5%AD%98%E5%AF%B9%E9%BD%90/:3:0","tags":["golang"],"title":"golang 结构体内存对齐","uri":"/posts/go_%E7%BB%93%E6%9E%84%E4%BD%93%E5%86%85%E5%AD%98%E5%AF%B9%E9%BD%90/"},{"categories":["笔记"],"content":"结构体类型对齐 go语言的结构体的对齐是先对结构体的每个字段进行对齐，然后对总体的大小按照最大对齐边界的整数倍进行对齐。有一个特殊的情况就是，如果空结构体嵌套到一个结构体尾部，那么这个结构体也是要额外对齐的，因为如果有指针指向该字段, 返回的地址将在结构体之外，如果此指针一直存活不释放对应的内存，就会有内存泄露的问题。 ","date":"2021-09-30","objectID":"/posts/go_%E7%BB%93%E6%9E%84%E4%BD%93%E5%86%85%E5%AD%98%E5%AF%B9%E9%BD%90/:4:0","tags":["golang"],"title":"golang 结构体内存对齐","uri":"/posts/go_%E7%BB%93%E6%9E%84%E4%BD%93%E5%86%85%E5%AD%98%E5%AF%B9%E9%BD%90/"},{"categories":["笔记"],"content":"怎么检查并优化 如果结构体字段比较多，靠人工去分析太慢而且还容易出错。 import \"golang.org/x/tools/go/analysis/analysistest\" \"golang.org/x/tools/go/analysis/passes/fieldalignment\" testdata := \"/Users/xingliuhua/Documents/goproject/hello\" analysistest.RunWithSuggestedFixes(t, testdata, fieldalignment.Analyzer, \"a\") analysis工具可以帮我们去检查并给出优化后代码。 案例 ","date":"2021-09-30","objectID":"/posts/go_%E7%BB%93%E6%9E%84%E4%BD%93%E5%86%85%E5%AD%98%E5%AF%B9%E9%BD%90/:5:0","tags":["golang"],"title":"golang 结构体内存对齐","uri":"/posts/go_%E7%BB%93%E6%9E%84%E4%BD%93%E5%86%85%E5%AD%98%E5%AF%B9%E9%BD%90/"},{"categories":["笔记"],"content":"案例1 type TestStruct1 struct { a int8 // 1 字节====\u003e max align 1 字节 b int32 // 4 字节====\u003e max align 4 字节 c []string // 24 字节====\u003e max align 8 字节 } TestStruct1在编译期就会进行字节对齐的优化。优化后各个变量的相对位置如下图(以64位字长下环境为例)： TestStruct1 内存占用大小分析：最大对齐边界为8，总体字节数 = 1 + （align 3） + 4 + 24 = 32, 由于32刚好是8的倍数，所以末尾无需额外填充，最后这个结构体的大小为32字节。 a后面为啥不直接补7个字节呢？因为b也不到8个字节，a+b可以合一起加一块8个字节，更省内存。 ","date":"2021-09-30","objectID":"/posts/go_%E7%BB%93%E6%9E%84%E4%BD%93%E5%86%85%E5%AD%98%E5%AF%B9%E9%BD%90/:6:0","tags":["golang"],"title":"golang 结构体内存对齐","uri":"/posts/go_%E7%BB%93%E6%9E%84%E4%BD%93%E5%86%85%E5%AD%98%E5%AF%B9%E9%BD%90/"},{"categories":["笔记"],"content":"案例2 type TestStruct2 struct { a []string // 24 字节====\u003e max align 8 字节 b int64 // 8 字节====\u003e max align 8 字节 c int32 // 4 字节====\u003e max align 4 字节 } TestStruct2 内存占用大小分析：最大对齐边界为8字节，总体字节数 = 24（a） + 8（b） + 4（c） + 4（填充） = 40, 由于40刚好是8的倍数，所以c字段填充完后无需额外填充了。 ","date":"2021-09-30","objectID":"/posts/go_%E7%BB%93%E6%9E%84%E4%BD%93%E5%86%85%E5%AD%98%E5%AF%B9%E9%BD%90/:7:0","tags":["golang"],"title":"golang 结构体内存对齐","uri":"/posts/go_%E7%BB%93%E6%9E%84%E4%BD%93%E5%86%85%E5%AD%98%E5%AF%B9%E9%BD%90/"},{"categories":["笔记"],"content":"案例3 type TestStruct3 struct { a int8 b int64 c struct{} } TestStruct3 内存占用大小分析：最大对齐边界为8字节，总体字节数 = 1（a）+ 7(填充) + 8（b） + 8（c填充）=24, 空结构体理论上不占字节，但是如果在另一个结构体尾部则需要进行额外字节对齐 。 a后面直接就是b,b是需要8个字节的。那a只能补7个了。 ","date":"2021-09-30","objectID":"/posts/go_%E7%BB%93%E6%9E%84%E4%BD%93%E5%86%85%E5%AD%98%E5%AF%B9%E9%BD%90/:8:0","tags":["golang"],"title":"golang 结构体内存对齐","uri":"/posts/go_%E7%BB%93%E6%9E%84%E4%BD%93%E5%86%85%E5%AD%98%E5%AF%B9%E9%BD%90/"},{"categories":["笔记"],"content":"案例4 type TestStruct4 struct { a struct{} b int8 c int32 } TestStruct4 内存占用大小分析：最大对齐边界为4字节，总体字节数 = 0(a)+ 1（b）+ 3(填充) + 4（c) = 8。 ","date":"2021-09-30","objectID":"/posts/go_%E7%BB%93%E6%9E%84%E4%BD%93%E5%86%85%E5%AD%98%E5%AF%B9%E9%BD%90/:9:0","tags":["golang"],"title":"golang 结构体内存对齐","uri":"/posts/go_%E7%BB%93%E6%9E%84%E4%BD%93%E5%86%85%E5%AD%98%E5%AF%B9%E9%BD%90/"},{"categories":["笔记"],"content":"趣谈linux笔记","date":"2021-08-23","objectID":"/posts/linux_%E8%B6%A3%E8%B0%88linux%E7%AC%94%E8%AE%B0/","tags":["linux"],"title":"趣谈linux笔记","uri":"/posts/linux_%E8%B6%A3%E8%B0%88linux%E7%AC%94%E8%AE%B0/"},{"categories":["笔记"],"content":"top命令 经常用来监控linux的系统状况，比如cpu、内存的使用。 第一行： 跟uptime的内容一样。平均负载信息。 第二行： Tasks — 任务（进程），系统现在共有135个进程，其中处于运行中的有1个，134个在休眠（sleep），stoped状态的有0个，zombie状态（僵尸）的有0个。 第三行：cpu状态，我们可以看出有多少个逻辑cpu 0.3% us — 用户空间占用CPU的百分比。 0.0% sy — 内核空间占用CPU的百分比。 0.0% ni — 改变过优先级的进程占用CPU的百分比 99.7% id — 空闲CPU百分比 0.0% wa — IO等待占用CPU的百分比 0.0% hi — 硬中断（Hardware IRQ）占用CPU的百分比 0.0% si — 软中断（Software Interrupts）占用CPU的百分比 第四行：内存状态，跟free功能类似 3808060k total — 物理内存总量（4GB） 3660048k used — 使用中的内存总量（3.6GB） 148012k free — 空闲内存总量（148M） 359760k buffers — 缓存的内存量 （359M） 第五行：swap交换分区 4184924k total — 交换区总量（4G） 0k used — 使用的交换区总量（0M） 4184924k free — 空闲交换区总量（4G） 2483956k cached — 缓冲的交换区总量（2483M） 第七行以下：各进程（任务）的状态监控 PID — 进程id USER — 进程所有者 PR — 进程优先级 NI — nice值。负值表示高优先级，正值表示低优先级 VIRT — 进程使用的虚拟内存总量，单位kb。VIRT=SWAP+RES RES — 进程使用的、未被换出的物理内存大小，单位kb。RES=CODE+DATA SHR — 共享内存大小，单位kb S — 进程状态。D=不可中断的睡眠状态 R=运行 S=睡眠 T=跟踪/停止 Z=僵尸进程 %CPU — 上次更新到现在的CPU时间占用百分比 %MEM — 进程使用的物理内存百分比 TIME+ — 进程使用的CPU时间总计，单位1/100秒 COMMAND — 进程名称（命令名/命令行） uptime命令查看平均负载 uptime命令能够打印系统总共运行了多长时间和系统的平均负载。uptime命令可以显示的信息显示依次为：现在时间、系统已经运行了多长时间、目前有多少登陆用户、系统在过去的1分钟、5分钟和15分钟内的平均负载。 那么什么是系统平均负载呢？ 平均负载是指单位时间内，系统处于可运行状态和不可中断状态的平均进程数，也就是平均活跃进程数，它和 CPU 使用率并没有直接关系。 所谓可运行状态的进程，是指正在使用 CPU 或者正在等待 CPU 的进程，也就是我们常用 ps 命令看到的，处于 R 状态（Running 或 Runnable）的进程。 不可中断状态的进程则是正处于内核态关键流程中的进程，并且这些流程是不可打断的，比如最常见的是等待硬件设备的 I/O 响应，也就是我们在 ps 命令中看到的 D 状态（Uninterruptible Sleep，也称为 Disk Sleep）的进程。 ","date":"2021-08-23","objectID":"/posts/linux_%E8%B6%A3%E8%B0%88linux%E7%AC%94%E8%AE%B0/:0:0","tags":["linux"],"title":"趣谈linux笔记","uri":"/posts/linux_%E8%B6%A3%E8%B0%88linux%E7%AC%94%E8%AE%B0/"},{"categories":["笔记"],"content":"平均负载为多少时合理 既然平均的是活跃进程数，那么最理想的，就是每个 CPU 上都刚好运行着一个进程，这样每个 CPU 都得到了充分利用。比如当平均负载为 2 时，意味着什么呢？ 在只有 2 个 CPU 的系统上，意味着所有的 CPU 都刚好被完全占用。 在 4 个 CPU 的系统上，意味着 CPU 有 50% 的空闲。而在只有 1 个 CPU 的系统中，则意味着有一半的进程竞争不到 CPU。 平均负载最理想的情况是等于 CPU 个数。所以在评判平均负载时， 首先你要知道系统有几个 CPU ，这可以通过 top 命令或者从文件 /proc/cpuinfo 中读取。 在我看来，当平均负载高于 CPU 数量 70% 的时候，你就应该分析排查负载高的问题了。一旦负载过高，就可能导致进程响应变慢，进而影响服务的正常功能。 ","date":"2021-08-23","objectID":"/posts/linux_%E8%B6%A3%E8%B0%88linux%E7%AC%94%E8%AE%B0/:1:0","tags":["linux"],"title":"趣谈linux笔记","uri":"/posts/linux_%E8%B6%A3%E8%B0%88linux%E7%AC%94%E8%AE%B0/"},{"categories":["笔记"],"content":"平均负载与 CPU 使用率区别 可能你会疑惑，既然平均负载代表的是活跃进程数，那平均负载高了，不就意味着 CPU 使用率高吗？ 我们还是要回到平均负载的含义上来，平均负载是指单位时间内，处于可运行状态和不可中断状态的进程数。所以，它不仅包括了 正在使用 CPU 的进程，还包括 等待 CPU 和 等待 I/O 的进程。 而 CPU 使用率，是单位时间内 CPU 繁忙情况的统计，跟平均负载并不一定完全对应。比如： CPU 密集型进程，使用大量 CPU 会导致平均负载升高，此时这两者是一致的； I/O 密集型进程，等待 I/O 也会导致平均负载升高，但 CPU 使用率不一定很高； 大量等待 CPU 的进程调度也会导致平均负载升高，此时的 CPU 使用率也会比较高。 uptime总结 平均负载提供了一个快速查看系统整体性能的手段，反映了整体的负载情况。但只看平均负载本身，我们并不能直接发现，到底是哪里出现了瓶颈。所以，在理解平均负载时，也要注意： 平均负载高有可能是 CPU 密集型进程导致的； 平均负载高并不一定代表 CPU 使用率高，还有可能是 I/O 更繁忙了； 当发现负载高的时候，你可以使用 mpstat、pidstat 等工具，辅助分析负载的来源。 Sysstat工具集 Sysstat 是一个软件包，包含监测系统性能及效率的一组工具，这些工具对于我们收集系统性能数据，比如CPU使用率、硬盘和网络吞吐数据，这些数据的收集和分析，有 利于我们判断系统是否正常运行，是提高系统运行效率、安全运行服务器的得力助手； Sysstat 软件包集成如下工具： iostat 工具提供CPU使用率及硬盘吞吐效率的数据； mpstat 工具提供单个处理器或多个处理器相关数据； sar 工具负责收集、报告并存储系统活跃的信息； sa1 工具负责收集并存储每天系统动态信息到一个二进制的文件中。它是通过计划任务工具cron来运行， 是为sadc所设计的程序前端程序； sa2 工具负责把每天的系统活跃性息写入总结性的报告中。它是为sar所设计的前端 ，要通过cron来调用 sadc 是系统动态数据收集工具，收集的数据被写一个二进制的文件中，它被用作sar工具的后端； sadf 显示被sar通过多种格式收集的数据； ","date":"2021-08-23","objectID":"/posts/linux_%E8%B6%A3%E8%B0%88linux%E7%AC%94%E8%AE%B0/:2:0","tags":["linux"],"title":"趣谈linux笔记","uri":"/posts/linux_%E8%B6%A3%E8%B0%88linux%E7%AC%94%E8%AE%B0/"},{"categories":["笔记"],"content":"mpstat处理器统计信息 1.不带任何参数的使用mpstat命令将会输出所有CPU的平均统计信息 $ mpstat Linux 4.4.0-132-generic 08/31/2021 _x86_64_ (2 CPU) 02:08:55 PM CPU %usr %nice %sys %iowait %irq %soft %steal %guest %gnice %idle 02:08:55 PM all 7.14 0.00 1.08 0.34 0.00 0.10 0.00 0.00 0.00 91.34 ","date":"2021-08-23","objectID":"/posts/linux_%E8%B6%A3%E8%B0%88linux%E7%AC%94%E8%AE%B0/:3:0","tags":["linux"],"title":"趣谈linux笔记","uri":"/posts/linux_%E8%B6%A3%E8%B0%88linux%E7%AC%94%E8%AE%B0/"},{"categories":["笔记"],"content":"pidstat进程和内核线程的统计信息 该命令是用于监控进程和当前受内核管理的线程。pidstat还可以检查子进程和线程的状态。 pidstat Linux 4.4.0-132-generic 08/31/2021 _x86_64_ (2 CPU) 02:11:03 PM UID PID %usr %system %guest %CPU CPU Command 02:11:03 PM 0 1 0.00 0.00 0.00 0.00 1 systemd 02:11:03 PM 0 2 0.00 0.00 0.00 0.00 1 kthreadd 02:11:03 PM 0 3 0.00 0.00 0.00 0.00 0 ksoftirqd/0 02:11:03 PM 0 7 0.00 0.04 0.00 0.04 0 rcu_sched ","date":"2021-08-23","objectID":"/posts/linux_%E8%B6%A3%E8%B0%88linux%E7%AC%94%E8%AE%B0/:4:0","tags":["linux"],"title":"趣谈linux笔记","uri":"/posts/linux_%E8%B6%A3%E8%B0%88linux%E7%AC%94%E8%AE%B0/"},{"categories":["笔记"],"content":"iostat块设备及其分区的 IO 统计信息 $ iostat Linux 4.4.0-132-generic 08/31/2021 _x86_64_ (2 CPU) avg-cpu: %user %nice %system %iowait %steal %idle 7.14 0.00 1.18 0.34 0.00 91.34 Device: tps kB_read/s kB_wrtn/s kB_read kB_wrtn vda 2.34 2.78 52.49 113137895 2136757300 vdb 10.66 10.50 189.38 427285083 7708672664 ","date":"2021-08-23","objectID":"/posts/linux_%E8%B6%A3%E8%B0%88linux%E7%AC%94%E8%AE%B0/:5:0","tags":["linux"],"title":"趣谈linux笔记","uri":"/posts/linux_%E8%B6%A3%E8%B0%88linux%E7%AC%94%E8%AE%B0/"},{"categories":["笔记"],"content":"sar 工具； sar 工具比较强大，既能收集系统CPU、硬盘、动态数据，也能显示动态显示，更能查看二进制数据文件；sar 的应用比较多，而且也比较复杂，数据更为精确。 示例：查看cpu使用率 sar -u 1 5 Linux 2.6.15-1.2054_FC5 (localhost.localdomain) 2006年05月12日 时间 CPU 利用率 nice值 系统占用 IO占用 空闲 11时19分34秒 CPU %user %nice %system %iowait %idle 11时19分35秒 all 2.97 0.00 0.00 0.00 97.03 11时19分36秒 all 11.11 0.00 9.09 0.00 79.80 11时19分37秒 all 21.78 0.00 6.93 0.00 71.29 11时19分38秒 all 15.00 0.00 0.00 0.00 85.00 11时19分39秒 all 8.00 0.00 0.00 0.00 92.00 Average: all 11.78 0.00 3.19 0.00 85.03 注解： CPU：表示机器内所有的CPU； %user 表示CPU的利用率； %nice 表示CPU在用户层优先级的百分比，0表示正常； %system 表示当系统运行时，在用户应用层上所占用的CPU百分比； %iowait 表示请求硬盘I/0数据流出时，所占用CPU的百分比； %idle 表示空闲CPU百分比，值越大系统负载越低； 上下文切换 每个任务运行前，CPU 都需要知道任务从哪里加载、又从哪里开始运行，也就是说，需要系统事先帮它设置好CPU 寄存器和程序计数器Program Counter，PC）。 CPU 寄存器，是 CPU 内置的容量小、但速度极快的内存。而程序计数器，则是用来存储 CPU 正在执行的指令位置、或者即将执行的下一条指令位置。它们都是 CPU 在运行任何任务前，必须的依赖环境，因此也被叫做CPU 上下文。 硬件通过触发信号，会导致中断处理程序的调用，也是一种常见的任务。 所以，根据任务的不同，CPU 的上下文切换就可以分为几个不同的场景，也就是进程上下文切换、线程上下文切换以及中断上下文切换。 ","date":"2021-08-23","objectID":"/posts/linux_%E8%B6%A3%E8%B0%88linux%E7%AC%94%E8%AE%B0/:6:0","tags":["linux"],"title":"趣谈linux笔记","uri":"/posts/linux_%E8%B6%A3%E8%B0%88linux%E7%AC%94%E8%AE%B0/"},{"categories":["笔记"],"content":"进程上下文切换 Linux 按照特权等级，把进程的运行空间分为内核空间和用户空间，分别对应着下图中， CPU 特权等级的 Ring 0 和 Ring 3。 内核空间（Ring 0）具有最高权限，可以直接访问所有资源； 用户空间（Ring 3）只能访问受限资源，不能直接访问内存等硬件设备，必须通过系统调用陷入到内核中，才能访问这些特权资源。 换个角度看，也就是说，进程既可以在用户空间运行，又可以在内核空间中运行。进程在用户空间运行时，被称为进程的用户态，而陷入内核空间的时候，被称为进程的内核态。 从用户态到内核态的转变，需要通过系统调用来完成。比如，当我们查看文件内容时，就需要多次系统调用来完成：首先调用 open() 打开文件，然后调用 read() 读取文件内容，并调用 write() 将内容写到标准输出，最后再调用 close() 关闭文件。 ","date":"2021-08-23","objectID":"/posts/linux_%E8%B6%A3%E8%B0%88linux%E7%AC%94%E8%AE%B0/:7:0","tags":["linux"],"title":"趣谈linux笔记","uri":"/posts/linux_%E8%B6%A3%E8%B0%88linux%E7%AC%94%E8%AE%B0/"},{"categories":["笔记"],"content":"进程的系统调用引起的cpu上下文切换 那么，系统调用的过程有没有发生 CPU 上下文的切换呢？答案自然是肯定的。 CPU 寄存器里原来用户态的指令位置，需要先保存起来。接着，为了执行内核态代码，CPU 寄存器需要更新为内核态指令的新位置。最后才是跳转到内核态运行内核任务。 而系统调用结束后，CPU 寄存器需要恢复原来保存的用户态，然后再切换到用户空间，继续运行进程。所以，一次系统调用的过程，其实是发生了两次 CPU 上下文切换。 不过，需要注意的是，系统调用过程中，并不会涉及到虚拟内存等进程用户态的资源，也不会切换进程。这跟我们通常所说的进程上下文切换是不一样的： 进程上下文切换，是指从一个进程切换到另一个进程运行。 而系统调用过程中一直是同一个进程在运行。 所以，系统调用过程通常称为特权模式切换，而不是上下文切换。但实际上，系统调用过程中，CPU 的上下文切换还是无法避免的。 ","date":"2021-08-23","objectID":"/posts/linux_%E8%B6%A3%E8%B0%88linux%E7%AC%94%E8%AE%B0/:7:1","tags":["linux"],"title":"趣谈linux笔记","uri":"/posts/linux_%E8%B6%A3%E8%B0%88linux%E7%AC%94%E8%AE%B0/"},{"categories":["笔记"],"content":"进程的上下文切换和系统调用 那么，进程上下文切换跟系统调用又有什么区别呢？ 首先，你需要知道，进程是由内核来管理和调度的，进程的切换只能发生在内核态。所以，进程的上下文不仅包括了虚拟内存、栈、全局变量等用户空间的资源，还包括了内核堆栈、寄存器等内核空间的状态。 因此，进程的上下文切换就比系统调用时多了一步：在保存当前进程的内核状态和 CPU 寄存器之前，需要先把该进程的虚拟内存、栈等保存下来；而加载了下一进程的内核态后，还需要刷新进程的虚拟内存和用户栈。 在进程上下文切换次数较多的情况下，很容易导致 CPU 将大量时间耗费在寄存器、内核栈以及虚拟内存等资源的保存和恢复上，进而大大缩短了真正运行进程的时间。这也正是导致平均负载升高的一个重要因素。 ","date":"2021-08-23","objectID":"/posts/linux_%E8%B6%A3%E8%B0%88linux%E7%AC%94%E8%AE%B0/:7:2","tags":["linux"],"title":"趣谈linux笔记","uri":"/posts/linux_%E8%B6%A3%E8%B0%88linux%E7%AC%94%E8%AE%B0/"},{"categories":["笔记"],"content":"进程什么时候上下文切换 Linux 为每个 CPU 都维护了一个就绪队列，将活跃进程（即正在运行和正在等待 CPU 的进程）按照优先级和等待 CPU 的时间排序，然后选择最需要 CPU 的进程，也就是优先级最高和等待 CPU 时间最长的进程来运行。 进程在什么时候才会被调度到 CPU 上运行： 其一，为了保证所有进程可以得到公平调度，CPU 时间被划分为一段段的时间片，这些时间片再被轮流分配给各个进程。这样，当某个进程的时间片耗尽了，就会被系统挂起，切换到其它正在等待 CPU 的进程运行。 其二，进程在系统资源不足（比如内存不足）时，要等到资源满足后才可以运行，这个时候进程也会被挂起，并由系统调度其他进程运行。 其三，当进程通过睡眠函数 sleep 这样的方法将自己主动挂起时，自然也会重新调度。 其四，当有优先级更高的进程运行时，为了保证高优先级进程的运行，当前进程会被挂起，由高优先级进程来运行。 最后一个，发生硬件中断时，CPU 上的进程会被中断挂起，转而执行内核中的中断服务程序。 了解这几个场景是非常有必要的，因为一旦出现上下文切换的性能问题，它们就是幕后凶手。 ","date":"2021-08-23","objectID":"/posts/linux_%E8%B6%A3%E8%B0%88linux%E7%AC%94%E8%AE%B0/:7:3","tags":["linux"],"title":"趣谈linux笔记","uri":"/posts/linux_%E8%B6%A3%E8%B0%88linux%E7%AC%94%E8%AE%B0/"},{"categories":["笔记"],"content":"线程的上下文切换 线程与进程最大的区别在于，线程是调度的基本单位，而进程则是资源拥有的基本单位。说白了，所谓内核中的任务调度，实际上的调度对象是线程；而进程只是给线程提供了虚拟内存、全局变量等资源。所以，对于线程和进程，我们可以这么理解： 当进程只有一个线程时，可以认为进程就等于线程。 当进程拥有多个线程时，这些线程会共享相同的虚拟内存和全局变量等资源。这些资源在上下文切换时是不需要修改的。 另外，线程也有自己的私有数据，比如栈和寄存器等，这些在上下文切换时也是需要保存的。 线程的上下文切换其实就可以分为两种情况： 第一种， 前后两个线程属于不同进程。此时，因为资源不共享，所以切换过程就跟进程上下文切换是一样。 第二种，前后两个线程属于同一个进程。此时，因为虚拟内存是共享的，所以在切换时，虚拟内存这些资源就保持不动，只需要切换线程的私有数据、寄存器等不共享的数据。 到这里你应该也发现了，虽然同为上下文切换，但同进程内的线程切换，要比多进程间的切换消耗更少的资源，而这，也正是多线程代替多进程的一个优势。 ","date":"2021-08-23","objectID":"/posts/linux_%E8%B6%A3%E8%B0%88linux%E7%AC%94%E8%AE%B0/:8:0","tags":["linux"],"title":"趣谈linux笔记","uri":"/posts/linux_%E8%B6%A3%E8%B0%88linux%E7%AC%94%E8%AE%B0/"},{"categories":["笔记"],"content":"中断上下文切换 除了前面两种上下文切换，还有一个场景也会切换 CPU 上下文，那就是中断。 跟进程上下文不同，中断上下文切换并不涉及到进程的用户态。所以，即便中断过程打断了一个正处在用户态的进程，也不需要保存和恢复这个进程的虚拟内存、全局变量等用户态资源。中断上下文，其实只包括内核态中断服务程序执行所必需的状态，包括 CPU 寄存器、内核堆栈、硬件中断参数等。 对同一个 CPU 来说，中断处理比进程拥有更高的优先级，所以中断上下文切换并不会与进程上下文切换同时发生。同样道理，由于中断会打断正常进程的调度和执行，所以大部分中断处理程序都短小精悍，以便尽可能快的执行结束。 另外，跟进程上下文切换一样，中断上下文切换也需要消耗 CPU，切换次数过多也会耗费大量的 CPU，甚至严重降低系统的整体性能。 ","date":"2021-08-23","objectID":"/posts/linux_%E8%B6%A3%E8%B0%88linux%E7%AC%94%E8%AE%B0/:9:0","tags":["linux"],"title":"趣谈linux笔记","uri":"/posts/linux_%E8%B6%A3%E8%B0%88linux%E7%AC%94%E8%AE%B0/"},{"categories":["笔记"],"content":"查看系统的上下文切换情况 通过前面学习我们知道，过多的上下文切换，会把 CPU 时间消耗在寄存器、内核栈以及虚拟内存等数据的保存和恢复上，缩短进程真正运行的时间，成了系统性能大幅下降的一个元凶。 可以使用 vmstat 这个工具，来查询系统的上下文切换情况。vmstat 是一个常用的系统性能分析工具，主要用来分析系统的内存使用情况，也常用来分析 CPU 上下文切换和中断的次数。 下面就是一个 vmstat 的使用示例： 1# 每隔 5 秒输出 1 组数据 2$ vmstat 5 3procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu----- 4 r b swpd free buff cache si so bi bo in cs us sy id wa st 5 0 0 0 7005360 91564 818900 0 0 0 0 25 33 0 0 100 0 cs（context switch）是每秒上下文切换的次数。 in（interrupt）则是每秒中断的次数。 r（Running or Runnable）是就绪队列的长度，也就是正在运行和等待 CPU 的进程数。 b（Blocked）则是处于不可中断睡眠状态的进程数。 vmstat 只给出了系统总体的上下文切换情况，要想查看每个进程的详细情况，就需要使用我们前面提到过的 pidstat 了。给它加上 -w 选项 这个结果中有两列内容是我们的重点关注对象。一个是 cswch ，表示每秒自愿上下文切换（voluntary context switches）的次数，另一个则是 nvcswch ，表示每秒非自愿上下文切换（non voluntary context switches）的次数。 这两个概念你一定要牢牢记住，因为它们意味着不同的性能问题： 所谓自愿上下文切换，是指进程无法获取所需资源，导致的上下文切换。比如说， I/O、内存等系统资源不足时，就会发生自愿上下文切换。 而非自愿上下文切换，则是指进程由于时间片已到等原因，被系统强制调度，进而发生的上下文切换。比如说，大量进程都在争抢 CPU 时，就容易发生非自愿上下文切换。 cpu使用率 计算 CPU 使用率，性能工具一般都会取间隔一段时间（比如 3 秒）的两次值，作差后，再计算出这段时间内的平均 CPU 使用率，即 进程的状态 top 和 ps 是最常用的查看进程状态的工具，我们就从 top 的输出开始。下面是一个 top 命令输出的示例，S 列（也就是 Status 列）表示进程的状态。从这个示例里，你可以看到 R、D、Z、S、I 等几个状态， R 是Running 或 Runnable 的缩写，表示进程在 CPU 的就绪队列中，正在运行或者正在等待运行。 D 是Disk Sleep 的缩写，也就是不可中断状态睡眠（Uninterruptible Sleep），一般表示进程正在跟硬件交互，并且交互过程不允许被其他进程或中断打断。 Z 是Zombie 的缩写，如果你玩过“植物大战僵尸”这款游戏，应该知道它的意思。它表示僵尸进程，也就是进程实际上已经结束了，但是父进程还没有回收它的资源（比如进程的描述符、PID 等）。 S 是Interruptible Sleep 的缩写，也就是可中断状态睡眠，表示进程因为等待某个事件而被系统挂起。当进程等待的事件发生时，它会被唤醒并进入 R 状态。 I 是Idle 的缩写，也就是空闲状态，用在不可中断睡眠的内核线程上。前面说了，硬件交互导致的不可中断进程用 D 表示，但对某些内核线程来说，它们有可能实际上并没有任何负载，用 Idle 正是为了区分这种情况。要注意，D 状态的进程会导致平均负载升高， I 状态的进程却不会。 X 也就是 Dead 的缩写，表示进程已经消亡，所以你不会在 top 或者 ps 命令中看到它。 T 或者t，也就是 Stopped 或 Traced 的缩写，表示进程处于暂停或者跟踪状态。 进程间通信 ","date":"2021-08-23","objectID":"/posts/linux_%E8%B6%A3%E8%B0%88linux%E7%AC%94%E8%AE%B0/:10:0","tags":["linux"],"title":"趣谈linux笔记","uri":"/posts/linux_%E8%B6%A3%E8%B0%88linux%E7%AC%94%E8%AE%B0/"},{"categories":["笔记"],"content":"管道模型 ps -ef | grep 关键字 | awk '{print $2}' | xargs kill -9 它会将前一个命令的输出，作为后一个命令的输入。从管道的这个名称可以看出来，管道是一种单向传输数据的机制，它其实是一段缓存，里面的数据只能从一端写入，从另一端读出。如果想互相通信，我们需要创建两个管道才行 管道分为两种类型，“|” 表示的管道称为匿名管道，意思就是这个类型的管道没有名字，用完了就销毁了。 另外一种类型是命名管道。这个类型的管道需要通过 mkfifo 命令显式地创建。 mkfifo hello # ls -l prw-r--r-- 1 root root 0 May 21 23:29 hello ","date":"2021-08-23","objectID":"/posts/linux_%E8%B6%A3%E8%B0%88linux%E7%AC%94%E8%AE%B0/:11:0","tags":["linux"],"title":"趣谈linux笔记","uri":"/posts/linux_%E8%B6%A3%E8%B0%88linux%E7%AC%94%E8%AE%B0/"},{"categories":["笔记"],"content":"消息队列 创建一个消息队列，使用msgget 函数。发送消息主要调用msgsnd 函数。收消息主要调用msgrcv 函数。 ","date":"2021-08-23","objectID":"/posts/linux_%E8%B6%A3%E8%B0%88linux%E7%AC%94%E8%AE%B0/:12:0","tags":["linux"],"title":"趣谈linux笔记","uri":"/posts/linux_%E8%B6%A3%E8%B0%88linux%E7%AC%94%E8%AE%B0/"},{"categories":["笔记"],"content":"共享内存 拿出一块虚拟地址空间来，映射到相同的物理内存中。这样这个进程写入的东西，另外一个进程马上就能看到了，都不需要拷贝来拷贝去，传来传去。 需要一种保护机制，使得同一个共享的资源，同时只能被一个进程访问。在 System V IPC 进程间通信机制体系中，早就想好了应对办法，就是信号量（Semaphore）。因此，信号量和共享内存往往要配合使用。 ","date":"2021-08-23","objectID":"/posts/linux_%E8%B6%A3%E8%B0%88linux%E7%AC%94%E8%AE%B0/:13:0","tags":["linux"],"title":"趣谈linux笔记","uri":"/posts/linux_%E8%B6%A3%E8%B0%88linux%E7%AC%94%E8%AE%B0/"},{"categories":["笔记"],"content":"信号 信号没有特别复杂的数据结构，就是用一个代号一样的数字。Linux 提供了几十种信号，分别代表不同的意义。信号之间依靠它们的值来区分。信号可以在任何时候发送给某一进程，进程需要为这个信号配置信号处理函数。当某个信号发生的时候，就默认执行这个函数就可以了。 一旦有信号产生，我们就有下面这几种，用户进程对信号的处理方式。 1.执行默认操作。Linux 对每种信号都规定了默认操作，例如，上面列表中的 Term，就是终止进程的意思。Core 的意思是 Core Dump，也即终止进程后，通过 Core Dump 将当前进程的运行状态保存在文件里面，方便程序员事后进行分析问题在哪里。 2.捕捉信号。我们可以为信号定义一个信号处理函数。当信号发生时，我们就执行相应的信号处理函数。 3.忽略信号。当我们不希望处理某些信号的时候，就可以忽略该信号，不做任何处理。有两个信号是应用进程无法捕捉和忽略的，即 SIGKILL 和 SEGSTOP，它们用于在任何时候中断或结束某一进程。 ","date":"2021-08-23","objectID":"/posts/linux_%E8%B6%A3%E8%B0%88linux%E7%AC%94%E8%AE%B0/:14:0","tags":["linux"],"title":"趣谈linux笔记","uri":"/posts/linux_%E8%B6%A3%E8%B0%88linux%E7%AC%94%E8%AE%B0/"},{"categories":["笔记"],"content":"信号的发送 有时候，我们在终端输入某些组合键的时候，会给进程发送信号，例如，Ctrl+C 产生 SIGINT 信号，Ctrl+Z 产生 SIGTSTP 信号 网络 应用层和内核互通的机制，就是通过 Socket 系统调用。所以经常有人会问，Socket 属于哪一层，其实它哪一层都不属于，它属于操作系统的概念，而非网络协议分层的概念。只不过操作系统选择对于网络协议的实现模式是，二到四层的处理代码在内核里面，七层的处理代码让应用自己去做，两者需要跨内核态和用户态通信，就需要一个系统调用完成这个衔接，这就是 Socket。 ","date":"2021-08-23","objectID":"/posts/linux_%E8%B6%A3%E8%B0%88linux%E7%AC%94%E8%AE%B0/:14:1","tags":["linux"],"title":"趣谈linux笔记","uri":"/posts/linux_%E8%B6%A3%E8%B0%88linux%E7%AC%94%E8%AE%B0/"},{"categories":["笔记"],"content":"发送数据包 网络分完层之后，对于数据包的发送，就是层层封装的过程。在 Linux 服务器 B 上部署的服务端 Nginx 和 Tomcat，都是通过 Socket 监听 80 和 8080 端口。这个时候，内核的数据结构就知道了。如果遇到发送到这两个端口的，就发送给这两个进程。 在 Linux 服务器 A 上的客户端，打开一个 Firefox 连接 Ngnix。也是通过 Socket，客户端会被分配一个随机端口 12345。同理，打开一个 Chrome 连接 Tomcat，同样通过 Socket 分配随机端口 12346。 在传输层有两个主流的协议 TCP 和 UDP，所以我们的 socket 程序设计也是主要操作这两个协议。这两个协议的区别是什么呢？通常的答案是下面这样的。 TCP 是面向连接的，UDP 是面向无连接的。 TCP 提供可靠交付，无差错、不丢失、不重复、并且按序到达；UDP 不提供可靠交付，不保证不丢失，不保证按顺序到达。 TCP 是面向字节流的，发送时发的是一个流，没头没尾；UDP 是面向数据报的，一个一个的发送。 TCP 是可以提供流量控制和拥塞控制的，既防止对端被压垮，也防止网络被压垮。 ","date":"2021-08-23","objectID":"/posts/linux_%E8%B6%A3%E8%B0%88linux%E7%AC%94%E8%AE%B0/:15:0","tags":["linux"],"title":"趣谈linux笔记","uri":"/posts/linux_%E8%B6%A3%E8%B0%88linux%E7%AC%94%E8%AE%B0/"},{"categories":["笔记"],"content":"linux","date":"2021-08-22","objectID":"/posts/linux_%E8%BD%AF%E7%A1%AC%E9%93%BE%E6%8E%A5/","tags":["linux"],"title":"linux软连接和硬连接","uri":"/posts/linux_%E8%BD%AF%E7%A1%AC%E9%93%BE%E6%8E%A5/"},{"categories":["笔记"],"content":"inode 文件还有元数据部分，例如名字、权限等，这就需要一个结构inode来存放。 什么是 inode 呢？inode 的“i”是 index 的意思，其实就是“索引”，类似图书馆的索引区域。既然如此，我们每个文件都会对应一个 inode；一个文件夹就是一个文件，也对应一个 inode。 struct ext4_inode { __le16 i_mode; /* File mode */ __le16 i_uid; /* Low 16 bits of Owner Uid */ __le32 i_size_lo; /* Size in bytes */ __le32 i_atime; /* Access time */ __le32 i_ctime; /* Inode Change time */ __le32 i_mtime; /* Modification time */ __le32 i_dtime; /* Deletion Time */ __le16 i_gid; /* Low 16 bits of Group Id */ __le16 i_links_count; /* Links count */ __le32 i_blocks_lo; /* Blocks count */ __le32 i_flags; /* File flags */ ...... __le32 i_block[EXT4_N_BLOCKS];/* Pointers to blocks */ __le32 i_generation; /* File version (for NFS) */ __le32 i_file_acl_lo; /* File ACL */ __le32 i_size_high; ...... }; 从这个数据结构中，我们可以看出，inode 里面有文件的读写权限 i_mode，属于哪个用户 i_uid，哪个组 i_gid，大小是多少 i_size_io，占用多少个块 i_blocks_io。咱们讲 ls 命令行的时候，列出来的权限、用户、大小这些信息，就是从这里面取出来的。 另外，这里面还有几个与文件相关的时间。i_atime 是 access time，是最近一次访问文件的时间；i_ctime 是 change time，是最近一次更改 inode 的时间；i_mtime 是 modify time，是最近一次更改文件的时间。 这里你需要注意区分几个地方。首先，访问了，不代表修改了，也可能只是打开看看，就会改变 access time。其次，修改 inode，有可能修改的是用户和权限，没有修改数据部分，就会改变 change time。只有数据也修改了，才改变 modify time。 我们平时使用的ls -l里面的数据其实就是取自inode。 硬连接和软连接 一种特殊的文件格式，硬链接（Hard Link）和软链接（Symbolic Link）。 ln [参数][源文件或目录][目标文件或目录] #为a生成一个软连接a1 ln -s a a1 #为a生成一个硬连接a2 ln a a2 硬连接时，a2和a共用一个inode,ls时，a2文件和a信息是一样的。但是 inode 是不跨文件系统的，每个文件系统都有自己的 inode 列表，因而硬链接是没有办法跨文件系统的。 但是软连接就不一样了，a1是单独的inode节点。 -rw-rw-r-- 2 xingliuhua xingliuhua 0 Aug 27 17:21 a lrwxrwxrwx 1 xingliuhua xingliuhua 1 Aug 27 17:22 a1 -\u003e a -rw-rw-r-- 2 xingliuhua xingliuhua 0 Aug 27 17:21 a2 还要注意，硬连接的开头并不是l,而软连接开头是l。 硬链接特点 具有相同inode节点号的多个文件互为硬链接文件； 删除硬链接文件或者删除源文件任意之一，文件实体并未被删除； 只有删除了源文件和所有对应的硬链接文件，文件实体才会被删除； 硬链接文件是文件的另一个入口； 可以通过给文件设置硬链接文件来防止重要文件被误删； 创建硬链接命令 ln 源文件 硬链接文件； 硬链接文件是普通文件，可以用rm删除； 对于静态文件（没有进程正在调用），当硬链接数为0时文件就被删除。注意：如果有进程正在调用，则无法删除或者即使文件名被删除但空间不会释放。 软链接特点 软链接类似windows系统的快捷方式； 软链接里面存放的是源文件的路径，指向源文件； 删除源文件，软链接依然存在，但无法访问源文件内容； 软链接失效时一般是白字红底闪烁； 创建软链接命令 ln -s 源文件 软链接文件； 软链接和源文件是不同的文件，文件类型也不同，inode号也不同； 软链接的文件类型是“l”，可以用rm删除。 硬链接和软链接的区别 原理上，硬链接和源文件的inode节点号相同，两者互为硬链接。 软连接和源文件的inode节点号不同，进而指向的block也不同，软连接block中存放了源文件的路径名。所以才能打开时看到是源文件的内容。 使用限制上，不能对目录创建硬链接，不能对不同文件系统创建硬链接，不能对不存在的文件创建硬链接；可以对目录创建软连接，可以跨文件系统创建软连接，可以对不存在的文件创建软连接。 ","date":"2021-08-22","objectID":"/posts/linux_%E8%BD%AF%E7%A1%AC%E9%93%BE%E6%8E%A5/:0:0","tags":["linux"],"title":"linux软连接和硬连接","uri":"/posts/linux_%E8%BD%AF%E7%A1%AC%E9%93%BE%E6%8E%A5/"},{"categories":["笔记"],"content":"golang slice \u0026 map","date":"2021-08-19","objectID":"/posts/go_%E5%BC%95%E7%94%A8%E7%A7%81%E6%9C%89%E4%BB%93%E5%BA%93/","tags":["golang"],"title":"golang 引用私有仓库","uri":"/posts/go_%E5%BC%95%E7%94%A8%E7%A7%81%E6%9C%89%E4%BB%93%E5%BA%93/"},{"categories":["笔记"],"content":"` ","date":"2021-08-19","objectID":"/posts/go_%E5%BC%95%E7%94%A8%E7%A7%81%E6%9C%89%E4%BB%93%E5%BA%93/:0:0","tags":["golang"],"title":"golang 引用私有仓库","uri":"/posts/go_%E5%BC%95%E7%94%A8%E7%A7%81%E6%9C%89%E4%BB%93%E5%BA%93/"},{"categories":["笔记"],"content":"直接使用go get 直接使用go get …添加私有仓库依赖时，会出现以下错误： get \"gitlab.com/xxx\": found meta tag get.metaImport{Prefix:\"gitlab.com/xxx\", VCS:\"git\", RepoRoot:\"https://gitlab.com/xxx.git\"} at //gitlab.com/xxx?go-get=1 go get gitlab.com/xxx: git ls-remote -q https://gitlab.com/xxx.git in /Users/sulin/go/pkg/mod/cache/vcs/91fae55e78195f3139c4f56af15f9b47ba7aa6ca0fa761efbd5b6e2b16d5159d: exit status 128: fatal: could not read Username for 'https://gitlab.com': terminal prompts disabled Confirm the import path was entered correctly. If this is a private repository, see https://golang.org/doc/faq#git_https for additional information. 从错误信息可以明显地看出来，我们使用私有仓库时通常会配置ssh-pubkey进行鉴权，但是go get使用https而非ssh的方式来下载依赖，从而导致鉴权失败。 如果配置了GOPROXY代理，错误信息则是如下样式： go get gitlab.com/xxx: module gitlab.com/xxx: reading https://goproxy.io/gitlab.com/xxx/@v/list: 404 Not Found 从错误信息可以看出，go get通过代理服务拉取私有仓库，而代理服务当然不可能访问到私有仓库，从而出现了404错误。 1.12版本解决方案 在1.11和1.12版本中，比较主流的解决方案是配置git强制采用ssh。 这个解决方案在许多博客、问答中都可以看到： git config --global url.\"git@gitlab.com:xxx/zz.git\".insteadof \"https://gitlab.com/xxx/zz.git\" 但是它与GOPROXY存在冲突，也就是说，在使用代理时，这个解决方案也是不生效的。 1.13版本解决方案 在1.13版本之后，前面介绍的解决方案又会导致go get出现另一种错误： get \"gitlab.com/xxx/zz\": found meta tag get.metaImport{Prefix:\"gitlab.com/xxx/zz\", VCS:\"git\", RepoRoot:\"https://gitlab.com/xxx/zz.git\"} at //gitlab.com/xxx/zz?go-get=1 verifying gitlab.com/xxx/zz@v0.0.1: gitlab.com/xxx/zz@v0.0.1: reading https://sum.golang.org/lookup/gitlab.com/xxx/zz@v0.0.1: 410 Gone 这个错误是因为新版本go mod会对依赖包进行checksum校验，但是私有仓库对sum.golang.org是不可见的，它当然没有办法成功执行checksum。 也就是说强制git采用ssh的解决办法在1.13版本之后GG了。 当然Golang在堵上窗户之前，也开了大门，它提供了一个更方便的解决方案：GOPRIVATE环境变量。解决以上的错误，可以这样配置： export GOPRIVATE=gitlab.com/xxx 它可以声明指定域名为私有仓库，go get在处理该域名下的所有依赖时，会直接跳过GOPROXY和CHECKSUM等逻辑，从而规避掉前文遇到的所有问题。 另外域名gitlab.com/xxx非常灵活，它默认是前缀匹配的，所有的gitlab.com/xxx前缀的依赖模块都会被视为private-modules，它对于企业、私有Group等有着一劳永逸的益处。 提示：如果你通过ssh公钥访问私有仓库，记得配置git拉取私有仓库时使用ssh而非https。 可以通过命令git config …的方式来配置。也可以像我这样，直接修改~/.gitconfig添加如下配置： [url \"git@github.com:\"] insteadOf = https://github.com/ [url \"git@gitlab.com:\"] insteadOf = https://gitlab.com/ 即可强制go get针对github.com与gitlab.com使用ssh而非https。 ","date":"2021-08-19","objectID":"/posts/go_%E5%BC%95%E7%94%A8%E7%A7%81%E6%9C%89%E4%BB%93%E5%BA%93/:0:1","tags":["golang"],"title":"golang 引用私有仓库","uri":"/posts/go_%E5%BC%95%E7%94%A8%E7%A7%81%E6%9C%89%E4%BB%93%E5%BA%93/"},{"categories":["笔记"],"content":"mysql innodb记录结构","date":"2021-07-27","objectID":"/posts/mysql_innodb%E8%AE%B0%E5%BD%95%E7%BB%93%E6%9E%84/","tags":["mysql"],"title":"mysql innodb记录结构","uri":"/posts/mysql_innodb%E8%AE%B0%E5%BD%95%E7%BB%93%E6%9E%84/"},{"categories":["笔记"],"content":"innodb记录结构 ","date":"2021-07-27","objectID":"/posts/mysql_innodb%E8%AE%B0%E5%BD%95%E7%BB%93%E6%9E%84/:0:0","tags":["mysql"],"title":"mysql innodb记录结构","uri":"/posts/mysql_innodb%E8%AE%B0%E5%BD%95%E7%BB%93%E6%9E%84/"},{"categories":["笔记"],"content":"页 我们知道读写磁盘的速度非常慢，和内存读写差了几个数量级，所以当我们想从表中获取某些记录时，InnoDB存储引擎需要一条一条的把记录 从磁盘上读出来么? 不，那样会慢死，InnoDB采取的方式是:将数据划分为若干个页，以页作为磁盘和内存之间交互的基本单位，InnoDB中页的大小一般为 16 KB。也就是在一般情况下，一次最少从磁 盘中读取16KB的内容到内存中，一次最少把内存中的16KB内容刷新到磁盘中。 ","date":"2021-07-27","objectID":"/posts/mysql_innodb%E8%AE%B0%E5%BD%95%E7%BB%93%E6%9E%84/:1:0","tags":["mysql"],"title":"mysql innodb记录结构","uri":"/posts/mysql_innodb%E8%AE%B0%E5%BD%95%E7%BB%93%E6%9E%84/"},{"categories":["笔记"],"content":"InnoDB行格式 我们平时是以记录为单位来向表中插入数据的，这些记录在磁盘上的存放方式也被称为行格式或者记录格式。设计InnoDB存储引擎的大叔们到现在为止设计了4种不同类型的行格式，分别 是Compact、Redundant、Dynamic和Compressed行格式。 我们可以在创建或修改表的语句中指定行格式: CREATE TABLE 表名 (列的信息) ROW_FORMAT=行格式名称 ALTER TABLE 表名 ROW_FORMAT=行格式名称 ","date":"2021-07-27","objectID":"/posts/mysql_innodb%E8%AE%B0%E5%BD%95%E7%BB%93%E6%9E%84/:2:0","tags":["mysql"],"title":"mysql innodb记录结构","uri":"/posts/mysql_innodb%E8%AE%B0%E5%BD%95%E7%BB%93%E6%9E%84/"},{"categories":["笔记"],"content":"行溢出数据现象 MySQL对一条记录占用的最大存储空间是有限制的，除了BLOB或者TEXT类型的列之外，其他所有的列(不包括隐藏列和记录头信息)占用的字节长度加起来不能超过65535个字 节。所以MySQL服务器建议我们把存储类型改为TEXT或者BLOB的类型。 MySQL中磁盘和内存交互的基本单位是页，也就是说MySQL是以页为基本单位来管理存储空间的， 我们的记录都会被分配到某个页中存储。而一个页的大小一般是16KB，也就是16384字节，而一个VARCHAR(M)类型的列就最多可以存储65532个字节，这样就可能造成一个页存放不了一条记录的尴尬情况。 在Compact和Reduntant行格式中，对于占用存储空间非常大的列，在记录的真实数据处只会存储该列的一部分数据，把剩余的数据分散存储在几个其他的页中，然后记录的真实数据处用20个字节存储指向这些 页的地址。 ","date":"2021-07-27","objectID":"/posts/mysql_innodb%E8%AE%B0%E5%BD%95%E7%BB%93%E6%9E%84/:2:1","tags":["mysql"],"title":"mysql innodb记录结构","uri":"/posts/mysql_innodb%E8%AE%B0%E5%BD%95%E7%BB%93%E6%9E%84/"},{"categories":["笔记"],"content":"行溢出的临界点 那发生行溢出的临界点是什么呢?也就是说在列存储多少字节的数据时就会发生行溢出? MySQL中规定一个页中至少存放两行记录,只要知道如果我们想一个行中存储了很大的数据时，可能发 生行溢出的现象。 ","date":"2021-07-27","objectID":"/posts/mysql_innodb%E8%AE%B0%E5%BD%95%E7%BB%93%E6%9E%84/:2:2","tags":["mysql"],"title":"mysql innodb记录结构","uri":"/posts/mysql_innodb%E8%AE%B0%E5%BD%95%E7%BB%93%E6%9E%84/"},{"categories":["笔记"],"content":"页结构 一个InnoDB数据页的存储空间大致被划分成了7个部分，有的部分占用的字节数是确定的，有的部分占用的字节数是不确定的: 不论我们怎么对页中的记录做增删改操作，InnoDB始终会维护一条记录的单链表，链表中的各个节点是按照主键值由小到大的顺序连接起来的。 主键值为2的记录被我们删掉了，但是存储空间却没有回收，如果我们再次把这条记录插入到表中，InnoDB并没有因为新记录的插入而为它申请新的存储空间，而是直接复用了原来被删除记录的存储空间 》每个数据页的File Header部分都有上一个和下一个页的编号，所以所有的数据页会组成一个双链表。 ","date":"2021-07-27","objectID":"/posts/mysql_innodb%E8%AE%B0%E5%BD%95%E7%BB%93%E6%9E%84/:3:0","tags":["mysql"],"title":"mysql innodb记录结构","uri":"/posts/mysql_innodb%E8%AE%B0%E5%BD%95%E7%BB%93%E6%9E%84/"},{"categories":["笔记"],"content":"数据页中的目录 主键值由小到大顺序串联成一个单链表，我们想根据主键值查找页中的某条记录需要遍历，有了目录就效率提高了。 在一个数据页中查找指定主键值的记录的过程分为两步: 通过二分法确定该记录所在的槽，并找到该槽中主键值最小的那条记录。 通过记录的next_record属性遍历该槽所在的组中的各个记录 ","date":"2021-07-27","objectID":"/posts/mysql_innodb%E8%AE%B0%E5%BD%95%E7%BB%93%E6%9E%84/:4:0","tags":["mysql"],"title":"mysql innodb记录结构","uri":"/posts/mysql_innodb%E8%AE%B0%E5%BD%95%E7%BB%93%E6%9E%84/"},{"categories":["笔记"],"content":"mysql配置","date":"2021-07-27","objectID":"/posts/mysql_%E9%85%8D%E7%BD%AE/","tags":["mysql"],"title":"mysql配置","uri":"/posts/mysql_%E9%85%8D%E7%BD%AE/"},{"categories":["笔记"],"content":"配置 除了在命令行启动的时候指定参数如：mysqld –default-storage-engine = MyISAM外，MySQL程序在启动时会寻找多个路径下的配置文件，这些路径有的是固定的，有的是可以在命令行指定的。根据操作系统的不 同，配置文件的路径也有所不同。 ","date":"2021-07-27","objectID":"/posts/mysql_%E9%85%8D%E7%BD%AE/:0:0","tags":["mysql"],"title":"mysql配置","uri":"/posts/mysql_%E9%85%8D%E7%BD%AE/"},{"categories":["笔记"],"content":"配置路径 在类UNIX操作系统中，MySQL会按照下列路径来寻找配置文件: 路径名 备注 /etc/my.cnf /etc/mysql/my.cnf SYSCONFDIR/my.cnf $MYSQL_HOME/my.cnf 特定于服务器的选项(仅限服务器) defaults-extra-file 命令行指定的额外配置文件路径 ~/.my.cnf 用户特定选项 ~/.mylogin.cnf 用户特定的登录路径选项(仅限客户端) ","date":"2021-07-27","objectID":"/posts/mysql_%E9%85%8D%E7%BD%AE/:1:0","tags":["mysql"],"title":"mysql配置","uri":"/posts/mysql_%E9%85%8D%E7%BD%AE/"},{"categories":["笔记"],"content":"配置分组 配置文件里面会分组：server、mysqld、mysqld_safe、client、mysql、mysqladmin。 配置文件中不同的选项组是给不同的启动命令使用的，如果选项组名称与程序名称相同，则组中的选项将专门应用于该程序。 |启动命令|类别|能读取的组| |-|-| | mysqld|启动服务器|[mysqld]、[server]| | mysqld_safe |服务 |[mysqld]、[server]、[mysqld_safe]| | mysql.server | 启动服务器 | [mysqld]、[server]、[mysql.server] | | mysql |启动客户端 | [mysql]、[client] | | mysqladmin | 启动客户端 | [mysqladmin]、[client]| | mysqldump | 启动客户端 | [mysqldump]、[client] | 如果我们想指定mysql.server程序的启动参数，则必须将它们放在配置文件中，而不是放在命令行中。 mysql.server仅支持start和stop作为命令行参数。 ","date":"2021-07-27","objectID":"/posts/mysql_%E9%85%8D%E7%BD%AE/:2:0","tags":["mysql"],"title":"mysql配置","uri":"/posts/mysql_%E9%85%8D%E7%BD%AE/"},{"categories":["笔记"],"content":"特定MySQL版本的专用选项组 我们可以在选项组的名称后加上特定的MySQL版本号。只有版本号为5.7的mysqld程序才能使用这个选项组中的选项[mysqld-5.7] ","date":"2021-07-27","objectID":"/posts/mysql_%E9%85%8D%E7%BD%AE/:3:0","tags":["mysql"],"title":"mysql配置","uri":"/posts/mysql_%E9%85%8D%E7%BD%AE/"},{"categories":["笔记"],"content":"配置文件加载顺序 MySQL将按照我们在上表中给定的顺序依次读取各个配置文件，如果该文件不存在则忽略。值得注 意的是，如果我们在多个配置文件中设置了相同的启动选项，那以最后一个配置文件中的为准。 同一个配置文件中多个组的优先级 我们说同一个命令可以访问配置文件中的多个组，那么，将以最后一个出现的组中的启动选项为准 如果同一个启动选项既出现在命令行中，又出现在配置文件中，那么以命令行中的启动选项为准 ","date":"2021-07-27","objectID":"/posts/mysql_%E9%85%8D%E7%BD%AE/:4:0","tags":["mysql"],"title":"mysql配置","uri":"/posts/mysql_%E9%85%8D%E7%BD%AE/"},{"categories":["笔记"],"content":"mysql统计数据的收集","date":"2021-07-27","objectID":"/posts/mysql_%E7%BB%9F%E8%AE%A1%E6%95%B0%E6%8D%AE%E7%9A%84%E6%94%B6%E9%9B%86/","tags":["mysql"],"title":"mysql统计数据的收集","uri":"/posts/mysql_%E7%BB%9F%E8%AE%A1%E6%95%B0%E6%8D%AE%E7%9A%84%E6%94%B6%E9%9B%86/"},{"categories":["笔记"],"content":"InnoDB 统计数据是如何收集的 通过SHOW TABLE STATUS可以看到关于表的统计数据，通过SHOW INDEX可以看到关于索引的统计数据，那么 这些统计数据是怎么来的呢?它们是以什么方式收集的呢? InnoDB提供了两种存储统计数据的方式: 永久性的统计数据 这种统计数据存储在磁盘上，也就是服务器重启之后这些统计数据还在。 非永久性的统计数据 这种统计数据存储在内存中，当服务器关闭时这些这些统计数据就都被清除掉了，等到服务器重启之后，在某些适当的场景下才会重新收集这些统计数据。 系统变量innodb_stats_persistent来控制到底采用哪种方式去存储统计数据。在MySQL 5.6.6之前，innodb_stats_persistent的值默 认是OFF，也就是说InnoDB的统计数据默认是存储到内存的，之后的版本中innodb_stats_persistent的值默认是ON，也就是统计数据默认被存储到磁盘中。 InnoDB默认是以表为单位来收集和存储统计数据的，也就是说我们可以把某些表的统计数据(以及该表的索引统计数据)存储在磁盘上，把另一些表的统计数据存 储在内存中。 我们可以在创建和修改表的时候通过指定STATS_PERSISTENT属性来指明该表的统计数据存储方式: CREATE TABLE 表名 (...) Engine=InnoDB, STATS_PERSISTENT = (1|0); ALTER TABLE 表名 Engine=InnoDB, STATS_PERSISTENT = (1|0); ","date":"2021-07-27","objectID":"/posts/mysql_%E7%BB%9F%E8%AE%A1%E6%95%B0%E6%8D%AE%E7%9A%84%E6%94%B6%E9%9B%86/:0:0","tags":["mysql"],"title":"mysql统计数据的收集","uri":"/posts/mysql_%E7%BB%9F%E8%AE%A1%E6%95%B0%E6%8D%AE%E7%9A%84%E6%94%B6%E9%9B%86/"},{"categories":["笔记"],"content":"InnoDB统计表中行数 InnoDB统计表中有多少行记录的套路是这样的: 按照一定算法(并不是纯粹随机的)选取几个叶子节点页面，计算每个页面中主键值记录数量，然后计算平均一个页面中主键值的记录数量乘以全部叶子节点的 数量就算是该表的n_rows值。 ","date":"2021-07-27","objectID":"/posts/mysql_%E7%BB%9F%E8%AE%A1%E6%95%B0%E6%8D%AE%E7%9A%84%E6%94%B6%E9%9B%86/:1:0","tags":["mysql"],"title":"mysql统计数据的收集","uri":"/posts/mysql_%E7%BB%9F%E8%AE%A1%E6%95%B0%E6%8D%AE%E7%9A%84%E6%94%B6%E9%9B%86/"},{"categories":["笔记"],"content":"更新统计数据 开启innodb_stats_auto_recalc。 系统变量innodb_stats_auto_recalc决定着服务器是否自动重新计算统计数据，它的默认值是ON，也就是该功能默认是开启的。每个表都维护了一个变量，该变 量记录着对该表进行增删改的记录条数，如果发生变动的记录数量超过了表大小的10%，并且自动重新计算统计数据的功能是打开的，那么服务器会重新进行一次 统计数据的计算，并且更新innodb_table_stats和innodb_index_stats表。不过自动重新计算统计数据的过程是异步发生的，也就是即使表中变动的记录数超过 了10%，自动重新计算统计数据也不会立即发生，可能会延迟几秒才会进行计算。 手动调用ANALYZE TABLE语句来更新统计信息 如果innodb_stats_auto_recalc系统变量的值为OFF的话，我们也可以手动调用ANALYZE TABLE语句来重新计算统计数据.ANALYZE TABLE语句会立即重新计算统计数据，也就是这个过程是同步的。 ","date":"2021-07-27","objectID":"/posts/mysql_%E7%BB%9F%E8%AE%A1%E6%95%B0%E6%8D%AE%E7%9A%84%E6%94%B6%E9%9B%86/:2:0","tags":["mysql"],"title":"mysql统计数据的收集","uri":"/posts/mysql_%E7%BB%9F%E8%AE%A1%E6%95%B0%E6%8D%AE%E7%9A%84%E6%94%B6%E9%9B%86/"},{"categories":["笔记"],"content":"mysql字符集","date":"2021-07-27","objectID":"/posts/mysql_%E5%AD%97%E7%AC%A6%E9%9B%86/","tags":["mysql"],"title":"mysql字符集","uri":"/posts/mysql_%E5%AD%97%E7%AC%A6%E9%9B%86/"},{"categories":["笔记"],"content":"字符集及编码 ","date":"2021-07-27","objectID":"/posts/mysql_%E5%AD%97%E7%AC%A6%E9%9B%86/:0:0","tags":["mysql"],"title":"mysql字符集","uri":"/posts/mysql_%E5%AD%97%E7%AC%A6%E9%9B%86/"},{"categories":["笔记"],"content":"MySQL中的 utf8和 utf8mb4 我们上边说utf8字符集表示一个字符需要使用1~4个字节，但是我们常用的一些字符使用1~3个字节就可以表示了。而在MySQL中字符集表示一 个字符所用最大字节长度在某些方面会影响系统的存储和性能，所以设计MySQL的大叔偷偷的定义了两个概念: utf8mb3:阉割过的utf8字符集，只使用1~3个字节表示字符。 utf8mb4:正宗的utf8字符集，使用1~4个字节表示字符。 有一点需要大家十分的注意，在MySQL中utf8是utf8mb3的别名，所以之后在MySQL中提到utf8就意味着使用1~3个字节来表示一个字符，如果大 家有使用4字节编码一个字符的情况，比如存储一些emoji表情啥的，那请使用utf8mb4。 ","date":"2021-07-27","objectID":"/posts/mysql_%E5%AD%97%E7%AC%A6%E9%9B%86/:1:0","tags":["mysql"],"title":"mysql字符集","uri":"/posts/mysql_%E5%AD%97%E7%AC%A6%E9%9B%86/"},{"categories":["笔记"],"content":"字符集相关配置 查看所支持的字符集。 SHOW (CHARACTER SET|CHARSET) [LIKE 匹配的模式]; 其中CHARACTER SET和CHARSET是同义词，用任意一个都可以。 查看MySQL中支持的比较规则的命令如下: SHOW COLLATION [LIKE 匹配的模式]; 我们前边说过一种字符集可能对应着若干种比较规则，MySQL支持的字符集就已经非常多了，所以支持的比较规则更多 各级别的字符集和比较规则 MySQL有4个级别的字符集和比较规则，分别是: 服务器级别 数据库级别 表级别 列级别 服务器级别 MySQL提供了两个系统变量来表示服务器级别的字符集和比较规则: character_set_server 服务器级别的字符集 collation_server 服务器级别的比较规则 SHOW VARIABLES LIKE ‘collation_server’; 数据库级别 我们在创建和修改数据库的时候可以指定该数据库的字符集和比较规则，具体语法如下: CREATE DATABASE 数据库名 [[DEFAULT] CHARACTER SET 字符集名称] [[DEFAULT] COLLATE 比较规则名称]; ALTER DATABASE 数据库名 [[DEFAULT] CHARACTER SET 字符集名称] [[DEFAULT] COLLATE 比较规则名称]; 表级别 我们也可以在创建和修改表的时候指定表的字符集和比较规则，语法如下: CREATE TABLE 表名 (列的信息) [[DEFAULT] CHARACTER SET 字符集名称] [COLLATE 比较规则名称]] ALTER TABLE 表名 [[DEFAULT] CHARACTER SET 字符集名称] [COLLATE 比较规则名称] 列级别 需要注意的是，对于存储字符串的列，同一个表中的不同的列也可以有不同的字符集和比较规则。我们在创建和修改列定义的时候可以指定该 列的字符集和比较规则，语法如下: CREATE TABLE 表名( 列名 字符串类型 [CHARACTER SET 字符集名称] [COLLATE 比较规则名称], 其他列…); ","date":"2021-07-27","objectID":"/posts/mysql_%E5%AD%97%E7%AC%A6%E9%9B%86/:2:0","tags":["mysql"],"title":"mysql字符集","uri":"/posts/mysql_%E5%AD%97%E7%AC%A6%E9%9B%86/"},{"categories":["笔记"],"content":"客户端和服务器通信中的字符集 说到底，字符串在计算机上的体现就是一个字节串，如果你使用不同字符集去解码这个字节串，最后得到的结果可能让你挠头。 从发送请求到返回结果这个过程中伴随着多次字符集 的转换，在这个过程中会用到3个系统变量，我们先把它们写出来看一下: 系统变量 描述 c character_set_client 服务器解码请求时使用的字符集 character_set_connection 服务器处理请求时会把请求字符串从character_set_client转为character_set_connection character_set_results 服务器向客户端返回数据时使用的字符集 ","date":"2021-07-27","objectID":"/posts/mysql_%E5%AD%97%E7%AC%A6%E9%9B%86/:3:0","tags":["mysql"],"title":"mysql字符集","uri":"/posts/mysql_%E5%AD%97%E7%AC%A6%E9%9B%86/"},{"categories":["笔记"],"content":"go sync.map","date":"2021-06-19","objectID":"/posts/go_syncmap/","tags":["golang"],"title":"go sync.map","uri":"/posts/go_syncmap/"},{"categories":["笔记"],"content":"实现安全的map 自go 1.6之后， 并发地读写map会报错，这在一些知名的开源库中都存在这个问题，所以go 1.9之前的解决方案是额外绑定一个锁，封装成一个新的struct或者单独使用锁都可以。 ","date":"2021-06-19","objectID":"/posts/go_syncmap/:0:0","tags":["golang"],"title":"go sync.map","uri":"/posts/go_syncmap/"},{"categories":["笔记"],"content":"直接加锁 type syncMap struct { items map[string]interface{} sync.RWMutex } 读读不会阻塞，读写、写写会阻塞。 ","date":"2021-06-19","objectID":"/posts/go_syncmap/:1:0","tags":["golang"],"title":"go sync.map","uri":"/posts/go_syncmap/"},{"categories":["笔记"],"content":"分段加锁 具体实现就是，基于上面的 syncMap 再包装一次，用多个 syncMap 来模拟实现一个 map： type SyncMap struct { shards []*syncMap } 源码解析 type Map struct { mu Mutex read atomic.Value // readOnly read map dirty map[interface{}]*entry // dirty map misses int } 总体图： 查询： 插入或更新： 删除： read map 的值是什么时间更新的 ？ Load/LoadOrStore/LoadAndDelete 时，当 misses 数量大于等于 dirty map 的元素个数时，会整体复制 dirty map 到 read map Store/LoadOrStore 时，当 read map 中存在这个key，则更新 Delete/LoadAndDelete 时，如果 read map 中存在这个key，则设置这个值为 nil。 dirty map 的值是什么时间更新的 ？ 完全是一个新 key， 第一次插入 sync.Map，必先插入 dirty map Store/LoadOrStore 时，当 read map 中不存在这个key，在 dirty map 存在这个key，则更新 Delete/LoadAndDelete 时，如果 read map 中不存在这个key，在 dirty map 存在这个key，则从 dirty map 中删除这个key 当 misses 数量大于等于 dirty map 的元素个数时，会整体复制 dirty map 到 read map，同时设置 dirty map 为 nil read map 和 dirty map 是什么时间删除的？ 当 read map 中存在某个 key 的时候，这个时候只会删除 read map， 并不会删除 dirty map（因为 dirty map 不存在这个值） 当 read map 中不存在时，才会去删除 dirty map 里面的值 疑问：如果按照这个删除方式，那岂不是 dirty map 中会有残余的 key，导致没删除掉？ 答：其实并不会。当 misses 数量大于等于 dirty map 的元素个数时，会整体复制 dirty map 到 read map。这个过程中还附带了另外一个操作：将 dirty map 置为 nil。 read map 与 dirty map 的关系 ？ 在 read map 中存在的值，在 dirty map 中可能不存在。 在 dirty map 中存在的值，在 read map 中也可能存在。 当访问多次，发现 dirty map 中存在，read map 中不存在，导致 misses 数量大于等于 dirty map 的元素个数时，会整体复制 dirty map 到 read map。 当出现 dirty map 向 read map 复制后，dirty map 会被置成 nil。 当出现 dirty map 向 read map 复制后，readOnly.amended 等于了 false。当新插入了一个值时，会将 read map 中的值，重新给 dirty map 赋值一遍 sync.Map 是如何提高性能的？ 通过源码解析，我们知道 sync.Map 里面有两个普通 map，read map主要是负责读，dirty map 是负责读和写（加锁）。在读多写少的场景下，read map 的值基本不发生变化，可以让 read map 做到无锁操作，就减少了使用 Mutex + Map 必须的加锁/解锁环节，因此也就提高了性能。 不过也能够看出来，read map 也是会发生变化的，如果某些 key 写操作特别频繁的话，sync.Map 基本也就退化成了 Mutex + Map（有可能性能还不如 Mutex + Map）。 所以，不是说使用了 sync.Map 就一定能提高程序性能，我们日常使用中尽量注意拆分粒度来使用 sync.Map。 ","date":"2021-06-19","objectID":"/posts/go_syncmap/:2:0","tags":["golang"],"title":"go sync.map","uri":"/posts/go_syncmap/"},{"categories":["笔记"],"content":"nginx案例","date":"2021-06-01","objectID":"/posts/niginx_%E6%A1%88%E4%BE%8B/","tags":["nginx"],"title":"nginx案例","uri":"/posts/niginx_%E6%A1%88%E4%BE%8B/"},{"categories":["笔记"],"content":"准备应用 go应用，返回环境变量 func InitGinRouter() { engine := gin.Default() engine.GET(\"/test\", func(ctx *gin.Context) { ctx.JSON(http.StatusOK, gin.H{ \"code\": 1, \"msg\": os.Getenv(\"SERVER_NODE_NAME\"), }) }) engine.Run(\":8800\") } 编译出二进制文件hello nginx配置文件 nginx.conf user root; worker_processes auto; error_log /var/log/nginx/error.log notice; pid /var/run/nginx.pid; events { worker_connections 1024; } http { include /etc/nginx/mime.types; default_type application/octet-stream; log_format main escape=json '$remote_addr-$time_iso8601-$request_uri-$request-$status'; access_log /var/log/nginx/access.log main; sendfile on; #tcp_nopush on; keepalive_timeout 65; #gzip on; include /etc/nginx/conf.d/*.conf; upstream myupstream { server hello-server:8800; server hello-server2:8800; // ip也可 } server { #监听端口 listen 80; server_name localhost; location /test { proxy_pass http://myupstream; } } } 在这里我们监听80端口，并指定路由/test，满足该路由就会反向代理到myupstream，然后请求轮询打到hello-server:8800，hello-server2:8800。 除了轮询外，还有其他策略。 当一个应用挂掉，会自动的停止给它请求，应用恢复后会自动再分配给它请求。 准备docker-compose hello应用的Dockerfile： FROM ubuntu COPY ./hello /hello/ WORKDIR /hello/ ENTRYPOINT [\"./hello\"] docker-compose.yml version: '3.2' services: hello-server: build: context: ./ dockerfile: ./Dockerfile restart: always environment: SERVER_NODE_NAME: \"hello1\" #这里自行设置root用户的密码 networks: - hello-net hello-server2: build: context: ./ dockerfile: ./Dockerfile restart: always environment: SERVER_NODE_NAME: \"hello2\" #这里自行设置root用户的密码 networks: - hello-net hello-nginx: image: nginx volumes: - /Users/lhx/Documents/go_project/hello/nginx.conf:/etc/nginx/nginx.conf - /Users/lhx/Documents/go_project/hello/log/nginx:/var/log/nginx/ ports: - 80:80 networks: - hello-net networks: hello-net: driver: bridge docker-compose up -d 运行后，访问http://localhost:81/test就能轮询服务器 ","date":"2021-06-01","objectID":"/posts/niginx_%E6%A1%88%E4%BE%8B/:0:0","tags":["nginx"],"title":"nginx案例","uri":"/posts/niginx_%E6%A1%88%E4%BE%8B/"},{"categories":["笔记"],"content":"nginx配置","date":"2021-06-01","objectID":"/posts/niginx_%E9%85%8D%E7%BD%AE/","tags":["nginx"],"title":"nginx配置","uri":"/posts/niginx_%E9%85%8D%E7%BD%AE/"},{"categories":["笔记"],"content":"#定义Nginx运行的用户和用户组 user www www; #nginx进程数，建议设置为等于CPU总核心数。 worker_processes 8; #全局错误日志定义类型，[ debug | info | notice | warn | error | crit ] error_log /usr/local/nginx/logs/error.log info; #进程pid文件 pid /usr/local/nginx/logs/nginx.pid; #指定进程可以打开的最大描述符：数目 #工作模式与连接数上限 #这个指令是指当一个nginx进程打开的最多文件描述符数目，理论值应该是最多打开文件数（ulimit -n）与nginx进程数相除，但是nginx分配请求并不是那么均匀，所以最好与ulimit -n 的值保持一致。 #现在在linux 2.6内核下开启文件打开数为65535，worker_rlimit_nofile就相应应该填写65535。 #这是因为nginx调度时分配请求到进程并不是那么的均衡，所以假如填写10240，总并发量达到3-4万时就有进程可能超过10240了，这时会返回502错误。 worker_rlimit_nofile 65535; events { #参考事件模型，use [ kqueue | rtsig | epoll | /dev/poll | select | poll ]; epoll模型 #是Linux 2.6以上版本内核中的高性能网络I/O模型，linux建议epoll，如果跑在FreeBSD上面，就用kqueue模型。 #补充说明： #与apache相类，nginx针对不同的操作系统，有不同的事件模型 #A）标准事件模型 #Select、poll属于标准事件模型，如果当前系统不存在更有效的方法，nginx会选择select或poll #B）高效事件模型 #Kqueue：使用于FreeBSD 4.1+, OpenBSD 2.9+, NetBSD 2.0 和 MacOS X.使用双处理器的MacOS X系统使用kqueue可能会造成内核崩溃。 #Epoll：使用于Linux内核2.6版本及以后的系统。 #/dev/poll：使用于Solaris 7 11/99+，HP/UX 11.22+ (eventport)，IRIX 6.5.15+ 和 Tru64 UNIX 5.1A+。 #Eventport：使用于Solaris 10。 为了防止出现内核崩溃的问题， 有必要安装安全补丁。 use epoll; #单个进程最大连接数（最大连接数=连接数*进程数） #根据硬件调整，和前面工作进程配合起来用，尽量大，但是别把cpu跑到100%就行。每个进程允许的最多连接数，理论上每台nginx服务器的最大连接数为。 worker_connections 65535; #keepalive超时时间。 keepalive_timeout 60; #客户端请求头部的缓冲区大小。这个可以根据你的系统分页大小来设置，一般一个请求头的大小不会超过1k，不过由于一般系统分页都要大于1k，所以这里设置为分页大小。 #分页大小可以用命令getconf PAGESIZE 取得。 #[root@web001 ~]# getconf PAGESIZE #4096 #但也有client_header_buffer_size超过4k的情况，但是client_header_buffer_size该值必须设置为“系统分页大小”的整倍数。 client_header_buffer_size 4k; #这个将为打开文件指定缓存，默认是没有启用的，max指定缓存数量，建议和打开文件数一致，inactive是指经过多长时间文件没被请求后删除缓存。 open_file_cache max=65535 inactive=60s; #这个是指多长时间检查一次缓存的有效信息。 #语法:open_file_cache_valid time 默认值:open_file_cache_valid 60 使用字段:http, server, location 这个指令指定了何时需要检查open_file_cache中缓存项目的有效信息. open_file_cache_valid 80s; #open_file_cache指令中的inactive参数时间内文件的最少使用次数，如果超过这个数字，文件描述符一直是在缓存中打开的，如上例，如果有一个文件在inactive时间内一次没被使用，它将被移除。 #语法:open_file_cache_min_uses number 默认值:open_file_cache_min_uses 1 使用字段:http, server, location 这个指令指定了在open_file_cache指令无效的参数中一定的时间范围内可以使用的最小文件数,如果使用更大的值,文件描述符在cache中总是打开状态. open_file_cache_min_uses 1; #语法:open_file_cache_errors on | off 默认值:open_file_cache_errors off 使用字段:http, server, location 这个指令指定是否在搜索一个文件时记录cache错误. open_file_cache_errors on; } #设定http服务器，利用它的反向代理功能提供负载均衡支持 http { #文件扩展名与文件类型映射表 include mime.types; #默认文件类型 default_type application/octet-stream; #默认编码 #charset utf-8; #服务器名字的hash表大小 #保存服务器名字的hash表是由指令server_names_hash_max_size 和server_names_hash_bucket_size所控制的。参数hash bucket size总是等于hash表的大小，并且是一路处理器缓存大小的倍数。在减少了在内存中的存取次数后，使在处理器中加速查找hash表键值成为可能。如果hash bucket size等于一路处理器缓存的大小，那么在查找键的时候，最坏的情况下在内存中查找的次数为2。第一次是确定存储单元的地址，第二次是在存储单元中查找键 值。因此，如果Nginx给出需要增大hash max size 或 hash bucket size的提示，那么首要的是增大前一个参数的大小. server_names_hash_bucket_size 128; #客户端请求头部的缓冲区大小。这个可以根据你的系统分页大小来设置，一般一个请求的头部大小不会超过1k，不过由于一般系统分页都要大于1k，所以这里设置为分页大小。分页大小可以用命令getconf PAGESIZE取得。 client_header_buffer_size 32k; #客户请求头缓冲大小。nginx默认会用client_header_buffer_size这个buffer来读取header值，如果header过大，它会使用large_client_header_buffers来读取。 large_client_header_buffers 4 64k; #设定通过nginx上传文件的大小 client_max_body_size 8m; #开启高效文件传输模式，sendfile指令指定nginx是否调用sendfile函数来输出文件，对于普通应用设为 on，如果用来进行下载等应用磁盘IO重负载应用，可设置为off，以平衡磁盘与网络I/O处理速度，降低系统的负载。注意：如果图片显示不正常把这个改成off。 #sendfile指令指定 nginx 是否调用sendfile 函数（zero copy 方式）来输出文件，对于普通应用，必须设为on。如果用来进行下载等应用磁盘IO重负载应用，可设置为off，以平衡磁盘与网络IO处理速度，降低系统uptime。 sendfile on; #开启目录列表访问，合适下载服务器，默认关闭。 autoindex on; #此选项允许或禁止使用socke的TCP_CORK的选项，此选项仅在使用sendfile的时候使用 tcp_nopush on; tcp_nodelay on; #长连接超时时间，单位是秒 keepalive_timeout 120; #FastCGI相关参数是为了改善网站的性能：减少资源占用，提高访问速度。下面参数看字面意思都能理解。 fastcgi_connect_timeout 300; fastcgi_send_timeout 300; fastcgi_read_timeout 300; fastcgi_buffer_size 64k; fastcgi_buffers 4 64k; fastcgi_busy_buffers_size 128k; fastcgi_temp_file_write_size 128k; #gzip模块设置 gzip on; #开启gzip压缩输出 gzip_min_length 1k; #最小压缩文件大小 gzip_buffers 4 16k; #压缩缓冲区 gzip_http_version 1.0; #压缩版本（默认1.1，前端如果是squid2.5请使用1.0） gzip_comp_level 2; #压缩等级 gzip_types text/plain application/x-j","date":"2021-06-01","objectID":"/posts/niginx_%E9%85%8D%E7%BD%AE/:0:0","tags":["nginx"],"title":"nginx配置","uri":"/posts/niginx_%E9%85%8D%E7%BD%AE/"},{"categories":["笔记"],"content":"nginx日志配置","date":"2021-06-01","objectID":"/posts/niginx_%E6%97%A5%E5%BF%97/","tags":["nginx"],"title":"nginx日志配置","uri":"/posts/niginx_%E6%97%A5%E5%BF%97/"},{"categories":["笔记"],"content":"Nginx日志主要分为两种：access_log(访问日志)和error_log(错误日志)。通过访问日志我们可以得到用户的IP地址、浏览器的信息，请求的处理时间等信息。 access_log 访问日志主要记录客户端的请求。客户端向Nginx服务器发起的每一次请求都记录在这里。客户端IP，浏览器信息，referer，请求处理时间，请求URL等都可以在访问日志中得到。当然具体要记录哪些信息，你可以通过log_format指令定义。 access_log path [format [buffer=size] [gzip[=level]] [flush=time] [if=condition]]; # 设置访问日志 access_log off; # 关闭访问日志 path 指定日志的存放位置。 format 指定日志的格式。默认使用预定义的combined。 buffer 用来指定日志写入时的缓存大小。默认是64k。 gzip 日志写入前先进行压缩。压缩率可以指定，从1到9数值越大压缩比越高，同时压缩的速度也越慢。默认是1。 flush 设置缓存的有效时间。如果超过flush指定的时间，缓存中的内容将被清空。 if 条件判断。如果指定的条件计算为0或空字符串，那么该请求不会写入日志。 另外，还有一个特殊的值off。如果指定了该值，当前作用域下的所有的请求日志都被关闭。 ","date":"2021-06-01","objectID":"/posts/niginx_%E6%97%A5%E5%BF%97/:0:0","tags":["nginx"],"title":"nginx日志配置","uri":"/posts/niginx_%E6%97%A5%E5%BF%97/"},{"categories":["笔记"],"content":"作用域 可以应用access_log指令的作用域分别有http，server，location，limit_except。也就是说，在这几个作用域外使用该指令，Nginx会报错。 ","date":"2021-06-01","objectID":"/posts/niginx_%E6%97%A5%E5%BF%97/:1:0","tags":["nginx"],"title":"nginx日志配置","uri":"/posts/niginx_%E6%97%A5%E5%BF%97/"},{"categories":["笔记"],"content":"用法 基本用法： access_log /var/logs/nginx-access.log ","date":"2021-06-01","objectID":"/posts/niginx_%E6%97%A5%E5%BF%97/:2:0","tags":["nginx"],"title":"nginx日志配置","uri":"/posts/niginx_%E6%97%A5%E5%BF%97/"},{"categories":["笔记"],"content":"使用log_format自定义日志格式 格式： log_format name [escape=default|json] string ...; 我们定义一个名为main的日志格式 http { include /etc/nginx/mime.types; default_type application/octet-stream; log_format main '$remote_addr-$time_iso8601-$request_uri-$request-$status'; access_log /var/log/nginx/access.log main; sendfile on; #tcp_nopush on; keepalive_timeout 65; #gzip on; include /etc/nginx/conf.d/*.conf; } Nginx预定义了名为combined日志格式，如果没有明确指定日志格式默认使用该格式： log_format combined '$remote_addr - $remote_user [$time_local] ' '\"$request\" $status $body_bytes_sent ' '\"$http_referer\" \"$http_user_agent\"'; log_format指令中常用的一些变量： bytes_sent 发送给客户端的总字节数 body_bytes_sent 发送给客户端的字节数，不包括响应头的大小 connection 连接序列号 connection_requests 当前通过连接发出的请求数量 msec 日志写入时间，单位为秒，精度是毫秒 pipe 如果请求是通过http流水线发送，则其值为\"p\"，否则为“.\" request_length 请求长度（包括请求行，请求头和请求体） request_time 请求处理时长，单位为秒，精度为毫秒，从读入客户端的第一个字节开始，直到把最后一个字符发送张客户端进行日志写入为止 status 响应状态码 time_iso8601 标准格式的本地时间,形如“2017-05-24T18:31:27+08:00” time_local 通用日志格式下的本地时间，如\"24/May/2017:18:31:27 +0800\" http_referer 请求的referer地址。 http_user_agent 客户端浏览器信息。 remote_addr 客户端IP http_x_forwarded_for 当前端有代理服务器时，设置web节点记录客户端地址的配置，此参数生效的前提是代理服务器也要进行相关的x_forwarded_for设置。 request 完整的原始请求行，如 “GET / HTTP/1.1” remote_user 客户端用户名称，针对启用了用户认证的请求 request_uri 完整的请求地址，如 “https://daojia.com/\" error_log 语法 配置错误日志文件的路径和日志级别。 error_log file [level]; level可以是debug, info, notice, warn, error, crit, alert,emerg中的任意值。可以看到其取值范围是按紧急程度从低到高排列的。只有日志的错误级别等于或高于level指定的值才会写入错误日志中。默认值是error。 ","date":"2021-06-01","objectID":"/posts/niginx_%E6%97%A5%E5%BF%97/:3:0","tags":["nginx"],"title":"nginx日志配置","uri":"/posts/niginx_%E6%97%A5%E5%BF%97/"},{"categories":["笔记"],"content":"基础","date":"2021-04-08","objectID":"/posts/mysql_join%E8%BF%87%E7%A8%8B/","tags":["mysql"],"title":"mysql join过程","uri":"/posts/mysql_join%E8%BF%87%E7%A8%8B/"},{"categories":["笔记"],"content":"被驱动表能用到索引 select * from t1 straight_join t2 on (t1.a=t2.a); 如果直接使用 join 语句，MySQL 优化器可能会选择表 t1 或 t2 作为驱动表，这样会影响我们分析 SQL 语句的执行过程。所以，为了便于分析执行过程中的性能问题，我改用 straight_join 让 MySQL 使用固定的连接方式执行查询，这样优化器只会按照我们指定的方式去 join。在这个语句里，t1 是驱动表，t2 是被驱动表。 在这条语句里，被驱动表 t2 的字段 a 上有索引，join 过程用上了这个索引，先遍历表 t1，然后根据从表 t1 中取出的每行数据中的 a 值，去表 t2 中查找满足条件的记录。在形式上，这个过程就跟我们写程序时的嵌套查询类似，并且可以用上被驱动表的索引，所以我们称之为“Index Nested-Loop Join”，简称 NLJ。 在这个 join 语句执行过程中，驱动表是走全表扫描，而被驱动表是走树搜索。 假设被驱动表的行数是 M。每次在被驱动表查一行数据，要先搜索索引 a，再搜索主键索引。每次搜索一棵树近似复杂度是以 2 为底的 M 的对数，记为 log2M，所以在被驱动表上查一行的时间复杂度是 2*log2M。 假设驱动表的行数是 N，执行过程就要扫描驱动表 N 行，然后对于每一行，到被驱动表上匹配一次。 因此整个执行过程，近似复杂度是 N + N2log2M。 显然，N 对扫描行数的影响更大，因此应该让小表来做驱动表。 被驱动表用不上索引 select * from t1 straight_join t2 on (t1.a=t2.b); 由于表 t2 的字段 b 上没有索引，因此再用图 2 的执行流程时，每次到 t2 去匹配的时候，就要做一次全表扫描。 你可以先设想一下这个问题，继续使用图 2 的算法，是不是可以得到正确的结果呢？如果只看结果的话，这个算法是正确的，而且这个算法也有一个名字，叫做“Simple Nested-Loop Join”。 如果 t1 和 t2 都是 10 万行的表（当然了，这也还是属于小表的范围），就要扫描 100 亿行，先拿出t1的一行，然后把t2的记录从磁盘一一读出来进行判断，那么t2表需要从内存中读取10次。 当然，MySQL 也没有使用这个 Simple Nested-Loop Join 算法，而是使用了另一个叫作“Block Nested-Loop Join”的算法，简称 BNL。 Block Nested-Loop Join 扫描一个表的过程其实是先把这个表从磁盘上加载到内存中，然后从内存中比较匹配条件是否满足。内存里可能并不能完全存放的下表中所有的记录，所以在扫描表前边记录的时候后边的记录可能还在磁盘上， 等扫描到后边记录的时候可能内存不足，所以需要把前边的记录从内存中释放掉。 我们前边又说过，采用嵌套循环连接算法的两表连接过程中，被驱动表可是要被访问好 多次的，如果这个被驱动表中的数据特别多而且不能使用索引进行访问，那就相当于要从磁盘上读好几次这个表，这个I/O代价就非常大了，所以我们得想办法:尽量减 少访问被驱动表的次数。 当被驱动表中的数据非常多时，每次访问被驱动表，被驱动表的记录会被加载到内存中，在内存中的每一条记录只会和驱动表结果集的一条记录做匹配，之后就会被从 内存中清除掉。然后再从驱动表结果集中拿出另一条记录，再一次把被驱动表的记录加载到内存中一遍，周而复始，驱动表结果集中有多少条记录，就得把被驱动表从 磁盘上加载到内存中多少次。所以我们可不可以在把被驱动表的记录加载到内存的时候，一次性和多条驱动表中的记录做匹配，这样就可以大大减少重复从磁盘上加载 被驱动表的代价了。所以设计MySQL的大叔提出了一个join buffer的概念，join buffer就是执行连接查询前申请的一块固定大小的内存，先把若干条驱动表结果集中的 记录装在这个join buffer中，然后开始扫描被驱动表，每一条被驱动表的记录一次性和join buffer中的多条驱动表记录做匹配，因为匹配的过程都是在内存中完成 的，所以这样可以显著减少被驱动表的I/O代价。 这时候，被驱动表上没有可用的索引，算法的流程是这样的： 把表 t1 的数据读入线程内存 join_buffer 中，由于我们这个语句中写的是 select *，因此是把整个表 t1 放入了内存； 扫描表 t2，把表 t2 中的每一行取出来，跟 join_buffer 中的数据做对比，满足 join 条件的，作为结果集的一部分返回。 在这个过程中，对表 t1 和 t2 都做了一次全表扫描，因此总的扫描行数是 1100。由于 join_buffer 是以无序数组的方式组织的，因此对表 t2 中的每一行，都要做 100 次判断，总共需要在内存中做的判断次数是：100*1000=10 万次。 如果使用 Simple Nested-Loop Join 算法进行查询，扫描行数也是 10 万行。因此，从时间复杂度上来说，这两个算法是一样的。但是，Block Nested-Loop Join 算法的这 10 万次判断是内存操作，速度上会快很多，性能也更好。 join_buffer 的大小是由参数 join_buffer_size 设定的，默认值是 256k。如果放不下表 t1 的所有数据话，策略很简单，就是分段放。 对于 优化被驱动表的查询来说，最好是为被驱动表加上效率高的索引，如果实在不能使用索引，并且自己的机器的内存也比较大可以尝试调大join_buffer_size的值来对连 接查询进行优化。 另外需要注意的是，驱动表的记录并不是所有列都会被放到join buffer中，只有查询列表中的列和过滤条件中的列才会被放到join buffer中，所以再次提醒我们，最 好不要把*作为查询列表，只需要把我们关心的列放到查询列表就好了，这样还可以在join buffer中放置更多的记录呢哈。 join成本选择 多表连接的成本分析 首先要考虑一下多表连接时可能产生出多少种连接顺序: 对于两表连接，比如表A和表B连接 只有 AB、BA这两种连接顺序。其实相当于2 × 1 = 2种连接顺序。 对于三表连接，比如表A、表B、表C进行连接 有ABC、ACB、BAC、BCA、CAB、CBA这么6种连接顺序。其实相当于3 × 2 × 1 = 6种连接顺序。 对于四表连接的话，则会有4 × 3 × 2 × 1 = 24种连接顺序。 对于n表连接的话，则有 n × (n-1) × (n-2) × ··· × 1种连接顺序，就是n的阶乘种连接顺序，也就是n!。 有n个表进行连接，MySQL查询优化器要每一种连接顺序的成本都计算一遍么?那可是n!种连接顺序呀。其实真的是要都算一遍，不过设计MySQL的大叔们想了很多办法减少计算非常多种连 接顺序的成本的方法: 提前结束某种顺序的成本评估 MySQL在计算各种链接顺序的成本之前，会维护一个全局的变量，这个变量表示当前最小的连接查询成本。如果在分析某个连接顺序的成本时，该成本已经超过当前最小的连接查询成 本，那就压根儿不对该连接顺序继续往下分析了。比方说A、B、C三个表进行连接，已经得到连接顺序ABC是当前的最小连接成本，比方说10.0，在计算连接顺序BCA时，发现B和C的 连接成本就已经大于10.0时，就不再继续往后分析BCA这个连接顺序的成本了。 系统变量optimizer_search_depth 为了防止无穷无尽的分析各种连接顺序的成本，设计MySQL的大叔们提出了optimizer_search_depth系统变量，如果连接表的个数小于该值，那么就继续穷举分析每一种连接顺序的成 本，否则只对与optimizer_search_depth值相同数量的表进行穷举分析。很显然，该值越大，成本分析的越精确，越容易得到好的执行计划，但是消耗的时间也就越长，否则得到不 是很好的执行计划，但可以省掉很多分析连接成本的时间。 根据某些规则压根儿就不考虑某些连接顺序 即使是有上边两条规则的限制，但是分析多个表不同连接顺序成本花费的时间还是会很长，所以设计MySQL的大叔干脆提出了一些所谓的启发式规则(就是根据以往经验指定的一些规 则)，凡是不满足这些规则的连接顺序压根儿就不分析，这样可以极大的减少需要分析的连接顺序的数量，但是也可能造成错失最优的执行计划。他们提供了一个系统变 量optimizer_prune_level来控制到底是不是用这些启发式规则。 总结 第一个问题：能不能使用 join 语句？ 如果可以使用 Index Nested-Loop Join 算法，也就是说可以用上被驱动表上的索引，其实是没问题的； 如果使用 Block Nested-Loop Join 算法，扫描行数就会过多。尤其是在大表上的 join 操作，这样可能要扫描被驱动表很多次，会占用大量的系统资源。所以这种 join 尽量不要用。 判断要不要使用 join 语句时，就是看 explain 结果里面，Extra 字段里面有没有出现“Block Nested Loop”字样。 第二个问题是：如果要使用 join，应该选择大表做驱动表还是选择小表做驱动表？ 如果是 Index Nested-Loop Join 算法，应该选择小表做驱动表； 如果是 Block Nested-Loop Join 算法： 在 join_buffer_size 足够大的时候，是一样的； 在 join_buffer_size 不够大的时候（这种情","date":"2021-04-08","objectID":"/posts/mysql_join%E8%BF%87%E7%A8%8B/:0:0","tags":["mysql"],"title":"mysql join过程","uri":"/posts/mysql_join%E8%BF%87%E7%A8%8B/"},{"categories":["笔记"],"content":"mysql是怎么运行的笔记","date":"2021-02-25","objectID":"/posts/mysql_%E6%98%AF%E6%80%8E%E6%A0%B7%E8%BF%90%E8%A1%8C%E7%9A%84/","tags":["mysql"],"title":"mysql是怎么运行的笔记","uri":"/posts/mysql_%E6%98%AF%E6%80%8E%E6%A0%B7%E8%BF%90%E8%A1%8C%E7%9A%84/"},{"categories":["笔记"],"content":"服务器层次划分 为了管理方便，人们把连接管理、查询缓存、语法解析、查询优化这些并不涉及真实数据存储的功能划分为MySQL server的功能，把真实存取数据的功能划 分为存储引擎的功能。 ","date":"2021-02-25","objectID":"/posts/mysql_%E6%98%AF%E6%80%8E%E6%A0%B7%E8%BF%90%E8%A1%8C%E7%9A%84/:0:0","tags":["mysql"],"title":"mysql是怎么运行的笔记","uri":"/posts/mysql_%E6%98%AF%E6%80%8E%E6%A0%B7%E8%BF%90%E8%A1%8C%E7%9A%84/"},{"categories":["笔记"],"content":"缓存 如果两个查询请求在任何字符上的不同(例如:空格、注释、大小写)，都会导致缓存不会命中。另外，如果查询 请求中包含某些系统函数、用户自定义变量和函数、一些系统表，如 mysql 、information_schema、 performance_schema 数据库中的表，那这个请求就不 会被缓存。 既然是缓存，那就有它缓存失效的时候。MySQL的缓存系统会监测涉及到的每张表，只要该表的结构或者数据被修改，如对该表使用了INSERT、 UPDATE、DELETE、TRUNCATE TABLE、ALTER TABLE、DROP TABLE或 DROP DATABASE语句，那使用该表的所有高速缓存查询都将变为无效并从高速缓存中 删除! 从MySQL 5.7.20开始，不推荐使用查询缓存，并在MySQL 8.0中删除。 ","date":"2021-02-25","objectID":"/posts/mysql_%E6%98%AF%E6%80%8E%E6%A0%B7%E8%BF%90%E8%A1%8C%E7%9A%84/:1:0","tags":["mysql"],"title":"mysql是怎么运行的笔记","uri":"/posts/mysql_%E6%98%AF%E6%80%8E%E6%A0%B7%E8%BF%90%E8%A1%8C%E7%9A%84/"},{"categories":["笔记"],"content":"查询优化 优化的结果就是生成一个执行计划，这个执行计划表明了应该使用哪些索引进行查询，表之间的连接顺序是啥样的。我们可以使 用EXPLAIN语句来查看某个语句的执行计划。 ","date":"2021-02-25","objectID":"/posts/mysql_%E6%98%AF%E6%80%8E%E6%A0%B7%E8%BF%90%E8%A1%8C%E7%9A%84/:2:0","tags":["mysql"],"title":"mysql是怎么运行的笔记","uri":"/posts/mysql_%E6%98%AF%E6%80%8E%E6%A0%B7%E8%BF%90%E8%A1%8C%E7%9A%84/"},{"categories":["笔记"],"content":"存储引擎 存储引擎原来叫表处理器 MySQL服务器把数据的存储和提取操作都封装到了一个叫存储引擎的模块 里。我们知道表是由一行一行的记录组成的，但这只是一个逻辑上的概念，物理上如何表示记录，怎么从表中读取数据，怎么把数据写入具体的物理 存储器上，这都是存储引擎负责的事情。 配置 除了在命令行启动的时候指定参数如：mysqld –default-storage-engine = MyISAM外，MySQL程序在启动时会寻找多个路径下的配置文件，这些路径有的是固定的，有的是可以在命令行指定的。根据操作系统的不 同，配置文件的路径也有所不同。 ","date":"2021-02-25","objectID":"/posts/mysql_%E6%98%AF%E6%80%8E%E6%A0%B7%E8%BF%90%E8%A1%8C%E7%9A%84/:3:0","tags":["mysql"],"title":"mysql是怎么运行的笔记","uri":"/posts/mysql_%E6%98%AF%E6%80%8E%E6%A0%B7%E8%BF%90%E8%A1%8C%E7%9A%84/"},{"categories":["笔记"],"content":"配置路径 在类UNIX操作系统中，MySQL会按照下列路径来寻找配置文件: 路径名 备注 /etc/my.cnf /etc/mysql/my.cnf SYSCONFDIR/my.cnf $MYSQL_HOME/my.cnf 特定于服务器的选项(仅限服务器) defaults-extra-file 命令行指定的额外配置文件路径 ~/.my.cnf 用户特定选项 ~/.mylogin.cnf 用户特定的登录路径选项(仅限客户端) ","date":"2021-02-25","objectID":"/posts/mysql_%E6%98%AF%E6%80%8E%E6%A0%B7%E8%BF%90%E8%A1%8C%E7%9A%84/:4:0","tags":["mysql"],"title":"mysql是怎么运行的笔记","uri":"/posts/mysql_%E6%98%AF%E6%80%8E%E6%A0%B7%E8%BF%90%E8%A1%8C%E7%9A%84/"},{"categories":["笔记"],"content":"配置分组 配置文件里面会分组：server、mysqld、mysqld_safe、client、mysql、mysqladmin。 配置文件中不同的选项组是给不同的启动命令使用的，如果选项组名称与程序名称相同，则组中的选项将专门应用于该程序。 |启动命令|类别|能读取的组| |-|-| | mysqld|启动服务器|[mysqld]、[server]| | mysqld_safe |服务 |[mysqld]、[server]、[mysqld_safe]| | mysql.server | 启动服务器 | [mysqld]、[server]、[mysql.server] | | mysql |启动客户端 | [mysql]、[client] | | mysqladmin | 启动客户端 | [mysqladmin]、[client]| | mysqldump | 启动客户端 | [mysqldump]、[client] | 如果我们想指定mysql.server程序的启动参数，则必须将它们放在配置文件中，而不是放在命令行中。 mysql.server仅支持start和stop作为命令行参数。 ","date":"2021-02-25","objectID":"/posts/mysql_%E6%98%AF%E6%80%8E%E6%A0%B7%E8%BF%90%E8%A1%8C%E7%9A%84/:5:0","tags":["mysql"],"title":"mysql是怎么运行的笔记","uri":"/posts/mysql_%E6%98%AF%E6%80%8E%E6%A0%B7%E8%BF%90%E8%A1%8C%E7%9A%84/"},{"categories":["笔记"],"content":"特定MySQL版本的专用选项组 我们可以在选项组的名称后加上特定的MySQL版本号。只有版本号为5.7的mysqld程序才能使用这个选项组中的选项[mysqld-5.7] ","date":"2021-02-25","objectID":"/posts/mysql_%E6%98%AF%E6%80%8E%E6%A0%B7%E8%BF%90%E8%A1%8C%E7%9A%84/:6:0","tags":["mysql"],"title":"mysql是怎么运行的笔记","uri":"/posts/mysql_%E6%98%AF%E6%80%8E%E6%A0%B7%E8%BF%90%E8%A1%8C%E7%9A%84/"},{"categories":["笔记"],"content":"配置文件加载顺序 MySQL将按照我们在上表中给定的顺序依次读取各个配置文件，如果该文件不存在则忽略。值得注 意的是，如果我们在多个配置文件中设置了相同的启动选项，那以最后一个配置文件中的为准。 同一个配置文件中多个组的优先级 我们说同一个命令可以访问配置文件中的多个组，那么，将以最后一个出现的组中的启动选项为准 如果同一个启动选项既出现在命令行中，又出现在配置文件中，那么以命令行中的启动选项为准 ","date":"2021-02-25","objectID":"/posts/mysql_%E6%98%AF%E6%80%8E%E6%A0%B7%E8%BF%90%E8%A1%8C%E7%9A%84/:7:0","tags":["mysql"],"title":"mysql是怎么运行的笔记","uri":"/posts/mysql_%E6%98%AF%E6%80%8E%E6%A0%B7%E8%BF%90%E8%A1%8C%E7%9A%84/"},{"categories":["笔记"],"content":"mysql系统变量 MySQL服务器程序运行过程中会用到许多影响程序行为的变量，它们被称为MySQL系统变量，比如允许同时连入的客户端数量用 系统变量max_connections表示，表的默认存储引擎用系统变量default_storage_engine表示，查询缓存的大小用系统变 量query_cache_size表示，MySQL服务器程序的系统变量有好几百条，我们就不一一列举了。每个系统变量都有一个默认值， 我们可以使用命令行或者配置文件中的选项在启动服务器时改变一些系统变量的值。大多数的系统变量的值也可以在程序运行 过程中修改，而无需停止并重新启动它。 查看系统变量：SHOW VARIABLES [LIKE 匹配的模式]; 设置的时候可以通过命令行、配置文件、甚至运行期间。 ","date":"2021-02-25","objectID":"/posts/mysql_%E6%98%AF%E6%80%8E%E6%A0%B7%E8%BF%90%E8%A1%8C%E7%9A%84/:8:0","tags":["mysql"],"title":"mysql是怎么运行的笔记","uri":"/posts/mysql_%E6%98%AF%E6%80%8E%E6%A0%B7%E8%BF%90%E8%A1%8C%E7%9A%84/"},{"categories":["笔记"],"content":"系统变量的作用范围 设计MySQL的大叔提出了系统变量的作用范围的概念，具体来说作用范围分为这两种: GLOBAL:全局变量，影响服务器的整体操作。 SESSION:会话变量，影响某个客户端连接的操作。 SET [GLOBAL|SESSION] 系统变量名 = 值; 通过启动选项设置的系统变量的作用范围都是GLOBAL的，也就是对所有客户端都有效的 我们的SHOW VARIABLES语句查看的是什么作用范围的系统变量呢? 答:默认查看的是SESSION作用范围的系统变量。 当然我们也可以在查看系统变量的语句上加上要查看哪个作用范围的系统变量，就像这样: SHOW [GLOBAL|SESSION] VARIABLES [LIKE 匹配的模式]; 有一些系统变量只具有SESSION作用范围，比如insert_id，表示在对某个包含AUTO_INCREMENT列的表进行插入时， 该列初始的值。 启动选项和系统变量的区别? 启动选项是在程序启动时我们程序员传递的一些参数，而系统变量是影响服务器程序运行行为的变量，它们之间的关系如下: 大部分的系统变量都可以被当作启动选项传入。 有些系统变量是在程序运行过程中自动生成的，是不可以当作启动选项来设置，比 如auto_increment_offset、character_set_client啥的。 有些启动选项也不是系统变量，比如defaults-file。 ","date":"2021-02-25","objectID":"/posts/mysql_%E6%98%AF%E6%80%8E%E6%A0%B7%E8%BF%90%E8%A1%8C%E7%9A%84/:8:1","tags":["mysql"],"title":"mysql是怎么运行的笔记","uri":"/posts/mysql_%E6%98%AF%E6%80%8E%E6%A0%B7%E8%BF%90%E8%A1%8C%E7%9A%84/"},{"categories":["笔记"],"content":"mysql状态变量 为了让我们更好的了解服务器程序的运行情况，MySQL服务器程序中维护了好多关于程序运行状态的变量，它们被称为状态变 量。比方说Threads_connected表示当前有多少客户端与服务器建立了连接，Handler_update表示已经更新了多少行记录。 SHOW [GLOBAL|SESSION] STATUS [LIKE 匹配的模式]; 不写明作用范围，默认的作用范围是SESSION 字符集及编码 ","date":"2021-02-25","objectID":"/posts/mysql_%E6%98%AF%E6%80%8E%E6%A0%B7%E8%BF%90%E8%A1%8C%E7%9A%84/:9:0","tags":["mysql"],"title":"mysql是怎么运行的笔记","uri":"/posts/mysql_%E6%98%AF%E6%80%8E%E6%A0%B7%E8%BF%90%E8%A1%8C%E7%9A%84/"},{"categories":["笔记"],"content":"MySQL中的 utf8和 utf8mb4 我们上边说utf8字符集表示一个字符需要使用1~4个字节，但是我们常用的一些字符使用1~3个字节就可以表示了。而在MySQL中字符集表示一 个字符所用最大字节长度在某些方面会影响系统的存储和性能，所以设计MySQL的大叔偷偷的定义了两个概念: utf8mb3:阉割过的utf8字符集，只使用1~3个字节表示字符。 utf8mb4:正宗的utf8字符集，使用1~4个字节表示字符。 有一点需要大家十分的注意，在MySQL中utf8是utf8mb3的别名，所以之后在MySQL中提到utf8就意味着使用1~3个字节来表示一个字符，如果大 家有使用4字节编码一个字符的情况，比如存储一些emoji表情啥的，那请使用utf8mb4。 ","date":"2021-02-25","objectID":"/posts/mysql_%E6%98%AF%E6%80%8E%E6%A0%B7%E8%BF%90%E8%A1%8C%E7%9A%84/:10:0","tags":["mysql"],"title":"mysql是怎么运行的笔记","uri":"/posts/mysql_%E6%98%AF%E6%80%8E%E6%A0%B7%E8%BF%90%E8%A1%8C%E7%9A%84/"},{"categories":["笔记"],"content":"字符集相关配置 查看所支持的字符集。 SHOW (CHARACTER SET|CHARSET) [LIKE 匹配的模式]; 其中CHARACTER SET和CHARSET是同义词，用任意一个都可以。 查看MySQL中支持的比较规则的命令如下: SHOW COLLATION [LIKE 匹配的模式]; 我们前边说过一种字符集可能对应着若干种比较规则，MySQL支持的字符集就已经非常多了，所以支持的比较规则更多 各级别的字符集和比较规则 MySQL有4个级别的字符集和比较规则，分别是: 服务器级别 数据库级别 表级别 列级别 服务器级别 MySQL提供了两个系统变量来表示服务器级别的字符集和比较规则: character_set_server 服务器级别的字符集 collation_server 服务器级别的比较规则 SHOW VARIABLES LIKE ‘collation_server’; 数据库级别 我们在创建和修改数据库的时候可以指定该数据库的字符集和比较规则，具体语法如下: CREATE DATABASE 数据库名 [[DEFAULT] CHARACTER SET 字符集名称] [[DEFAULT] COLLATE 比较规则名称]; ALTER DATABASE 数据库名 [[DEFAULT] CHARACTER SET 字符集名称] [[DEFAULT] COLLATE 比较规则名称]; 表级别 我们也可以在创建和修改表的时候指定表的字符集和比较规则，语法如下: CREATE TABLE 表名 (列的信息) [[DEFAULT] CHARACTER SET 字符集名称] [COLLATE 比较规则名称]] ALTER TABLE 表名 [[DEFAULT] CHARACTER SET 字符集名称] [COLLATE 比较规则名称] 列级别 需要注意的是，对于存储字符串的列，同一个表中的不同的列也可以有不同的字符集和比较规则。我们在创建和修改列定义的时候可以指定该 列的字符集和比较规则，语法如下: CREATE TABLE 表名( 列名 字符串类型 [CHARACTER SET 字符集名称] [COLLATE 比较规则名称], 其他列…); ","date":"2021-02-25","objectID":"/posts/mysql_%E6%98%AF%E6%80%8E%E6%A0%B7%E8%BF%90%E8%A1%8C%E7%9A%84/:11:0","tags":["mysql"],"title":"mysql是怎么运行的笔记","uri":"/posts/mysql_%E6%98%AF%E6%80%8E%E6%A0%B7%E8%BF%90%E8%A1%8C%E7%9A%84/"},{"categories":["笔记"],"content":"客户端和服务器通信中的字符集 说到底，字符串在计算机上的体现就是一个字节串，如果你使用不同字符集去解码这个字节串，最后得到的结果可能让你挠头。 从发送请求到返回结果这个过程中伴随着多次字符集 的转换，在这个过程中会用到3个系统变量，我们先把它们写出来看一下: 系统变量 描述 c character_set_client 服务器解码请求时使用的字符集 character_set_connection 服务器处理请求时会把请求字符串从character_set_client转为character_set_connection character_set_results 服务器向客户端返回数据时使用的字符集 innodb记录结构 ","date":"2021-02-25","objectID":"/posts/mysql_%E6%98%AF%E6%80%8E%E6%A0%B7%E8%BF%90%E8%A1%8C%E7%9A%84/:12:0","tags":["mysql"],"title":"mysql是怎么运行的笔记","uri":"/posts/mysql_%E6%98%AF%E6%80%8E%E6%A0%B7%E8%BF%90%E8%A1%8C%E7%9A%84/"},{"categories":["笔记"],"content":"页 我们知道读写磁盘的速度非常慢，和内存读写差了几个数量级，所以当我们想从表中获取某些记录时，InnoDB存储引擎需要一条一条的把记录 从磁盘上读出来么? 不，那样会慢死，InnoDB采取的方式是:将数据划分为若干个页，以页作为磁盘和内存之间交互的基本单位，InnoDB中页的大小一般为 16 KB。也就是在一般情况下，一次最少从磁 盘中读取16KB的内容到内存中，一次最少把内存中的16KB内容刷新到磁盘中。 ","date":"2021-02-25","objectID":"/posts/mysql_%E6%98%AF%E6%80%8E%E6%A0%B7%E8%BF%90%E8%A1%8C%E7%9A%84/:13:0","tags":["mysql"],"title":"mysql是怎么运行的笔记","uri":"/posts/mysql_%E6%98%AF%E6%80%8E%E6%A0%B7%E8%BF%90%E8%A1%8C%E7%9A%84/"},{"categories":["笔记"],"content":"InnoDB行格式 我们平时是以记录为单位来向表中插入数据的，这些记录在磁盘上的存放方式也被称为行格式或者记录格式。设计InnoDB存储引擎的大叔们到现在为止设计了4种不同类型的行格式，分别 是Compact、Redundant、Dynamic和Compressed行格式。 我们可以在创建或修改表的语句中指定行格式: CREATE TABLE 表名 (列的信息) ROW_FORMAT=行格式名称 ALTER TABLE 表名 ROW_FORMAT=行格式名称 ","date":"2021-02-25","objectID":"/posts/mysql_%E6%98%AF%E6%80%8E%E6%A0%B7%E8%BF%90%E8%A1%8C%E7%9A%84/:14:0","tags":["mysql"],"title":"mysql是怎么运行的笔记","uri":"/posts/mysql_%E6%98%AF%E6%80%8E%E6%A0%B7%E8%BF%90%E8%A1%8C%E7%9A%84/"},{"categories":["笔记"],"content":"行溢出数据现象 MySQL对一条记录占用的最大存储空间是有限制的，除了BLOB或者TEXT类型的列之外，其他所有的列(不包括隐藏列和记录头信息)占用的字节长度加起来不能超过65535个字 节。所以MySQL服务器建议我们把存储类型改为TEXT或者BLOB的类型。 MySQL中磁盘和内存交互的基本单位是页，也就是说MySQL是以页为基本单位来管理存储空间的， 我们的记录都会被分配到某个页中存储。而一个页的大小一般是16KB，也就是16384字节，而一个VARCHAR(M)类型的列就最多可以存储65532个字节，这样就可能造成一个页存放不了一条记录的尴尬情况。 在Compact和Reduntant行格式中，对于占用存储空间非常大的列，在记录的真实数据处只会存储该列的一部分数据，把剩余的数据分散存储在几个其他的页中，然后记录的真实数据处用20个字节存储指向这些 页的地址。 ","date":"2021-02-25","objectID":"/posts/mysql_%E6%98%AF%E6%80%8E%E6%A0%B7%E8%BF%90%E8%A1%8C%E7%9A%84/:14:1","tags":["mysql"],"title":"mysql是怎么运行的笔记","uri":"/posts/mysql_%E6%98%AF%E6%80%8E%E6%A0%B7%E8%BF%90%E8%A1%8C%E7%9A%84/"},{"categories":["笔记"],"content":"行溢出的临界点 那发生行溢出的临界点是什么呢?也就是说在列存储多少字节的数据时就会发生行溢出? MySQL中规定一个页中至少存放两行记录,只要知道如果我们想一个行中存储了很大的数据时，可能发 生行溢出的现象。 ","date":"2021-02-25","objectID":"/posts/mysql_%E6%98%AF%E6%80%8E%E6%A0%B7%E8%BF%90%E8%A1%8C%E7%9A%84/:14:2","tags":["mysql"],"title":"mysql是怎么运行的笔记","uri":"/posts/mysql_%E6%98%AF%E6%80%8E%E6%A0%B7%E8%BF%90%E8%A1%8C%E7%9A%84/"},{"categories":["笔记"],"content":"mysql实战45讲笔记","date":"2021-01-20","objectID":"/posts/mysql_45%E8%AE%B2%E7%AC%94%E8%AE%B0/","tags":["mysql"],"title":"mysql实战45讲笔记","uri":"/posts/mysql_45%E8%AE%B2%E7%AC%94%E8%AE%B0/"},{"categories":["笔记"],"content":"查询语句执行流程 MySQL 的基本架构示意图： Server 层包括连接器、查询缓存、分析器、优化器、执行器等，涵盖 MySQL 的大多数核心服务功能，以及所有的内置函数（如日期、时间、数学和加密函数等），所有跨存储引擎的功能都在这一层实现，比如存储过程、触发器、视图等。 存储引擎层负责数据的存储和提取。其架构模式是插件式的，支持 InnoDB、MyISAM、Memory 等多个存储引擎。现在最常用的存储引擎是 InnoDB，它从 MySQL 5.5.5 版本开始成为了默认存储引擎。 ","date":"2021-01-20","objectID":"/posts/mysql_45%E8%AE%B2%E7%AC%94%E8%AE%B0/:0:0","tags":["mysql"],"title":"mysql实战45讲笔记","uri":"/posts/mysql_45%E8%AE%B2%E7%AC%94%E8%AE%B0/"},{"categories":["笔记"],"content":"连接器 连接器负责跟客户端建立连接、获取权限、维持和管理连接。 mysql -h ip -P port -u user -p 连接命令中的 mysql 是客户端工具。 连接器会到权限表里面查出你拥有的权限。之后，这个连接里面的权限判断逻辑，都将依赖于此时读到的权限。 这就意味着，一个用户成功建立连接后，即使你用管理员账号对这个用户的权限做了修改，也不会影响已经存在连接的权限。修改完成后，只有再新建的连接才会使用新的权限设置。 连接完成后，如果你没有后续的动作，这个连接就处于空闲状态，你可以在 show processlist 命令中看到它。文本中这个图是 show processlist 的结果，其中的 Command 列显示为“Sleep”的这一行，就表示现在系统里面有一个空闲连接。 客户端如果太长时间没动静，连接器就会自动将它断开。这个时间是由参数 wait_timeout 控制的，默认值是 8 小时。 ","date":"2021-01-20","objectID":"/posts/mysql_45%E8%AE%B2%E7%AC%94%E8%AE%B0/:1:0","tags":["mysql"],"title":"mysql实战45讲笔记","uri":"/posts/mysql_45%E8%AE%B2%E7%AC%94%E8%AE%B0/"},{"categories":["笔记"],"content":"查询缓存 MySQL 拿到一个查询请求后，会先到查询缓存看看，之前是不是执行过这条语句。之前执行过的语句及其结果可能会以 key-value 对的形式，被直接缓存在内存中。key 是查询的语句，value 是查询的结果。如果你的查询能够直接在这个缓存中找到 key，那么这个 value 就会被直接返回给客户端。 返回缓存内容到客户端时会校验是否对该表有权限。 每当表有更新时都会把该表的缓存给清空，所以缓存适合不经常变的表。我们可以指定查询时使用缓存： select SQL_CACHE * from T where ID=10； MySQL 8.0 版本直接将查询缓存的整块功能删掉了。 ","date":"2021-01-20","objectID":"/posts/mysql_45%E8%AE%B2%E7%AC%94%E8%AE%B0/:2:0","tags":["mysql"],"title":"mysql实战45讲笔记","uri":"/posts/mysql_45%E8%AE%B2%E7%AC%94%E8%AE%B0/"},{"categories":["笔记"],"content":"分析器 如果没有命中查询缓存，就要开始真正执行语句了。首先，MySQL 需要知道你要做什么，因此需要对 SQL 语句做解析。 分析器先会做“词法分析”，然后做“语法分析”。根据词法分析的结果，语法分析器会根据语法规则，判断你输入的这个 SQL 语句是否满足 MySQL 语法。 查询的字段并不在表中，也会在分析阶段中报错。 ","date":"2021-01-20","objectID":"/posts/mysql_45%E8%AE%B2%E7%AC%94%E8%AE%B0/:3:0","tags":["mysql"],"title":"mysql实战45讲笔记","uri":"/posts/mysql_45%E8%AE%B2%E7%AC%94%E8%AE%B0/"},{"categories":["笔记"],"content":"优化器 优化器是在表里面有多个索引的时候，决定使用哪个索引；或者在一个语句有多表关联（join）的时候，决定各个表的连接顺序。 ","date":"2021-01-20","objectID":"/posts/mysql_45%E8%AE%B2%E7%AC%94%E8%AE%B0/:4:0","tags":["mysql"],"title":"mysql实战45讲笔记","uri":"/posts/mysql_45%E8%AE%B2%E7%AC%94%E8%AE%B0/"},{"categories":["笔记"],"content":"执行器 MySQL 通过分析器知道了你要做什么，通过优化器知道了该怎么做，于是就进入了执行器阶段，开始执行语句。 开始执行的时候，要先判断一下你对这个表 T 有没有执行查询的权限。 如果有权限，就打开表继续执行。打开表的时候，执行器就会根据表的引擎定义，去使用这个引擎提供的接口。 更新语句执行流程 你执行语句前要先连接数据库，这是连接器的工作。 分析器会通过词法和语法解析知道这是一条更新语句。 优化器决定要使用 ID 这个索引。 执行器负责具体执行，找到这一行，然后更新。 更新流程还涉及两个重要的日志模块：redo log（重做日志）和 binlog（归档日志）。 这两种日志有以下三点不同。 redo log 是 InnoDB 引擎特有的；binlog 是 MySQL 的 Server 层实现的，所有引擎都可以使用。 redo log 是物理日志，记录的是“在某个数据页上做了什么修改”；binlog 是逻辑日志，记录的是这个语句的原始逻辑，比如“给 ID=2 这一行的 c 字段加 1 ”。 redo log 是循环写的，空间固定会用完；binlog 是可以追加写入的。“追加写”是指 binlog 文件写到一定大小后会切换到下一个，并不会覆盖以前的日志。 我们再来看执行器和 InnoDB 引擎在执行这个简单的 update 语句时的内部流程。 执行器先找引擎取 ID=2 这一行。ID 是主键，引擎直接用树搜索找到这一行。如果 ID=2 这一行所在的数据页本来就在内存中，就直接返回给执行器；否则，需要先从磁盘读入内存，然后再返回。 执行器拿到引擎给的行数据，把这个值加上 1，比如原来是 N，现在就是 N+1，得到新的一行数据，再调用引擎接口写入这行新数据。 引擎将这行新数据更新到内存中，同时将这个更新操作记录到 redo log 里面，此时 redo log 处于 prepare 状态。然后告知执行器执行完成了，随时可以提交事务。 执行器生成这个操作的 binlog，并把 binlog 写入磁盘。 执行器调用引擎的提交事务接口，引擎把刚刚写入的 redo log 改成提交（commit）状态，更新完成。 redolog也并不是每次都直接写到磁盘中，虽然是顺序写，也没有必要每次都立即刷到磁盘中，而是先写到Log Buffer中，再批量刷到磁盘中。什么时候刷取决于innodb_flush_log_at_trx_commit配置。 刷脏页 MySQL实际更新的不是磁盘，而是内存。当内存数据页 和 磁盘数据页内容不一致的时候，这个内存页 就为 “脏页”；内存数据页写入磁盘后，内存数据页 和 磁盘数据页内容一致，称之为 “干净页”。 那么，什么情况会引发数据库的 flush 过程呢？ 第一种场景是InnoDB 的 redo log 写满了，这时候系统会停止所有更新操作，把 checkpoint 往前推进，redo log 留出空间可以继续写。 第二种场景对应的就是系统内存不足。当需要新的内存页，而内存不够用的时候，就要淘汰一些数据页，空出内存给别的数据页使用。如果淘汰的是“脏页”，就要先将脏页写到磁盘。 第三种场景对应的就是 MySQL 认为系统“空闲”的时候。当然，MySQL忙起来可是会很快就能把redo log 记满的，所以要合理地安排时间，即使是忙的时候，也要见缝插针地找时间，只要有机会就刷一点“脏页”。 第四种场景对应的就是 MySQL 正常关闭的情况。这时候，MySQL 会把内存的脏页都 flush 到磁盘上，这样下次 MySQL 启动的时候，就可以直接从磁盘上读数据，启动速度会很快 事务隔离 数据库里面会创建一个视图，访问的时候以视图的逻辑结果为准。 在“可重复读”隔离级别下，这个视图是在事务开始的时候创建的（begin语句时并不会，第一条查询语句才会创建read view，一致性视图是在第执行第一个快照读语句时创建的，如果你想要马上启动一个事务，可以使用 start transaction with consistent snapshot 这个命令。），整个事务存在期间都用这个视图。 在“读提交”隔离级别下，这个视图是在每个 SQL 语句开始执行的时候创建的。 这里需要注意的是，“读未提交”隔离级别下直接返回记录上的最新值，没有视图概念，是用的读锁。 而“串行化”隔离级别下直接用加锁的方式来避免并行访问。 ","date":"2021-01-20","objectID":"/posts/mysql_45%E8%AE%B2%E7%AC%94%E8%AE%B0/:5:0","tags":["mysql"],"title":"mysql实战45讲笔记","uri":"/posts/mysql_45%E8%AE%B2%E7%AC%94%E8%AE%B0/"},{"categories":["笔记"],"content":"undolog 每条记录在更新的时候都会同时记录一条回滚操作。记录上的最新值，通过回滚操作，都可以得到前一个状态的值。 回滚日志总不能一直保留吧，什么时候删除呢？答案是，在不需要的时候才删除。也就是说，系统会判断，当没有事务再需要用到这些回滚日志时，回滚日志会被删除。 什么时候才不需要了呢？就是当系统里没有比这个回滚日志更早的 read-view 的时候。 基于上面的说明，我们来讨论一下为什么建议你尽量不要使用长事务。 长事务意味着系统里面会存在很老的事务视图。由于这些事务随时可能访问数据库里面的任何数据，所以这个事务提交之前，数据库里面它可能用到的回滚记录都必须保留，这就会导致大量占用存储空间。 索引 索引维护中可能会造成页的分裂，有分裂就有合并。当相邻两个页由于删除了数据，利用率很低之后，会将数据页做合并。 树高其实取决于叶子树（数据行数）和“N叉树”的N。 而N是由页大小和索引大小决定的。 N 叉树中非叶子节点存放的是索引信息，索引包含 Key 和 Point 指针。Point 指针固定为 6 个字节，假如 Key 为 10 个字节，那么单个索引就是 16 个字节。如果 B + 树中页大小为 16 K，那么一个页就可以存储 1024 个索引，此时 N 就等于 1024。 InnoDB会把主键字段放到索引定义字段后面， 当然同时也会去重。 所以，当联合主键是(a,b)的时候， 定义为c的索引，实际上是（c,a,b); 定义为(c,a)的索引，实际上是(c,a,b) 定义为(c,b）的索引，实际上是（c,b,a) 锁 ","date":"2021-01-20","objectID":"/posts/mysql_45%E8%AE%B2%E7%AC%94%E8%AE%B0/:6:0","tags":["mysql"],"title":"mysql实战45讲笔记","uri":"/posts/mysql_45%E8%AE%B2%E7%AC%94%E8%AE%B0/"},{"categories":["笔记"],"content":"全局锁 顾名思义，全局锁就是对整个数据库实例加锁。MySQL 提供了一个加全局读锁的方法，命令是 Flush tables with read lock (FTWRL)。当你需要让整个库处于只读状态的时候，可以使用这个命令，之后其他线程的以下语句会被阻塞：数据更新语句（数据的增删改）、数据定义语句（包括建表、修改表结构等）和更新类事务的提交语句。 全局锁的典型使用场景是，做全库逻辑备份。 ","date":"2021-01-20","objectID":"/posts/mysql_45%E8%AE%B2%E7%AC%94%E8%AE%B0/:7:0","tags":["mysql"],"title":"mysql实战45讲笔记","uri":"/posts/mysql_45%E8%AE%B2%E7%AC%94%E8%AE%B0/"},{"categories":["笔记"],"content":"表级锁 MySQL 里面表级别的锁有两种：一种是表锁，一种是元数据锁（meta data lock，MDL)。 表锁的语法是 lock tables … read/write。与 FTWRL 类似，可以用 unlock tables 主动释放锁，也可以在客户端断开的时候自动释放。需要注意，lock tables 语法除了会限制别的线程的读写外，也限定了本线程接下来的操作对象。 举个例子, 如果在某个线程 A 中执行 lock tables t1 read, t2 write; 这个语句，则其他线程写 t1、读写 t2 的语句都会被阻塞。同时，线程 A 在执行 unlock tables 之前，也只能执行读 t1、读写 t2 的操作。连写 t1 都不允许，自然也不能访问其他表。 另一类表级的锁是 MDL（metadata lock)。MDL 不需要显式使用，在访问一个表的时候会被自动加上。MDL 的作用是，保证读写的正确性。你可以想象一下，如果一个查询正在遍历一个表中的数据，而执行期间另一个线程对这个表结构做变更，删了一列，那么查询线程拿到的结果跟表结构对不上，肯定是不行的。 因此，在 MySQL 5.5 版本中引入了 MDL，当对一个表做增删改查操作的时候，加 MDL 读锁；当要对表做结构变更操作的时候，加 MDL 写锁。 来个案例： 我们可以看到 session A 先启动，这时候会对表 t 加一个 MDL 读锁。由于 session B 需要的也是 MDL 读锁，因此可以正常执行。 之后 session C 会被 blocked，是因为 session A 的 MDL 读锁还没有释放，而 session C 需要 MDL 写锁，因此只能被阻塞。 如果只有 session C 自己被阻塞还没什么关系，但是之后所有要在表 t 上新申请 MDL 读锁的请求也会被 session C 阻塞。前面我们说了，所有对表的增删改查操作都需要先申请 MDL 读锁，就都被锁住，等于这个表现在完全不可读写了。 如何安全地给小表加字段？ 首先我们要解决长事务，事务不提交，就会一直占着 MDL 锁。在 MySQL 的 information_schema 库的 innodb_trx 表中，你可以查到当前执行中的事务。如果你要做 DDL 变更的表刚好有长事务在执行，要考虑先暂停 DDL，或者 kill 掉这个长事务。 ","date":"2021-01-20","objectID":"/posts/mysql_45%E8%AE%B2%E7%AC%94%E8%AE%B0/:8:0","tags":["mysql"],"title":"mysql实战45讲笔记","uri":"/posts/mysql_45%E8%AE%B2%E7%AC%94%E8%AE%B0/"},{"categories":["笔记"],"content":"行锁 MySQL 的行锁是在引擎层由各个引擎自己实现的。但并不是所有的引擎都支持行锁，比如 MyISAM 引擎就不支持行锁。不支持行锁意味着并发控制只能使用表锁，对于这种引擎的表，同一张表上任何时刻只能有一个更新在执行，这就会影响到业务并发度。 在 InnoDB 事务中，行锁是在需要的时候才加上的，但并不是不需要了就立刻释放，而是要等到事务结束时才释放。这个就是两阶段锁协议。 mvcc视图 InnoDB 在实现 MVCC 时用到的一致性读视图，即 consistent read view，用于支持 RC（Read Committed，读提交）和 RR（Repeatable Read，可重复读）隔离级别的实现。 它没有物理结构，作用是事务执行期间用来定义“我能看到什么数据”。 在可重复读隔离级别下，事务在启动的时候就“拍了个快照”。注意，这个快照是基于整库的。 如果一个库有 100G，那么我启动一个事务，MySQL 就要拷贝 100G 的数据出来，这个过程得多慢啊。可是，我平时的事务执行起来很快啊。 实际上，我们并不需要拷贝出这 100G 的数据。 InnoDB 里面每个事务有一个唯一的事务 ID，叫作 transaction id。它是在事务开始的时候向 InnoDB 的事务系统申请的，是按申请顺序严格递增的。 而每行数据也都是有多个版本的，每个数据版本有自己的 row trx_id。每次事务更新数据的时候，都会生成一个新的数据版本，并且把 transaction id 赋值给这个数据版本的事务 ID，记为 row trx_id。同时，旧的数据版本要保留，并且在新的数据版本中，能够有信息可以直接拿到它。 也就是说，数据表中的一行记录，其实可能有多个版本 (row)，每个版本有自己的 row trx_id。 如图 所示，就是一个记录被多个事务连续更新后的状态。 图中的三个虚线箭头，就是 undo log；而 V1、V2、V3 并不是物理上真实存在的，而是每次需要的时候根据当前版本和 undo log 计算出来的 当前读 更新数据都是先读后写的，而这个读，只能读当前的值，称为“当前读”（current read）。 其实，除了 update 语句外，select 语句如果加锁，也是当前读。 换句话说当前读就是读的数据最新的内容，不管什么快照可见不可见。 而读已提交和可重复读级别下普通查询语句是一致性读，一致性读会根据 row trx_id 和一致性视图确定数据版本的可见性。 更新过程和change buffer 当需要更新一个数据页时，如果数据页在内存中就直接更新，然后写redolog，内存中数据也磁盘中数据不一致，不要紧，刷脏页即可。 而如果这个数据页还没有在内存中的话，在不影响数据一致性的前提下，InooDB 会将这些更新操作缓存在 change buffer 中，这样就不需要从磁盘中读入这个数据页了。在下次查询需要访问这个数据页的时候，将数据页读入内存，然后执行 change buffer 中与这个页有关的操作。通过这种方式就能保证这个数据逻辑的正确性。 需要说明的是，虽然名字叫作 change buffer，实际上它是可以持久化的数据。也就是说，change buffer 在内存中有拷贝，也会被写入到磁盘上。 将 change buffer 中的操作应用到原数据页，得到最新结果的过程称为** merge**。除了访问这个数据页会触发 merge 外，系统有后台线程会定期 merge。在数据库正常关闭（shutdown）的过程中，也会执行 merge 操作。 显然，如果能够将更新操作先记录在 change buffer，减少读磁盘，语句的执行速度会得到明显的提升。而且，数据读入内存是需要占用 buffer pool 的，所以这种方式还能够避免占用内存，提高内存利用率。 在MySQL5.5之前，叫插入缓冲(insert buffer)，只针对insert做了优化；现在对delete和update也有效，叫做写缓冲(change buffer)。 它是一种应用在非唯一普通索引页(non-unique secondary index page)不在缓冲池中，对页进行了写操作，并不会立刻将磁盘页加载到缓冲池，而仅仅记录缓冲变更(buffer changes)，等未来数据被读取时，再将数据合并(merge)恢复到缓冲池中的技术。写缓冲的目的是降低写操作的磁盘IO，提升数据库性能。 change buffer适用场景？ 因为 merge 的时候是真正进行数据更新的时刻，而 change buffer 的主要目的就是将记录的变更动作缓存下来，所以在一个数据页做 merge 之前，change buffer 记录的变更越多（也就是这个页面上要更新的次数越多），收益就越大。 因此，对于写多读少的业务来说，页面在写完以后马上被访问到的概率比较小，此时 change buffer 的使用效果最好。这种业务模型常见的就是账单类、日志类的系统。 反过来，假设一个业务的更新模式是写入之后马上会做查询，那么即使满足了条件，将更新先记录在 change buffer，但之后由于马上要访问这个数据页，会立即触发 merge 过程。这样随机访问 IO 的次数不会减少，反而增加了 change buffer 的维护代价。所以，对于这种业务模式来说，change buffer 反而起到了副作用。 总结更新过程 案例： 我们假设当前 k 索引树的状态，查找到位置后，k1 所在的数据页在内存 (InnoDB buffer pool) 中，k2 所在的数据页不在内存中。 这条更新语句做了如下的操作： Page 1 在内存中，直接更新内存； Page 2 没有在内存中，就在内存的 change buffer 区域，记录下“我要往 Page 2 插入一行”这个信息 将上述两个动作记入 redo log 中 普通和唯一索引选择 对于唯一索引来说，所有的更新操作都要先判断这个操作是否违反唯一性约束。比如，要插入 (4,400) 这个记录，就要先判断现在表中是否已经存在 k=4 的记录，而这必须要将数据页读入内存才能判断。如果都已经读入到内存了，那直接更新内存会更快，就没必要使用 change buffer 了。 因此，唯一索引的更新就不能使用 change buffer，实际上也只有普通索引可以使用。 ","date":"2021-01-20","objectID":"/posts/mysql_45%E8%AE%B2%E7%AC%94%E8%AE%B0/:9:0","tags":["mysql"],"title":"mysql实战45讲笔记","uri":"/posts/mysql_45%E8%AE%B2%E7%AC%94%E8%AE%B0/"},{"categories":["笔记"],"content":"插入记录的区别 如果要在这张表中插入一个新记录 (4,400) 的话，InnoDB 的处理流程是怎样的。 第一种情况是，这个记录要更新的目标页在内存中。这时，InnoDB 的处理流程如下： 对于唯一索引来说，找到 3 和 5 之间的位置，判断到没有冲突，插入这个值，语句执行结束； 对于普通索引来说，找到 3 和 5 之间的位置，插入这个值，语句执行结束。 这样看来，普通索引和唯一索引对更新语句性能影响的差别，只是一个判断，只会耗费微小的 CPU 时间。 但，这不是我们关注的重点。 第二种情况是，这个记录要更新的目标页不在内存中。这时，InnoDB 的处理流程如下： 对于唯一索引来说，需要将数据页读入内存，判断到没有冲突，插入这个值，语句执行结束； 对于普通索引来说，则是将更新记录在 change buffer，语句执行就结束了。 将数据从磁盘读入内存涉及随机 IO 的访问，是数据库里面成本最高的操作之一。change buffer 因为减少了随机磁盘访问，所以对更新性能的提升是会很明显的。 ","date":"2021-01-20","objectID":"/posts/mysql_45%E8%AE%B2%E7%AC%94%E8%AE%B0/:10:0","tags":["mysql"],"title":"mysql实战45讲笔记","uri":"/posts/mysql_45%E8%AE%B2%E7%AC%94%E8%AE%B0/"},{"categories":["笔记"],"content":"普通索引和唯一索引选择总结 其实，这两类索引在查询能力上是没差别的，InnoDB 的数据是按数据页为单位来读写的。 也就是说，当需要读一条记录的时候，并不是将这个记录本身从磁盘读出来，而是以页为单位，将其整体读入内存。在 InnoDB 中，每个数据页的大小默认是 16KB。当找到 k=5 的记录的时候，它所在的数据页就都在内存里了。那么，对于普通索引来说，要多做的那一次“查找和判断下一条记录”的操作，就只需要一次指针寻找和一次计算。 当找到 k=5 的记录的时候，它所在的数据页就都在内存里了。那么，对于普通索引来说，要多做的那一次“查找和判断下一条记录”的操作，就只需要一次指针寻找和一次计算。 主要考虑的是对更新性能的影响。所以，我建议你尽量选择普通索引。 如果所有的更新后面，都马上伴随着对这个记录的查询，那么你应该关闭 change buffer。而在其他情况下，change buffer 都能提升更新性能。 索引的选择 选择索引是优化器的工作。 而优化器选择索引的目的，是找到一个最优的执行方案，并用最小的代价去执行语句。在数据库里面，扫描行数是影响执行代价的因素之一。 不过扫描行数并不是唯一的判断标准，优化器还会结合是否使用临时表、是否排序等因素进行综合判断。 ","date":"2021-01-20","objectID":"/posts/mysql_45%E8%AE%B2%E7%AC%94%E8%AE%B0/:11:0","tags":["mysql"],"title":"mysql实战45讲笔记","uri":"/posts/mysql_45%E8%AE%B2%E7%AC%94%E8%AE%B0/"},{"categories":["笔记"],"content":"扫描行数的预判 MySQL 在真正开始执行语句之前，并不能精确地知道满足这个条件的记录有多少条，而只能根据统计信息来估算记录数。 这个统计信息就是索引的“区分度”。显然，一个索引上不同的值越多，这个索引的区分度就越好。而一个索引上不同的值的个数，我们称之为“基数”（cardinality）。也就是说，这个基数越大，索引的区分度越好。 可以使用 show index 方法，看到一个索引的基数。 MySQL 是怎样得到索引的基数的呢？ 采样统计！ 采样统计的时候，InnoDB 默认会选择 N 个数据页，统计这些页面上的不同值，得到一个平均值，然后乘以这个索引的页面数，就得到了这个索引的基数。 analyze table t 命令，可以用来重新统计索引信息。 索引统计只是一个输入，对于一个具体的语句来说，优化器还要判断，执行这个语句本身要扫描多少行。 explain语句可以看到预计扫描行数。 字符串索引 字符串列当索引时，可以指定索引的长度，叫做前缀索引 alter table user add index idx_email(email(6)) 合适的前缀索引可以节省空间。 查找前缀索引后，必须回表再次判断是否完全相等，所以覆盖索引的好处就没有了。这也是你在选择是否使用前缀索引时需要考虑的一个因素。 刷脏页 InnoDB 在处理更新语句的时候，只做了写日志这一个磁盘操作。这个日志叫作 redo log（重做日志），也就是《孔乙己》里咸亨酒店掌柜用来记账的粉板，在更新内存写完 redo log 后，就返回给客户端，本次更新成功。 做下类比的话，掌柜记账的账本是数据文件，记账用的粉板是日志文件（redo log），掌柜的记忆就是内存。 掌柜总要找时间把账本更新一下，这对应的就是把内存里的数据写入磁盘的过程，术语就是 flush。在这个 flush 操作执行之前，孔乙己的赊账总额，其实跟掌柜手中账本里面的记录是不一致的。因为孔乙己今天的赊账金额还只在粉板上，而账本里的记录是老的，还没把今天的赊账算进去。 当内存数据页跟磁盘数据页内容不一致的时候，我们称这个内存页为“脏页”。内存数据写入到磁盘后，内存和磁盘上的数据页的内容就一致了，称为“干净页”。 不论是脏页还是干净页，都在内存中。在这个例子里，内存对应的就是掌柜的记忆。 什么情况会引发数据库的 flush 过程? 第一种场景是，粉板满了，记不下了。这时候如果再有人来赊账，掌柜就只得放下手里的活儿，将粉板上的记录擦掉一些，留出空位以便继续记账。当然在擦掉之前，他必须先将正确的账目记录到账本中才行。 这个场景，对应的就是 InnoDB 的 redo log 写满了。 第二种场景是，这一天生意太好，要记住的事情太多，掌柜发现自己快记不住了，赶紧找出账本把孔乙己这笔账先加进去。 这种场景，对应的就是系统内存不足。 第三种场景是，生意不忙的时候，或者打烊之后。这时候柜台没事，掌柜闲着也是闲着，不如更新账本。 这种场景，对应的就是 MySQL 认为系统“空闲”的时候。 第四种场景是，年底了咸亨酒店要关门几天，需要把账结清一下。这时候掌柜要把所有账都记到账本上，这样过完年重新开张的时候，就能就着账本明确账目情况了。 这种场景，对应的就是 MySQL 正常关闭的情况。 表空间回收 ","date":"2021-01-20","objectID":"/posts/mysql_45%E8%AE%B2%E7%AC%94%E8%AE%B0/:12:0","tags":["mysql"],"title":"mysql实战45讲笔记","uri":"/posts/mysql_45%E8%AE%B2%E7%AC%94%E8%AE%B0/"},{"categories":["笔记"],"content":"innodb_file_per_table配置 表数据既可以存在共享表空间里，也可以是单独的文件。这个行为是由参数 innodb_file_per_table 控制的： 这个参数设置为 OFF 表示的是，表的数据放在系统共享表空间，也就是跟数据字典放在一起； 这个参数设置为 ON 表示的是，每个 InnoDB 表数据存储在一个以 .ibd 为后缀的文件中。 从 MySQL 5.6.6 版本开始，它的默认值就是 ON 了。 我建议你不论使用 MySQL 的哪个版本，都将这个值设置为 ON。因为，一个表单独存储为一个文件更容易管理，而且在你不需要这个表的时候，通过 drop table 命令，系统就会直接删除这个文件。而如果是放在共享表空间中，即使表删掉了，空间也是不会回收的。 ","date":"2021-01-20","objectID":"/posts/mysql_45%E8%AE%B2%E7%AC%94%E8%AE%B0/:13:0","tags":["mysql"],"title":"mysql实战45讲笔记","uri":"/posts/mysql_45%E8%AE%B2%E7%AC%94%E8%AE%B0/"},{"categories":["笔记"],"content":"数据删除流程 delete 命令其实只是把记录的位置，或者数据页标记为了“可复用”，但磁盘文件的大小是不会变的。也就是说，通过 delete 命令是不能回收表空间的。这些可以复用，而没有被使用的空间，看起来就像是“空洞”。 当某条记录标记为已删除的时候可以被符合条件的数据再次插入复用。比如ID为10的记录标记为删除，再插入ID为10的数据时就能复用，如果插入ID为12的数据，是不能被复用的。 大量增删改的操作，都是可能是存在空洞的。所以，如果能够把这些空洞去掉，就能达到收缩表空间的目的。当整个页从 B+ 树里面摘掉以后，可以复用到任何位置。 ","date":"2021-01-20","objectID":"/posts/mysql_45%E8%AE%B2%E7%AC%94%E8%AE%B0/:14:0","tags":["mysql"],"title":"mysql实战45讲笔记","uri":"/posts/mysql_45%E8%AE%B2%E7%AC%94%E8%AE%B0/"},{"categories":["笔记"],"content":"重建表 你可以使用 **alter table A engine=InnoDB **命令来重建表。 在整个 DDL 过程中，表 A 中不能有更新。也就是说，这个 DDL 不是 Online 的。而在MySQL 5.6 版本开始引入的 Online DDL，对这个操作流程做了优化。 alter 语句在启动的时候需要获取 MDL 写锁，但是这个写锁在真正拷贝数据之前就退化成读锁了。 为什么要退化呢？为了实现 Online，MDL 读锁不会阻塞增删改操作。 那为什么不干脆直接解锁呢？为了保护自己，禁止其他线程对这个表同时做 DDL。 而对于一个大表来说，Online DDL 最耗时的过程就是拷贝数据到临时表的过程，这个步骤的执行期间可以接受增删改操作。所以，相对于整个 DDL 过程来说，锁的时间非常短。对业务来说，就可以认为是 Online 的。 count(*) 在不同的 MySQL 引擎中，count(*) 有不同的实现方式。 MyISAM 引擎把一个表的总行数存在了磁盘上，因此执行 count(*) 的时候会直接返回这个数，效率很高； 而 InnoDB 引擎就麻烦了，它执行 count(*) 的时候，需要把数据一行一行地从引擎里面读出来，然后累积计数。 这里需要注意的是，我们在这篇文章里讨论的是没有过滤条件的 count(*)，如果加了 where 条件的话，MyISAM 表也是不能返回得这么快的。 那为什么 InnoDB 不跟 MyISAM 一样，也把数字存起来呢？ 这是因为即使是在同一个时刻的多个查询，由于多版本并发控制（MVCC）的原因，InnoDB 表“应该返回多少行”也是不确定的。 show table status 命令里面有TABLE_ROWS，是通过采样估算的，不准确。 ","date":"2021-01-20","objectID":"/posts/mysql_45%E8%AE%B2%E7%AC%94%E8%AE%B0/:15:0","tags":["mysql"],"title":"mysql实战45讲笔记","uri":"/posts/mysql_45%E8%AE%B2%E7%AC%94%E8%AE%B0/"},{"categories":["笔记"],"content":"count函数 count() 是一个聚合函数，对于返回的结果集，一行行地判断，如果 count 函数的参数不是 NULL，累计值就加 1，否则不加。最后返回累计值。 count(*)、count(主键 id) 和 count(1) 都表示返回满足条件的结果集的总行数；而 count(字段），则表示返回满足条件的数据行里面，参数“字段”不为 NULL 的总个数。 对于 count(主键 id) 来说，InnoDB 引擎会遍历整张表，把每一行的 id 值都取出来，返回给 server 层。server 层拿到 id 后，判断是不可能为空的，就按行累加。 对于** count(1)** 来说，InnoDB 引擎遍历整张表，但不取值。server 层对于返回的每一行，放一个数字“1”进去，判断是不可能为空的，按行累加。 单看这两个用法的差别的话，你能对比出来，count(1) 执行得要比 count(主键 id) 快。因为从引擎返回 id 会涉及到解析数据行，以及拷贝字段值的操作。 对于 **count(字段) **来说： 如果这个“字段”是定义为 not null 的话，一行行地从记录里面读出这个字段，判断不能为 null，按行累加； 如果这个“字段”定义允许为 null，那么执行的时候，判断到有可能是 null，还要把值取出来再判断一下，不是 null 才累加。 对于count(*) 是例外，并不会把全部字段取出来，而是专门做了优化，不取值。count(*) 肯定不是 null，按行累加。 按照效率排序的话，count(字段)\u003ccount(主键 id)\u003ccount(1)≈count(*)，所以我建议你，尽量使用 count(*)。 更新行数问题 当 MySQL 去更新一行，但是要修改的值跟原来的值是相同的，这时候 MySQL 会真的去执行一次修改吗？还是看到值相同就直接返回呢？ 假设，当前表 t 里的值是 (1,2)。执行update t set a=2 where id =1;会出现什么情况。 案例1： session B 的 update 语句被 blocked 了，加锁这个动作是 InnoDB 才能做的。 InnoDB 认真执行了“把这个值修改成 (1,2)“这个操作，该加锁的加锁，该更新的更新。 案例2： session A 的第二个 select 语句是一致性读（快照读)，它是不能看见 session B 的更新的。 案例3： 现在它返回的是 (1,3)，表示它看见了某个新的版本，这个版本只能是 session A 自己的 update 语句做更新的时候生成。 在这个语句里面，MySQL 认为读出来的值，只有一个确定的 (id=1), 而要写的是 (a=3)，只从这两个信息是看不出来“不需要修改”的。而update t set a =3 where id =1 and a=3；就不同了，更新的目的和条件是一样的，明显浪费 索引失效 ","date":"2021-01-20","objectID":"/posts/mysql_45%E8%AE%B2%E7%AC%94%E8%AE%B0/:16:0","tags":["mysql"],"title":"mysql实战45讲笔记","uri":"/posts/mysql_45%E8%AE%B2%E7%AC%94%E8%AE%B0/"},{"categories":["笔记"],"content":"条件字段函数操作 select count(*) from tradelog where month(t_modified)=7; //不能使用索引 为什么条件是 where t_modified=‘2018-7-1’的时候可以用上索引，而改成 where month(t_modified)=7 的时候就不行了？ B+ 树提供的这个快速定位能力，来源于同一层兄弟节点的有序性。 但是，如果计算 month() 函数的话，你会看到传入 7 的时候，在树的第一层就不知道该怎么办了。只能一个一个来遍历t_modified索引。 ","date":"2021-01-20","objectID":"/posts/mysql_45%E8%AE%B2%E7%AC%94%E8%AE%B0/:17:0","tags":["mysql"],"title":"mysql实战45讲笔记","uri":"/posts/mysql_45%E8%AE%B2%E7%AC%94%E8%AE%B0/"},{"categories":["笔记"],"content":"隐式类型转换 字符串和数字进行对比的时候，要进行转换，规则是字符串转为数字。 示例： user表age int; phone varchar均分别有索引。 select * from user where age='5'; //会使用到索引 select * from user where phone = 123456;//索引失效 ","date":"2021-01-20","objectID":"/posts/mysql_45%E8%AE%B2%E7%AC%94%E8%AE%B0/:18:0","tags":["mysql"],"title":"mysql实战45讲笔记","uri":"/posts/mysql_45%E8%AE%B2%E7%AC%94%E8%AE%B0/"},{"categories":["笔记"],"content":"隐式字符编码转换 字符集不同可能用不上索引。 字符集 utf8mb4 是 utf8 的超集，所以当这两个类型的字符串在做比较的时候，MySQL 内部的操作是，先把 utf8 字符串转成 utf8mb4 字符集，再做比较。 字符集不同只是条件之一，连接过程中要求在被驱动表的索引字段上加函数操作，是直接导致对被驱动表做全表扫描的原因。 解决幻读 产生幻读的原因是，行锁只能锁住行，但是新插入记录这个动作，要更新的是记录之间的“间隙”。因此，为了解决幻读问题，InnoDB 只好引入新的锁，也就是间隙锁 (Gap Lock)。 ","date":"2021-01-20","objectID":"/posts/mysql_45%E8%AE%B2%E7%AC%94%E8%AE%B0/:19:0","tags":["mysql"],"title":"mysql实战45讲笔记","uri":"/posts/mysql_45%E8%AE%B2%E7%AC%94%E8%AE%B0/"},{"categories":["笔记"],"content":"间隙锁 间隙锁，锁的就是两个值之间的空隙。比如文章开头的表 t，初始化插入了 6 个记录，这就产生了 7 个间隙。 CREATE TABLE `t` ( `id` int(11) NOT NULL, `c` int(11) DEFAULT NULL, `d` int(11) DEFAULT NULL, PRIMARY KEY (`id`), KEY `c` (`c`) ) ENGINE=InnoDB; 这样，当你执行 select * from t where d=5 for update 的时候，就不止是给数据库中已有的 6 个记录加上了行锁，还同时加了 7 个间隙锁。这样就确保了无法再插入新的记录。 跟行锁有冲突关系的是“另外一个行锁”。但是间隙锁不一样，跟间隙锁存在冲突关系的，是“往这个间隙中插入一个记录”这个操作。间隙锁之间都不存在冲突关系。 select * from t where c=7 lock in share mode; select * from t where c=7 for update; 因为表 t 里并没有 c=7 这个记录，因此 session A 加的是间隙锁 (5,10)。而 session B 也是在这个间隙加的间隙锁。它们有共同的目标，即：保护这个间隙，不允许插入值。但，它们之间是不冲突的。 ","date":"2021-01-20","objectID":"/posts/mysql_45%E8%AE%B2%E7%AC%94%E8%AE%B0/:20:0","tags":["mysql"],"title":"mysql实战45讲笔记","uri":"/posts/mysql_45%E8%AE%B2%E7%AC%94%E8%AE%B0/"},{"categories":["笔记"],"content":"next-key锁 间隙锁和行锁合称 next-key lock，每个 next-key lock 是前开后闭区间。加锁过程是要分成间隙锁和行锁两段来执行的。 举个例子，“加 next-key lock(5,10] ”操作，实际上分成了两步，先是加 (5,10) 的间隙锁，加锁成功；然后加 c=10 的行锁。 也就是说，我们的表 t 初始化以后，如果用 select * from t for update 要把整个表所有记录锁起来，就形成了 7 个 next-key lock，分别是 (-∞,0]、(0,5]、(5,10]、(10,15]、(15,20]、(20, 25]、(25, +supremum]。 ","date":"2021-01-20","objectID":"/posts/mysql_45%E8%AE%B2%E7%AC%94%E8%AE%B0/:21:0","tags":["mysql"],"title":"mysql实战45讲笔记","uri":"/posts/mysql_45%E8%AE%B2%E7%AC%94%E8%AE%B0/"},{"categories":["笔记"],"content":"next-key加锁规则 两个“原则”、两个“优化”和一个“bug”。 原则 1：加锁的基本单位是 next-key lock。希望你还记得，next-key lock 是前开后闭区间。 原则 2：查找过程中访问到的对象才会加锁。 优化 1：索引上的等值查询，给唯一索引加锁的时候，next-key lock 退化为行锁。 优化 2：索引上的等值查询，向右遍历时且最后一个值不满足等值条件的时候，next-key lock 退化为间隙锁。 一个 bug：唯一索引上的范围查询会访问到不满足条件的第一个值为止。 通过一些案例理解，先看表 CREATE TABLE `t` ( `id` int(11) NOT NULL, `c` int(11) DEFAULT NULL, `d` int(11) DEFAULT NULL, PRIMARY KEY (`id`), KEY `c` (`c`) ) ENGINE=InnoDB; insert into t values(0,0,0),(5,5,5), (10,10,10),(15,15,15),(20,20,20),(25,25,25); 案例1：等值查询间隙锁 update t set d = d+1 where id =7; 表 t 中没有 id=7 的记录，所以用我们上面提到的加锁规则判断一下的话： 根据原则 1，加锁单位是 next-key lock，session A 加锁范围就是 (5,10]； 同时根据优化 2，这是一个等值查询 (id=7)，而 id=10 不满足查询条件，next-key lock 退化成间隙锁，因此最终加锁的范围是 (5,10)。 所以，此时锁是加在主键上的(5,10)，session B 要往这个间隙里面插入 id=8 的记录会被锁住。 案例2：非唯一索引等值锁 这是关于覆盖索引上的锁： sessionA sessionB sessionC begin;select id from t where c =5 lock in share mode; update t set d =d d+1 where id =5;(query ok) insert into t values(7,7,7);(block) session A 要给索引 c 上 c=5 的这一行加上读锁。 根据原则 1，加锁单位是 next-key lock，因此会给 (0,5] 加上 next-key lock。 要注意 c 是普通索引，因此仅访问 c=5 这一条记录是不能马上停下来的，需要向右遍历，查到 c=10 才放弃。根据原则 2，访问到的都要加锁，因此要给 (5,10] 加 next-key lock。 但是同时这个符合优化 2：等值判断，向右遍历，最后一个值不满足 c=5 这个等值条件，因此退化成间隙锁 (5,10)。 根据原则 2 ，只有访问到的对象才会加锁，这个查询使用覆盖索引，并不需要访问主键索引，所以主键索引上没有加任何锁，这就是为什么 session B 的 update 语句可以执行完成。 但 session C 要插入一个 (7,7,7) 的记录，就会被 session A 的间隙锁 (5,10) 锁住。 在这个例子中，lock in share mode 只锁覆盖索引，但是如果是 for update 就不一样了。 执行 for update 时，系统会认为你接下来要更新数据，因此会顺便给主键索引上满足条件的行加上行锁。 案例3：主键索引范围锁 select * from t where id\u003e=10 and id\u003c11 for update; 开始执行的时候，要找到第一个 id=10 的行，因此本该是 next-key lock(5,10]。 根据优化 1， 主键 id 上的等值条件，退化成行锁，只加了 id=10 这一行的行锁。 范围查找就往后继续找，找到 id=15 这一行停下来，因此需要加 next-key lock(10,15]。 所以，session A 这时候锁的范围就是主键索引上，行锁 id=10 和 next-key lock(10,15]。这样，session B 和 session C 的结果你就能理解了。 这里你需要注意一点，首次 session A 定位查找 id=10 的行的时候，是当做等值查询来判断的，而向右扫描到 id=15 的时候，用的是范围查询判断。 案例4：非唯一索引范围锁 select * from t where c \u003e= 10 and c \u003c11 for update; 在第一次用 c=10 定位记录的时候，索引 c 上加了 (5,10] 这个 next-key lock 后，由于索引 c 是非唯一索引，没有优化规则，也就是说不会蜕变为行锁，因此最终 sesion A 加的锁是，索引 c 上的 (5,10] 和 (10,15] 这两个 next-key lock。 案例5：唯一索引范围锁 bug select * from t where id \u003e10 and id \u003c=15 for update; session A 是一个范围查询，按照原则 1 的话，应该是索引 id 上只加 (10,15] 这个 next-key lock，并且因为 id 是唯一键，所以循环判断到 id=15 这一行就应该停止了。 但是实现上，InnoDB 会往前扫描到第一个不满足条件的行为止，也就是 id=20。而且由于这是个范围扫描，因此索引 id 上的 (15,20] 这个 next-key lock 也会被锁上。 所以你看到了，session B 要更新 id=20 这一行，是会被锁住的。 案例7：limit 语句加锁 delete from t where c =10 limit 2; ession A 的 delete 语句加了 limit 2。你知道表 t 里 c=10 的记录其实只有两条，因此加不加 limit 2，删除的效果都是一样的，但是加锁的效果却不同。 delete 语句明确加了 limit 2 的限制，因此在遍历到 (c=10, id=30) 这一行之后，满足条件的语句已经有两条，循环就结束了。 案例八：一个死锁的例子 session A session B begin;select id from t where c =10 lock in share mode; update t set d = d+1 where c =10;(blocked) insert into t values(8,8,8); session A 启动事务后执行查询语句加 lock in share mode，在索引 c 上加了 next-key lock(5,10] 和间隙锁 (10,15)； session B 的 update 语句也要在索引 c 上加 next-key lock(5,10] ，进入锁等待； 然后 session A 要再插入 (8,8,8) 这一行，被 session B 的间隙锁锁住。由于出现了死锁，InnoDB 让 session B 回滚。 其实，session B 的“加 next-key lock(5,10] ”操作，实际上分成了两步，先是加 (5,10) 的间隙锁，加锁成功；然后加 c=10 的行锁，这时候才被锁住的。 也就是说，我们在分析加锁规则的时候可以用 next-key lock 来分析。但是要知道，具体执行的时候，是要分成间隙锁和行锁两段来执行的。 案例9：带排序的 session A session B begin;select * from t where c \u003e=15 and c\u003c=20 order by c desc lock in share mode; insert into t values(6,6,6);(blocked) 由于是 order by c desc，第一个要定位的是索引 c 上“最右边的”c=20 的行，所以会加上间隙锁 (20,25) 和 next-key lock (15,20]。 在索引 c 上向左遍历，要扫描到 c=10 才停下来，所以 next-key lock 会加到 (5,10]，这正是阻塞 session B 的 insert 语句的原因。 在扫描过程中，c=20、c=15、c=10 这三行都存在值，由于是 select *，所以会在主键 id 上加三个行锁。 因此，session A 的 select 语句锁的范围就是： 索引 c 上 (5, 25)； 主键索引上 id=15、20 两个行锁。 最终的加锁是根据实际执行情况来的。所以，如果一个 select * from … for update 语句，优化器决定使用全表扫描，那么就会把主键索引上 next-key lock 全加上。 binlog redolog写入和落盘 ","date":"2021-01-20","objectID":"/posts/mysql_45%E8%AE%B2%E7%AC%94%E8%AE%B0/:22:0","tags":["mysql"],"title":"mysql实战45讲笔记","uri":"/posts/mysql_45%E8%AE%B2%E7%AC%94%E8%AE%B0/"},{"categories":["笔记"],"content":"binglog写入机制 binlog 的写入逻辑比较简单：事务执行过程中，先把日志写到 binlog cache，事务提交的时候，再把 binlog cache 写到 binlog 文件中。 一个事务的 binlog 是不能被拆开的，因此不论这个事务多大，也要确保一次性写入。 系统给 binlog cache 分配了一片内存，每个线程一个，参数 binlog_cache_size 用于控制单个线程内 binlog cache 所占内存的大小。如果超过了这个参数规定的大小，就要暂存到磁盘。 事务提交的时候，执行器把 binlog cache 里的完整事务写入到 binlog 中，并清空 binlog cache。 write，指的就是指把日志写入到文件系统的 page cache，并没有把数据持久化到磁盘，所以速度比较快。 fsync，才是将数据持久化到磁盘的操作。一般情况下，我们认为 fsync 才占磁盘的 IOPS。 write 和 fsync 的时机，是由参数 sync_binlog 控制的： sync_binlog=0 的时候，表示每次提交事务都只 write，不 fsync； sync_binlog=1 的时候，表示每次提交事务都会执行 fsync； sync_binlog=N(N\u003e1) 的时候，表示每次提交事务都 write，但累积 N 个事务后才 fsync。 在出现 IO 瓶颈的场景里，将 sync_binlog 设置成一个比较大的值，可以提升性能。在实际的业务场景中，考虑到丢失日志量的可控性，一般不建议将这个参数设成 0，比较常见的是将其设置为 100~1000 中的某个数值。 对应的风险是：如果主机发生异常重启，会丢失最近 N 个事务的 binlog 日志。 ","date":"2021-01-20","objectID":"/posts/mysql_45%E8%AE%B2%E7%AC%94%E8%AE%B0/:23:0","tags":["mysql"],"title":"mysql实战45讲笔记","uri":"/posts/mysql_45%E8%AE%B2%E7%AC%94%E8%AE%B0/"},{"categories":["笔记"],"content":"redolog写入机制 事务在执行过程中，生成的 redo log 是要先写到 redo log buffer 的。 如果事务执行期间 MySQL 发生异常重启，那这部分日志就丢了。由于事务并没有提交，所以这时日志丢了也不会有损失。 redo log 可能存在的三种状态： 存在 redo log buffer 中，物理上是在 MySQL 进程内存中，就是图中的红色部分； 写到磁盘 (write)，但是没有持久化（fsync)，物理上是在文件系统的 page cache 里面，也就是图中的黄色部分； 持久化到磁盘，对应的是 hard disk，也就是图中的绿色部分。 日志写到 redo log buffer 是很快的，wirte 到 page cache 也差不多，但是持久化到磁盘的速度就慢多了。 为了控制 redo log 的写入策略，InnoDB 提供了 innodb_flush_log_at_trx_commit 参数，它有三种可能取值： 设置为 0 的时候，表示每次事务提交时都只是把 redo log 留在 redo log buffer 中 ; 设置为 1 的时候，表示每次事务提交时都将 redo log 直接持久化到磁盘； 设置为 2 的时候，表示每次事务提交时都只是把 redo log 写到 page cache。 InnoDB 有一个后台线程，每隔 1 秒，就会把 redo log buffer 中的日志，调用 write 写到文件系统的 page cache，然后调用 fsync 持久化到磁盘。 事务执行中间过程的 redo log 也是直接写在 redo log buffer 中的，这些 redo log 也会被后台线程一起持久化到磁盘。也就是说，一个没有提交的事务的 redo log，也是可能已经持久化到磁盘的。 除了后台线程每秒一次的轮询操作外，还有两种场景会让一个没有提交的事务的 redo log 写入到磁盘中。 一种是，redo log buffer 占用的空间即将达到 innodb_log_buffer_size 一半的时候，后台线程会主动写盘。注意，由于这个事务并没有提交，所以这个写盘动作只是 write，而没有调用 fsync，也就是只留在了文件系统的 page cache。 另一种是，并行的事务提交的时候，顺带将这个事务的 redo log buffer 持久化到磁盘。 通常我们说 MySQL 的“双 1”配置，指的就是 sync_binlog 和 innodb_flush_log_at_trx_commit 都设置成 1。也就是说，一个事务完整提交前，需要等待两次刷盘，一次是 redo log（prepare 阶段），一次是 binlog。 WAL 机制主要得益于两个方面： redo log 和 binlog 都是顺序写，磁盘的顺序写比随机写速度要快； 组提交机制，可以大幅度降低磁盘的 IOPS 消耗。 主从同步 ","date":"2021-01-20","objectID":"/posts/mysql_45%E8%AE%B2%E7%AC%94%E8%AE%B0/:24:0","tags":["mysql"],"title":"mysql实战45讲笔记","uri":"/posts/mysql_45%E8%AE%B2%E7%AC%94%E8%AE%B0/"},{"categories":["笔记"],"content":"同步过程 节点 A 到 B 这条线的内部流程是什么样的： 备库 B 跟主库 A 之间维持了一个长连接。主库 A 内部有一个线程，专门用于服务备库 B 的这个长连接。一个事务日志同步的完整过程是这样的： 在备库 B 上通过 change master 命令，设置主库 A 的 IP、端口、用户名、密码，以及要从哪个位置开始请求 binlog，这个位置包含文件名和日志偏移量。 在备库 B 上执行 start slave 命令，这时候备库会启动两个线程，就是图中的 io_thread 和 sql_thread。其中 io_thread 负责与主库建立连接。 主库 A 校验完用户名、密码后，开始按照备库 B 传过来的位置，从本地读取 binlog，发给 B。 备库 B 拿到 binlog 后，写到本地文件，称为中转日志（relay log）。 sql_thread 读取中转日志，解析出日志里的命令，并执行。 ","date":"2021-01-20","objectID":"/posts/mysql_45%E8%AE%B2%E7%AC%94%E8%AE%B0/:25:0","tags":["mysql"],"title":"mysql实战45讲笔记","uri":"/posts/mysql_45%E8%AE%B2%E7%AC%94%E8%AE%B0/"},{"categories":["笔记"],"content":"binlog 的三种格式对比 binlog 有两种格式，一种是 statement，一种是 row。可能你在其他资料上还会看到有第三种格式，叫作 mixed，其实它就是前两种格式的混合。 delete from t /*comment*/ where a\u003e=4 and t_modified\u003c='2018-11-10' limit 1; 通过这个sql语句看binlog： 当 binlog_format=statement 时，binlog 里面记录的就是 SQL 语句的原文。 当sql语句在从库执行的时候可能删除的是另外一行，因为从库执行是使用的索引可能会不一致。 row 格式的 binlog 里没有了 SQL 语句的原文，而是替换成了两个 event：Table_map 和 Delete_rows。 Table_map event，用于说明接下来要操作的表是 test 库的表 t; Delete_rows event，用于定义删除的行为。 binlog 里面记录了真实删除行的主键 id，这样 binlog 传到备库去的时候，就肯定会删除 id=4 的行，不会有主备删除不同行的问题。 MySQL 就取了个折中方案，也就是有了 **mixed **格式的 binlog。mixed 格式的意思是，MySQL 自己会判断这条 SQL 语句是否可能引起主备不一致，如果有可能，就用 row 格式，否则就用 statement 格式。 现在越来越多的场景要求把 MySQL 的 binlog 格式设置成 row。这么做的理由有很多，我来给你举一个可以直接看出来的好处：恢复数据。 服务端返回数据 取数据和发数据的流程是这样的： 获取一行，写到 net_buffer 中。这块内存的大小是由参数 net_buffer_length 定义的，默认是 16k。 重复获取行，直到 net_buffer 写满，调用网络接口发出去。 如果发送成功，就清空 net_buffer，然后继续取下一行，并写入 net_buffer。 如果发送函数返回 EAGAIN 或 WSAEWOULDBLOCK，就表示本地网络栈（socket send buffer）写满了，进入等待。直到网络栈重新可写，再继续发送。 也就是说，MySQL 是“边读边发的”，这个概念很重要。这就意味着，如果客户端接收得慢，会导致 MySQL 服务端由于结果发不出去，这个事务的执行时间变长。 join ","date":"2021-01-20","objectID":"/posts/mysql_45%E8%AE%B2%E7%AC%94%E8%AE%B0/:26:0","tags":["mysql"],"title":"mysql实战45讲笔记","uri":"/posts/mysql_45%E8%AE%B2%E7%AC%94%E8%AE%B0/"},{"categories":["笔记"],"content":"Index Nested-Loop Join CREATE TABLE `t1` ( `id` int(11) NOT NULL, `a` int(11) DEFAULT NULL, `b` int(11) DEFAULT NULL, PRIMARY KEY (`id`), KEY `a` (`a`) ) ENGINE=InnoDB; t1和t2一样。 select * from t1 straight_join t2 on (t1.a=t2.a); 在这个语句里，强制t1 是驱动表，t2 是被驱动表。 被驱动表 t2 的字段 a 上有索引，join 过程用上了这个索引。这个语句的执行流程是这样的： 从表 t1 中读入一行数据 R； 从数据行 R 中，取出 a 字段到表 t2 里去查找； 取出表 t2 中满足条件的行，跟 R 组成一行，作为结果集的一部分； 重复执行步骤 1 到 3，直到表 t1 的末尾循环结束。 这个过程就跟我们写程序时的嵌套查询类似，并且可以用上被驱动表的索引，所以我们称之为“Index Nested-Loop Join”，简称 NLJ。 结论： 使用 join 语句，性能比强行拆成多个单表执行 SQL 语句的性能要好； 如果使用 join 语句的话，需要让小表做驱动表。 但是，你需要注意，这个结论的前提是“可以使用被驱动表的索引”。 ","date":"2021-01-20","objectID":"/posts/mysql_45%E8%AE%B2%E7%AC%94%E8%AE%B0/:27:0","tags":["mysql"],"title":"mysql实战45讲笔记","uri":"/posts/mysql_45%E8%AE%B2%E7%AC%94%E8%AE%B0/"},{"categories":["笔记"],"content":"Simple Nested-Loop Join select * from t1 straight_join t2 on (t1.a=t2.b); 表 t2 的字段 b 上没有索引，因此再用图 2 的执行流程时，每次到 t2 去匹配的时候，就要做一次全表扫描。 如果只看结果的话，这个算法是正确的，而且这个算法也有一个名字，叫做“Simple Nested-Loop Join”。 如果 t1 和 t2 都是 10 万行的表（当然了，这也还是属于小表的范围），就要扫描 100 亿行，这个算法看上去太“笨重”了。 当然，MySQL 也没有使用这个 Simple Nested-Loop Join 算法，而是使用了另一个叫作“Block Nested-Loop Join”的算法，简称 BNL。 ","date":"2021-01-20","objectID":"/posts/mysql_45%E8%AE%B2%E7%AC%94%E8%AE%B0/:28:0","tags":["mysql"],"title":"mysql实战45讲笔记","uri":"/posts/mysql_45%E8%AE%B2%E7%AC%94%E8%AE%B0/"},{"categories":["笔记"],"content":"Block Nested-Loop Join 被驱动表上没有可用的索引，算法的流程是这样的： 把表 t1 的数据读入线程内存 join_buffer 中，由于我们这个语句中写的是 select *，因此是把整个表 t1 放入了内存； 扫描表 t2，把表 t2 中的每一行取出来，跟 join_buffer 中的数据做对比，满足 join 条件的，作为结果集的一部分返回。 通过explain可以看到extra列中写的是useing join buffer(block nested loop) 对表 t1 和 t2 都做了一次全表扫描，因此总的扫描行数是 1100。由于 join_buffer 是以无序数组的方式组织的，因此对表 t2 中的每一行，都要做 100 次判断，总共需要在内存中做的判断次数是：100*1000=10 万次。 如果使用 Simple Nested-Loop Join 算法进行查询，扫描行数也是 10 万行。因此，从时间复杂度上来说，这两个算法是一样的。但是，Block Nested-Loop Join 算法的这 10 万次判断是内存操作，速度上会快很多，性能也更好。 这个例子里表 t1 才 100 行，要是表 t1 是一个大表，join_buffer 放不下怎么办呢？ join_buffer 的大小是由参数 join_buffer_size 设定的，默认值是 256k。如果放不下表 t1 的所有数据话，策略很简单，就是分段放。 执行过程就变成了： 扫描表 t1，顺序读取数据行放入 join_buffer 中，放完第 88 行 join_buffer 满了，继续第 2 步； 扫描表 t2，把 t2 中的每一行取出来，跟 join_buffer 中的数据做对比，满足 join 条件的，作为结果集的一部分返回； 清空 join_buffer； 继续扫描表 t1，顺序读取最后的 12 行数据放入 join_buffer 中，继续执行第 2 步。 这个流程才体现出了这个算法名字中“Block”的由来，表示“分块去 join”。 join_buffer_size 越大，一次可以放入的行越多，分成的段数也就越少，对被驱动表的全表扫描次数就越少。你可能会看到一些建议告诉你，如果你的 join 语句很慢，就把 join_buffer_size 改大。 此种情形，大表还是小表做驱动表好呢？ 假设驱动表的数据行数是 N，需要分 K 段才能完成算法流程，被驱动表的数据行数是 M。 注意，这里的 K 不是常数，N 越大 K 就会越大，因此把 K 表示为λ*N，显然λ的取值范围是 (0,1)。 所以，在这个算法的执行过程中： 扫描行数是 N+λNM； 内存判断 N*M 次。 显然，内存判断次数是不受选择哪个表作为驱动表影响的。而考虑到扫描行数，在 M 和 N 大小确定的情况下，N 小一些，整个算式的结果会更小。应该让小表当驱动表。 总结： 能不能使用 join 语句？ 如果可以使用 Index Nested-Loop Join 算法，也就是说可以用上被驱动表上的索引，其实是没问题的； 如果使用 Block Nested-Loop Join 算法，扫描行数就会过多。尤其是在大表上的 join 操作，这样可能要扫描被驱动表很多次，会占用大量的系统资源。所以这种 join 尽量不要用。 如果要使用 join，应该选择大表做驱动表还是选择小表做驱动表？ 如果是 Index Nested-Loop Join 算法，应该选择小表做驱动表； 如果是 Block Nested-Loop Join 算法： 在 join_buffer_size 足够大的时候，是一样的； 在 join_buffer_size 不够大的时候（这种情况更常见），应该选择小表做驱动表。 ","date":"2021-01-20","objectID":"/posts/mysql_45%E8%AE%B2%E7%AC%94%E8%AE%B0/:29:0","tags":["mysql"],"title":"mysql实战45讲笔记","uri":"/posts/mysql_45%E8%AE%B2%E7%AC%94%E8%AE%B0/"},{"categories":["笔记"],"content":"Multi-Range Read 优化 回表是指，InnoDB 在普通索引 a 上查到主键 id 的值后，再根据一个个主键 id 的值到主键索引上去查整行数据的过程。 回表过程是一行行地查数据，还是批量地查数据？ 如果随着 a 的值递增顺序查询的话，id 的值就变成随机的，那么就会出现随机访问，性能相对较差。虽然“按行查”这个机制不能改，但是调整查询的顺序，还是能够加速的。 因为大多数的数据都是按照主键递增顺序插入得到的，所以我们可以认为，如果按照主键的递增顺序查询的话，对磁盘的读比较接近顺序读，能够提升读性能。 这，就是 MRR 优化的设计思路。 语句的执行流程变成了这样： 根据索引 a，定位到满足条件的记录，将 id 值放入 read_rnd_buffer 中 ; 将 read_rnd_buffer 中的 id 进行递增排序； 排序后的 id 数组，依次到主键 id 索引中查记录，并作为结果返回。 这里，read_rnd_buffer 的大小是由 read_rnd_buffer_size 参数控制的。 如果步骤 1 中，read_rnd_buffer 放满了，就会先执行完步骤 2 和 3，然后清空 read_rnd_buffer。之后继续找索引 a 的下个记录，并继续循环。 自增主键 ","date":"2021-01-20","objectID":"/posts/mysql_45%E8%AE%B2%E7%AC%94%E8%AE%B0/:30:0","tags":["mysql"],"title":"mysql实战45讲笔记","uri":"/posts/mysql_45%E8%AE%B2%E7%AC%94%E8%AE%B0/"},{"categories":["笔记"],"content":"自增值修改机制 show create table中AUTO_INCREMENT表示下次插入时主键的值。 在 MySQL 里面，如果字段 id 被定义为 AUTO_INCREMENT，在插入一行数据的时候，自增值的行为如下： 如果插入数据时 id 字段指定为 0、null 或未指定值，那么就把这个表当前的 AUTO_INCREMENT 值填到自增字段； 如果插入数据时 id 字段指定了具体的值，就直接使用语句里指定的值。 根据要插入的值和当前自增值的大小关系，自增值的变更结果也会有所不同。假设，某次要插入的值是 X，当前的自增值是 Y。 如果 X\u003cY，那么这个表的自增值不变； 如果 X≥Y，就需要把当前自增值修改为新的自增值。 新的自增值生成算法是：从 auto_increment_offset 开始，以 auto_increment_increment 为步长，持续叠加，直到找到第一个大于 X 的值，作为新的自增值。 ","date":"2021-01-20","objectID":"/posts/mysql_45%E8%AE%B2%E7%AC%94%E8%AE%B0/:31:0","tags":["mysql"],"title":"mysql实战45讲笔记","uri":"/posts/mysql_45%E8%AE%B2%E7%AC%94%E8%AE%B0/"},{"categories":["笔记"],"content":"自增值的修改时机 这个表的自增值改成 3，是在真正执行插入数据的操作之前。这个语句真正执行的时候，因为碰到唯一键 c 冲突，所以 id=2 这一行并没有插入成功，但也没有将自增值再改回去。 row_id,Xid,tx_id ","date":"2021-01-20","objectID":"/posts/mysql_45%E8%AE%B2%E7%AC%94%E8%AE%B0/:32:0","tags":["mysql"],"title":"mysql实战45讲笔记","uri":"/posts/mysql_45%E8%AE%B2%E7%AC%94%E8%AE%B0/"},{"categories":["笔记"],"content":"InnoDB 系统自增 row_id 如果你创建的 InnoDB 表没有指定主键，那么 InnoDB 会给你创建一个不可见的，长度为 6 个字节的 row_id。 InnoDB 维护了一个全局的 dict_sys.row_id 值，所有无主键的 InnoDB 表，每插入一行数据，都将当前的 dict_sys.row_id 值作为要插入数据的 row_id，然后把 dict_sys.row_id 的值加 1。 row_id 写入表中的值范围，是从 0 到 2^48-1； 当 dict_sys.row_id=248时，如果再有插入数据的行为要来申请 row_id，拿到以后再取最后 6 个字节的话就是 0。 在 InnoDB 逻辑里，申请到 row_id=N 后，就将这行数据写入表中；如果表中已经存在 row_id=N 的行，新写入的行就会覆盖原有的行。 从这个角度看，我们还是应该在 InnoDB 表中主动创建自增主键。因为，表自增 id 到达上限后，再插入数据时报主键冲突错误，是更能被接受的。 没有指定自增主键，到达上限会覆盖；指定主键后到达上限再插入会报主键冲突。 ","date":"2021-01-20","objectID":"/posts/mysql_45%E8%AE%B2%E7%AC%94%E8%AE%B0/:33:0","tags":["mysql"],"title":"mysql实战45讲笔记","uri":"/posts/mysql_45%E8%AE%B2%E7%AC%94%E8%AE%B0/"},{"categories":["笔记"],"content":"Xid 介绍 redo log 和 binlog 相配合的时候，提到了它们有一个共同的字段叫作 Xid。它在 MySQL 中是用来对应事务的。 那么，Xid 在 MySQL 内部是怎么生成的呢？ MySQL 内部维护了一个全局变量 global_query_id，每次执行语句的时候将它赋值给 Query_id，然后给这个变量加 1。如果当前语句是这个事务执行的第一条语句，那么 MySQL 还会同时把 Query_id 赋值给这个事务的 Xid。 而 global_query_id 是一个纯内存变量，重启之后就清零了。所以你就知道了，在同一个数据库实例中，不同事务的 Xid 也是有可能相同的。 ","date":"2021-01-20","objectID":"/posts/mysql_45%E8%AE%B2%E7%AC%94%E8%AE%B0/:34:0","tags":["mysql"],"title":"mysql实战45讲笔记","uri":"/posts/mysql_45%E8%AE%B2%E7%AC%94%E8%AE%B0/"},{"categories":["笔记"],"content":"Innodb trx_id Xid 和 InnoDB 的 trx_id 是两个容易混淆的概念。 Xid 是由 server 层维护的。InnoDB 内部使用 Xid，就是为了能够在 InnoDB 事务和 server 之间做关联。但是，InnoDB 自己的 trx_id，是另外维护的。 InnoDB 内部维护了一个 max_trx_id 全局变量，每次需要申请一个新的 trx_id 时，就获得 max_trx_id 的当前值，然后并将 max_trx_id 加 1。 InnoDB 数据可见性的核心思想是：每一行数据都记录了更新它的 trx_id，当一个事务读到一行数据的时候，判断这个数据是否可见的方法，就是通过事务的一致性视图与这行数据的 trx_id 做对比。 对于正在执行的事务，你可以从 information_schema.innodb_trx 表中看到事务的 trx_id。 ","date":"2021-01-20","objectID":"/posts/mysql_45%E8%AE%B2%E7%AC%94%E8%AE%B0/:35:0","tags":["mysql"],"title":"mysql实战45讲笔记","uri":"/posts/mysql_45%E8%AE%B2%E7%AC%94%E8%AE%B0/"},{"categories":["笔记"],"content":"thread_id 接下来，我们再看看线程 id（thread_id）。其实，线程 id 才是 MySQL 中最常见的一种自增 id。平时我们在查各种现场的时候，show processlist 里面的第一列，就是 thread_id。 thread_id 的逻辑很好理解：系统保存了一个全局变量 thread_id_counter，每新建一个连接，就将 thread_id_counter 赋值给这个新连接的线程变量。 案例 ","date":"2021-01-20","objectID":"/posts/mysql_45%E8%AE%B2%E7%AC%94%E8%AE%B0/:36:0","tags":["mysql"],"title":"mysql实战45讲笔记","uri":"/posts/mysql_45%E8%AE%B2%E7%AC%94%E8%AE%B0/"},{"categories":["笔记"],"content":"先插入、再更新阻塞 先插入一条记录并不提交，另一个事务更新会阻塞 CREATE TABLE `user` ( `id` int NOT NULL AUTO_INCREMENT, `name` varchar(20) NOT NULL DEFAULT '', `age` int NOT NULL DEFAULT '0', PRIMARY KEY (`id`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4; 事务1 事务2 begin; insert user (name,age) value(“x”,2); begin; update user set name = ‘bb’ where age = 2;//阻塞 为啥会阻塞？ InnoDb 在插入记录时，是不加锁的。如果事务 A 插入记录且未提交，这时事务 B 尝试对这条记录加锁，事务 B 会先去判断记录上保存的事务 id 是否活跃，如果活跃的话，那么就帮助事务 A 去建立一个锁对象，然后自身进入等待事务 A 状态，这就是所谓的隐式锁转换为显式锁。 问题的脉络已经很清晰了： 执行 insert 语句，判断是否有和插入意向锁冲突的锁，如果有，加插入意向锁，进入锁等待；如果没有，直接写数据，不加任何锁； 执行 select … lock in share mode 语句，判断记录上是否存在活跃的事务，如果存在，则为 insert 事务创建一个排他记录锁，并将自己加入到锁等待队列； ","date":"2021-01-20","objectID":"/posts/mysql_45%E8%AE%B2%E7%AC%94%E8%AE%B0/:37:0","tags":["mysql"],"title":"mysql实战45讲笔记","uri":"/posts/mysql_45%E8%AE%B2%E7%AC%94%E8%AE%B0/"},{"categories":["笔记"],"content":"先插入后提交、再更新 重复读结果不一样 CREATE TABLE `user` ( `id` int NOT NULL AUTO_INCREMENT, `name` varchar(20) NOT NULL DEFAULT '', `age` int NOT NULL DEFAULT '0', PRIMARY KEY (`id`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4; 事务1 事务2 begin; begin; select * from user where age = 2; //无数据 select * from user where age = 2; //无数据 insert user (name,age) value(“x”,2); commit; update user set name = ‘bb’ where age = 2; //却更新到了一行，因为的更新是先读再更新，而且这个读是当前读，并把这条记录的最新的row trx_id更新了 select * from user where age = 2; //查到了一行，两次select不一致，因为上个update语句，这个时候读是能感知到当前事务的变化的，因为它能读到当前刚更新的row trx_id快照 commit; ","date":"2021-01-20","objectID":"/posts/mysql_45%E8%AE%B2%E7%AC%94%E8%AE%B0/:38:0","tags":["mysql"],"title":"mysql实战45讲笔记","uri":"/posts/mysql_45%E8%AE%B2%E7%AC%94%E8%AE%B0/"},{"categories":["笔记"],"content":"系统学docker笔记","date":"2021-01-20","objectID":"/posts/docker_%E7%B3%BB%E7%BB%9F%E5%AD%A6docker%E7%AC%94%E8%AE%B0/","tags":["docker"],"title":"系统学docker笔记","uri":"/posts/docker_%E7%B3%BB%E7%BB%9F%E5%AD%A6docker%E7%AC%94%E8%AE%B0/"},{"categories":["笔记"],"content":"什么是docker Docker 使用 Google 公司推出的 Go 语言 进行开发实现，基于 Linux 内核的 cgroup，namespace，以及 OverlayFS 类的 Union FS 等技术，对进程进行封装隔离，属于 操作系统层面的虚拟化技术。由于隔离的进程独立于宿主和其它的隔离的进程，因此也称其为容器。 Docker 是当前流行的 Linux 容器解决方案，利用 Namespaces 、Cgroups 以及联合文件系统UnionFS 实现了同一主机上容器进程间的相互隔离。 NameSpaces：隔离进程，让进程只能访问到本命名空间里的挂载目录、PID、NetWork 等资源 Cgroups: 限制进程能使用的计算机系统各项资源的上限，包括 CPU、内存、磁盘、网络带宽等等 联合文件系统UnionFS : 保存一个操作系统的所有文件和目录，在它基础之上添加应用运行依赖的文件。创建容器进程的时候给进程指定Mount Namespace 把镜像文件挂载到容器里，用 chroot 把进程的 Root目录切换到挂载的目录里，从而让容器进程各自拥有独立的操作系统目录。 虚拟化和docker区别 传统虚拟机技术是虚拟出一套硬件后，在其上运行一个完整操作系统，在该系统上再运行所需应用进程；而容器内的应用进程直接运行于宿主的内核，容器内没有自己的内核，而且也没有进行硬件虚拟。因此容器要比传统虚拟机更为轻便。 docker cs架构 docker是cs结构，Docker桌面版安装包括了Docker Client和Docker Engine。 主要包含： 后台进程（dockerd） rest api server cli接口（docker） Docker的Client和Engine之间的通讯有一下几种方式 Unix Socket 这是类unix系统进程间通讯的一种方式，当Client操作本机的Engine是就是使用这种方式。缺省的socket文件是unix:///var/run/docker.sock Systemd socket activation : 这是systemd提供的一种为了服务并行启动设计的socket， TCP : 上面两种都是只能连接本地Engine，需要连接远程Engine，必须在服务端开始TCP连接。此连接为不安全连接，数据通过明文进行传输。缺省端口2375。 TCP_TLS : 在TCP的基础之上加上了SSL的安全证书，以保证连接安全。缺省端口2376。 docker 底层技术支持 Namespaces：隔离pid,net,ipc,mnt,uts Control groups：做资源限制 Union file systems：Conatiner和image的分层 镜像 ","date":"2021-01-20","objectID":"/posts/docker_%E7%B3%BB%E7%BB%9F%E5%AD%A6docker%E7%AC%94%E8%AE%B0/:0:0","tags":["docker"],"title":"系统学docker笔记","uri":"/posts/docker_%E7%B3%BB%E7%BB%9F%E5%AD%A6docker%E7%AC%94%E8%AE%B0/"},{"categories":["笔记"],"content":"Dockerfile命令 ","date":"2021-01-20","objectID":"/posts/docker_%E7%B3%BB%E7%BB%9F%E5%AD%A6docker%E7%AC%94%E8%AE%B0/:1:0","tags":["docker"],"title":"系统学docker笔记","uri":"/posts/docker_%E7%B3%BB%E7%BB%9F%E5%AD%A6docker%E7%AC%94%E8%AE%B0/"},{"categories":["笔记"],"content":"workdir workdir等于linux cd命令 workdir /test # 如果没有会自动创建 workdir demo run pwd # 输出结果为/test/demo 推荐workdir，不要 run cd。尽量使用绝对路径。 ","date":"2021-01-20","objectID":"/posts/docker_%E7%B3%BB%E7%BB%9F%E5%AD%A6docker%E7%AC%94%E8%AE%B0/:1:1","tags":["docker"],"title":"系统学docker笔记","uri":"/posts/docker_%E7%B3%BB%E7%BB%9F%E5%AD%A6docker%E7%AC%94%E8%AE%B0/"},{"categories":["笔记"],"content":"ADD COPY 对于目录而言，COPY 和 ADD 命令具有相同的特点：只复制目录中的内容而不包含目录自身。 ADD 命令可以完成 COPY 命令的所有功能，是copy的增强版，并且还可以完成两类超酷的功能： 解压压缩文件并把它们添加到镜像中 从 url 拷贝文件到镜像中 当然，这些功能也让 ADD 命令用起来复杂一些，不如 COPY 命令那么直观。 docker 官方建议我们当需要从远程复制文件时，最好使用 curl 或 wget 命令来代替 ADD 命令。原因是，当使用 ADD 命令时，会创建更多的镜像层，当然镜像的 size 也会更大 ","date":"2021-01-20","objectID":"/posts/docker_%E7%B3%BB%E7%BB%9F%E5%AD%A6docker%E7%AC%94%E8%AE%B0/:1:2","tags":["docker"],"title":"系统学docker笔记","uri":"/posts/docker_%E7%B3%BB%E7%BB%9F%E5%AD%A6docker%E7%AC%94%E8%AE%B0/"},{"categories":["笔记"],"content":"EXPOSE expose是对端口的暴露声明，记住是声明，不写也没事。 写了expose后，运行容器时如果加-P参数，就会对expose声明的端口进行和宿主主机端口随机映射。 ","date":"2021-01-20","objectID":"/posts/docker_%E7%B3%BB%E7%BB%9F%E5%AD%A6docker%E7%AC%94%E8%AE%B0/:1:3","tags":["docker"],"title":"系统学docker笔记","uri":"/posts/docker_%E7%B3%BB%E7%BB%9F%E5%AD%A6docker%E7%AC%94%E8%AE%B0/"},{"categories":["笔记"],"content":"VOLUME 定义匿名数据卷 valume /test/data 这里会自动生成一个volume，然后/test/data和自动生成的那个目录映射。 docker run的时候如果使用-v，–mount可以覆盖dockerfile里面的这个命令。 ","date":"2021-01-20","objectID":"/posts/docker_%E7%B3%BB%E7%BB%9F%E5%AD%A6docker%E7%AC%94%E8%AE%B0/:1:4","tags":["docker"],"title":"系统学docker笔记","uri":"/posts/docker_%E7%B3%BB%E7%BB%9F%E5%AD%A6docker%E7%AC%94%E8%AE%B0/"},{"categories":["笔记"],"content":"CMD Docker 不是虚拟机，容器就是进程。既然是进程，那么在启动容器的时候，需要指定所运行的程序及参数。CMD 指令就是用于指定默认的容器主进程的启动命令的。 多个cmd命令会覆盖。 docker run时如果指定入口程序也会覆盖Dcokerfile中的cmd。比如 docker run -it hello /bin/bash // /bin/bash就会覆盖cmd命令 ","date":"2021-01-20","objectID":"/posts/docker_%E7%B3%BB%E7%BB%9F%E5%AD%A6docker%E7%AC%94%E8%AE%B0/:1:5","tags":["docker"],"title":"系统学docker笔记","uri":"/posts/docker_%E7%B3%BB%E7%BB%9F%E5%AD%A6docker%E7%AC%94%E8%AE%B0/"},{"categories":["笔记"],"content":"ENTRYPOINT ENTRYPOINT 的目的和 CMD 一样，都是在指定容器启动程序及参数。 ENTRYPOINT 在运行时也可以替代，不过比 CMD 要略显繁琐，需要通过 docker run 的参数 –entrypoint 来指定。 当指定了 ENTRYPOINT 后，CMD 的含义就发生了改变，不再是直接的运行其命令，而是将 CMD 的内容作为参数传给 ENTRYPOINT 指令，换句话说实际执行时，将变为： \u003cENTRYPOINT\u003e \"\u003cCMD\u003e\" docker run -it hello /bin/bashz这个例子中的/bin/bash也会当做entrypoint的参数。 容器 看几个特点： 通过image创建（copy） image是只读的。再image layer之上建立一个container layer（可读写） image和container类比面向对象的类和示例 image负责APP的存储和分发，container负责运行APP image是只读的，那从Dockerfile构建image的过程中，docker build是怎么执行命令修改的呢？ 答：构建过程中会生成临时的contailer，其实是在容器上执行的操作，后面再把临时容器删除，又容器commit成image。 数据管理 把写到容器里面的东西写到宿主主机上。 分两种： volume类型，通过docker volume ls可以看到 bind类型，这种不会不会生成volume volume类型匿名挂载： docker run -v 容器内路径 DockerFile中volume命令 volume类型具名挂载： -v 卷名:容器内路径 bind类型 docker run -v /主机路径:容器内路径 # 指定路径挂载 docker –mount type=bind,source=/src/webapp,target=/usr/share/ ","date":"2021-01-20","objectID":"/posts/docker_%E7%B3%BB%E7%BB%9F%E5%AD%A6docker%E7%AC%94%E8%AE%B0/:1:6","tags":["docker"],"title":"系统学docker笔记","uri":"/posts/docker_%E7%B3%BB%E7%BB%9F%E5%AD%A6docker%E7%AC%94%E8%AE%B0/"},{"categories":["笔记"],"content":"基础","date":"2021-01-08","objectID":"/posts/%E5%9F%BA%E7%A1%80-%E5%AD%97%E7%AC%A6%E7%BC%96%E7%A0%81/","tags":["基础"],"title":"字符编码","uri":"/posts/%E5%9F%BA%E7%A1%80-%E5%AD%97%E7%AC%A6%E7%BC%96%E7%A0%81/"},{"categories":["笔记"],"content":"最早的计算机系统都是使用 EBCDIC(扩展的二进制的十进制转换码) 和 ASCII 编码，因为那时候只是用一些英文字母数字，加减号和其他一些字符，字符并不多，但是随着Internet的发展，网络遍布全球。全球有大概6000种语言（其中3000种在巴布亚新几内亚…） ，为了更好地服务更多的人，我们需要为不同语言的用户提供不同的语言支持。如果世界只需要 ASCII 编码，那样将会简单很多。但事实却是非常复杂的。 字符集 character code 字符编码就是将一个字符映射到一个整数，比如最常见的 ASCII编码，将 a 编码为 97（编码点,code point）， A编码为65.编码仍然是抽象的，它仍不是我们在文本或者TCP包中多见到的。 字符集常用的有ascii字符集,unicode字符集 ASCII 我们说ascii的时候，其实包含了ascii字符集和ascii编码。 ASCII 编码大家都很熟悉了，大一的时候专业基础课接触到的就是这些内容。作为最常用的编码，ASCII使用的编码点使用7-bit。所以他一共有128个编码。 这里问题来了，ASCII是一个字节，7位就能表示字符，那最左边的那一位有什么用？ 回答：最高位是表示扩展字符集的。换句话说当最高位为0时，表示这一个字节就表示一个字符；如果是1，那就代表要和下一个字节连起来看，这也就是ASCII码表示汉字需要两个字节的原因。 ISO 8859 现在一个字节的标准是8个比特位，作为ASCII的扩展多出来128个编码点。一些列不同的编码集使用了这128个编码点，它们被一些欧洲语言使用，合起来就是 ISO-8859系列。 ISO-8859-1也就是常说的Latin-1，它涵盖了大部分的欧洲语言。 ISO-8859是一个系列，系列中所有的低128个编码点就是ASCII编码，以实现兼容。 早期的HTML标准推荐使用ISO-8859-1字符集。不过在 HTML 4 之后就推荐使用 Unicode了。 Unicode 像ASCII和ISO8859这样的编码在象形文字（中日韩）语言面前显得就太小气了。中文常用的字多大几千个，至少需要两个字节才能涵括。最初没有统一的国际标准，因此出现了很多的2字节编码方案，比如台湾的Big5，国内的GB2312，BGK；再考虑日本的JIS X 0208等等，字符集简直是一个大乱斗。 Unicode是一个包含所有主要当前在用字符的新的标准，它包括了欧洲，亚洲，印度等等各种语言，Unicode的好处是它是可扩展的。到5.2版本，一共有超过 107000个字符。 Unicode编码点是兼容ISO8859的，也就是说它的前256个编码点就是ISO 8859-1.要在计算机系统表示一个Unicode字符，需要使用一种编码方式。UCS（通用编码方式）使用两个字节进行编码，然而随着Unicod囊括的字符越来越多，UCS不再使用，而是使用 UTF-*的编码。 unicode字符集其实就是一个字符和整型数的映射库。它本身并不会指定需要用几个字节去代表一个字符。 utf UTF即 Unicode Transformation format.常见的编码方式如下： UTF-32 4字节编码，不常用，尤其是 HTML5 规范明确反对使用它 UTF-16 UTF-16是一种可变长度字符编码方式，以16-bit 为单元，使用2个或4个字节为每个字符编码。 UTF-16的编码规则如下： Unicode编码范围 （十六进制） UTF-16编码占用字节 UTF-16 编码（二进制） U+0000 - U+FFFF 2 xxxxxxxx xxxxxxxx U+10000 - U+10FFFF 4 110110yyyyyyyyyy 110111xxxxxxxxxx UTF-8 一个字符使用1-4字节，不定长度，最常用的，对英文字符基本和ISO-8859对应，中文编码为3-4字符，相UTF-8 的编码规则很简单，只有二条： 1）对于单字节的符号，字节的第一位设为0，后面7位为这个符号的 Unicode 码。因此对于英语字母，UTF-8 编码和 ASCII 码是相同的。 2）对于n字节的符号（n \u003e 1），第一个字节的前n位都设为1，第n + 1位设为0，后面字节的前两位一律设为10。剩下的没有提及的二进制位，全部为这个符号的 Unicode 码。 如果一个字节的第一位是0，则这个字节单独就是一个字符；如果第一位是1，则连续有多少个1，就表示当前字符占用多少个字节。 Unicode编码范围（十六进制） UTF-8编码占用字节 UTF-8 编码（二进制） U+0000 - U+007F 1 0xxxxxxx U+0080 - U+07FF 2 110xxxxx 10xxxxxx U+0800 - U+FFFF 3 1110xxxx 10xxxxxx 10xxxxxx U+10000 - U+10FFFF 4 11110xxx 10xxxxxx 10xxxxxx 10xxxxxx UTF-7 有时使用，但不常用 要理解Unicode和UTF-8之间的区别和联系。知道UTF的名字是Unicode转换格式就可以了。Unicode是一个字符集，UTF-8是这个字符集的一种编码方式。 Go与字符集 UTF-8 Go的字符串的每一个字符称为一个 rune，它是 int32的别名，因为一个Unicode字符的长度可能是1，2，3，4个字节，如果要统计字符数，就需要计算的是rune的个数而不是字节数了。字符数和字节数只有在是字符串只由ASCII字符组成时才是一样的。 Rune选择int32而不是uint32呢？ This has been asked several times. rune occupies 4 bytes and not just one because it is supposed to store unicode codepoints and not just ASCII characters. Like array indices, the datatype is signed so that you can easily detect overflows or other errors while doing arithmetic with those types. 意思技术方便检测溢出或错误。 ","date":"2021-01-08","objectID":"/posts/%E5%9F%BA%E7%A1%80-%E5%AD%97%E7%AC%A6%E7%BC%96%E7%A0%81/:0:0","tags":["基础"],"title":"字符编码","uri":"/posts/%E5%9F%BA%E7%A1%80-%E5%AD%97%E7%AC%A6%E7%BC%96%E7%A0%81/"},{"categories":["笔记"],"content":"mysq 日志和crash-safe","date":"2020-12-28","objectID":"/posts/mysql_%E6%97%A5%E5%BF%97%E5%92%8Ccrash_safe/","tags":["mysql"],"title":"mysq 日志和crash-safe","uri":"/posts/mysql_%E6%97%A5%E5%BF%97%E5%92%8Ccrash_safe/"},{"categories":["笔记"],"content":"mysql的crash-safe MySQL 保证数据不会丢的能力主要体现在两方面： 能够恢复到任何时间点的状态； 能够保证MySQL在任何时间段突然奔溃，重启后之前提交的记录都不会丢失； 对于第一点将MySQL恢复到任何时间点的状态，相信很多人都知道，只要保留有足够的binlog，就能通过重跑binlog来实现。对于第二点的能力，也就是本文标题所讲的crash-safe。即在 InnoDB 存储引擎中，事务提交过程中任何阶段，MySQL突然奔溃，重启后都能保证事务的完整性，已提交的数据不会丢失，未提交完整的数据会自动进行回滚。这个能力依赖的就是redo log和unod log两个日志。 更新语句在MySQL中是怎么执行的，来看执行器和 InnoDB 引擎在执行这个简单的 update 语句时的内部流程。简单进行总结一下： 执行器先找引擎取 ID=2 这一行。ID 是主键，引擎直接用树搜索找到这一行。如果 ID=2 这一行所在的数据页本来就在内存中，就直接返回给执行器；否则，需要先从磁盘读入内存，然后再返回。 执行器拿到引擎给的行数据，把这个值加上 1，比如原来是 N，现在就是 N+1，得到新的一行数据，再调用引擎接口写入这行新数据。 引擎将这行新数据更新到内存中，同时将这个更新操作记录到 redo log 里面，此时 redo log 处于 prepare 状态。然后告知执行器执行完成了，随时可以提交事务。 执行器生成这个操作的 binlog，并把 binlog 写入磁盘。 执行器调用引擎的提交事务接口，引擎把刚刚写入的 redo log 改成提交（commit）状态，更新完成。 对于内存中的数据和日志，都是由后台线程，当触发到落盘规则后再异步进行刷盘； WAL机制 为什么不直接更改磁盘中的数据，而要在内存中更改，然后还需要写日志，最后再落盘这么复杂？ MySQL更改数据的时候，之所以不直接写磁盘文件中的数据，最主要就是性能问题。因为直接写磁盘文件是随机写，开销大性能低，没办法满足MySQL的性能要求。所以才会设计成先在内存中对数据进行更改，再异步落盘。但是内存总是不可靠，万一断电重启，还没来得及落盘的内存数据就会丢失，所以还需要加上写日志这个步骤，万一断电重启，还能通过日志中的记录进行恢复。 写日志虽然也是写磁盘，但是它是顺序写，相比随机写开销更小，能提升语句执行的性能。 日志先行的技术，指的是对数据文件进行修改前，必须将修改先记录日志。保证了数据一致性和持久性，并且提升语句执行性能。 核心日志模块 更新SQL语句执行流程中，总共需要写3个日志，这3个是不是都需要，能不能进行简化？更新SQL执行过程中，总共涉及MySQL日志模块其中的三个核心日志，分别是redo log（重做日志）、undo log（回滚日志）、binlog（归档日志）。这里提前预告，crash-safe的能力主要依赖的就是这三大日志。 ","date":"2020-12-28","objectID":"/posts/mysql_%E6%97%A5%E5%BF%97%E5%92%8Ccrash_safe/:0:0","tags":["mysql"],"title":"mysq 日志和crash-safe","uri":"/posts/mysql_%E6%97%A5%E5%BF%97%E5%92%8Ccrash_safe/"},{"categories":["笔记"],"content":"重做日志 redo log redo log也称为事务日志，由InnoDB存储引擎层产生。记录的是数据库中每个页的修改，而不是某一行或某几行修改成怎样，可以用来恢复提交后的物理数据页（恢复数据页，且只能恢复到最后一次提交的位置，因为修改会覆盖之前的）。 前面提到的WAL技术，redo log就是WAL的典型应用，MySQL在有事务提交对数据进行更改时，只会在内存中修改对应的数据页和记录redo log日志，完成后即表示事务提交成功，至于磁盘数据文件的更新则由后台线程异步处理。由于redo log的加入，保证了MySQL数据一致性和持久性（即使数据刷盘之前MySQL奔溃了，重启后仍然能通过redo log里的更改记录进行重放，重新刷盘），此外还能提升语句的执行性能（写redo log是顺序写，相比于更新数据文件的随机写，日志的写入开销更小，能显著提升语句的执行性能，提高并发量），由此可见redo log是必不可少的。 redo log是固定大小的，所以只能循环写，从头开始写，写到末尾就又回到开头，相当于一个环形。当日志写满了，就需要对旧的记录进行擦除，但在擦除之前，需要确保这些要被擦除记录对应在内存中的数据页都已经刷到磁盘中了。在redo log满了到擦除旧记录腾出新空间这段期间，是不能再接收新的更新请求，所以有可能会导致MySQL卡顿。（所以针对并发量大的系统，适当设置redo log的文件大小非常重要！！！） ","date":"2020-12-28","objectID":"/posts/mysql_%E6%97%A5%E5%BF%97%E5%92%8Ccrash_safe/:1:0","tags":["mysql"],"title":"mysq 日志和crash-safe","uri":"/posts/mysql_%E6%97%A5%E5%BF%97%E5%92%8Ccrash_safe/"},{"categories":["笔记"],"content":"redo 参数 innodb_log_files_in_group redo log 文件的个数，命名方式如：ib_logfile0，iblogfile1… iblogfilen。默认2个，最大100个。 innodb_log_file_size 文件设置大小，默认值为 48M，最大值为512G，注意最大值指的是整个 redo log系列文件之和，即（innodb_log_files_in_group * innodb_log_file_size ）不能大于最大值512G。 innodb_log_group_home_dir 文件存放路径 innodb_log_buffer_size Redo Log 缓存区，默认8M，可设置1-8M。延迟事务日志写入磁盘，把redo log 放到该缓冲区，然后根据 innodb_flush_log_at_trx_commit参数的设置，再把日志从buffer 中flush 到磁盘中。 innodb_flush_log_at_trx_commit innodb_flush_log_at_trx_commit=0，事务发生过程，日志一直记录在redo log buffer中，跟其他设置一样，但是在事务提交时，不产生redo 写操作，而是MySQL内部每秒操作一次，从redo log buffer，把数据写入到系统中去。如果发生crash，即丢失1s内的事务修改操作。 innodb_flush_log_at_trx_commit=1，每次commit都会把redo log从redo log buffer写入到system，并fsync刷新到磁盘文件中。 innodb_flush_log_at_trx_commit=2，每次事务提交时MySQL会把日志从redo log buffer写入到system，但只写入到file system buffer，由系统内部来fsync到磁盘文件。如果数据库实例crash，不会丢失redo log，但是如果服务器crash，由于file system buffer还来不及fsync到磁盘文件，所以会丢失这一部分的数据。 简而言之：为了满足不用业务对于吞吐量与一致性的需求，MySQL事务提交时刷redo log有三种策略： 0：每秒write一次OS cache，同时fsync刷磁盘，性能好； 1：每次都write入OS cache，同时fsync刷磁盘，一致性好； 2：每次都write入OS cache，每秒fsync刷磁盘，折衷； 注意：由于进程调度策略问题,这个“每秒执行一次 flush(刷到磁盘)操作”并不是保证100%的“每秒”。 ","date":"2020-12-28","objectID":"/posts/mysql_%E6%97%A5%E5%BF%97%E5%92%8Ccrash_safe/:1:1","tags":["mysql"],"title":"mysq 日志和crash-safe","uri":"/posts/mysql_%E6%97%A5%E5%BF%97%E5%92%8Ccrash_safe/"},{"categories":["笔记"],"content":"回滚日志 undo log undo log顾名思义，主要就是提供了回滚的作用，是逻辑日志。但其还有另一个主要作用，就是多个行版本控制(MVCC)，保证事务的原子性。在数据修改的流程中，会记录一条与当前操作相反的逻辑日志到undo log中（可以认为当delete一条记录时，undo log中会记录一条对应的insert记录，反之亦然，当update一条记录时，它记录一条对应相反的update记录），如果因为某些原因导致事务异常失败了，可以借助该undo log进行回滚，保证事务的完整性，所以undo log也必不可少。 每条记录在更新的时候都会同时记录一条回滚操作。记录上的最新值，通过回滚操作，都可以得到前一个状态的值。 回滚日志总不能一直保留吧，什么时候删除呢？答案是，在不需要的时候才删除。也就是说，系统会判断，当没有事务再需要用到这些回滚日志时，回滚日志会被删除。 什么时候才不需要了呢？就是当系统里没有比这个回滚日志更早的 read-view 的时候。 基于上面的说明，我们来讨论一下为什么建议你尽量不要使用长事务。 长事务意味着系统里面会存在很老的事务视图。由于这些事务随时可能访问数据库里面的任何数据，所以这个事务提交之前，数据库里面它可能用到的回滚记录都必须保留，这就会导致大量占用存储空间。 ","date":"2020-12-28","objectID":"/posts/mysql_%E6%97%A5%E5%BF%97%E5%92%8Ccrash_safe/:2:0","tags":["mysql"],"title":"mysq 日志和crash-safe","uri":"/posts/mysql_%E6%97%A5%E5%BF%97%E5%92%8Ccrash_safe/"},{"categories":["笔记"],"content":"归档日志 bin log binlog在MySQL的server层产生，不属于任何引擎，主要记录用户对数据库操作的SQL语句（除了查询语句）。之所以将binlog称为归档日志，是因为binlog不会像redo log一样擦掉之前的记录循环写，而是一直记录（超过有效期才会被清理），如果超过单日志的最大值（默认1G，可以通过变量 max_binlog_size 设置），则会新起一个文件继续记录。但由于日志可能是基于事务来记录的(如InnoDB表类型)，而事务是绝对不可能也不应该跨文件记录的，如果正好binlog日志文件达到了最大值但事务还没有提交则不会切换新的文件记录，而是继续增大日志，所以 max_binlog_size 指定的值和实际的binlog日志大小不一定相等。 Binlog有三种模式： ","date":"2020-12-28","objectID":"/posts/mysql_%E6%97%A5%E5%BF%97%E5%92%8Ccrash_safe/:3:0","tags":["mysql"],"title":"mysq 日志和crash-safe","uri":"/posts/mysql_%E6%97%A5%E5%BF%97%E5%92%8Ccrash_safe/"},{"categories":["笔记"],"content":"statement模式 statement格式的话是记sql语句。每一条会修改数据的sql都会记录到master的bin-log中。slave在复制的时候sql进程会解析成和原来master端执行过的相同的sql来再次执行。 优点是减少bin-log日志量，节约IO，提高性能。缺点是修改数据的时候使用了某些定的函数或者功能的时候会出现错误。 ","date":"2020-12-28","objectID":"/posts/mysql_%E6%97%A5%E5%BF%97%E5%92%8Ccrash_safe/:3:1","tags":["mysql"],"title":"mysq 日志和crash-safe","uri":"/posts/mysql_%E6%97%A5%E5%BF%97%E5%92%8Ccrash_safe/"},{"categories":["笔记"],"content":"row模式 row格式会记录行的内容，记两条，更新前和更新后都有。 优点是不会出现某些特定的情况下的存储过程或function，以及trigger的调用和触发无法被正确复制的问题。缺点是row level，所有的执行的语句当记录到日志中的时候，都将以每行记录的修改来记录，会产生大量的日志内容。 ","date":"2020-12-28","objectID":"/posts/mysql_%E6%97%A5%E5%BF%97%E5%92%8Ccrash_safe/:3:2","tags":["mysql"],"title":"mysq 日志和crash-safe","uri":"/posts/mysql_%E6%97%A5%E5%BF%97%E5%92%8Ccrash_safe/"},{"categories":["笔记"],"content":"Mixed 自动模式 在Mixed模式下，MySQL会根据执行的每一条具体的sql语句来区分对待记录的日志格式，也就是在Statement和Row之间选择一种。 两阶段提交 问题：为什么redo log要分两步写，中间再穿插写binlog呢？ 从上面可以看出，因为redo log影响主库的数据，binlog影响从库的数据，所以redo log和binlog必须保持一致才能保证主从数据一致，这是前提。 事务的提交过程有两个阶段，就是将redo log的写入拆成了两个步骤：prepare和commit，中间再穿插写入binlog。 如果只有binlog，那么不管先写binlog日志还是先写库，都有可能数据和日志不一致，可是主从同步就是利用的binlog，那么主从就不一致了。 数据恢复流程 我们先来看一下崩溃恢复时的判断规则： 如果 redo log 里面的事务是完整的，也就是已经有了 commit 标识，则直接提交； 如果 redo log 里面的事务只有完整的 prepare，则判断对应的事务 binlog 是否存在并完整： a. 如果是，则提交事务； b. 否则，回滚事务。 如果在图中时刻 A 的地方，也就是写入 redo log 处于 prepare 阶段之后、写 binlog 之前，发生了崩溃（crash），由于此时 binlog 还没写，redo log 也还没提交，所以崩溃恢复的时候，这个事务会回滚。这时候，binlog 还没写，所以也不会传到备库。到这里，大家都可以理解。 时刻 B，也就是 binlog 写完，redo log 还没 commit 前发生 crash。时刻 B 发生 crash 对应的就是 前面2(a) 的情况，崩溃恢复过程中事务会被提交。 追问 ","date":"2020-12-28","objectID":"/posts/mysql_%E6%97%A5%E5%BF%97%E5%92%8Ccrash_safe/:3:3","tags":["mysql"],"title":"mysq 日志和crash-safe","uri":"/posts/mysql_%E6%97%A5%E5%BF%97%E5%92%8Ccrash_safe/"},{"categories":["笔记"],"content":"追问 1：MySQL 怎么知道 binlog 是完整的? 回答：一个事务的 binlog 是有完整格式的： statement 格式的 binlog，最后会有 COMMIT； row 格式的 binlog，最后会有一个 XID event。 另外，在 MySQL 5.6.2 版本以后，还引入了 binlog-checksum 参数，用来验证 binlog 内容的正确性。对于 binlog 日志由于磁盘原因，可能会在日志中间出错的情况，MySQL 可以通过校验 checksum 的结果来发现。所以，MySQL 还是有办法验证事务 binlog 的完整性的。 ","date":"2020-12-28","objectID":"/posts/mysql_%E6%97%A5%E5%BF%97%E5%92%8Ccrash_safe/:4:0","tags":["mysql"],"title":"mysq 日志和crash-safe","uri":"/posts/mysql_%E6%97%A5%E5%BF%97%E5%92%8Ccrash_safe/"},{"categories":["笔记"],"content":"追问 2：redo log 和 binlog 是怎么关联起来的? 回答：它们有一个共同的数据字段，叫 XID。崩溃恢复的时候，会按顺序扫描 redo log： 如果碰到既有 prepare、又有 commit 的 redo log，就直接提交； 如果碰到只有 parepare、而没有 commit 的 redo log，就拿着 XID 去 binlog 找对应的事务。 ","date":"2020-12-28","objectID":"/posts/mysql_%E6%97%A5%E5%BF%97%E5%92%8Ccrash_safe/:5:0","tags":["mysql"],"title":"mysq 日志和crash-safe","uri":"/posts/mysql_%E6%97%A5%E5%BF%97%E5%92%8Ccrash_safe/"},{"categories":["笔记"],"content":"追问 3：处于 prepare 阶段的 redo log 加上完整 binlog，重启就能恢复，MySQL 为什么要这么设计? 回答：其实，这个问题还是跟我们在反证法中说到的数据与备份的一致性有关。在时刻 B，也就是 binlog 写完以后 MySQL 发生崩溃，这时候 binlog 已经写入了，之后就会被从库（或者用这个 binlog 恢复出来的库）使用。 所以，在主库上也要提交这个事务。采用这个策略，主库和备库的数据就保证了一致性。 ","date":"2020-12-28","objectID":"/posts/mysql_%E6%97%A5%E5%BF%97%E5%92%8Ccrash_safe/:6:0","tags":["mysql"],"title":"mysq 日志和crash-safe","uri":"/posts/mysql_%E6%97%A5%E5%BF%97%E5%92%8Ccrash_safe/"},{"categories":["笔记"],"content":"追问 4：如果这样的话，为什么还要两阶段提交呢？干脆先 redo log 写完，再写 binlog。崩溃恢复的时候，必须得两个日志都完整才可以。是不是一样的逻辑？ 回答：其实，两阶段提交是经典的分布式系统问题，并不是 MySQL 独有的。 如果必须要举一个场景，来说明这么做的必要性的话，那就是事务的持久性问题。 对于 InnoDB 引擎来说，如果 redo log 提交完成了，事务就不能回滚（如果这还允许回滚，就可能覆盖掉别的事务的更新）。而如果 redo log 直接提交，然后 binlog 写入的时候失败，InnoDB 又回滚不了，数据和 binlog 日志又不一致了。 两阶段提交就是为了给所有人一个机会，当每个人都说“我 ok”的时候，再一起提交。 ","date":"2020-12-28","objectID":"/posts/mysql_%E6%97%A5%E5%BF%97%E5%92%8Ccrash_safe/:7:0","tags":["mysql"],"title":"mysq 日志和crash-safe","uri":"/posts/mysql_%E6%97%A5%E5%BF%97%E5%92%8Ccrash_safe/"},{"categories":["笔记"],"content":"追问 5：不引入两个日志，也就没有两阶段提交的必要了。只用 binlog 来支持崩溃恢复，又能支持归档，不就可以了？ 回答：这位同学的意思是，只保留 binlog，然后可以把提交流程改成这样：… -\u003e “数据更新到内存” -\u003e “写 binlog” -\u003e “提交事务”，是不是也可以提供崩溃恢复的能力？ 答案是不可以。 ","date":"2020-12-28","objectID":"/posts/mysql_%E6%97%A5%E5%BF%97%E5%92%8Ccrash_safe/:8:0","tags":["mysql"],"title":"mysq 日志和crash-safe","uri":"/posts/mysql_%E6%97%A5%E5%BF%97%E5%92%8Ccrash_safe/"},{"categories":["笔记"],"content":"追问 6：那能不能反过来，只用 redo log，不要 binlog？ 回答：如果只从崩溃恢复的角度来讲是可以的。你可以把 binlog 关掉，这样就没有两阶段提交了，但系统依然是 crash-safe 的。 但是，如果你了解一下业界各个公司的使用场景的话，就会发现在正式的生产库上，binlog 都是开着的。因为 binlog 有着 redo log 无法替代的功能。 一个是归档。redo log 是循环写，写到末尾是要回到开头继续写的。这样历史日志没法保留，redo log 也就起不到归档的作用。 一个就是 MySQL 系统依赖于 binlog。binlog 作为 MySQL 一开始就有的功能，被用在了很多地方。其中，MySQL 系统高可用的基础，就是 binlog 复制。 ","date":"2020-12-28","objectID":"/posts/mysql_%E6%97%A5%E5%BF%97%E5%92%8Ccrash_safe/:9:0","tags":["mysql"],"title":"mysq 日志和crash-safe","uri":"/posts/mysql_%E6%97%A5%E5%BF%97%E5%92%8Ccrash_safe/"},{"categories":["笔记"],"content":"追问 9：redo log buffer 是什么？是先修改内存，还是先写 redo log 文件？ 回答：这两个问题可以一起回答。 在一个事务的更新过程中，日志是要写多次的。比如下面这个事务： begin; insert into t1 ... insert into t2 ... commit; 这个事务要往两个表中插入记录，插入数据的过程中，生成的日志都得先保存起来，但又不能在还没 commit 的时候就直接写到 redo log 文件里。 所以，redo log buffer 就是一块内存，用来先存 redo 日志的。也就是说，在执行第一个 insert 的时候，数据的内存被修改了，redo log buffer 也写入了日志。 但是，真正把日志写到 redo log 文件（文件名是 ib_logfile+ 数字），是在执行 commit 语句的时候做的。 扩展 在MySQL内部，在事务提交时利用两阶段提交(内部XA的两阶段提交)很好地解决了上面提到的binlog和redo log的一致性问题： 第一阶段： InnoDB Prepare阶段。此时SQL已经成功执行，并生成事务ID(xid)信息及redo和undo的内存日志。此阶段InnoDB会写事务的redo log，但要注意的是，此时redo log只是记录了事务的所有操作日志，并没有记录提交（commit）日志，因此事务此时的状态为Prepare。此阶段对binlog不会有任何操作。 第二阶段：commit 阶段，这个阶段又分成两个步骤。第一步写binlog（先调用write()将binlog内存日志数据写入文件系统缓存，再调用fsync()将binlog文件系统缓存日志数据永久写入磁盘）；第二步完成事务的提交（commit），此时在redo log中记录此事务的提交日志（增加commit 标签）。 可以看出，此过程中是先写redo log再写binlog的。但需要注意的是，在第一阶段并没有记录完整的redo log（不包含事务的commit标签），而是在第二阶段记录完binlog后再写入redo log的commit 标签。还要注意的是，在这个过程中是以第二阶段中binlog的写入与否作为事务是否成功提交的标志。 ","date":"2020-12-28","objectID":"/posts/mysql_%E6%97%A5%E5%BF%97%E5%92%8Ccrash_safe/:10:0","tags":["mysql"],"title":"mysq 日志和crash-safe","uri":"/posts/mysql_%E6%97%A5%E5%BF%97%E5%92%8Ccrash_safe/"},{"categories":["笔记"],"content":"mysql ACID实现原理","date":"2020-12-25","objectID":"/posts/mysql_acid%E5%AE%9E%E7%8E%B0/","tags":["mysql"],"title":"mysql ACID实现原理","uri":"/posts/mysql_acid%E5%AE%9E%E7%8E%B0/"},{"categories":["笔记"],"content":"事务 典型的MySQL事务是如下操作的： start transaction; # 一条或多条sql语句 commit; 其中start transaction标识事务开始，commit提交事务，将执行结果写入到数据库。如果sql语句执行出现问题，会调用rollback，回滚所有已经执行成功的sql语句。当然，也可以在事务中直接使用rollback语句进行回滚。 MySQL中默认采用的是自动提交（autocommit）模式。在自动提交模式下，如果没有start transaction显式地开始一个事务，那么每个sql语句都会被当做一个事务执行提交操作。 针对某次连接也可以临时指定不自动提交，set autocommit =0即可。 特殊操作： 在MySQL中，存在一些特殊的命令，如果在事务中执行了这些命令，会马上强制执行commit提交事务；如DDL语句(create table/drop table/alter/table)、lock tables语句等等。 不过，常用的select、insert、update和delete命令，都不会强制提交事务。 ACID特性 ACID是衡量事务的四个特性： 原子性（Atomicity，或称不可分割性） 一致性（Consistency） 隔离性（Isolation） 持久性（Durability） 按照严格的标准，只有同时满足ACID特性才是事务；但是在各大数据库厂商的实现中，真正满足ACID的事务少之又少。因此与其说ACID是事务必须满足的条件，不如说它们是衡量事务的四个维度。 原子性 实现原理：undo log。 MySQL的日志有很多种，如二进制日志、错误日志、查询日志、慢查询日志等，此外InnoDB存储引擎还提供了两种事务日志：redo log(重做日志)和undo log(回滚日志)。其中redo log用于保证事务持久性；undo log则是事务原子性和隔离性实现的基础。undo log属于逻辑日志，它记录的是sql执行相关的信息。 实现原子性的关键，是当事务回滚时能够撤销所有已经成功执行的sql语句。InnoDB实现回滚，靠的是undo log：当事务对数据库进行修改时，InnoDB会生成对应的undo log；如果事务执行失败或调用了rollback，导致事务需要回滚，便可以利用undo log中的信息将数据回滚到修改之前的样子。 持久性 持久性是指事务一旦提交，它对数据库的改变就应该是永久性的。接下来的其他操作或故障不应该对其有任何影响。 实现原理：redo log。 redo log和undo log都属于InnoDB特有的事务日志。redo log是物理日志。 ","date":"2020-12-25","objectID":"/posts/mysql_acid%E5%AE%9E%E7%8E%B0/:0:0","tags":["mysql"],"title":"mysql ACID实现原理","uri":"/posts/mysql_acid%E5%AE%9E%E7%8E%B0/"},{"categories":["笔记"],"content":"redo log的来历 InnoDB作为MySQL的存储引擎，数据是存放在磁盘中的，但如果每次读写数据都需要磁盘IO，效率会很低。为此，InnoDB提供了缓存(Buffer Pool)，Buffer Pool中包含了磁盘中部分数据页的映射，作为访问数据库的缓冲：当从数据库读取数据时，会首先从Buffer Pool中读取，如果Buffer Pool中没有，则从磁盘读取后放入Buffer Pool；当向数据库写入数据时，会首先写入Buffer Pool，Buffer Pool中修改的数据会定期刷新到磁盘中（这一过程称为刷脏）。 Buffer Pool的使用大大提高了读写数据的效率，但是也带了新的问题：如果MySQL宕机，而此时Buffer Pool中修改的数据还没有刷新到磁盘，就会导致数据的丢失，事务的持久性无法保证。于是，redo log被引入来解决这个问题：当数据修改时，除了修改Buffer Pool中的数据，还会在redo log记录这次操作；当事务提交时，会调用fsync接口对redo log进行刷盘。如果MySQL宕机，重启时可以读取redo log中的数据，对数据库进行恢复。redo log采用的是WAL（Write-ahead logging，预写式日志），所有修改先写入日志，再更新到Buffer Pool，保证了数据不会因MySQL宕机而丢失，从而满足了持久性要求。 ","date":"2020-12-25","objectID":"/posts/mysql_acid%E5%AE%9E%E7%8E%B0/:1:0","tags":["mysql"],"title":"mysql ACID实现原理","uri":"/posts/mysql_acid%E5%AE%9E%E7%8E%B0/"},{"categories":["笔记"],"content":"为什么binlog不能做到crash-safe 简单来说一句话：binlog可能和实际的库不一致。 假如只有binlog，有可能先提交事务再写binlog，有可能事务提交数据更新之后数据库崩了，还没来得及写binlog。我们都知道binlog一般用来做数据库的主从复制或恢复数据库，这样就导致主从数据库不一致或者无法恢复数据库了。同样即使先写binlog再提交事务更新数据库，还是有可能写binlog成功之后数据库崩掉而导致数据库更新失败，这样也会导致主从数据库不一致或者无法恢复数据库。所以只有binlog做不到crash-safe。为了支持crash-safe，需要redolog，而且为了保证逻辑一致，事务提交需要两个阶段：prepare阶段和commit阶段。写redolog并落入磁盘(prepare状态)–\u003e写binlog–\u003ecommit。commit的时候是不会落盘的。 如果binlog写成功之后，将redolog置成commit的时候数据库崩了，如果在commit的时候redolog才落盘，由于事务是否成功以binlog为依据，上面的情况下事务是成功的，但是redolog没有写到磁盘，丢了。恢复之后数据库与binlog就不一致了。如果在prepare阶段落盘，上面的情况下redolog已经写入到文件了（在prepare阶段已经写盘了），恢复的时候不会丢数据。 同样的，如果不分两个阶段。假如redolog和binlog独立，那么还是会出现“为什么binlog不能做到crash-safe”里面描述的问题：数据库与binlog不一致。 ","date":"2020-12-25","objectID":"/posts/mysql_acid%E5%AE%9E%E7%8E%B0/:2:0","tags":["mysql"],"title":"mysql ACID实现原理","uri":"/posts/mysql_acid%E5%AE%9E%E7%8E%B0/"},{"categories":["笔记"],"content":"为什么有了redo log可以做到crash-safe 先写redo log日志在改库,再加上二阶段。 ","date":"2020-12-25","objectID":"/posts/mysql_acid%E5%AE%9E%E7%8E%B0/:3:0","tags":["mysql"],"title":"mysql ACID实现原理","uri":"/posts/mysql_acid%E5%AE%9E%E7%8E%B0/"},{"categories":["笔记"],"content":"redo log与binlog 我们知道，在MySQL中还存在binlog(二进制日志)也可以记录写操作并用于数据的恢复，但二者是有着根本的不同的： （1）作用不同：redo log是用于crash recovery的，保证MySQL宕机也不会影响持久性；binlog是用于point-in-time recovery的，保证服务器可以基于时间点恢复数据，此外binlog还用于主从复制。 （2）层次不同：redo log是InnoDB存储引擎实现的，而binlog是MySQL的服务器层(可以参考文章前面对MySQL逻辑架构的介绍)实现的，同时支持InnoDB和其他存储引擎。 （3）内容不同：redo log是物理日志，内容基于磁盘的Page；binlog的内容是二进制的，根据binlog_format参数的不同，可能基于sql语句、基于数据本身或者二者的混合。 （4）写入时机不同：binlog在事务提交时写入；redo log的写入时机相对多元： 当事务提交时会调用fsync对redo log进行刷盘；这是默认情况下的策略，修改innodb_flush_log_at_trx_commit参数可以改变该策略，但事务的持久性将无法保证。 除了事务提交时，还有其他刷盘时机：如master thread每秒刷盘一次redo log等，这样的好处是不一定要等到commit时刷盘，commit速度大大加快。 innodb_flush_log_at_trx_commit=0：每秒一次将Log Buffer中数据写入到Log File中，并且Flush到磁盘。事务提交不会主动触发写磁盘操作。 innodb_flush_log_at_trx_commit=1：每次事务提交时将Log Buffer数据写入到Log File中，并且Flush到磁盘。 innodb_flush_log_at_trx_commit=2：每次事务提交时将Log Buffer数据写入到Log File中，但不立即Flush到磁盘，MySQL会每秒一次刷新到磁盘。 由于进程调度问题，每条一次操作不能保证每一秒都执行一次。 sync_binlog=0：每次事务提交后，将Binlog Cache中的数据写入到Binlog文件，但不立即刷新到磁盘。由文件系统(file system)决定何时刷新到磁盘中。 sync_binlog=N：每N次事务提交后，将Binlog Cache中的数据写入到Binlog文件,调用fdatasync()函数将数据刷新到磁盘中。 隔离性 隔离性研究的是不同事务之间的相互影响。隔离性是指，事务内部的操作与其他事务是隔离的，并发执行的各个事务之间不能互相干扰。严格的隔离性，对应了事务隔离级别中的Serializable (可串行化)，但实际应用中出于性能方面的考虑很少会使用可串行化。 隔离性追求的是并发情形下事务之间互不干扰。简单起见，我们主要考虑最简单的读操作和写操作(加锁读等特殊读操作会特殊说明)，那么隔离性的探讨，主要可以分为两个方面： (一个事务)写操作对(另一个事务)写操作的影响：锁机制保证隔离性 (一个事务)写操作对(另一个事务)读操作的影响：MVCC保证隔离性 不过需要说明的是，RR虽然避免了幻读问题，但是毕竟不是Serializable，不能保证完全的隔离: 例子，如果在事务中第一次读取采用非加锁读，第二次读取采用加锁读，则如果在两次读取之间数据发生了变化，两次读取到的结果不一样，因为加锁读时不会采用MVCC。 一致性 一致性是指事务执行结束后，数据库的完整性约束没有被破坏，事务执行的前后都是合法的数据状态。 可以说，一致性是事务追求的最终目标：前面提到的原子性、持久性和隔离性，都是为了保证数据库状态的一致性。 总结 原子性A实现原理：undo log。 持久性D实现原理：redo log。 隔离性I实现原理：mvcc + 锁。mvcc是解决读的隔离性，可以不用锁，更快。但是写写之间还是要靠排他锁。 ","date":"2020-12-25","objectID":"/posts/mysql_acid%E5%AE%9E%E7%8E%B0/:4:0","tags":["mysql"],"title":"mysql ACID实现原理","uri":"/posts/mysql_acid%E5%AE%9E%E7%8E%B0/"},{"categories":["笔记"],"content":"mysql 各隔离级别实现","date":"2020-12-25","objectID":"/posts/mysql_%E5%90%84%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB%E5%AE%9E%E7%8E%B0/","tags":["mysql"],"title":"mysql 各隔离级别实现","uri":"/posts/mysql_%E5%90%84%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB%E5%AE%9E%E7%8E%B0/"},{"categories":["笔记"],"content":"什么时候会加锁？ 在数据库增删改查四种操作中，insert、delete和update都是会加排它锁(Exclusive Locks)的，而select只有显式声明才会加锁: select: 即最常用的查询，是不加任何锁的 select … lock in share mode: 会加共享锁(Shared Locks) select … for update: 会加排它锁 READ UNCOMMITTED 顾名思义，事务之间可以读取彼此未提交的数据。机智如你会记得，在前文有说到所有写操作都会加排它锁，那还怎么读未提交呢？ 机智如你，前面我们介绍排它锁的时候，有这种说明： 排他锁会阻止其它事务再对其锁定的数据加读或写的锁，但是对不加锁的读就不起作用了。 READ UNCOMMITTED隔离级别下, 读不会加任何锁。而写会加排他锁，并到事务结束之后释放。 READ COMMITTED 顾名思义，事务之间可以读取彼此已提交的数据。 InnoDB在该隔离级别(READ COMMITTED)写数据时，使用排它锁, 读取数据不加锁而是使用了MVCC机制。 因此，在读已提交的级别下，都会通过MVCC获取当前数据的最新快照，不加任何锁，也无视任何锁(因为历史数据是构造出来的，身上不可能有锁)。 但是，该级别下还是遗留了不可重复读和幻读问题： MVCC版本的生成时机: 是每次select时。这就意味着，如果我们在事务A中执行多次的select，在每次select之间有其他事务更新了我们读取的数据并提交了，那就出现了不可重复读 REPEATABLE READ mysql在可重复读隔离级别实现了可重复读靠两种技术：mvcc + next lock ","date":"2020-12-25","objectID":"/posts/mysql_%E5%90%84%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB%E5%AE%9E%E7%8E%B0/:0:0","tags":["mysql"],"title":"mysql 各隔离级别实现","uri":"/posts/mysql_%E5%90%84%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB%E5%AE%9E%E7%8E%B0/"},{"categories":["笔记"],"content":"多版本并发控制（MVCC）（快照读/一致性读） 多数数据库都实现了多版本并发控制，并且都是靠保存数据快照来实现的。以 InnoDB 为例，每一行中都冗余了两个字断。 一个是行的创建版本，一个是行的删除（过期）版本。具体的版本号（trx_id）存在 information_schema.INNODB_TRX 表中。版本号（trx_id）随着每次事务的开启自增。 事务每次取数据的时候都会取创建版本小于当前事务版本的数据，以及过期版本大于当前版本的数据。 普通的 select 就是快照读。 select * from T where number = 1; 原理：将历史数据存一份快照，所以其他事务增加与删除数据，对于当前事务来说是不可见的。 ","date":"2020-12-25","objectID":"/posts/mysql_%E5%90%84%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB%E5%AE%9E%E7%8E%B0/:1:0","tags":["mysql"],"title":"mysql 各隔离级别实现","uri":"/posts/mysql_%E5%90%84%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB%E5%AE%9E%E7%8E%B0/"},{"categories":["笔记"],"content":"next-key 锁 （当前读） next-key 锁包含两部分： 记录锁（行锁） 间隙锁 记录锁是加在索引上的锁，间隙锁是加在索引之间的。 select * from T where number = 1 for update; select * from T where number = 1 lock in share mode; 原理：将当前数据行与上一条数据和下一条数据之间的间隙锁定，保证此范围内读取的数据是一致的。 ","date":"2020-12-25","objectID":"/posts/mysql_%E5%90%84%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB%E5%AE%9E%E7%8E%B0/:2:0","tags":["mysql"],"title":"mysql 各隔离级别实现","uri":"/posts/mysql_%E5%90%84%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB%E5%AE%9E%E7%8E%B0/"},{"categories":["笔记"],"content":"go 类型直接比较","date":"2020-12-08","objectID":"/posts/go_%E7%B1%BB%E5%9E%8B%E7%9B%B4%E6%8E%A5%E6%AF%94%E8%BE%83/","tags":["golang"],"title":"go 类型直接比较","uri":"/posts/go_%E7%B1%BB%E5%9E%8B%E7%9B%B4%E6%8E%A5%E6%AF%94%E8%BE%83/"},{"categories":["笔记"],"content":"结构体能否比较？数组能否比较？slice、map、channel能否比较？接口能否比较？函数能否比较？ 结构体比较 能用\"==“直接比较的我们才称其为能比较。 类型不同的都不能直接比较，类型别名可以。不同类型可以先强转再比较。 相同类型时，字段都是可比较类型才能直接比较。 type T2 struct { Name string Age int } type T22 T2 type T222 = T2 type T3 struct { Name string Age int } func main() { t2 := T2{} t22 := T22{} t222 := T222{} t3 := T3{} fmt.Println(t2==t222) // =起别名能比较 fmt.Println(t2==t22) // 类型定义不能比较 fmt.Println(t2==t3) // 不同类型，哪怕字段完全相同不能比较 t33:=T3(t2) fmt.Println(t3==t33) // 不同类型，可以先强转再比较 } 结构体字段顺序、类型、字段名完全相同才可以强转。 数组比较 数组可以直接比较。前提是数组长度和类型相同，而且数组里面的元素是可以直接比较的。 func main() { arr1:=[2]int{} arr2:=[2]int{} fmt.Println(arr1==arr2) // 可以比较 arr11:=[2][]int{} arr22:=[2][]int{} fmt.Println(arr11==arr22) // invalid operation: arr11 == arr22 ([2][]int cannot be compared) } channel比较 channel可以直接比较，比较的是channel的地址 func main() { c1:=make(chan int) c2:=make(chan int) c11:=make(chan []int) c22:=make(chan []int) fmt.Println(c1==c2) // false fmt.Println(c11==c22) // false } slice、map比较 slice不能直接比较。slice之间之所以不能进行比较，是因为slice的元素是间接引用的。slice引用的底层数组的元素随时可能会被修改，即slice在不同的时间可能包含不同的值，所以无法进行比较。 map不能直接比较。 接口比较 接口类型的变量，包含该接口变量存储的值和值的类型两部分组成，分别称为接口的动态类型和动态值。只有动态类型和动态值都相同时，两个接口变量才相同 type Person interface { getName() string } type Student struct { Name string } type Teacher struct { Name string } func (s Student) getName() string { return s.Name } func (t Teacher) getName() string { return t.Name } func compare(s, t Person) bool { return s == t } func main() { s1 := Student{\"minping\"} s2 := Student{\"minping\"} t := Teacher{\"minping\"} fmt.Println(compare(s1, s2)) //true fmt.Println(compare(s1, t)) //false,类型不同 } 接口的动态类型必须要是可比较的，如果不能比较(比如slice，map)，则运行时会报panic。因为编译器在编译时无法获取接口的动态类型，所以编译能通过，但是运行时直接panic 函数比较 golang的func作为一等公民，也是一种类型，而且不可比较 type GetName func () type GetName2 func () func main() { var f1 GetName var f2 GetName fmt.Println(f1==f2) // invalid operation: f1 == f2 (func can only be compared to nil) } 总结 1.结构体等复合类型，只有每个元素(成员)可比较且相同类型时能比较。类型和值都相等时，两个复合元素才相等 2.slice，map不可比较，但是可以用reflect或者cmp包来比较 3.func作为golnag的一等公民，也是一个类型，也不能比较。 4.channel引用类型的比较是看指向的是不是同一个变量 5.类型再定义(type A string)不可比较，是两种不同的类型 6.类型别名(type A = string)可比较，是同一种类型。 7.reflect.DeepEqual函数可以用来比较两个任意类型的变量 ","date":"2020-12-08","objectID":"/posts/go_%E7%B1%BB%E5%9E%8B%E7%9B%B4%E6%8E%A5%E6%AF%94%E8%BE%83/:0:0","tags":["golang"],"title":"go 类型直接比较","uri":"/posts/go_%E7%B1%BB%E5%9E%8B%E7%9B%B4%E6%8E%A5%E6%AF%94%E8%BE%83/"},{"categories":["笔记"],"content":"curl","date":"2020-11-11","objectID":"/posts/linux_curl/","tags":["linux"],"title":"curl笔记","uri":"/posts/linux_curl/"},{"categories":["笔记"],"content":"快速入门 curl 是常用的命令行工具，用来请求 Web 服务器。它的名字就是客户端（client）的 URL 工具的意思. -i参数可以显示http response的头信息，连同网页代码一起。 curl -i www.sina.com -v参数可以显示一次http通信的整个过程，包括端口连接和http request头信息。 HTTP动词 curl默认的HTTP动词是GET，使用-X参数可以支持其他动词。 curl -X POST www.example.com cookie 使用--cookie参数，可以让curl发送cookie。 curl --cookie \"name=xxx\" www.example.com -c cookie-file可以保存服务器返回的cookie到文件，-b cookie-file可以使用这个文件作为cookie信息，进行后续的请求 curl -c cookies http://example.com curl -b cookies http://example.com 增加头信息 有时需要在http request之中，自行增加一个头信息。--header参数就可以起到这个作用。也可以缩写为-H curl --header \"Content-Type:application/json\" http://example.com User-Agent -A参数指定客户端的用户代理标头，即User-Agent。curl 的默认用户代理字符串是curl/[version]。 curl -A 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/76.0.3809.100 Safari/537.36' https://google.com 也可以直接指定头字段 curl -H 'User-Agent: php/1.0' https://google.com -d -d参数用于发送 POST 请求的数据体。也就是–data的缩写 $ curl -d'login=emma＆password=123'-X POST https://google.com/login # 或者 $ curl -d 'login=emma' -d 'password=123' -X POST https://google.com/login 使用-d参数以后，HTTP 请求会自动加上标头Content-Type : application/x-www-form-urlencoded。并且会自动将请求转为 POST 方法，因此可以省略-X POST。 -d参数可以读取本地文本文件的数据，向服务器发送。 curl -d '@data.txt' https://google.com/login –data-urlencode参数等同于-d，发送 POST 请求的数据体，区别在于会自动将发送的数据进行 URL 编码。 -F 上传二进制文件。 -F参数用来向服务器上传二进制文件。 curl -F 'file=@photo.png' https://google.com/profile 上面命令会给 HTTP 请求加上标头Content-Type: multipart/form-data，然后将文件photo.png作为file字段上传。 -F参数可以指定 MIME 类型。 curl -F 'file=@photo.png;type=image/png' https://google.com/profile 上面命令指定 MIME 类型为image/png，否则 curl 会把 MIME 类型设为application/octet-stream。 -F参数也可以指定文件名。 -k参数指定跳过 SSL 检测 不会检查服务器的 SSL 证书是否正确。 curl -k https://www.example.com –limit-rate限制速度 –limit-rate用来限制 HTTP 请求和回应的带宽，模拟慢网速的环境。 curl --limit-rate 200k https://google.com 限制在每秒 200K 字节。 -o保存响应 -o参数将服务器的回应保存成文件，等同于wget命令。 curl -o example.html https://www.example.com ","date":"2020-11-11","objectID":"/posts/linux_curl/:0:0","tags":["linux"],"title":"curl笔记","uri":"/posts/linux_curl/"},{"categories":["笔记"],"content":"go test","date":"2020-06-20","objectID":"/posts/go_test/","tags":["golang"],"title":"go test","uri":"/posts/go_test/"},{"categories":["笔记"],"content":"概述 我们可以为 Go 程序编写三类测试，即：功能测试（test）、基准测试（benchmark，也称性能测试），以及示例测试（example） 测试源码文件的主名称应该以被测源码文件的主名称为前导，并且必须以“_test”为后缀。例如，如果被测源码文件的名称为 demo52.go，那么针对它的测试源码文件的名称就应该是 demo52_test.go。 Go 语言对测试函数的名称和签名都有哪些规定？ 对于功能测试函数来说，其名称必须以Test为前缀，并且参数列表中只应有一个*testing.T类型的参数声明。 对于性能测试函数来说，其名称必须以Benchmark为前缀，并且唯一参数的类型必须是*testing.B类型的。 对于示例测试函数来说，其名称必须以Example为前缀，但对函数的参数列表没有强制规定。 测试流程 go test命令就会针对每个被测代码包，依次地进行构建、执行包中符合要求的测试函数，清理临时文件，打印测试结果。这就是通常情况下的主要测试流程。 输入了go test puzzlers/article20/q2，这表示我想对导入路径为puzzlers/article20/q2的代码包进行测试。 功能测试 t.Log方法以及t.Logf方法的作用，就是打印常规的测试日志，只不过当测试成功的时候，go test命令就不会打印这类日志了。如果你想在测试结果中看到所有的常规测试日志，那么可以在运行go test命令的时候加入标记-v。 若我们想让某个测试函数在执行的过程中立即失败，则可以在该函数中调用t.FailNow方法。我在下面把TestFail函数中的t.Fail()改为t.FailNow()。与t.Fail()不同，在t.FailNow()执行之后，当前函数会立即终止执行。 如果你想在测试失败的同时打印失败测试日志，那么可以直接调用t.Error方法或者t.Errorf方法。前者相当于t.Log方法和t.Fail方法的连续调用 性能测试 在运行go test命令的时候加了两个标记。 第一个标记及其值为-bench=.，只有有了这个标记，命令才会进行性能测试。该标记的值.表明需要执行任意名称的性能测试函数，当然了，函数名称还是要符合 Go 程序测试的基本规则的。 第二个标记及其值是-run=^$，这个标记用于表明需要执行哪些功能测试函数，这同样也是以函数名称为依据的。该标记的值^$意味着：只执行名称为空的功能测试函数，换句话说，不执行任何功能测试函数。 如果运行go test命令的时候不加-run标记，那么就会使它执行被测代码包中的所有功能测试函数。(可是我们现在不想测试功能测试，只想性能测试) $ go test -bench=. -run=^$ puzzlers/article20/q3 goos: darwin goarch: amd64 pkg: puzzlers/article20/q3 BenchmarkGetPrimes-8 500000 2314 ns/op PASS ok puzzlers/article20/q3 1.192s ","date":"2020-06-20","objectID":"/posts/go_test/:0:0","tags":["golang"],"title":"go test","uri":"/posts/go_test/"},{"categories":["笔记"],"content":"核心数 BenchmarkGetPrimes-8被称为单个性能测试的名称，它表示命令执行了性能测试函数BenchmarkGetPrimes，并且当时所用的最大 P 数量为8。 最大 P 数量相当于可以同时运行 goroutine 的逻辑 CPU 的最大个数。这里的逻辑 CPU，也可以被称为 CPU 核心，但它并不等同于计算机中真正的 CPU 核心，只是 Go 语言运行时系统内部的一个概念，代表着它同时运行 goroutine 的能力。 顺便说一句，一台计算机的 CPU 核心的个数，意味着它能在同一时刻执行多少条程序指令，代表着它并行处理程序指令的能力。 我们可以通过调用 runtime.GOMAXPROCS函数改变最大 P 数量，也可以在运行go test命令时，加入标记-cpu来设置一个最大 P 数量的列表，以供命令在多次测试时使用。 -cpu的值应该是一个正整数的列表,比如1,2,4。针对于此值中的每一个正整数，go test命令都会先设置最大 P 数量为该数，然后再执行测试函数。 go test命令会先以1,2,4为最大 P 数量分别去执行第一个测试函数，之后再用同样的方式执行第二个测试函数，以此类推。 ","date":"2020-06-20","objectID":"/posts/go_test/:1:0","tags":["golang"],"title":"go test","uri":"/posts/go_test/"},{"categories":["笔记"],"content":"执行次数 它指的是被测函数的执行次数，而不是性能测试函数的执行次数。 go test命令在执行性能测试函数的时候会给它一个正整数，若该测试函数的唯一参数的名称为b，则该正整数就由b.N代表。我们应该在测试函数中配合着编写代码，比如： for i := 0; i \u003c b.N; i++ { GetPrimes() } go test命令会先尝试把b.N设置为1，然后执行测试函数。如果测试函数的执行时间没有超过上限，此上限默认为 1 秒，那么命令就会改大b.N的值，然后再次执行测试函数，如此往复，直到这个时间大于或等于上限为止。 当某次执行的时间大于或等于上限时，我们就说这是命令此次对该测试函数的最后一次执行。这时的b.N的值就会被包含在测试结果中，也就是上述测试结果中的500000。 参数 go test 后面可以跟参数： -cpu p的数量，可以是个列表，如果不设置默认是机器核心数 -count标记是专门用于重复执行测试函数的。它的值必须大于或等于0，并且默认值为1。 如果我们在运行go test命令的时候追加了-count 5，那么对于每一个测试函数，命令都会在预设的不同条件下（比如不同的最大 P 数量下）分别重复执行五次。 注意：这是测试函数的执行数量，不是被测试函数 -benchmem 输出基准测试的内存分配统计信息。 -benchtime 用于指定基准测试的探索式测试执行时间上限 -coverprofile=xxxx.out 输出覆盖率的out文件，使用go tool cover -html=xxxx.out 命令转换成Html的覆盖率测试报告。 覆盖率测试将被测试的代码拷贝一份，在每个语句块中加入bool标识变量，测试结束后统计覆盖率并输出成out文件，因此性能上会有一定的影响。 ","date":"2020-06-20","objectID":"/posts/go_test/:2:0","tags":["golang"],"title":"go test","uri":"/posts/go_test/"},{"categories":["笔记"],"content":"Redis学习-lru和redis实现","date":"2020-06-16","objectID":"/posts/redis_lru%E5%92%8Credis%E5%AE%9E%E7%8E%B0/","tags":["redis"],"title":"Redis学习-lru和redis实现","uri":"/posts/redis_lru%E5%92%8Credis%E5%AE%9E%E7%8E%B0/"},{"categories":["笔记"],"content":"lru实现 数组实现 用一个数组来存储数据，给每一个数据项标记一个访问时间戳，每次插入新数据项的时候，先把数组中存在的数据项的时间戳自增，并将新数据项的时间戳置为0并插入到数组中。每次访问数组中的数据项的时候，将被访问的数据项的时间戳置为0。当数组空间已满时，将时间戳最大的数据项淘汰。 链表实现 利用一个链表来实现，每次新插入数据的时候将新数据插到链表的头部；每次缓存命中（即数据被访问），则将数据移到链表头部；那么当链表满的时候，就将链表尾部的数据丢弃。 hashmap + 链表 整体的设计思路是，可以使用 HashMap 存储 key，这样可以做到 save 和 get key的时间都是 O(1)，而 HashMap 的 Value 指向双向链表实现的 LRU 的 Node 节点， 其中 head 代表双向链表的表头，tail 代表尾部。首先预先设置 LRU 的容量，如果存储满了，可以通过 O(1) 的时间淘汰掉双向链表的尾部，每次新增和访问数据，都可以通过 O(1)的效率把新的节点增加到对头，或者把已经存在的节点移动到队头。 ","date":"2020-06-16","objectID":"/posts/redis_lru%E5%92%8Credis%E5%AE%9E%E7%8E%B0/:0:1","tags":["redis"],"title":"Redis学习-lru和redis实现","uri":"/posts/redis_lru%E5%92%8Credis%E5%AE%9E%E7%8E%B0/"},{"categories":["笔记"],"content":"Redis的LRU实现 redis服务器实际使用的是惰性删除和定期删除两种策略：通过配合使用这两种删除策略，服务器可很好的使用CPU的时间和避免浪费内存空间之间取得平衡。 如果按照HashMap和双向链表实现，需要额外的存储存放 next 和 prev 指针，牺牲比较大的存储空间，显然是不划算的。所以Redis采用了一个近似的做法，就是随机取出若干个key，然后按照访问时间排序后，淘汰掉最不经常使用的. redis中有很多数据类型（以后会出一个redis系列），为了实现key-value新老判断，不能像上面算法题中简单的链表就能实现. Redis采用了一个全局时钟在redisServer这个struct中的lruclock，这个时钟供每个object更新自己object的时间。其中存储了服务器自启动之后的lru时钟，该时钟是全局的lru时钟。 默认的LRU时钟的分辨率是1秒，可以通过改变REDIS_LRU_CLOCK_RESOLUTION宏的值来改变，Redis会在serverCron()中调用updateLRUClock定期的更新LRU时钟，更新的频率和hz参数有关，默认为100ms一次。 Redis最为一款优秀的内存数据库，用途非常广泛，其缓存代码设计和实现很值得学习，实现步骤主要有： 用一个全局时钟作为参照 对每个object初始化和操作的时候都更新它各自的lru时钟 随机挑选几个key，根据lru时钟计算idle的时间排序放入EvictionPool中，最终挑选idle时间最长的free，以释放空间。至于为什么随机和只选择5个，是为了性能考虑，如果做到全局一个一个排序就非常消耗CPU，而实际应用中没必要这么精确。 ","date":"2020-06-16","objectID":"/posts/redis_lru%E5%92%8Credis%E5%AE%9E%E7%8E%B0/:0:2","tags":["redis"],"title":"Redis学习-lru和redis实现","uri":"/posts/redis_lru%E5%92%8Credis%E5%AE%9E%E7%8E%B0/"},{"categories":["笔记"],"content":"redis lru配置 Redis配置中和LRU有关的有三个： maxmemory: 配置Redis存储数据时指定限制的内存大小，比如100m。当缓存消耗的内存超过这个数值时, 将触发数据淘汰。该数据配置为0时，表示缓存的数据量没有限制, 即LRU功能不生效。64位的系统默认值为0，32位的系统默认内存限制为3GB maxmemory_policy: 触发数据淘汰后的淘汰策略 maxmemory_samples: 随机采样的精度，也就是随即取出key的数目。该数值配置越大, 越接近于真实的LRU算法，但是数值越大，相应消耗也变高，对性能有一定影响，样本值默认为5。 ","date":"2020-06-16","objectID":"/posts/redis_lru%E5%92%8Credis%E5%AE%9E%E7%8E%B0/:0:3","tags":["redis"],"title":"Redis学习-lru和redis实现","uri":"/posts/redis_lru%E5%92%8Credis%E5%AE%9E%E7%8E%B0/"},{"categories":["笔记"],"content":"Redis学习-与数据库一致性问题","date":"2020-06-16","objectID":"/posts/redis_%E4%B8%8E%E6%95%B0%E6%8D%AE%E5%BA%93%E4%B8%80%E8%87%B4%E6%80%A7/","tags":["redis"],"title":"Redis学习-与数据库一致性问题","uri":"/posts/redis_%E4%B8%8E%E6%95%B0%E6%8D%AE%E5%BA%93%E4%B8%80%E8%87%B4%E6%80%A7/"},{"categories":["笔记"],"content":"背景 我们使用redis作为缓存，查询的时候先去redis查，如果有数据直接返回，降低数据库的压力。如果没有的话，查数据库，如果数据库中查到了数据则把数据写入到redis,并返回给调用方。 如果只是这样的读就没有啥问题了，如果是更新呢？ 先操作数据库？还是先操作缓存？ 操作缓存是直接删缓存还是更新缓存？ 是更新还是删除缓存 一般我们都是采取删除缓存缓存策略的，原因如下： 高并发环境下，无论是先操作数据库还是后操作数据库而言，如果加上更新缓存，那就更加容易导致数据库与缓存数据不一致问题。(删除缓存直接和简单很多) 如果每次更新了数据库，都要更新缓存【这里指的是频繁更新的场景，这会耗费一定的性能】，倒不如直接删除掉。等再次读取时，缓存里没有，那我到数据库找，在数据库找到再写到缓存里边 (体现懒加载) 先数据库还是先缓存 不管是先写MySQL数据库，再删除Redis缓存；还是先删除缓存，再写库，都有可能出现数据不一致的情况。举一个例子： 如果删除了缓存Redis，还没有来得及写库MySQL，另一个线程就来读取，发现缓存为空，则去数据库中读取数据写入缓存，此时缓存中为脏数据。 如果先写了库，在删除缓存前，写库的线程宕机了，没有删除掉缓存，则也会出现数据不一致情况。 缓存和数据库一致性解决方案 1.第一种方案：采用延时双删策略 在写库前后都进行redis.del(key)操作，并且设定合理的超时时间。 public void write( String key, Object data ) { redis.delKey( key ); db.updateData( data ); Thread.sleep( 500 ); redis.delKey( key ); } 2、第二种方案：异步更新缓存(基于订阅binlog的同步机制) 技术整体思路：MySQL binlog增量订阅消费+消息队列+增量数据更新到redis 读取binlog后分析 ，利用消息队列,推送更新各台的redis缓存数据。这样一旦MySQL中产生了新的写入、更新、删除等操作，就可以把binlog相关的消息推送至Redis，Redis再根据binlog中的记录，对Redis进行更新。其实这种机制，很类似MySQL的主从备份机制，因为MySQL的主备也是通过binlog来实现的数据一致性。 这里可以结合使用canal(阿里的一款开源框架)，通过该框架可以对MySQL的binlog进行订阅，而canal正是模仿了mysql的slave数据库的备份请求，使得Redis的数据更新达到了相同的效果。 ","date":"2020-06-16","objectID":"/posts/redis_%E4%B8%8E%E6%95%B0%E6%8D%AE%E5%BA%93%E4%B8%80%E8%87%B4%E6%80%A7/:0:0","tags":["redis"],"title":"Redis学习-与数据库一致性问题","uri":"/posts/redis_%E4%B8%8E%E6%95%B0%E6%8D%AE%E5%BA%93%E4%B8%80%E8%87%B4%E6%80%A7/"},{"categories":["笔记"],"content":"mysql分布式事务","date":"2020-06-15","objectID":"/posts/mysql_%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1/","tags":["mysql"],"title":"mysql分布式事务","uri":"/posts/mysql_%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1/"},{"categories":["笔记"],"content":"mysql的事务 ","date":"2020-06-15","objectID":"/posts/mysql_%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1/:0:0","tags":["mysql"],"title":"mysql分布式事务","uri":"/posts/mysql_%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1/"},{"categories":["笔记"],"content":"undolog 实现原子性A。 UndoLog的原理很简单，为了满足事务的原子性，在操作任何数据之前，首先将数据备份到一个地方（这个存储数据备份的地方称为UndoLog）。然后进行数据的修改。如果出现了错误或者用户执行了ROLLBACK语句，系统可以利用Undo Log中的备份将数据恢复到事务开始之前的状态。 ","date":"2020-06-15","objectID":"/posts/mysql_%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1/:1:0","tags":["mysql"],"title":"mysql分布式事务","uri":"/posts/mysql_%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1/"},{"categories":["笔记"],"content":"redolog 实现持久性D。 和Undo Log相反，RedoLog记录的是新数据的备份。在事务提交前，只要将RedoLog持久化即可，不需要将数据持久化。当系统崩溃时，虽然数据没有持久化，但是RedoLog已经持久化。系统可以根据RedoLog的内容，将所有数据恢复到最新的状态。 ","date":"2020-06-15","objectID":"/posts/mysql_%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1/:2:0","tags":["mysql"],"title":"mysql分布式事务","uri":"/posts/mysql_%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1/"},{"categories":["笔记"],"content":"mvcc+锁 mysql的隔离性I。 mvcc+锁来实现事务的隔离性，各个事务看到的是自己的快照。 ","date":"2020-06-15","objectID":"/posts/mysql_%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1/:3:0","tags":["mysql"],"title":"mysql分布式事务","uri":"/posts/mysql_%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1/"},{"categories":["笔记"],"content":"一致性 mysql的一致性是通过上面的原子性、隔离性、持久性来实现的。 分布式事务 分布式事务就是指事务的参与者、支持事务的服务器、资源服务器以及事务管理器分别位于不同的分布式系统的不同节点之上。简单的说，就是一次大的操作由不同的小操作组成，这些小的操作分布在不同的服务器上，且属于不同的应用，分布式事务需要保证这些小操作要么全部成功，要么全部失败。本质上来说，分布式事务就是为了保证不同数据库的数据一致性。 ","date":"2020-06-15","objectID":"/posts/mysql_%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1/:4:0","tags":["mysql"],"title":"mysql分布式事务","uri":"/posts/mysql_%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1/"},{"categories":["笔记"],"content":"CAP CAP定理，又被叫作布鲁尔定理。对于设计分布式系统来说(不仅仅是分布式事务)的架构师来说，CAP就是你的入门理论。 C (一致性):对某个指定的客户端来说，读操作能返回最新的写操作。对于数据分布在不同节点上的数据上来说，如果在某个节点更新了数据，那么在其他节点如果都能读取到这个最新的数据，那么就称为强一致，如果有某个节点没有读取到，那就是分布式不一致。 A (可用性)：也就是满足高并发。非故障的节点在合理的时间内返回合理的响应(不是错误和超时的响应)。可用性的两个关键一个是合理的时间，一个是合理的响应。合理的时间指的是请求不能无限被阻塞，应该在合理的时间给出返回。合理的响应指的是系统应该明确返回结果并且结果是正确的，这里的正确指的是比如应该返回50，而不是返回40。 P (分区容错性):当出现网络分区后，系统能够继续工作。打个比方，这里个集群有多台机器，有台机器网络出现了问题，但是这个集群仍然可以正常工作。 cap同时最多满足两个。试想，又要多个服务器，又要高并发，那只能牺牲一致性了。 ","date":"2020-06-15","objectID":"/posts/mysql_%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1/:5:0","tags":["mysql"],"title":"mysql分布式事务","uri":"/posts/mysql_%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1/"},{"categories":["笔记"],"content":"BASE base就是牺牲了一致性，满足ap，但是会最终一致性。 BASE 是 Basically Available(基本可用)、Soft state(软状态)和 Eventually consistent (最终一致性)三个短语的缩写。是对CAP中AP的一个扩展。 基本可用:分布式系统在出现故障时，允许损失部分可用功能，保证核心功能可用。 软状态:允许系统中存在中间状态，这个状态不影响系统可用性，这里指的是CAP中的不一致。 最终一致:最终一致是指经过一段时间后，所有节点数据都将会达到一致。 BASE解决了CAP中理论没有网络延迟，在BASE中用软状态和最终一致，保证了延迟后的一致性。BASE和 ACID 是相反的，它完全不同于ACID的强一致性模型，而是通过牺牲强一致性来获得可用性，并允许数据在一段时间内是不一致的，但最终达到一致状态。 ","date":"2020-06-15","objectID":"/posts/mysql_%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1/:6:0","tags":["mysql"],"title":"mysql分布式事务","uri":"/posts/mysql_%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1/"},{"categories":["笔记"],"content":"具体解决方案 分布式事务又很多具体的解决方案， ","date":"2020-06-15","objectID":"/posts/mysql_%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1/:7:0","tags":["mysql"],"title":"mysql分布式事务","uri":"/posts/mysql_%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1/"},{"categories":["笔记"],"content":"TCC 有TCC（Try-Confirm-Cancel）的概念； Try阶段：尝试执行,完成所有业务检查（一致性）,预留必须业务资源（准隔离性） Confirm阶段：确认执行真正执行业务，不作任何业务检查，只使用Try阶段预留的业务资源，Confirm操作满足幂等性。要求具备幂等设计，Confirm失败后需要进行重试。 Cancel阶段：取消执行，释放Try阶段预留的业务资源 Cancel操作满足幂等性Cancel阶段的异常和Confirm阶段异常处理方案基本上一致。 ","date":"2020-06-15","objectID":"/posts/mysql_%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1/:7:1","tags":["mysql"],"title":"mysql分布式事务","uri":"/posts/mysql_%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1/"},{"categories":["笔记"],"content":"2pc 2PC就不得不聊数据库分布式事务中的 XA Transactions。 在XA协议中分为两阶段: 第一阶段：事务管理器要求每个涉及到事务的数据库预提交(precommit)此操作，并反映是否可以提交. 第二阶段：事务协调器要求每个数据库提交数据，或者回滚数据。 优点： 尽量保证了数据的强一致，实现成本较低，在各大主流数据库都有自己实现，对于MySQL是从5.5开始支持。 缺点: 单点问题:事务管理器在整个流程中扮演的角色很关键，如果其宕机，比如在第一阶段已经完成，在第二阶段正准备提交的时候事务管理器宕机，资源管理器就会一直阻塞，导致数据库无法使用。 同步阻塞:在准备就绪之后，资源管理器中的资源一直处于阻塞，直到提交完成，释放资源。 数据不一致:两阶段提交协议虽然为分布式数据强一致性所设计，但仍然存在数据不一致性的可能，比如在第二阶段中，假设协调者发出了事务commit的通知，但是因为网络问题该通知仅被一部分参与者所收到并执行了commit操作，其余的参与者则因为没有收到通知一直处于阻塞状态，这时候就产生了数据的不一致性。 总的来说，XA协议比较简单，成本较低，但是其单点问题，以及不能支持高并发(由于同步阻塞)依然是其最大的弱点。 ","date":"2020-06-15","objectID":"/posts/mysql_%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1/:7:2","tags":["mysql"],"title":"mysql分布式事务","uri":"/posts/mysql_%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1/"},{"categories":["笔记"],"content":"本地消息表 本地消息表这个方案最初是ebay提出的 ebay的完整方案https://queue.acm.org/detail.cfm?id=1394128。 此方案的核心是将需要分布式处理的任务通过消息日志的方式来异步执行。消息日志可以存储到本地文本、数据库或消息队列，再通过业务规则自动或人工发起重试。人工重试更多的是应用于支付场景，通过对账系统对事后问题的处理。 1.当你扣钱的时候，你需要在你扣钱的服务器上新增加一个本地消息表，你需要把你扣钱和写入减去水的库存到本地消息表放入同一个事务(依靠数据库本地事务保证一致性。 2.这个时候有个定时任务去轮询这个本地事务表，把没有发送的消息，扔给商品库存服务器，叫他减去水的库存，到达商品服务器之后这个时候得先写入这个服务器的事务表，然后进行扣减，扣减成功后，更新事务表中的状态。 3.商品服务器通过定时任务扫描消息表或者直接通知扣钱服务器，扣钱服务器本地消息表进行状态更新。 4.针对一些异常情况，定时扫描未成功处理的消息，进行重新发送，在商品服务器接到消息之后，首先判断是否是重复的，如果已经接收，在判断是否执行，如果执行在马上又进行通知事务，如果未执行，需要重新执行需要由业务保证幂等，也就是不会多扣一瓶水。 怎么保证幂等性？ 我们可以把减库存+扣钱消息表的主键ID通过消息传到商品服务器，商品服务器去减库存，同时商品服务器也有一个处理历史表，把主键ID存进去，以后再来消息先判断历史记录表有没有处理过。 本地消息队列是BASE理论，是最终一致模型，适用于对一致性要求不高的。实现这个模型时需要注意重试的幂等。 参考资料： https://juejin.im/post/5b5a0bf9f265da0f6523913b ","date":"2020-06-15","objectID":"/posts/mysql_%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1/:8:0","tags":["mysql"],"title":"mysql分布式事务","uri":"/posts/mysql_%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1/"},{"categories":["笔记"],"content":"sql执行顺序","date":"2020-06-14","objectID":"/posts/mysql_sql%E6%89%A7%E8%A1%8C%E9%A1%BA%E5%BA%8F/","tags":["mysql"],"title":"sql执行顺序","uri":"/posts/mysql_sql%E6%89%A7%E8%A1%8C%E9%A1%BA%E5%BA%8F/"},{"categories":["笔记"],"content":"sql执行顺序 (1)from (3) join (2) on (4) where (5)group by(开始使用select中的别名，后面的语句中都可以使用) (6) avg,sum…. (7)having (8) select (9) distinct (10) order by (11) limit 从这个顺序中我们不难发现，所有的 查询语句都是从from开始执行的，在执行过程中，每个步骤都会为下一个步骤生成一个虚拟表，这个虚拟表将作为下一个执行步骤的输入。 第一步：首先对from子句中的前两个表执行一个笛卡尔乘积，此时生成虚拟表 vt1（选择相对小的表做基础表） 第二步：接下来便是应用on筛选器，on 中的逻辑表达式将应用到 vt1 中的各个行，筛选出满足on逻辑表达式的行，生成虚拟表 vt2 第三步：如果是outer join 那么这一步就将添加外部行，left outer jion 就把左表在第二步中过滤的添加进来，如果是right outer join 那么就将右表在第二步中过滤掉的行添加进来，这样生成虚拟表 vt3 第四步：如果 from 子句中的表数目多余两个表，那么就将vt3和第三个表连接从而计算笛卡尔乘积，生成虚拟表，该过程就是一个重复1-3的步骤，最终得到一个新的虚拟表 vt3。 第五步：应用where筛选器，对上一步生产的虚拟表引用where筛选器，生成虚拟表vt4，在这有个比较重要的细节不得不说一下，对于包含outer join子句的查询，就有一个让人感到困惑的问题，到底在on筛选器还是用where筛选器指定逻辑表达式呢？on和where的最大区别在于，如果在on应用逻辑表达式那么在第三步outer join中还可以把移除的行再次添加回来，而where的移除的最终的。举个简单的例子，有一个学生表（班级,姓名）和一个成绩表(姓名,成绩)，我现在需要返回一个x班级的全体同学的成绩，但是这个班级有几个学生缺考，也就是说在成绩表中没有记录。为了得到我们预期的结果我们就需要在on子句指定学生和成绩表的关系（学生.姓名=成绩.姓名）那么我们是否发现在执行第二步的时候，对于没有参加考试的学生记录就不会出现在vt2中，因为他们被on的逻辑表达式过滤掉了,但是我们用left outer join就可以把左表（学生）中没有参加考试的学生找回来，因为我们想返回的是x班级的所有学生，如果在on中应用学生.班级=‘x’的话，left outer join会把x班级的所有学生记录找回（感谢网友康钦谋__康钦苗的指正），所以只能在where筛选器中应用学生.班级=‘x’ 因为它的过滤是最终的。 第六步：group by 子句将中的唯一的值组合成为一组，得到虚拟表vt5。如果应用了group by，那么后面的所有步骤都只能得到的vt5的列或者是聚合函数（count、sum、avg等）。原因在于最终的结果集中只为每个组包含一行。这一点请牢记。 第七步：应用cube或者rollup选项，为vt5生成超组，生成vt6. 第八步：应用having筛选器，生成vt7。having筛选器是第一个也是为唯一一个应用到已分组数据的筛选器。 第九步：处理select子句。将vt7中的在select中出现的列筛选出来。生成vt8. 第十步：应用distinct子句，vt8中移除相同的行，生成vt9。事实上如果应用了group by子句那么distinct是多余的，原因同样在于，分组的时候是将列中唯一的值分成一组，同时只为每一组返回一行记录，那么所以的记录都将是不相同的。 第十一步：应用order by子句。按照order_by_condition排序vt9，此时返回的一个游标，而不是虚拟表。sql是基于集合的理论的，集合不会预先对他的行排序，它只是成员的逻辑集合，成员的顺序是无关紧要的。对表进行排序的查询可以返回一个对象，这个对象包含特定的物理顺序的逻辑组织。这个对象就叫游标。正因为返回值是游标，那么使用order by 子句查询不能应用于表表达式。排序是很需要成本的，除非你必须要排序，否则最好不要指定order by，最后，在这一步中是第一个也是唯一一个可以使用select列表中别名的步骤。 第十二步：应用top选项（limit）。此时才返回结果给请求者即用户。 ","date":"2020-06-14","objectID":"/posts/mysql_sql%E6%89%A7%E8%A1%8C%E9%A1%BA%E5%BA%8F/:0:0","tags":["mysql"],"title":"sql执行顺序","uri":"/posts/mysql_sql%E6%89%A7%E8%A1%8C%E9%A1%BA%E5%BA%8F/"},{"categories":["笔记"],"content":"protobuf v2","date":"2020-06-01","objectID":"/posts/protobuf_v2/","tags":["protobuf"],"title":"protobuf v2","uri":"/posts/protobuf_v2/"},{"categories":["笔记"],"content":"protobuf 是什么 Protocol Buffer (简称Protobuf) 是Google出品的性能优异、跨语言、跨平台的序列化库。 2001年初，Protobuf首先在Google内部创建，很多项目也采用Protobuf进行消息的通讯，还有基于Protobuf的微服务框架GRPC。 可以看做xml、json等序列化的又一种形式，只不过序列化后它是二进制的。 Protobuf支持很多语言，比如C++、C#、Dart、Go、Java、Python、Rust等，同时也是跨平台的。 序列化(serialization、marshalling)的过程是指将数据结构或者对象的状态转换成可以存储(比如文件、内存)或者传输的格式(比如网络)。反向操作就是反序列化(deserialization、unmarshalling)的过程。 ","date":"2020-06-01","objectID":"/posts/protobuf_v2/:1:0","tags":["protobuf"],"title":"protobuf v2","uri":"/posts/protobuf_v2/"},{"categories":["笔记"],"content":"protobuf 为什么要有 二十世纪九十年代后期，XML开始流行，它是一种人类易读的基于文本的编码方式，易于阅读和理解。 JSON是一种更轻量级的基于文本的编码方式，经常用在client/server端的通讯中。 除此之外还有很多序列化格式。 protobuf序列化和反序列化速度更快； 文件更小存储需要更少的空间，传输时间短。 ","date":"2020-06-01","objectID":"/posts/protobuf_v2/:2:0","tags":["protobuf"],"title":"protobuf v2","uri":"/posts/protobuf_v2/"},{"categories":["笔记"],"content":"protobuf 基础 这个教程主要介绍proto2的开发。 使用protobuf需要一个.proto文件，在这里定义要序列化的格式。可以理解为我们工作中和服务端定义的接口文档或者java bean。 举例： user.proto syntax = \"proto2\"; //生成java类所在的包名 package com.example.protobuftest.bean; message SearchRequest { required string query = 1; optional int32 page_number = 2; optional int32 result_per_page = 3; } 第一行指定protobuf的版本，可以指定为proto2或3。如果没有指定，默认以proto2格式定义。 在这里我们定义了一个User类型，包括name和age字段。 ","date":"2020-06-01","objectID":"/posts/protobuf_v2/:3:0","tags":["protobuf"],"title":"protobuf v2","uri":"/posts/protobuf_v2/"},{"categories":["笔记"],"content":"package package是可选的。对于生成的java语言对于Java，包声明符会变为java的一个包，除非在.proto文件中提供了一个明确有java_package； 如果不写package,默认是文件名作为包名。 写了包名后，就可以用包名加以区分。比如test.model.UserInfo。 显示设置包名后生成的对应语言文件就按照这个来生成包名了。 option java_package = \"test.protobuf.sample\"; option go_package = \"test.protobuf.sample\"; ","date":"2020-06-01","objectID":"/posts/protobuf_v2/:3:1","tags":["protobuf"],"title":"protobuf v2","uri":"/posts/protobuf_v2/"},{"categories":["笔记"],"content":"字段规则 所指定的消息字段修饰符必须是如下之一： required：一个格式良好的消息一定要含有1个这种字段。表示该值是必须要设置的； optional：消息格式中该字段可以有0个或1个值（不超过1个）。 repeated：在一个格式良好的消息中，这种字段可以重复任意多次（包括0次）。重复的值的顺序会被保留。表示该值可以重复，相当于java中的List。 required是永久性的：在将一个字段标识为required的时候，应该特别小心。如果在某些情况下不想写入或者发送一个required的字段，将原始该字段修饰符更改为optional可能会遇到问题——旧版本的使用者会认为不含该字段的消息是不完整的，从而可能会无目的的拒绝解析。 换句话说，字段设置了required，如果不设置就会序列化失败，同理，如果也会反序列化失败。 ","date":"2020-06-01","objectID":"/posts/protobuf_v2/:3:2","tags":["protobuf"],"title":"protobuf v2","uri":"/posts/protobuf_v2/"},{"categories":["笔记"],"content":"字段类型 .proto Type Notes Java Type Go Type double double float64 float float float32 int32 使用可变长度编码。编码负数的效率低 - 如果您的字段可能有负值，请改用sint32。 int int32 int64 使用可变长度编码。编码负数的效率低 - 如果您的字段可能有负值，请改用sint64。 long int64 uint32 使用可变长度编码 int uint32 uint64 使用可变长度编码. long uint64 sint32 使用可变长度编码。签名的int值。这些比常规int32更有效地编码负数。 int int32 sint64 使用可变长度编码。签名的int值。这些比常规int64更有效地编码负数。 long int64 fixed32 总是四个字节。如果值通常大于228，则比uint32更有效。 int uint32 fixed64 总是八个字节。如果值通常大于256，则比uint64更有效 long uint64 sfixed32 总是四个字节 int int32 sfixed64 总是八个字节 long int64 bool boolean bool string String string bytes 可以包含不超过232的任意字节序列。 String []byte ","date":"2020-06-01","objectID":"/posts/protobuf_v2/:3:3","tags":["protobuf"],"title":"protobuf v2","uri":"/posts/protobuf_v2/"},{"categories":["笔记"],"content":"标识号 正如上述文件格式，在消息定义中，每个字段都有唯一的一个数字标识符。这些标识符是用来在消息的二进制格式中识别各个字段的，一旦开始使用就不能够再改变。注：[1,15]之内的标识号在编码的时候会占用一个字节。[16,2047]之内的标识号则占用2个字节。所以应该为那些频繁出现的消息元素保留 [1,15]之内的标识号。切记：要为将来有可能添加的、频繁出现的标识号预留一些标识号。 最小的标识号可以从1开始，最大到2^29 - 1, or 536,870,911。不可以使用其中的[19000－19999]的标识号， Protobuf协议实现中对这些进行了预留。如果非要在.proto文件中使用这些预留标识号，编译时就会报警。 ","date":"2020-06-01","objectID":"/posts/protobuf_v2/:3:4","tags":["protobuf"],"title":"protobuf v2","uri":"/posts/protobuf_v2/"},{"categories":["笔记"],"content":"类型嵌套及导入 在一个.proto文件中可以定义多个消息类型,可以引用，也可以嵌套 message SearchResponse { repeated Result result = 1; } message Result { required string url = 1; optional string title = 2; repeated string snippets = 3; } 要导入其他.proto文件的定义，你需要在你的文件中添加一个导入声明，如： import \"myproject/other_protos.proto\"; 嵌套使用： message SearchResponse { message Result { required string url = 1; optional string title = 2; repeated string snippets = 3; } repeated Result result = 1; } 如果你想在它的父消息类型的外部重用这个消息类型，你需要以Parent.Type的形式使用它 Optional的字段和默认值 如上所述，消息描述中的一个元素可以被标记为“可选的”（optional）。一个格式良好的消息可以包含0个或一个optional的元素。当解 析消息时，如果它不包含optional的元素值，那么解析出来的对象中的对应字段就被置为默认值。默认值可以在消息描述文件中指定。 optional int32 result_per_page = 3 [default = 10]; 如果没有为optional的元素指定默认值，就会使用与特定类型相关的默认值：对string来说，默认值是空字符串。对bool来说，默认值是false。对数值类型来说，默认值是0。对枚举来说，默认值是枚举类型定义中的第一个值。 ","date":"2020-06-01","objectID":"/posts/protobuf_v2/:3:5","tags":["protobuf"],"title":"protobuf v2","uri":"/posts/protobuf_v2/"},{"categories":["笔记"],"content":"枚举 message SearchRequest { required string query = 1; optional int32 page_number = 2; optional int32 result_per_page = 3 [default = 10]; enum Corpus { UNIVERSAL = 0; WEB = 1; IMAGES = 2; LOCAL = 3; NEWS = 4; PRODUCTS = 5; VIDEO = 6; } optional Corpus corpus = 4 [default = UNIVERSAL]; } 枚举常量必须在32位整型值的范围内。因为enum值是使用可变编码方式的，对负数不够高效，因此不推荐在enum中使用负数。 ","date":"2020-06-01","objectID":"/posts/protobuf_v2/:3:6","tags":["protobuf"],"title":"protobuf v2","uri":"/posts/protobuf_v2/"},{"categories":["笔记"],"content":"扩展 通过扩展，可以将一个范围内的字段标识号声明为可被第三方扩展所用。然后，其他人就可以在他们自己的.proto文件中为该消息类型声明新的字段，而不必去编辑原始文件了。 message Foo { // ... extensions 100 to 199; } 在消息Foo中，范围[100,199]之内的字段标识号被保留为扩展用。现在，其他人就可以在他们自己的.proto文件中添加新字段到Foo里了。 extend Foo { optional int32 bar = 126; } 消息Foo现在有一个名为bar的optional int32字段。然而，要在程序代码中访问扩展字段的方法与访问普通的字段稍有不同。 如果你的消息中有很多可选字段， 并且同时至多一个字段会被设置， 你可以加强这个行为，使用oneof特性节省内存. ","date":"2020-06-01","objectID":"/posts/protobuf_v2/:3:7","tags":["protobuf"],"title":"protobuf v2","uri":"/posts/protobuf_v2/"},{"categories":["笔记"],"content":"Oneof Oneof字段就像可选字段， 除了它们会共享内存， 至多一个字段会被设置。 设置其中一个字段会清除其它oneof字段。 message SampleMessage { oneof test_oneof { string name = 4; SubMessage sub_message = 9; } } oneof中字段不能使用 required, optional, repeated 关键字. ","date":"2020-06-01","objectID":"/posts/protobuf_v2/:3:8","tags":["protobuf"],"title":"protobuf v2","uri":"/posts/protobuf_v2/"},{"categories":["笔记"],"content":"Map 如果你希望创建一个关联映射，protocol buffer提供了一种快捷的语法： map\u003ckey_type, value_type\u003e map_field = N; 其中key_type可以是任意Integer或者string类型（所以，除了floating和bytes的任意标量类型都是可以的）value_type可以是任意类型。 Map的字段不可以是repeated，optional,required。 序列化后的顺序和map迭代器的顺序是不确定的，所以你不要期望以固定顺序处理Map ","date":"2020-06-01","objectID":"/posts/protobuf_v2/:3:9","tags":["protobuf"],"title":"protobuf v2","uri":"/posts/protobuf_v2/"},{"categories":["笔记"],"content":"定义服务(Service) 如果想要将消息类型用在RPC(远程方法调用)系统中，可以在.proto文件中定义一个RPC服务接口。 service SearchService { rpc Search (SearchRequest) returns (SearchResponse); } ","date":"2020-06-01","objectID":"/posts/protobuf_v2/:3:10","tags":["protobuf"],"title":"protobuf v2","uri":"/posts/protobuf_v2/"},{"categories":["笔记"],"content":"protobuf v3","date":"2020-06-01","objectID":"/posts/protobuf_v3/","tags":["protobuf"],"title":"protobuf v3","uri":"/posts/protobuf_v3/"},{"categories":["笔记"],"content":"protobuf 是什么 Protocol Buffer (简称Protobuf) 是Google出品的性能优异、跨语言、跨平台的序列化库。 2001年初，Protobuf首先在Google内部创建，很多项目也采用Protobuf进行消息的通讯，还有基于Protobuf的微服务框架GRPC。 可以看做xml、json等序列化的又一种形式，只不过序列化后它是二进制的。 Protobuf支持很多语言，比如C++、C#、Dart、Go、Java、Python、Rust等，同时也是跨平台的。 序列化(serialization、marshalling)的过程是指将数据结构或者对象的状态转换成可以存储(比如文件、内存)或者传输的格式(比如网络)。反向操作就是反序列化(deserialization、unmarshalling)的过程。 ","date":"2020-06-01","objectID":"/posts/protobuf_v3/:0:1","tags":["protobuf"],"title":"protobuf v3","uri":"/posts/protobuf_v3/"},{"categories":["笔记"],"content":"protobuf 为什么要有 二十世纪九十年代后期，XML开始流行，它是一种人类易读的基于文本的编码方式，易于阅读和理解。 JSON是一种更轻量级的基于文本的编码方式，经常用在client/server端的通讯中。 除此之外还有很多序列化格式。 protobuf序列化和反序列化速度更快； 文件更小存储需要更少的空间，传输时间短。 ","date":"2020-06-01","objectID":"/posts/protobuf_v3/:0:2","tags":["protobuf"],"title":"protobuf v3","uri":"/posts/protobuf_v3/"},{"categories":["笔记"],"content":"protobuf 基础 官方推荐新代码采用proto3,这个教程主要介绍proto3的开发。 使用protobuf需要一个.proto文件，在这里定义要序列化的格式。可以理解为我们工作中和服务端定义的接口文档或者java bean。 举例： user.proto syntax = \"proto3\"; //生成java类所在的包名 package com.example.protobuftest.bean; message SearchRequest { required string query = 1; optional int32 page_number = 2; optional int32 result_per_page = 3; } 第一行指定protobuf的版本，可以指定为proto2或3。如果没有指定，默认以proto2格式定义。 在这里我们定义了一个User类型，包括name和age字段。 package package是可选的。对于生成的java语言对于Java，包声明符会变为java的一个包，除非在.proto文件中提供了一个明确有java_package； 如果不写package,默认是文件名作为包名。 写了包名后，就可以用包名加以区分。比如test.model.UserInfo。 显示设置包名后生成的对应语言文件就按照这个来生成包名了。 option java_package = \"test.protobuf.sample\"; option go_package = \"test.protobuf.sample\"; 字段规则 所指定的消息字段修饰符必须是如下之一： singular：一个格式良好的消息应该有0个或者1个这种字段（但是不能超过1个）。这是proto3语法的默认字段规则。 repeated：在一个格式良好的消息中，这种字段可以重复任意多次（包括0次）。重复的值的顺序会被保留。 protobuf2中的required、optional规则已经不能使用了。 字段类型 .proto Type Notes Java Type Go Type double double float64 float float float32 int32 使用可变长度编码。编码负数的效率低 - 如果您的字段可能有负值，请改用sint32。 int int32 int64 使用可变长度编码。编码负数的效率低 - 如果您的字段可能有负值，请改用sint64。 long int64 uint32 使用可变长度编码 int uint32 uint64 使用可变长度编码. long uint64 sint32 使用可变长度编码。签名的int值。这些比常规int32更有效地编码负数。 int int32 sint64 使用可变长度编码。签名的int值。这些比常规int64更有效地编码负数。 long int64 fixed32 总是四个字节。如果值通常大于228，则比uint32更有效。 int uint32 fixed64 总是八个字节。如果值通常大于256，则比uint64更有效 long uint64 sfixed32 总是四个字节 int int32 sfixed64 总是八个字节 long int64 bool boolean bool string String string bytes 可以包含不超过232的任意字节序列。 String []byte 标识号 正如上述文件格式，在消息定义中，每个字段都有唯一的一个数字标识符。这些标识符是用来在消息的二进制格式中识别各个字段的，一旦开始使用就不能够再改变。注：[1,15]之内的标识号在编码的时候会占用一个字节。[16,2047]之内的标识号则占用2个字节。所以应该为那些频繁出现的消息元素保留 [1,15]之内的标识号。切记：要为将来有可能添加的、频繁出现的标识号预留一些标识号。 最小的标识号可以从1开始，最大到2^29 - 1, or 536,870,911。不可以使用其中的[19000－19999]的标识号， Protobuf协议实现中对这些进行了预留。如果非要在.proto文件中使用这些预留标识号，编译时就会报警。 ","date":"2020-06-01","objectID":"/posts/protobuf_v3/:0:3","tags":["protobuf"],"title":"protobuf v3","uri":"/posts/protobuf_v3/"},{"categories":["笔记"],"content":"保留标识符（Reserved） 如果你通过删除或者注释所有域，以后的用户在更新这个类型的时候可能重用这些标识号。如果你使用旧版本加载相同的.proto文件会导致严重的问题，包括数据损坏、隐私错误等等。现在有一种确保不会发生这种情况的方法就是为字段tag（reserved name可能会JSON序列化的问题）指定reserved标识符，protocol buffer的编译器会警告未来尝试使用这些域标识符的用户。 message Foo { reserved 2, 15, 9 to 11; reserved \"foo\", \"bar\"; } 注：不要在同一行reserved声明中同时声明域名字和tag number。 类型嵌套及导入 在一个.proto文件中可以定义多个消息类型,可以引用，也可以嵌套 message SearchResponse { repeated Result result = 1; } message Result { required string url = 1; optional string title = 2; repeated string snippets = 3; } 要导入其他.proto文件的定义，你需要在你的文件中添加一个导入声明，如： import \"myproject/other_protos.proto\"; 嵌套使用： message SearchResponse { message Result { required string url = 1; optional string title = 2; repeated string snippets = 3; } repeated Result result = 1; } 如果你想在它的父消息类型的外部重用这个消息类型，你需要以Parent.Type的形式使用它 枚举 message SearchRequest { required string query = 1; optional int32 page_number = 2; optional int32 result_per_page = 3 [default = 10]; enum Corpus { UNIVERSAL = 0; WEB = 1; IMAGES = 2; LOCAL = 3; NEWS = 4; PRODUCTS = 5; VIDEO = 6; } optional Corpus corpus = 4 [default = UNIVERSAL]; } protbuf3中第一个字段必须是0，枚举类的第一个值总是默认值。 Oneof Oneof字段就像可选字段， 除了它们会共享内存， 至多一个字段会被设置。 设置其中一个字段会清除其它oneof字段。 message SampleMessage { oneof test_oneof { string name = 4; SubMessage sub_message = 9; } } oneof中字段不能使用 required, optional, repeated 关键字. Map 如果你希望创建一个关联映射，protocol buffer提供了一种快捷的语法： map\u003ckey_type, value_type\u003e map_field = N; 其中key_type可以是任意Integer或者string类型（所以，除了floating和bytes的任意标量类型都是可以的）value_type可以是任意类型。 Map的字段不可以是repeated。 序列化后的顺序和map迭代器的顺序是不确定的，所以你不要期望以固定顺序处理Map 定义服务(Service) 如果想要将消息类型用在RPC(远程方法调用)系统中，可以在.proto文件中定义一个RPC服务接口。 service SearchService { rpc Search (SearchRequest) returns (SearchResponse); } ","date":"2020-06-01","objectID":"/posts/protobuf_v3/:0:4","tags":["protobuf"],"title":"protobuf v3","uri":"/posts/protobuf_v3/"},{"categories":["笔记"],"content":"基础","date":"2020-04-08","objectID":"/posts/%E5%9F%BA%E7%A1%80-%E5%8F%8D%E7%A0%81%E8%A1%A5%E7%A0%81/","tags":["基础"],"title":"反码补码","uri":"/posts/%E5%9F%BA%E7%A1%80-%E5%8F%8D%E7%A0%81%E8%A1%A5%E7%A0%81/"},{"categories":["笔记"],"content":"反码补码 原码：第一位表示符号位，0是正，1是负。其余表示值。 反码：正数的反码和原码相同。负数时第一个符号位不变，其他取反。 补码：正数的补码与与原码相同。负数时第一个符号位不变，其余各位取反，然后再加1。即负数时补码为反码加1。 原码 +1 0000 0001 -1 1000 0001 反码 +1 0000 0001 -1 1111 1110 补码 +1 0000 0001 -1 1111 1111 为何要有反码、补码 我们知道, 根据运算法则减去一个正数等于加上一个负数, 即: 1-1 = 1 + (-1) = 0 , 所以机器可以只有加法而没有减法, 这样计算机运算的设计就更简单了。 如果用原码来计算1-1=0 1 - 1 = 1 + (-1) = [00000001]原 + [10000001]原 = [10000010]原 = -2 如果用原码表示, 让符号位也参与计算, 显然对于减法来说, 结果是不正确的.这也就是为何计算机内部不使用原码表示一个数. 为了解决原码做减法的问题, 出现了反码: 1 - 1 = 1 + (-1) = [0000 0001]原 + [1000 0001]原 = [0000 0001]反 + [1111 1110]反 = [1111 1111]反 = [1000 0000]原 = -0 发现用反码计算减法, 结果的真值部分是正确的. 而唯一的问题其实就出现在\"0\"这个特殊的数值上. 虽然人们理解上+0和-0是一样的, 但是0带符号是没有任何意义的. 而且会有[0000 0000]原和[1000 0000]原两个编码表示0. 于是补码的出现, 解决了0的符号以及两个编码的问题: 1-1 = 1 + (-1) = [0000 0001]原 + [1000 0001]原 = [0000 0001]补 + [1111 1111]补 = [0000 0000]补= [0000 0000]原 这样0用[0000 0000]表示, 而以前出现问题的-0则不存在了.而且可以用[1000 0000]表示-128: (-1) + (-127) = [1000 0001]原 + [1111 1111]原 = [1111 1111]补 + [1000 0001]补 = [1000 0000]补 -1-127的结果应该是-128, 在用补码运算的结果中, [1000 0000]补 就是-128. 但是注意因为实际上是使用以前的-0的补码来表示-128, 所以-128并没有原码和反码表示.(对-128的补码表示[1000 0000]补算出来的原码是[0000 0000]原, 这是不正确的) 简单总结：原码运算不能让符号位参与运算、反码运算会出现 使用补码, 不仅仅修复了0的符号以及存在两个编码的问题, 而且还能够多表示一个最低数. 这就是为什么8位二进制, 使用原码或反码表示的范围为[-127, +127], 而使用补码表示的范围为[-128, 127]。 一个+0表示为：00000000，一个-0表示为：1000000，因为符号位不算在里面，所以就会有两个0，所以从一开始发明二进制的时候，就把-0规定为-128，之所以这样是因为[1000 0000]补 就是-128. （这是国内教材中的解释） 补码可以表示负数，且与其他数运算的时候符号位也一起参与运算。这样减法可以用加法实现。除法可以用减法实现，10 /3 可化为 10 -3 -3 -3最后余1，见了3次，就是商3余1。乘法直接用加法实现即可。 ","date":"2020-04-08","objectID":"/posts/%E5%9F%BA%E7%A1%80-%E5%8F%8D%E7%A0%81%E8%A1%A5%E7%A0%81/:0:0","tags":["基础"],"title":"反码补码","uri":"/posts/%E5%9F%BA%E7%A1%80-%E5%8F%8D%E7%A0%81%E8%A1%A5%E7%A0%81/"},{"categories":["笔记"],"content":"linux ssh","date":"2020-02-22","objectID":"/posts/linux_ssh/","tags":["linux"],"title":"linux ssh","uri":"/posts/linux_ssh/"},{"categories":["笔记"],"content":"ssh远程登录 ","date":"2020-02-22","objectID":"/posts/linux_ssh/:0:0","tags":["linux"],"title":"linux ssh","uri":"/posts/linux_ssh/"},{"categories":["笔记"],"content":"密码登录 整个过程所示： （1）远程主机收到用户的登录请求，把自己的公钥发给用户。 （2）用户使用这个公钥，将登录密码加密后，发送回来。 （3）远程主机用自己的私钥，解密登录密码，如果密码正确，就同意用户登录。 问题 这个过程本身是安全的，但是实施的时候存在一个风险： Client端如何保证接受到的公钥就是目标Server端的？ 如果一个攻击者中途拦截Client的登录请求，向其发送自己的公钥，Client端用攻击者的公钥进行数据加密。攻击者接收到加密信息后再用自己的私钥进行解密，不就窃取了Client的登录信息了吗？因为不像https协议，SSH协议的公钥是没有证书中心（CA）公证的，也就是说，都是自己签发的。 通常在第一次登录的时候，系统会出现下面提示信息：无法确认主机host(12.18.429.21)的真实性，不过知道它的公钥指纹，询问你是否继续连接？ 当远程主机的公钥被接受以后，它就会被保存在文件$HOME/.ssh/known_hosts之中。下次再连接这台主机，系统就会认出它的公钥已经保存在本地了，从而跳过警告部分，直接提示输入密码。 ","date":"2020-02-22","objectID":"/posts/linux_ssh/:1:0","tags":["linux"],"title":"linux ssh","uri":"/posts/linux_ssh/"},{"categories":["笔记"],"content":"公钥登录（基于公钥的认证） 使用密码登录，每次都必须输入密码，非常麻烦。好在SSH提供了另外一种可以免去输入密码过程的登录方式：公钥登录。 所谓\"公钥登录\"，原理很简单，就是用户将自己的公钥储存在远程主机上。 公钥认证流程： Client端用户TopGun将自己的公钥存放在Server上，追加在文件authorized_keys中。 Server收到登录请求后，随机生成一个字符串str1，并发送给Client。 Client用自己的私钥对字符串str1进行加密。 将加密后字符串发送给Server。 Server用之前存储的公钥进行解密，比较解密后的str2和str1。 根据比较结果，返回客户端登陆结果。 ","date":"2020-02-22","objectID":"/posts/linux_ssh/:2:0","tags":["linux"],"title":"linux ssh","uri":"/posts/linux_ssh/"},{"categories":["笔记"],"content":"生成公钥 1、在本机生成密钥对 使用ssh-keygen命令生成密钥对： ssh-keygen -t rsa #-t表示类型选项，这里采用rsa加密算法 然后根据提示一步步的按enter键即可（其中有一个提示是要求设置私钥口令passphrase，不设置则为空，这里看心情吧，如果不放心私钥的安全可以设置一下），执行结束以后会在 /home/当前用户 目录下生成一个 .ssh 文件夹,其中包含私钥文件 id_rsa 和公钥文件 id_rsa.pub。 2、将公钥复制到远程主机中 使用ssh-copy-id命令将公钥复制到远程主机。ssh-copy-id会将公钥写到远程主机的 ~/ .ssh/authorized_key 文件中 ssh-copy-id ldz@192.168.0.1 经过以上两个步骤，以后再登录这个远程主机就不用再输入密码了。 SSH端口转发 SSH 不仅仅能够自动加密和解密 SSH 客户端与服务端之间的网络数据，同时，SSH 还能够提供了一个非常有用的功能，那就是端口转发，即将TCP 端口的网络数据，转发到指定的主机某个端口上，在转发的同时会对数据进行相应的加密及解密。如果工作环境中的防火墙限制了一些网络端口的使用，但是允许 SSH 的连接，那么也是能够通过使用SSH转发后的端口进行通信。转发，主要分为本地转发与远程转发两种类型。 转发的参数： -C：压缩数据 -f ：后台认证用户/密码，通常和-N连用，不用登录到远程主机。 -N ：不执行脚本或命令，通常与-f连用。 -g ：在-L/-R/-D参数中，允许远程主机连接到建立的转发的端口，如果不加这个参数，只允许本地主机建立连接。 -L : 本地端口:目标IP:目标端口 -D : 动态端口转发 -R : 远程端口转发 -T ：不分配 TTY 只做代理用 -q ：安静模式，不输出 错误/警告 信息 ","date":"2020-02-22","objectID":"/posts/linux_ssh/:3:0","tags":["linux"],"title":"linux ssh","uri":"/posts/linux_ssh/"},{"categories":["笔记"],"content":"本地转发 有本地网络服务器的某个端口，转发到远程服务器某个端口。说白了就是，将发送到本地端口的请求，转发到目标端口。格式如下： ssh -L 本地网卡地址:本地端口:目标地址:目标端口 用户@目标地址。 案例：B服务器上的mysql只允许127.0.0.1连接，A服务器想要连接mysql只能 ssh -L 127.0.0.1:3306:127.0.0.1:3306 root@192.168.13.142 因为本地网卡地址是可以省略的，上面的转发，可以简写为： ssh -L 3306:127.0.0.1:3306 root@192.168.13.142 ","date":"2020-02-22","objectID":"/posts/linux_ssh/:4:0","tags":["linux"],"title":"linux ssh","uri":"/posts/linux_ssh/"},{"categories":["笔记"],"content":"远程转发 由远程服务器的某个端口，转发到本地网络的服务器某个端口。说白了，就是将发送到远程端口的请求，转发到目标端口。格式如下： ssh -R 远程网卡地址:远程端口:目标地址:目标端口 ","date":"2020-02-22","objectID":"/posts/linux_ssh/:5:0","tags":["linux"],"title":"linux ssh","uri":"/posts/linux_ssh/"},{"categories":["笔记"],"content":"go rpc系列1-rpc","date":"2019-11-05","objectID":"/posts/go_rpc_rpc/","tags":["golang"],"title":"go rpc系列1-rpc","uri":"/posts/go_rpc_rpc/"},{"categories":["笔记"],"content":"RPC RPC是远程过程调用（Remote Procedure Call）的缩写形式，是分布式系统中不同节点间流行的通信方式。 一个完整的RPC架构里面包含了四个核心的组件，分别是Client ,Server,Client Stub以及Server Stub，这个Stub大家可以理解为存根。分别说说这几个组件： 客户端（Client），服务的调用方。 服务端（Server），真正的服务提供者。 客户端存根，存放服务端的地址消息，再将客户端的请求参数打包成网络消息，然后通过网络远程发送给服务方。 服务端存根，接收客户端发送过来的消息，将消息解包，并调用本地的方法。 示例 Go 语言的 RPC 包的路径为 net/rpc，可以猜测该RPC包是建立在 net 包基础之上的。下面我们尝试基于 rpc 实现一个打印的例子。 先构造一个 HelloService 类型，其中的 Hello 方法用于实现打印功能，服务端代码： package main import ( \"log\" \"net\" \"net/rpc\" ) // HelloService is rpc server obj type HelloService struct {} func (p *HelloService) Hello(request string, reply *string) error { *reply = \"hello:\" + request return nil } //将HelloService类型的对象注册为一个RPC服务 func main(){ rpc.RegisterName(\"HelloService\", new(HelloService)) listener, err := net.Listen(\"tcp\", \":1234\") if err != nil { log.Fatal(\"ListenTCP error:\", err) } conn, err := listener.Accept() if err != nil { log.Fatal(\"Accept error\", err) } rpc.ServeConn(conn) } 其中Hello方法必须满足Go语言的RPC规则： 方法只能有两个可序列化的参数，其中第二个参数是指针 类型 并且返回一个error类型，同时必须是公开的方法。 rpc.Register 函数调用会将对象类型中所有满足 RPC 规则的对象方法注册为 RPC 函数，所有注册 的方法会放在 “HelloService” 服务空间之下。 建立一个唯一的TCP链接，并且通过 rpc.ServeConn 函数在该 TCP 链接上为对方提供 RPC 服务。 客户端请求HelloService服务的代码： package main import ( \"fmt\" \"log\" \"net/rpc\" ) func main() { client, err := rpc.Dial(\"tcp\", \"localhost:1234\") if err != nil { log.Fatal(\"dialing err:\", err) } var reply string err = client.Call(\"HelloService.Hello\", \"test_rpc\", \u0026reply) if err != nil { log.Fatal(err) } fmt.Println(reply) } 通过rpc.Dial拨号RPC服务，然后通过client.Call调用具体的RPC方法。在调用 client.Call时，第一个参数是用点号链接的RPC服务名字和方法名字，第二和第三个参数分别我们定 义RPC方法的两个参数。 ","date":"2019-11-05","objectID":"/posts/go_rpc_rpc/:0:0","tags":["golang"],"title":"go rpc系列1-rpc","uri":"/posts/go_rpc_rpc/"},{"categories":["笔记"],"content":"go rpc系列2-protobuf","date":"2019-11-05","objectID":"/posts/go_rpc_protobuf/","tags":["golang"],"title":"go rpc系列2-protobuf","uri":"/posts/go_rpc_protobuf/"},{"categories":["笔记"],"content":"protobuf 语法 Protobuf 是 Protocol Buffers 的简称，是一种与语言、平台无关，可扩展的序列化结构化数据的数据描述语言，Protobuf作为接口规范的描述语言，可以作为设计安全的跨语言PRC接口的基础工具。 hello.proto 文件 syntax = \"proto3\"; package main; message String { string value = 1; } syntax = \"proto3\"; package greeter; option go_package=\"proto1/greeter\"; service Greeter { rpc SayHello (HelloRequest) returns (HelloReply) {} } message HelloRequest { string name = 1; } message HelloReply { string message = 1; } 第一行声明使用 proto3 语法。否则，默认使用 proto2 语法，目前主流推荐使用 v3 版本。此声明必须是文件的非空、非注释的第一行。 package 指令指明当前是 main 包，用户也可以针对不同的语言定制对应的包路径和名称。 message 关键字定义一个 String 类型消息体，在最终生成的Go语言代码中对应一个 String 结构体。每一个消息体的字段包含三个属性：类型、字段名称、字段编号。在消息体的定义上，除类型以外均不可重复。此处 String 类型中只有一个字符串类型的 value 成员，该成员编码时用1编号代替名字。 Protobuf 中最基本的数据单元是 message，类似 Go 语言中的结构体。在 message 中可以嵌套 message 或其它的基础数据类型的成员。 package proto文件（非protoc）有两个易混参数，即package和xx_package，xx指的是你的编译语言，比如你要编程成Go语言，对应的就是go_package。 ","date":"2019-11-05","objectID":"/posts/go_rpc_protobuf/:0:0","tags":["golang"],"title":"go rpc系列2-protobuf","uri":"/posts/go_rpc_protobuf/"},{"categories":["笔记"],"content":"package package参数针对的是protobuf，是proto文件的命名空间，它的作用是为了避免我们定义的接口，或者message出现冲突。 举一个小栗子，假设我有A.proto和B.proto两份文件，如下 A.proto message UserInfo { uint32 uid = 1; string name = 2; } B.proto message UserInfo { uint32 uid = 1; string name = 2; uint32 age = 3; string work = 4; } 如上，两份文件同时有一个UserInfo的message，这时候如果我需要在A文件引用B文件，如果没有指定package，就无法区分是要调A的UserInfo还是调B的。 ","date":"2019-11-05","objectID":"/posts/go_rpc_protobuf/:1:0","tags":["golang"],"title":"go rpc系列2-protobuf","uri":"/posts/go_rpc_protobuf/"},{"categories":["笔记"],"content":"xx_package 这里以go_package进行举例说明，该参数主要声明Go代码的存放位置，也可以说它解决的是包名问题（因为proto文件编译后会生成一份.pb.go文件，既然是go文件，就有包名问题） .pb.go常规的存放路径一般是放在同名proto文件下，但也有些人不想这么做，比如他想把所有.pb.go文件都存放在一个特定文件夹下，比如上述的 pb_go，那么他有两种办法： 第一种： 修改 –go_out，go_package 保持不变 $ protoc –proto_path=. –go_out=./proto1/pb_go proto1/greeter/greeter.proto 这样生成的pb文件在 pb_go/proto1/greeter 目录下，文件目录有点冗余，不过pb文件的包名仍然是 greeter 第二种： 修改 go_package， go_out 保持不变 option go_package=“proto1/pb_go”; $ protoc –proto_path=. –go_out=. proto1/greeter/greeter_v2.proto 这样生成的pb文件在 pb_go 目录下，pb文件的包名为 pb_go 关于标识号 消息体中字段定义了唯一的数字值。这些数字是用来在消息的二进制格式中识别各个字段的，一旦开始使用就不能够再改变。注：[1,15]之内的标识号在编码的时候会占用一个字节。[16,2047]之内的标识号则占用2个字节。所以应该为那些频繁出现的消息元素保留 [1,15]之内的标识号。 最小的标识号可以从1开始，最大到2^29 - 1, or 536,870,911。不可以使用其中的[19000－19999]的标识号， Protobuf 协议实现中对这些进行了预留。如果非要在 .proto 文件中使用这些预留标识号，编译时就会报警。类似地，你不能使用之前保留的任何标识符。 添加注释 .proto 文件添加注释，可以使用C/C++风格的 // 和 /* … */ 语法格式 保留字段 如果从前面定义的消息中删除了 和 字段，应保留其字段编号，使用关键字 reserved: syntax \"proto3\"; message Stock { reserved 3, 4; // ... } 还可以将 reserved 关键字用作将来可能添加的字段的占位符。可以使用 to 关键字将连续字段号占位。 syntax \"proto3\"; message Info { reserved 2, 9 to 11, 15; // ... } protoc命令 ","date":"2019-11-05","objectID":"/posts/go_rpc_protobuf/:2:0","tags":["golang"],"title":"go rpc系列2-protobuf","uri":"/posts/go_rpc_protobuf/"},{"categories":["笔记"],"content":"安装 $ brew install protoc ","date":"2019-11-05","objectID":"/posts/go_rpc_protobuf/:3:0","tags":["golang"],"title":"go rpc系列2-protobuf","uri":"/posts/go_rpc_protobuf/"},{"categories":["笔记"],"content":"查看版本 $ protoc –version libprotoc 3.7.1 ","date":"2019-11-05","objectID":"/posts/go_rpc_protobuf/:4:0","tags":["golang"],"title":"go rpc系列2-protobuf","uri":"/posts/go_rpc_protobuf/"},{"categories":["笔记"],"content":"参数 protoc --proto_path=. --go_out=. proto1/greeter/greeter.proto 上面的指令可以拆解为三部分，分别对应protoc的三个重要参数，我们首先来看看protoc提供了哪些参数： $ protoc --help Usage: protoc [OPTION] PROTO_FILES -IPATH, --proto_path=PATH 指定搜索路径 --plugin=EXECUTABLE: .... --cpp_out=OUT_DIR Generate C++ header and source. --csharp_out=OUT_DIR Generate C# source file. --java_out=OUT_DIR Generate Java source file. --js_out=OUT_DIR Generate JavaScript source. --objc_out=OUT_DIR Generate Objective C header and source. --php_out=OUT_DIR Generate PHP source file. --python_out=OUT_DIR Generate Python source file. --ruby_out=OUT_DIR Generate Ruby source file @\u003cfilename\u003e proto文件的具体位置 搜索路径参数 第一个比较重要的参数就是搜索路径参数，即上述展示的-IPATH, –proto_path=PATH。它表示的是我们要在哪个路径下搜索.proto文件，这个参数既可以用-I指定，也可以使用–proto_path=指定。 如果不指定该参数，则默认在当前路径下进行搜索；另外，该参数也可以指定多次，这也意味着我们可以指定多个路径进行搜索。 语言插件参数 语言参数即上述的–cpp_out=，–python_out=等，protoc支持的语言长达13种，且都是比较常见的。像上面出现的语言参数，说明protoc本身已经内置该语言对应的编译插件，我们无需安装。 而如果上面没出现的，比如–go_out=，就得自己单独安装语言插件，比如–go_out=对应的是protoc-gen-go ","date":"2019-11-05","objectID":"/posts/go_rpc_protobuf/:5:0","tags":["golang"],"title":"go rpc系列2-protobuf","uri":"/posts/go_rpc_protobuf/"},{"categories":["笔记"],"content":"–go_out详细解读 想必大家在使用的时候，应该遇到过这些写法：–go_out=paths=import:.、–go_out=paths=source_relative:.，或者–go_out=plugins=grpc:.。 这样写表达的是啥意思呢？ 所以我们需要知道，–go_out参数是用来指定 protoc-gen-go 插件的工作方式和Go代码的生成位置，而上面的写法正是表明该插件的工作方式。 –go_out主要的两个参数为plugins 和 paths，分别表示生成Go代码所使用的插件，以及生成的Go代码的位置。–go_out的写法是，参数之间用逗号隔开，最后加上冒号来指定代码的生成位置，比如–go_out=plugins=grpc,paths=import:. plugins参数有不带grpc和带grpc两种（应该还有其它的，目前知道的有这两种），两者的区别如下，带grpc的会多一些跟gRPC相关的代码，实现gRPC通信。 paths参数有两个选项，分别是 import 和 source_relative，默认为 import，表示按照生成的Go代码的包的全路径去创建目录层级，source_relative 表示按照 proto源文件的目录层级去创建Go代码的目录层级，如果目录已存在则不用创建。 生成相应的Go代码 Protobuf 核心的工具集是 C++ 语言开发的，官方的 protoc 编译器中并不支持Go语言。要想基于上面 的 hello.proto 文件生成相应的Go代码，需要安装相应的插件。 安装官方的 protoc 工具，可以从 https://github.com/google/protobuf/releases 下载。 安装针对Go语言的代码生成插件，通过 go get github.com/golang/protobuf/protoc-gen-go 命令安装。 通过以下命令生成相应的Go代码： $ protoc –go_out=. hello.proto go_out 参数告知 protoc 编译器去加载对应的 protoc-gen-go 工具，生成的代码放到当前目录。最后是一系列要处理的protobuf文件的列表。 plugins=plugin1+plugin2：指定要加载的子插件列表，我们定义的 proto 文件是涉及了 RPC 服务的，而默认是不会生成 RPC 代码的，因此需要在 go_out 中给出 plugins 参数传递给 protoc-gen-go，告诉编译器，请支持 RPC（这里指定了内置的 grpc 插件）。 todo 基本数据类型 protobuf 所生成出来的数据类型并非与原始的类型完全一致，下面是一些常见的类型映射： Protobuf 和 RPC组合 基于 String 类型，重新实现 HelloService 服务 package main import ( \"log\" \"net\" \"net/rpc\" \"rpc/protoc\" ) // HelloService is rpc server obj type HelloService struct{} //Hello方法的输入参数和输出的参数均改用 Protobuf 定义的 String 类型表示。 //因为新的输入参数为结构体类型，因此改用指针类型作为输入参数，函数的内部代码同时也做了相应的调整。 func (p *HelloService) Hello(request *protoc.String, reply *protoc.String) error { reply.Value = \"hello:\" + request.GetValue() return nil } func main() { rpc.RegisterName(\"HelloService\", new(HelloService)) listener, err := net.Listen(\"tcp\", \":1234\") if err != nil { log.Fatal(\"ListenTCP error:\", err) } conn, err := listener.Accept() if err != nil { log.Fatal(\"Accept error\", err) } rpc.ServeConn(conn) } 客户端请求HelloService服务的代码 client.go： package main import ( \"fmt\" \"log\" \"net/rpc\" \"rpc/protoc\" ) func main() { client, err := rpc.Dial(\"tcp\", \"localhost:1234\") if err != nil { log.Fatal(\"dialing err:\", err) } var reply = \u0026protoc.String{} var param = \u0026protoc.String{ Value: \"hello wekenw\", } err = client.Call(\"HelloService.Hello\", \u0026param, \u0026reply) if err != nil { log.Fatal(err) } fmt.Println(reply) } ","date":"2019-11-05","objectID":"/posts/go_rpc_protobuf/:6:0","tags":["golang"],"title":"go rpc系列2-protobuf","uri":"/posts/go_rpc_protobuf/"},{"categories":["笔记"],"content":"go rpc系列3-grpc","date":"2019-11-05","objectID":"/posts/go_rpc_grpc/","tags":["golang"],"title":"go rpc系列3-grpc","uri":"/posts/go_rpc_grpc/"},{"categories":["笔记"],"content":"我们用过 rpc 来实现过简单的服务，现在我们改用 gRPC 试试。 安装 在我们的项目根下，在命令行执行 Go 语言的 gRPC 库的安装命令，如下： $ go get -u google.golang.org/grpc@v1.29.1 使用 hello.proto 文件，新增了 HelloService 接口： syntax = \"proto3\"; package proto; message String { string value = 1; } service HelloService { rpc Hello (String) returns (String); } 然后使用 protoc-gen-go 内置的 gRPC 插件生成 gRPC 代码： $ protoc –go_out=plugins=grpc:. ./proto/*.proto 查看生产的 hello.pb.go 文件，gRPC 插件为服务端和客户端生成不同的接口： // HelloServiceServer is the server API for HelloService service. type HelloServiceServer interface { Hello(context.Context, *String) (*String, error) } // HelloServiceClient is the client API for HelloService service. type HelloServiceClient interface { Hello(ctx context.Context, in *String, opts ...grpc.CallOption) (*String, error) } gRPC 通过 context.Context 参数，为每个方法调用提供了上下文支持。 基于服务端的 HelloServiceServer 接口，我们重新来实现 HelloService 服务： package main import ( \"context\" \"google.golang.org/grpc\" \"log\" \"net\" pb \"rpc/proto\" // 设置引用别名 ) type HelloServiceImpl struct{} func (p *HelloServiceImpl) Hello(ctx context.Context, args *pb.String) (*pb.String, error) { reply := \u0026pb.String{Value: \"hello:\" + args.GetValue()} return reply, nil } func main() { grpcServer := grpc.NewServer() pb.RegisterHelloServiceServer(grpcServer, new(HelloServiceImpl)) lis, err := net.Listen(\"tcp\", \":1234\") if err != nil { log.Fatal(err) } grpcServer.Serve(lis) } 客户端链接 gRPC 服务： package main import ( \"context\" \"fmt\" \"google.golang.org/grpc\" \"log\" pb \"rpc/proto\" // 设置引用别名 ) func main() { conn, err := grpc.Dial(\"localhost:1234\", grpc.WithInsecure()) if err != nil { log.Fatal(\"dialing err:\", err) } defer conn.Close() client := pb.NewHelloServiceClient(conn) reply, err := client.Hello(context.Background(), \u0026pb.String{Value: \"wekenw\"}) if err != nil { log.Fatal(err) } fmt.Println(reply.GetValue()) } 上面的是grpc 的一元 RPC(Unary RPC)调用方式。 Server-side streaming RPC：服务端流式 RPC 服务器端流式 RPC，单向流，Server 为 Stream，Client 为普通的一元 RPC 请求。 简单来讲就是客户端发起一次普通的 RPC 请求，服务端通过流式响应多次发送数据集，客户端 Recv 接收数据集。 syntax = \"proto3\"; package proto; message String { string value = 1; } service HelloService { rpc Hello (String) returns (stream String){}; } Server: package main import ( \"google.golang.org/grpc\" \"log\" \"net\" pb \"rpc/proto\" // 设置引用别名 \"strconv\" ) // HelloServiceImpl 定义我们的服务 type HelloServiceImpl struct{} //实现Hello方法 func (p *HelloServiceImpl) Hello(req *pb.String, srv pb.HelloService_HelloServer) error { for n := 0; n \u003c 5; n++ { // 向流中发送消息， 默认每次send送消息最大长度为`math.MaxInt32`bytes err := srv.Send(\u0026pb.String{ Value: req.Value + strconv.Itoa(n), }) if err != nil { return err } } return nil } func main() { // 新建gRPC服务器实例 grpcServer := grpc.NewServer() // 在gRPC服务器注册我们的服务 pb.RegisterHelloServiceServer(grpcServer, new(HelloServiceImpl)) lis, err := net.Listen(\"tcp\", \":1234\") if err != nil { log.Fatal(err) } log.Println(\" net.Listing...\") //用服务器 Serve() 方法以及我们的端口信息区实现阻塞等待，直到进程被杀死或者 Stop() 被调用 err = grpcServer.Serve(lis) if err != nil { log.Fatalf(\"grpcServer.Serve err: %v\", err) } } Server 端，主要留意 stream.Send 方法，通过阅读源码，可得知是 protoc 在生成时，根据定义生成了各式各样符合标准的接口方法。最终再统一调度内部的 SendMsg 方法，该方法涉及以下过程: 消息体（对象）序列化。 压缩序列化后的消息体。 对正在传输的消息体增加 5 个字节的 header（标志位）。 判断压缩 + 序列化后的消息体总字节长度是否大于预设的 maxSendMessageSize（预设值为 math.MaxInt32），若超出则提示错误。 写入给流的数据集。 Client: package main import ( \"context\" \"google.golang.org/grpc\" \"io\" \"log\" pb \"rpc/proto\" // 设置引用别名 ) // SayHello 调用服务端的 Hello 方法 func SayHello(client pb.HelloServiceClient, r *pb.String) error { stream, _ := client.Hello(context.Background(), r) for { resp, err := stream.Recv() if err == io.EOF { break } if err != nil { return err } log.Printf(\"resp: %v\", resp) } return nil } func main() { conn, err := grpc.Dial(\"localhost:1234\", grpc.WithInsecure()) if err != nil { log.Fatal(\"dialing err:\", err) } defer conn.Close() // 建立gRPC连接 client := pb.NewHelloServiceClient(conn) // 创建发送结构体 req := pb.String{ Value: \"stream server grpc \", } SayHello(client, \u0026req) } 在 Client 端，主要留意 stream.Recv() 方法，此方法，是对 ClientStream.RecvMsg方法的封装，而 RecvMsg 方法会从流中读取完整的 gRPC 消息体，我们可得知： RecvMsg 是阻塞等待的","date":"2019-11-05","objectID":"/posts/go_rpc_grpc/:0:0","tags":["golang"],"title":"go rpc系列3-grpc","uri":"/posts/go_rpc_grpc/"},{"categories":["笔记"],"content":"go select","date":"2019-08-13","objectID":"/posts/go_select/","tags":["golang"],"title":"go select","uri":"/posts/go_select/"},{"categories":["笔记"],"content":"select使用 select只会执行一次 case语句必须是对channel的操作 case语句不管是接收还是发送，语句表达式都会执行（执行顺序是从左到右，从上到下，这里只是语句表达式，而不是发送和接收操作的顺序，判断发送和接收是随机的顺序） 会对所有case语句进行判断，如果多个都符合，随机选一个 如果case都不符合，default（如果存在，不存在就会阻塞等着）执行 func test1(ctx context.Context) { for { select { case \u003c-ctx.Done(): //context到期或主动取消时，channel就被关闭，零值被取出，语句得到执行 fmt.Println(ctx.Err() == context.DeadlineExceeded) fmt.Println(\"test1 stop\") return case \u003c-time.Tick(time.Second): fmt.Println(\"tick\") default: fmt.Println(\"test1\") time.Sleep(time.Second) } } } case表达式都会执行，验证： func main() { select { case getChan(\"this is 1 get chan\") \u003c- getInt(\"this is 1 get int\"): //getChan(),getInt都会执行，从左到右，从上到下 fmt.Println(\"1 被选中\") case getChan(\"this is 2 get chan\") \u003c- getInt(\"this is 2 get int\"): fmt.Println(\"2 被选中\") default: fmt.Println(\"default 被选中\") } } func getChan(s string) chan int { fmt.Println(s) c1 := make(chan int, 1) return c1 } func getInt(s string) int { fmt.Println(s) return 1 } select原理 定义了一个数据结构表示每个case语句(含defaut，default实际上是一种特殊的case)，select执行过程可以类比成一个函数，函数输入case数组，输出选中的case，然后程序流程转到选中的case块。 case数据结构 type scase struct { c *hchan // chan kind uint16 elem unsafe.Pointer // data element } scase.c为当前case语句所操作的channel指针，这也说明了一个case语句只能操作一个channel。 scase.kind表示该case的类型，分为读channel、写channel和default，三种类型分别由常量定义： caseRecv：case语句中尝试读取scase.c中的数据； caseSend：case语句中尝试向scase.c中写入数据； caseDefault： default语句 scase.elem表示缓冲区地址，跟据scase.kind不同，有不同的用途： scase.kind == caseRecv ： scase.elem表示读出channel的数据存放地址； scase.kind == caseSend ： scase.elem表示将要写入channel的数据存放地址； 真正选择case的函数是selectgo函数， 总结 select 结构的执行过程与实现原理，首先在编译期间，Go 语言会对 select 语句进行优化，以下是根据 select 中语句的不同选择了不同的优化路径： 空的 select 语句会被直接转换成 block 函数的调用，直接挂起当前 Goroutine； 如果 select 语句中只包含一个 case，就会被转换成 if ch == nil { block }; n; 表达式； 首先判断操作的 Channel 是不是空的； 然后执行 case 结构中的内容； 如果 select 语句中只包含两个 case 并且其中一个是 default，那么 Channel 和接收和发送操作都会使用 selectnbrecv 和 selectnbsend 非阻塞地执行接收和发送操作； 在默认情况下会通过 selectgo 函数选择需要执行的 case 并通过多个 if 语句执行 case 中的表达式； 在编译器已经对 select 语句进行优化之后，Go 语言会在运行时执行编译期间展开的 selectgo 函数，这个函数会按照以下的过程执行： 随机生成一个遍历的轮询顺序 pollOrder 并根据 Channel 地址生成一个用于遍历的锁定顺序 lockOrder； 根据 pollOrder 遍历所有的 case 查看是否有可以立刻处理的 Channel 消息； 如果有消息就直接获取 case 对应的索引并返回； 如果没有消息就会创建 sudog 结构体，将当前 Goroutine 加入到所有相关 Channel 的 sendq 和 recvq 队列中并调用 gopark 触发调度器的调度； 当调度器唤醒当前 Goroutine 时就会再次按照 lockOrder 遍历所有的 case，从中查找需要被处理的 sudog 结构并返回对应的索引； 然而并不是所有的 select 控制结构都会走到 selectgo 上，很多情况都会被直接优化掉，没有机会调用 selectgo 函数。 ","date":"2019-08-13","objectID":"/posts/go_select/:0:0","tags":["golang"],"title":"go select","uri":"/posts/go_select/"},{"categories":["笔记"],"content":"gitlab cicd","date":"2019-07-25","objectID":"/posts/%E9%A1%B9%E7%9B%AE_cicd/","tags":["项目"],"title":"gitlab cicd","uri":"/posts/%E9%A1%B9%E7%9B%AE_cicd/"},{"categories":["笔记"],"content":"cicd工作原理 gitlab-runner 使用流程 安装gitlab服务器 安装gitlab-runner 注册gitlab-runner，在这个步骤中，我们要知道执行器类型，执行器才是真正执行.gitlab-ci.yml脚本的。类型包括shell,docker,k8s等，我们使用的docker。 项目根目录下创建.gitlab-ci.yml文件 .gitlab-cli.yml 一个.gitlab-cli.yml我们可以理解为对应一个流水线，也就是pipeline,一个pipeline包含一个或多个作业（job）, 每一个作业必须包含一个名称，属于一个阶段（stage),至少包含一个script,作业在特定条件下执行。 stages: - build - test build-code-job: stage: build script: - echo \"Check the ruby version, then build some Ruby project files:\" - ruby -v - rake test-code-job1: stage: test script: - echo \"If the files are built successfully, test some files with one command:\" - rake test1 test-code-job2: stage: test script: - echo \"If the files are built successfully, test other files with a different command:\" - rake test2 build 阶段的 build-code-job 作业首先运行。 它输出作业使用的 Ruby 版本，然后运行 rake 来构建项目文件。 如果此作业成功完成，则 test 阶段中的两个 test-code-job 作业将并行启动并对文件运行测试。 示例中的完整流水线由三个作业组成，分为两个阶段，build 和 test。 每次将更改推送到项目中的任何分支时，流水线都会运行。我们可以指定触发的条件必须git分支名。 除了可以指定script之外，还可以指定before-script,after-script。before-script在script在一个shell中执行，before-script失败了会导致整个作业失败，而script失败并不会影响after-script的运行。 作业（job)还可以自动tag,我们知道可以安装注册多个Runner，Runner注册期间可以指定tag,这样我们就可以指定job由哪个runner来执行了。 test-code-job1: stage: test script: - echo \"If the files are built successfully, test some files with one command:\" - rake test1 tags: - windows ","date":"2019-07-25","objectID":"/posts/%E9%A1%B9%E7%9B%AE_cicd/:0:0","tags":["项目"],"title":"gitlab cicd","uri":"/posts/%E9%A1%B9%E7%9B%AE_cicd/"},{"categories":["笔记"],"content":"唯一id","date":"2019-07-25","objectID":"/posts/%E9%A1%B9%E7%9B%AE_unique_id/","tags":["项目"],"title":"唯一id","uri":"/posts/%E9%A1%B9%E7%9B%AE_unique_id/"},{"categories":["笔记"],"content":"UUID UUID是由一组32位数的16进制数字所构成，所以UUID理论上的总数为16^32=2^128。 以连字号分为五段，形式为8-4-4-4-12的32个字符。示例： 550e8400-e29b-41d4-a716-446655440000 为保证UUID的唯一性，规范定义了包括网卡MAC地址、时间戳、名字空间（Namespace）、随机或伪随机数、时序等元素。 UUID具有多个版本，每个版本的算法不同，应用范围也不同。 基于时间的UUID 基于时间的UUID通过计算当前时间戳、随机数和机器MAC地址得到。由于在算法中使用了MAC地址，这个版本的UUID可以保证在全球范围的唯一性。 DCE安全的UUID DCE（Distributed Computing Environment）安全的UUID和基于时间的UUID算法相同，但会把时间戳的前4位置换为POSIX的UID或GID。这个版本的UUID在实际中较少用到。 基于名字的UUID（MD5） 基于名字的UUID通过计算名字和名字空间的MD5散列值得到。这个版本的UUID保证了：相同名字空间中不同名字生成的UUID的唯一性；不同名字空间中的UUID的唯一性；相同名字空间中相同名字的UUID重复生成是相同的。 随机UUID 根据随机数，或者伪随机数生成UUID。这种UUID产生重复的概率是可以计算出来的。 snowflake(雪花id) 时间戳。时间戳段位共41位，单位毫秒，可以使用约70年。为了增加剩余可用期限，一般都会把起始日期尽量后移而不是直接使用1970-01-01。（ps：如果是使用1970，你的程序只能支持到2039年了） 机器id。用于区分集群内不同机器，因为Snowflake生成ID是在每台机器上进行的。5个bit是数据中心，5个bit的机器ID 序列号。由于高并发的特性，即使时间戳精确到了毫秒，也有可能出现重复。序列号用于同一时间戳下生成多个id。12位的长度，同一机器一毫秒可以有2^12=4096个id。 总结 uuid参数的数据长度32位，比较长；无序，没有规律； snowflake长度为64bit位二进制，可以转为int64；总体呈现递增趋势；如果时间回拨，会有重复的风险。 ","date":"2019-07-25","objectID":"/posts/%E9%A1%B9%E7%9B%AE_unique_id/:0:0","tags":["项目"],"title":"唯一id","uri":"/posts/%E9%A1%B9%E7%9B%AE_unique_id/"},{"categories":["笔记"],"content":"go内存分配（上）","date":"2019-06-24","objectID":"/posts/go_memory%E4%B8%8A/","tags":["golang"],"title":"go内存分配（上）","uri":"/posts/go_memory%E4%B8%8A/"},{"categories":["笔记"],"content":"概述 为了方便自主管理内存，做法便是先向系统申请一块内存，然后将内存切割成小块，通过一定的内存分配算法管理内存。 预申请的内存划分为spans、bitmap、arena三部分。其中arena即为所谓的堆区，应用中需要的内存从这里分配。 其中spans和bitmap是为了管理arena区而存在的。 arena的大小为512G，为了方便管理把arena区域划分成一个个的page，每个page为8KB,一共有512GB/8KB个页； spans区域存放span的指针，每个指针对应一个page，所以span区域的大小为(512GB/8KB)*指针大小8byte = 512M bitmap区域大小也是通过arena计算出来，不过主要用于GC。 spans span是内存管理的基本单位,每个span用于管理特定的class对象, 每个class都代表一个固定大小的对象，跟据对象大小，span将一个或多个页拆分成多个块进行管理。 换句话说为了更好的应对不同大小的对象的分配，有了class的概念，一共67中class，每种class所拥有的page数量是不同的。 src/runtime/mheap.go:mspan定义了其数据结构： type mspan struct { next *mspan //链表前向指针，用于将span链接起来 prev *mspan //链表前向指针，用于将span链接起来 startAddr uintptr // 起始地址，也即所管理页的地址 npages uintptr // 管理的页数 nelems uintptr // 块个数，也即有多少个块可供分配 allocBits *gcBits //分配位图，每一位代表一个块是否已分配 allocCount uint16 // 已分配块的个数 spanclass spanClass // class表中的class ID elemsize uintptr // class表中的对象大小，也即块大小 } 以class 10为例，span和管理的内存如下图所示： chache 有了管理内存的基本单位span，还要有个数据结构来管理span，这个数据结构叫mcentral，各线程需要内存时从mcentral管理的span中申请内存，为了避免多线程申请内存时不断的加锁，Golang为每个线程分配了span的缓存，这个缓存即是cache。 central cache作为线程的私有资源为单个线程服务，而central则是全局资源，为多个线程服务，当某个线程内存不足时会向central申请，当某个线程释放内存时又会回收进central。 heap central内存不够时，从heap申请。 内存分配过程 以申请size为n的内存为例，分配步骤如下： 获取当前线程的私有缓存mcache 跟据size计算出适合的class的ID 从mcache的alloc[class]链表中查询可用的span 如果mcache没有可用的span则从mcentral申请一个新的span加入mcache中 如果mcentral中也没有可用的span则从mheap中申请一个新的span加入mcentral 从该span中获取到空闲对象地址并返回 总结 Golang程序启动时申请一大块内存，并划分成spans、bitmap、arena区域 arena区域按页划分成一个个小块 span管理一个或多个页 mcentral管理多个span供线程申请使用 mcache作为线程私有资源，资源来源于mcentral ","date":"2019-06-24","objectID":"/posts/go_memory%E4%B8%8A/:0:0","tags":["golang"],"title":"go内存分配（上）","uri":"/posts/go_memory%E4%B8%8A/"},{"categories":["笔记"],"content":"go内存分配（下）","date":"2019-06-24","objectID":"/posts/go_memory%E4%B8%8B/","tags":["golang"],"title":"go内存分配（下）","uri":"/posts/go_memory%E4%B8%8B/"},{"categories":["笔记"],"content":"栈 栈区的内存一般是由编译器自动进行分配和释放，其中存储着函数的入参及局部变量，这些参数会随着函数的创建而创建，函数的销毁而销毁。 堆 堆区的内存一般有编译器和工程师自己共同进行管理分配，交给runtimeGc来释放。堆上分配必须找到一块足够大的内存来存放新的变量数据。后续释放时，垃圾回收器扫描堆空间寻找不再被使用的对象。 内存基础 内存： cpu与硬盘速度不匹配，引入内存作为中间缓冲。 cache: cpu与内存速度也不匹配，引入chache作为中间缓冲，并逐渐发展为3级cache。L1、L2、L3。其中l1速度最快 虚拟内存： 虚拟内存是当代操作系统必备的一项重要功能了，它向进程屏蔽了底层了RAM和磁盘，并向进程提供了远超物理内存大小的内存空间。 进程访问数据，当Cache没有命中的时候，访问虚拟内存获取数据，当前要访问的虚拟内存地址，是否已经加载到了物理内存，如果已经在物理内存，则取物理内存数据，如果没有对应的物理内存，则从磁盘加载数据到物理内存，并把物理内存地址和虚拟内存地址更新到页表。 在没有虚拟内存的时代，物理内存对所有进程是共享的，多进程同时访问同一个物理内存存在并发访问问题。引入虚拟内存后，每个进程都要各自的虚拟内存，内存的并发访问问题的粒度从多进程级别，可以降低到多线程级别。 堆和栈： 虚拟内存中的栈和堆，也就是进程对内存的管理。 栈和堆相比有这么几个好处： 栈的内存管理简单，分配比堆上快。 栈的内存不需要回收，而堆需要，无论是主动free，还是被动的垃圾回收，这都需要花费额外的CPU。 TCMalloc TCMalloc是go内存分配的前辈，Go的内存管理是借鉴了TCMalloc，随着Go的迭代，Go的内存管理与TCMalloc不一致地方在不断扩大，但其主要思想、原理和概念都是和TCMalloc一致的。 引入虚拟内存后，让内存的并发访问问题的粒度从多进程级别，降低到多线程级别。但是同一个进程中多个线程申请内存时需要加锁，如果不加锁就存在同一块内存被2个线程同时访问的问题。 TCMalloc的做法是什么呢？为每个线程预分配一块缓存，线程申请小内存时，可以从缓存分配内存，这样有2个好处： 以后线程在再分配内存就是在用户态，不需要系统调用即可。 线程利用自己的内存缓存，不用加锁了。 ","date":"2019-06-24","objectID":"/posts/go_memory%E4%B8%8B/:0:0","tags":["golang"],"title":"go内存分配（下）","uri":"/posts/go_memory%E4%B8%8B/"},{"categories":["笔记"],"content":"TCMalloc的几个重要概念 Page：操作系统对内存管理以页为单位，TCMalloc也是这样，只不过TCMalloc里的Page大小与操作系统里的大小并不一定相等，x64下Page大小是8KB。 Span：一组连续的Page被称为Span，比如可以有2个页大小的Span，也可以有16页大小的Span，Span比Page高一个层级，是为了方便管理一定大小的内存区域，Span是TCMalloc中内存管理的基本单位。 ThreadCache：每个线程各自的Cache。由于每个线程有自己的ThreadCache，所以ThreadCache访问是无锁的。 CentralCache：是所有线程共享的缓存，也是保存的空闲内存块链表，链表的数量与ThreadCache中链表数量相同，当ThreadCache内存块不足时，可以从CentralCache取，当ThreadCache内存块多时，可以放回CentralCache。由于CentralCache是共享的，所以它的访问是要加锁的。 PageHeap：PageHeap是堆内存的抽象，当CentralCache没有内存的时，会从PageHeap取，把1个Span拆成若干内存块，添加到对应大小的链表中，当CentralCache内存多的时候，会放回PageHeap。 TCMalloc中有小、中、大对象概念，Go内存管理中也有类似的概念，我们瞄一眼TCMalloc的定义： 小对象大小：0~256KB 中对象大小：257~1MB 大对象大小：\u003e1MB 小对象的分配流程：ThreadCache -\u003e CentralCache -\u003e HeapPage，大部分时候，ThreadCache缓存都是足够的，不需要去访问CentralCache和HeapPage，无锁分配加无系统调用，分配效率是非常高的。 中对象分配流程：直接在PageHeap中选择适当的大小即可，128 Page的Span所保存的最大内存就是1MB。 大对象分配流程：从large span set选择合适数量的页面组成span，用来存储数据。 Go内存管理 通过TCMalloc的了解，可以看出他的精髓是内存的分层。go借鉴了他。 ","date":"2019-06-24","objectID":"/posts/go_memory%E4%B8%8B/:1:0","tags":["golang"],"title":"go内存分配（下）","uri":"/posts/go_memory%E4%B8%8B/"},{"categories":["笔记"],"content":"go内存的概念 Page：与TCMalloc中的Page相同，x64下1个Page的大小是8KB。 pan：与TCMalloc中的Span相同，Span是内存管理的基本单位，代码中为mspan，一组连续的Page组成1个Span。 mcache：mcache与TCMalloc中的ThreadCache类似，mcache保存的是各种大小的Span，并按Span class分类，小对象直接从mcache分配内存，它起到了缓存的作用，并且可以无锁访问。 但mcache与ThreadCache也有不同点，TCMalloc中是每个线程1个ThreadCache，Go中是每个P拥有1个mcache，因为在Go程序中，当前最多有GOMAXPROCS个线程在用户态运行，所以最多需要GOMAXPROCS个mcache就可以保证各线程对mcache的无锁访问，线程的运行又是与P绑定的，把mcache交给P刚刚好。 mcentral：mcentral与TCMalloc中的CentralCache类似，是所有线程共享的缓存，需要加锁访问，它按Span class对Span分类，串联成链表，当mcache的某个级别Span的内存被分配光时，它会向mcentral申请1个当前级别的Span。 mheap：mheap与TCMalloc中的PageHeap类似，它是堆内存的抽象，把从OS申请出的内存页组织成Span，并保存起来。当mcentral的Span不够用时会向mheap申请，mheap的Span不够用时会向OS申请。 ","date":"2019-06-24","objectID":"/posts/go_memory%E4%B8%8B/:2:0","tags":["golang"],"title":"go内存分配（下）","uri":"/posts/go_memory%E4%B8%8B/"},{"categories":["笔记"],"content":"对象的分级 为了更好的应对不同大小的对象的分配，有了class的概念，一共67中class，每种class所拥有的page数量是不同的。size class 0实际并未使用到。 object size：代码里简称size，指申请内存的对象大小。 size class：代码里简称class，它是size的级别，相当于把size归类到一定大小的区间段，比如size[1,8]属于size class 1，size(8,16]属于size class 2。 span class：指span的级别，但span class的大小与span的大小并没有正比关系。span class主要用来和size class做对应，1个size class对应2个span class，2个span class的span大小相同，只是功能不同，1个用来存放包含指针的对象，一个用来存放不包含指针的对象，不包含指针对象的Span就无需GC扫描了。 num of page：代码里简称npage，代表Page的数量，其实就是Span包含的页数，用来分配内存。 go内存分配 Go中的内存分类并不像TCMalloc那样分成小、中、大对象，但是它的小对象里又细分了一个Tiny对象，Tiny对象指大小在1Byte到16Byte之间并且不包含指针的对象。小对象和大对象只用大小划定，无其他区分。 小对象是在mcache中分配的，而大对象是直接从mheap分配的。 ","date":"2019-06-24","objectID":"/posts/go_memory%E4%B8%8B/:3:0","tags":["golang"],"title":"go内存分配（下）","uri":"/posts/go_memory%E4%B8%8B/"},{"categories":["笔记"],"content":"小对象分配 寻找span的流程如下： 计算对象所需内存大小size 根据size到size class映射，计算出所需的size class 根据size class和对象是否包含指针计算出span class 获取该span class指向的span。 对象size-\u003e size class -\u003e span class -\u003e span class -\u003e span -\u003e 从span分配地址。 ","date":"2019-06-24","objectID":"/posts/go_memory%E4%B8%8B/:4:0","tags":["golang"],"title":"go内存分配（下）","uri":"/posts/go_memory%E4%B8%8B/"},{"categories":["笔记"],"content":"大对象分配 大对象的分配比小对象省事多了，99%的流程与mcentral向mheap申请内存的相同，所以不重复介绍了，不同的一点在于mheap会记录一点大对象的统计信息。 ","date":"2019-06-24","objectID":"/posts/go_memory%E4%B8%8B/:5:0","tags":["golang"],"title":"go内存分配（下）","uri":"/posts/go_memory%E4%B8%8B/"},{"categories":["笔记"],"content":"go绝知—unsafe包","date":"2019-06-09","objectID":"/posts/go_unsafe_package/","tags":["golang"],"title":"go绝知—unsafe包","uri":"/posts/go_unsafe_package/"},{"categories":["笔记"],"content":"go中指针 先看下go中的指针，go中的指针与c语言中的指针有很大不同： go指针不支持运算 不同类型无法转换 不同类型指针不可比较 我们不难看出，go中的指针更加安全，但是却失去了灵活性，而且有些通过指针高效的处理数据的能力也失去了。 unsafe包 unsafe包的出现，可以让我们更高效的处理数据，但是和他的名字一样，不安全！ 我们可以利用unsafe包拿到结构体struct中的未导出字段。 使用unsafe 阅读unsafe包文档中列出的规则： 任何类型的指针值都可以转换为unsafe.Pointer。 unsafe.Pointer可以转换为任何类型的指针值。 uintptr可以转换为unsafe.Pointer。 unsafe.Pointer可以转换为uintptr。 简单而言就是：unsafe.Pointer可以与任何类型指针值转换，unintptr可以与unsafe.Pointer互转。 unsafe 包还有其他三个函数： func Sizeof(x ArbitraryType) uintptr func Offsetof(x ArbitraryType) uintptr func Alignof(x ArbitraryType) uintptr ArbitraryType是任意的意思。 ","date":"2019-06-09","objectID":"/posts/go_unsafe_package/:0:0","tags":["golang"],"title":"go绝知—unsafe包","uri":"/posts/go_unsafe_package/"},{"categories":["笔记"],"content":"获取slice长度 我们换种方式来取slice的长度 type slice struct { array unsafe.Pointer // 元素指针 len int // 长度 cap int // 容量 } func main() { s := make([]int, 9, 20) var Len = *(*int)(unsafe.Pointer(uintptr(unsafe.Pointer(\u0026s)) + uintptr(8))) fmt.Println(Len, len(s)) // 9 9 var Cap = *(*int)(unsafe.Pointer(uintptr(unsafe.Pointer(\u0026s)) + uintptr(16))) fmt.Println(Cap, cap(s)) // 20 20 } ","date":"2019-06-09","objectID":"/posts/go_unsafe_package/:1:0","tags":["golang"],"title":"go绝知—unsafe包","uri":"/posts/go_unsafe_package/"},{"categories":["笔记"],"content":"Offsetof 获取成员偏移量 对于一个结构体，通过 offset 函数可以获取结构体成员的偏移量，进而获取成员的地址，读写该地址的内存，就可以达到改变成员值的目的。 这里有一个内存分配相关的事实：结构体会被分配一块连续的内存，结构体的地址也代表了第一个成员的地址。 type Programmer struct { name string language string } func main() { p := Programmer{\"stefno\", \"go\"} fmt.Println(p) name := (*string)(unsafe.Pointer(\u0026p)) *name = \"qcrao\" lang := (*string)(unsafe.Pointer(uintptr(unsafe.Pointer(\u0026p)) + unsafe.Offsetof(p.language))) *lang = \"Golang\" fmt.Println(p) } 这里name成员的地址就是结构体的地址。 ","date":"2019-06-09","objectID":"/posts/go_unsafe_package/:2:0","tags":["golang"],"title":"go绝知—unsafe包","uri":"/posts/go_unsafe_package/"},{"categories":["笔记"],"content":"string 和 slice 的相互转换 string和slice的转换直接强转即可。 func main() { str := \"abc\" var s []byte = []byte(str) fmt.Println(cap(s)) str2 := string(s) fmt.Println(str2) } 这里转换的时候是有内存重新分配转换的。他们底层虽然都是数组，但是string的底层数组是只读的，不能修改的，slice底层数组是可以修改的。 这是一个非常精典的例子。实现字符串和 bytes 切片之间的转换，要求是 zero-copy。想一下，一般的做法，都需要遍历字符串或 bytes 切片，再挨个赋值。 完成这个任务，我们需要了解 slice 和 string 的底层数据结构： type StringHeader struct { Data uintptr Len int } type SliceHeader struct { Data uintptr Len int Cap int } 我们可以看到，string本质还是个结构体，只需要共享底层 []byte 数组就可以实现 zero-copy。 func string2bytes(s string) []byte { stringHeader := (*reflect.StringHeader)(unsafe.Pointer(\u0026s)) bh := reflect.SliceHeader{ Data: stringHeader.Data, Len: stringHeader.Len, Cap: stringHeader.Len, } return *(*[]byte)(unsafe.Pointer(\u0026bh)) } func bytes2string(b []byte) string{ sliceHeader := (*reflect.SliceHeader)(unsafe.Pointer(\u0026b)) sh := reflect.StringHeader{ Data: sliceHeader.Data, Len: sliceHeader.Len, } return *(*string)(unsafe.Pointer(\u0026sh)) } 总结 unsafe 包绕过了 Go 的类型系统，达到直接操作内存的目的，使用它有一定的风险性。但是在某些场景下，使用 unsafe 包提供的函数会提升代码的效率，Go 源码中也是大量使用 unsafe 包。 ","date":"2019-06-09","objectID":"/posts/go_unsafe_package/:3:0","tags":["golang"],"title":"go绝知—unsafe包","uri":"/posts/go_unsafe_package/"},{"categories":["笔记"],"content":"docker-compose","date":"2019-05-27","objectID":"/posts/docker-compose/","tags":["docker"],"title":"docker-compose","uri":"/posts/docker-compose/"},{"categories":["笔记"],"content":"配置文件 一份标准配置文件应该包含 version、services、networks 三大部分，其中最关键的就是 services 和 networks 两个部分。 version: '2' services: web: image: dockercloud/hello-world ports: - 8080 networks: - front-tier - back-tier redis: image: redis links: - web networks: - back-tier lb: image: dockercloud/haproxy ports: - 80:80 links: - web networks: - front-tier - back-tier volumes: - /var/run/docker.sock:/var/run/docker.sock networks: front-tier: driver: bridge back-tier: driver: bridge services services下的一级标签是用户自己定义的，是服务名。 image 则是指定服务的镜像名称或镜像 ID。 build 镜像还可以根据自己的dockerfile生成。build指定Dockerfile文件所在的文件夹路径。可以是相对路径。 command 使用 command 可以覆盖容器启动后默认执行的命令 container_name Compose 的容器名称格式是：\u003c项目名称\u003e\u003c服务名称\u003e\u003c序号\u003e,这个可以指定名字 depends_on 容器依赖，被依赖的容器先执行 environment 这个标签的作用是设置镜像变量，它可以保存变量到镜像里面，也就是说启动的容器也会包含这些变量设置 expose 暴露接口 volumes 挂载目录 networks 加入指定网络 networks networks: front: # Use a custom driver driver: custom-driver-1 back: # Use a custom driver which takes special options driver: custom-driver-2 driver_opts: foo: \"1\" bar: \"2\" 这样就定义了两个网络。 一些场景下，我们并不需要创建新的网络，而只需加入已存在的网络，此时可使用external选项。 ","date":"2019-05-27","objectID":"/posts/docker-compose/:0:0","tags":["docker"],"title":"docker-compose","uri":"/posts/docker-compose/"},{"categories":["笔记"],"content":"git","date":"2019-05-06","objectID":"/posts/git/","tags":["git"],"title":"git","uri":"/posts/git/"},{"categories":["笔记"],"content":"快速入门 ","date":"2019-05-06","objectID":"/posts/git/:0:0","tags":["git"],"title":"git","uri":"/posts/git/"},{"categories":["笔记"],"content":"克隆仓库 git clone http://git.oschina.net/yiibai/git-start.git //如果想在克隆远程仓库的时候，自定义本地仓库的名字，可以使用如下命令： git clone http://git.oschina.net/yiibai/git-start.git mygit-start ","date":"2019-05-06","objectID":"/posts/git/:1:0","tags":["git"],"title":"git","uri":"/posts/git/"},{"categories":["笔记"],"content":"在现有目录中初始化仓库 如果不克隆现有的仓库，而是打算使用 Git 来对现有的项目进行管理。假设有一个项目的目录是：D:\\worksp\\git_sample，只需要进入该项目的目录并输入： git init ","date":"2019-05-06","objectID":"/posts/git/:2:0","tags":["git"],"title":"git","uri":"/posts/git/"},{"categories":["笔记"],"content":"更新提交到仓库 工作目录下的每一个文件都不外乎这两种状态：已跟踪或未跟踪。 已跟踪的文件是指那些被纳入了版本控制的文件，在上一次快照中有它们的记录，在工作一段时间后，它们的状态可能处于未修改，已修改或已放入暂存区。 工作目录中除已跟踪文件以外的所有其它文件都属于未跟踪文件，它们既不存在于上次快照的记录中，也没有放入暂存区。 初次克隆某个仓库的时候，工作目录中的所有文件都属于已跟踪文件，并处于未修改状态 ","date":"2019-05-06","objectID":"/posts/git/:3:0","tags":["git"],"title":"git","uri":"/posts/git/"},{"categories":["笔记"],"content":"检查当前文件状态 git status git status -s 命令或 git status –short 命令，将得到一种更为紧凑的格式输出。 ","date":"2019-05-06","objectID":"/posts/git/:4:0","tags":["git"],"title":"git","uri":"/posts/git/"},{"categories":["笔记"],"content":"跟踪新文件 git add mytext.txt git add . ","date":"2019-05-06","objectID":"/posts/git/:5:0","tags":["git"],"title":"git","uri":"/posts/git/"},{"categories":["笔记"],"content":"忽略文件 文件 .gitignore 的格式规范如下： 所有空行或者以 ＃ 开头的行都会被 Git 忽略。 可以使用标准的 glob 模式匹配。 匹配模式可以以(/)开头防止递归。 匹配模式可以以(/)结尾指定目录。 要忽略指定模式以外的文件或目录，可以在模式前加上惊叹号(!)取反 所谓的 glob 模式是指 shell 所使用的简化了的正则表达式。 星号()匹配零个或多个任意字符；[abc]匹配任何一个列在方括号中的字符(这个例子要么匹配一个字符 a，要么匹配一个字符 b，要么匹配一个字符 c)；问号(?)只匹配一个任意字符；如果在方括号中使用短划线分隔两个字符，表示所有在这两个字符范围内的都可以匹配(比如 [0-9] 表示匹配所有 0 到 9 的数字)。 使用两个星号() 表示匹配任意中间目录，比如a/**/z 可以匹配 a/z, a/b/z 或 a/b/c/z等。 ","date":"2019-05-06","objectID":"/posts/git/:6:0","tags":["git"],"title":"git","uri":"/posts/git/"},{"categories":["笔记"],"content":"查看修改内容 git diff ","date":"2019-05-06","objectID":"/posts/git/:7:0","tags":["git"],"title":"git","uri":"/posts/git/"},{"categories":["笔记"],"content":"提交 把暂存区的内容提交前先确定文件是否都添加到暂存区内了 Git commit -m \"提交信息\" Git commit -a 参数可以把变化的文件先添加到暂存区并提交，换句话说Git 就会自动把所有已经跟踪过的文件暂存起来一并提交，从而跳过 git add，新增的为追踪的不行。 ","date":"2019-05-06","objectID":"/posts/git/:8:0","tags":["git"],"title":"git","uri":"/posts/git/"},{"categories":["笔记"],"content":"移除文件 git rm会删除本地文件，推送后也会把仓库里的删除。 如果文件已经添加到缓存区了，那么git rm命令是删除不了的 可以使用下面两个命令： git rm -f 文件名 // 缓存区和工作目录的都会删除 git rm --cached 文件名 // 只删除缓存区，工作目录的保留 ","date":"2019-05-06","objectID":"/posts/git/:9:0","tags":["git"],"title":"git","uri":"/posts/git/"},{"categories":["笔记"],"content":"查看提交历史 git log 会按提交时间列出所有的更新，最近的更新排在最上面。 Git log -p // 用来显示每次提交的内容差异 另外一个常用的选项是 –pretty。 这个选项可以指定使用不同于默认格式的方式展示提交历史。 这个选项有一些内建的子选项供你使用。 比如用 oneline 将每个提交放在一行显示，查看的提交数很大时非常有用。 另外还有 short，full 和 fuller 可以用，展示的信息或多或少有些不同 git log --pretty=oneline ","date":"2019-05-06","objectID":"/posts/git/:10:0","tags":["git"],"title":"git","uri":"/posts/git/"},{"categories":["笔记"],"content":"远程仓库 可以有好几个远程仓库，通常有些仓库对你只读，有些则可以读写。 与他人协作涉及管理远程仓库以及根据需要推送或拉取数据。 ","date":"2019-05-06","objectID":"/posts/git/:11:0","tags":["git"],"title":"git","uri":"/posts/git/"},{"categories":["笔记"],"content":"查看远程仓库 git remote 命令。 它会列出你指定的每一个远程服务器的简写。 如果已经克隆了自己的仓库，那么至少应该能看到 origin - 这是 Git 给你克隆的仓库服务器的默认名字。 也可以指定选项 -v，会显示需要读写远程仓库使用的 Git 保存的简写与其对应的 URL。 ","date":"2019-05-06","objectID":"/posts/git/:12:0","tags":["git"],"title":"git","uri":"/posts/git/"},{"categories":["笔记"],"content":"添加远程仓库 运行 git remote add 添加一个新的远程 Git 仓库，同时指定一个可以轻松引用的简写 ","date":"2019-05-06","objectID":"/posts/git/:13:0","tags":["git"],"title":"git","uri":"/posts/git/"},{"categories":["笔记"],"content":"从远程仓库中抓取与拉取 就如刚才所见，从远程仓库中获得数据，可以执行: git fetch [remote-name] 这个命令会访问远程仓库，从中拉取所有还没有的数据。执行完成后，将会拥有那个远程仓库中所有分支的引用，可以随时合并或查看。 必须注意 git fetch 命令会将数据拉取到本地仓库 - 它并不会自动合并或修改当前的工作 如果你有一个分支设置为跟踪一个远程分支，可以使用 git pull 命令来自动的抓取然后合并远程分支到当前分支。 这对你来说可能是一个更简单或更舒服的工作流程；默认情况下，git clone 命令会自动设置本地 master 分支跟踪克隆的远程仓库的 master。 ","date":"2019-05-06","objectID":"/posts/git/:14:0","tags":["git"],"title":"git","uri":"/posts/git/"},{"categories":["笔记"],"content":"推送到远程仓库 git push [remote-name] [branch-name]。 当你想要将 master 分支推送到 origin 服务器时(再次说明，克隆时通常会自动帮你设置好那两个名字) git push origin master 只有当你有所克隆服务器的写入权限，并且之前没有人推送过时，这条命令才能生效。 当你和其他人在同一时间克隆，他们先推送到上游然后你再推送到上游，你的推送就会毫无疑问地被拒绝。 你必须先将他们的工作拉取下来并将其合并进你的工作后才能推送。 ","date":"2019-05-06","objectID":"/posts/git/:15:0","tags":["git"],"title":"git","uri":"/posts/git/"},{"categories":["笔记"],"content":"查看远程仓库 如果想要查看某一个远程仓库的更多信息，可以使用 git remote show [remote-name] 命令 git remote show origin * remote origin Fetch URL: http://git.oschina.net/yiibai/git-start.git Push URL: http://git.oschina.net/yiibai/git-start.git HEAD branch: master Remote branch: master tracked Local branch configured for 'git pull': master merges with remote master Local ref configured for 'git push': master pushes to master (fast-forwardable) 它同样会列出远程仓库的 URL 与跟踪分支的信息。 这些信息非常有用，它告诉你正处于 master 分支，并且如果运行 git pull，就会抓取所有的远程引用，然后将远程 master 分支合并到本地 master 分支。 它也会列出拉取到的所有远程引用。 它也同样地列出了哪些远程分支不在你的本地，哪些远程分支已经从服务器上移除了，还有当你执行 git pull 时哪些分支会自动合并。 ","date":"2019-05-06","objectID":"/posts/git/:16:0","tags":["git"],"title":"git","uri":"/posts/git/"},{"categories":["笔记"],"content":"远程仓库的移除与重命名 如果想要重命名引用的名字可以运行 git remote rename 去修改一个远程仓库的简写名。 例如，想要将 gs 重命名为 newgs，可以用 git remote rename 这样做： git remote rename gs newgs 值得注意的是这同样也会修改你的远程分支名字。 那些过去引用 gs/master 的现在会引用 newgs/master。 删除远程库: git remote rm newgs 命令详解 ","date":"2019-05-06","objectID":"/posts/git/:17:0","tags":["git"],"title":"git","uri":"/posts/git/"},{"categories":["笔记"],"content":"git add git add 命令。 这是个多功能命令：可以用它开始跟踪新文件，或者把已跟踪的文件放到暂存区，还能用于合并时把有冲突的文件标记为已解决状态等。 将这个命令理解为“添加内容到下一次提交中”而不是“将一个文件添加到项目中”要更加合适。 ","date":"2019-05-06","objectID":"/posts/git/:18:0","tags":["git"],"title":"git","uri":"/posts/git/"},{"categories":["笔记"],"content":"查看更改记录 git log //查看更改记录 git show 版本号 // 查看某次提交详细信息 ","date":"2019-05-06","objectID":"/posts/git/:19:0","tags":["git"],"title":"git","uri":"/posts/git/"},{"categories":["笔记"],"content":"git rm 删除了文件，也属于文件变化，可以使用： git add 文件 //或者git rm 文件 git commit 来提交变化。 如果想撤销删除，可以参考上面的撤销工作区、暂存区内容。 git rm –cached newfile_name 就可以将这个文件从暂存区移除掉，但是在工作区里没有消失，如果不加 –cached 参数，就会从工作区和版本库暂存区同时删除，相当于执行了 rm newfile_name 和 git add new_file 两条命令。 ","date":"2019-05-06","objectID":"/posts/git/:20:0","tags":["git"],"title":"git","uri":"/posts/git/"},{"categories":["笔记"],"content":"git pull 执行git pull命令以将其本地存储库与远程存储库同步。 拉取远程代码时别人已经提的可能会与你的冲突 打开冲突文件，eg: a = 10 b = 20 \u003c\u003c\u003c\u003c\u003c\u003c\u003c HEAD c = a + b print(\"The value of c is \", c) def mul(a, b): return (a * b) ======= def sum(a, b): return (a+b) c = sum(a, b) print(\"The value of c is \", c) \u003e\u003e\u003e\u003e\u003e\u003e\u003e 01c54624879782e4657dd6c166ce8818f19e8251 并发现其它开发人员的提交的详细信息，提交ID为：01c54624879782e4657dd6c166ce8818f19e8251 解决完冲突，把冲突文件重新add 一次，再重新push一次， ","date":"2019-05-06","objectID":"/posts/git/:21:0","tags":["git"],"title":"git","uri":"/posts/git/"},{"categories":["笔记"],"content":"暂存stash 代码写到一半需要切换分支，但是写到一半提交上去又不合适，可以先把改变暂存起来，等后面再还原。暂存起来的分支可以在其他分支还原。 git stash // 把当前分支的改变暂存 git stash list // 命令来查看已存在更改的列表。 git stash pop // 命令即可从堆栈中删除更改并将其放置在当前工作目录中 git stash show [\u003cstash\u003e] // 查看某次暂存内容 git stash drop [-q|--quiet] [\u003cstash\u003e] // 删除暂存 git stash apply [-q|--quiet] [\u003cstash\u003e] // 还原暂存 git stash clear git stash save [\u003cmessage\u003e] // 带信息的暂存 ","date":"2019-05-06","objectID":"/posts/git/:22:0","tags":["git"],"title":"git","uri":"/posts/git/"},{"categories":["笔记"],"content":"checkout Git checkout用于切换分支或恢复文件。 Git checkout -b dev origin/dev 生成一个新的dev分支，远程分支对应origin/dev Git branch -D 分支名 删除本地分支（不管有没有合并） Git checkout 文件 // 恢复工作区修改 Git checkout head 文件 // 恢复已经提交到暂存区的修改,工作区也不会保留修改 ","date":"2019-05-06","objectID":"/posts/git/:23:0","tags":["git"],"title":"git","uri":"/posts/git/"},{"categories":["笔记"],"content":"git commit –amend 修改当前最新的提交，可以修改提交信息、文件。注意，只能改当前最新的那次，如果想改以前的，那就麻烦一点，需要先回到那个版本才行。 eg: Git commit –amend -m “这才是正确的，刚才那次是错误的” Git commit -a -m \"xx\" // 添加到暂存区并提交文件 标准的git commit分为3个部分，header、body、footer,后两个一般不用。 heder结构为 type(scope):subject Type一般有几种类型： feat:新功能 fix:修复bug style:格式 refactor:代码重构 chore:项目构建 比如fix(pay):修复了xxbug ","date":"2019-05-06","objectID":"/posts/git/:24:0","tags":["git"],"title":"git","uri":"/posts/git/"},{"categories":["笔记"],"content":"git merge Git merge dev // 从dev合并到当前分支 合并后可能会有冲突，如果想放弃合并回到刚才，使用git merger –abort命令。 ","date":"2019-05-06","objectID":"/posts/git/:25:0","tags":["git"],"title":"git","uri":"/posts/git/"},{"categories":["笔记"],"content":"git tag 使用git tag命令来标记当前HEAD指针。在创建标签时需要提供-a选项的标签名称，并提供带-m选项的标签消息。 Git tag -a 'v1.0.0' -m \"v1.0.0 tag\" 如果要标记特定提交，则使用相应的COMMIT ID而不是HEAD指针。使用以下命令将标签推送到远程存储库。 Git tag -a 'v1.0.0' -m \"v1.0.0 tag\" 1852b48f8d1d86bce1694f99039833e3eaa55bc9 Git push origin tag v1.0.0 // 推送tag至远程 Git show v1.0.0 // 查看tag详细信息 Git tag -d v1.0.0 // 删除标签 ","date":"2019-05-06","objectID":"/posts/git/:26:0","tags":["git"],"title":"git","uri":"/posts/git/"},{"categories":["笔记"],"content":"git config git的配置分为全局和各仓库的单独配置。 查看配置 git config –list 如果当前目录是git仓库就能看到该仓库的单独配置。 git config –global user.name配置全局用户名 git config –global user.email配置全局邮箱 git config –local user.name在仓库地址路径配置该仓库的特定用户名，如果不指定–local默认也是该效果，配置邮箱等信息同理。 如果不对各仓库配置自己的邮箱，可能会造成邮箱不必要的暴露。 git各区 ","date":"2019-05-06","objectID":"/posts/git/:27:0","tags":["git"],"title":"git","uri":"/posts/git/"},{"categories":["笔记"],"content":"git本地仓库 各区之间数据状态转换流程 ","date":"2019-05-06","objectID":"/posts/git/:28:0","tags":["git"],"title":"git","uri":"/posts/git/"},{"categories":["笔记"],"content":"工作区和暂存区 我们先来理解下Git 工作区、暂存区和版本库概念 工作区：就是你在电脑里能看到的目录,工作区就是项目所在的目录 暂存区：英文叫stage, 或index。一般存放在\"git目录\"下的index文件（.git/index）中，所以我们把暂存区有时也叫作索引（index）。文件修改后，用add命令放到暂存区，然后在通过commit命令放到分支上。每次的修改只有先add到暂存区才能被提交。 版本库：工作区有一个隐藏目录.git，这个不算工作区，而是Git的版本库。 总结起来一个文件的状态通常可以分为： 不受版本控制的 untracked 状态 受版本控制并且已修改的 modified 状态 受版本控制已修改并提交到暂存区的 staged 状态 从暂存区已经提交到本地仓库的 committed 状态 提交到本地仓库未修改或者从远程仓库克隆下来的 unmodified 状态 Git回退命令 ","date":"2019-05-06","objectID":"/posts/git/:29:0","tags":["git"],"title":"git","uri":"/posts/git/"},{"categories":["笔记"],"content":"git checkout 这个命令又出现了，上次是总结 git branch 分支操作的时候，git checkout 可以用来新建或者切换分支，这次总结回退版本的命令，git checkout 也可以用来回退文件版本，很神奇吧。 其实这个命令的作用就是它单词的本义——检出，他的常用操作也取自这个意思，比如 git checkout branch_name 切换分支操作，实际上就是把指定分支在仓库中对应的所有文件检出来覆盖当前工作区，最终表现就是切换了分支。 而针对于文件的检出可以使用 git checkout – file_name，当不指定 commit id 就是将暂存区的内容恢复到工作区，也就可以达到回退本地修改的作用。 不过，这个身兼数职的 git checkout 命令现在可以轻松一些了，从 Git 2.23 版本开始引入了两个新的命令： git switch 用来切换分支，git restore用来还原工作区的文件 ","date":"2019-05-06","objectID":"/posts/git/:30:0","tags":["git"],"title":"git","uri":"/posts/git/"},{"categories":["笔记"],"content":"git restore 这个命令是 Git 2.23 版本之后新加的，用来分担之前 git checkout 命令的功能, git restore –staged 从暂存区撤销，回到工作区，就是git add 的逆操作。 git restore 从工作区撤销，就是放弃更改 ","date":"2019-05-06","objectID":"/posts/git/:31:0","tags":["git"],"title":"git","uri":"/posts/git/"},{"categories":["笔记"],"content":"git reset reset 重新设置的意思，其实就是用来设置分支的头部指向，当进行了一系列的提交之后，忽然发现最近的几次提交有问题，想从提交记录中删除，这是就会用到 git reset 命令，这个命令后面跟 commit id，表示当前分支回退到这个 commit id 对应的状态，之后的日志记录被删除，工作区中的文件状态根据参数的不同会恢复到不同的状态。 –soft: 被回退的那些版本的修改会被放在暂存区，可以再次提交。 –mixed: 默认选项，被回退的那些版本的修改会放在工作目录，可以先加到暂存区，然后再提交。 –hard: 被回退的那些版本的修改会直接舍弃，好像它们没有来过一样。 这样来看，git reset 命令好像是用来回退版本的，但是如果使用 git rest HEAD file_name 命令就可以将一个文件回退到 HEAD 指向版本所对应的状态，其实就是当前版本库中的状态，也就相当于还原了本地的修改。 git每次commit都是一个快照，方便以后查找或恢复。 在Git中，用HEAD表示当前版本，上一个版本就是HEAD^，上上一个版本就是HEAD^^，当然往上100个版本写100个^比较容易数不过来，所以写成HEAD~100。 git reset –hard HEAD^ 这样就回到了上个快照，在查看git log发现刚提交的那条记录不在了。当然如果你还能记住刚才那个最新的快照id还是可以回去的。 如果记不住了就再也回不去了吗？ 当然不是，Git提供了一个命令git reflog用来记录你的每一次命令。 然后 git reset –hard 快照id 就可以再回来啦。 git revert 也可以回退一个已经提交到仓库的版本 git revert 版本号 即可。会生成一个提交记录，提交记录明确记录着revert 的是哪个版本，原来的版本记录都是在的。如果对revert再revert一次，那就等于负负得正，回到原来的版本中。 注意，revert针对最后一次的commit比较方便，如果想revert上上一次或更前面的，就会冲突。 如果修改已经push到远程了，需要本地回退后，在push一次 merge和rebase git rebase和git merge区别： git rebase后变成了一个线，且不会生成新的合并节点。 git merge后两个分支记录依然各自存在，会生成新的合并节点。 git merge冲突后需要手动add并commit，git rebase自动生成一次提交。 当完成一个特性的开发并将其合并到 master 分支时，我们有两种方式：git merge 和 git rebase。 看merge图解： rebase图解： 我们可以看到，merge后会产生一个多余的commit,但是每个提交记录先后顺序不变。 rebase没有生产新的commit，把两个分支合成一个线性的记录，可读性更强，但是提交记录的先后顺序变了。 总结： 当需要保留详细的合并信息的时候建议使用git merge，特别是需要将分支合并进入master分支时；当发现自己修改某个功能时，频繁进行了git commit提交时，发现其实过多的提交信息没有必要时，可以尝试git rebase。 ","date":"2019-05-06","objectID":"/posts/git/:32:0","tags":["git"],"title":"git","uri":"/posts/git/"},{"categories":["笔记"],"content":"树结构","date":"2019-04-12","objectID":"/posts/tree/","tags":["数据结构"],"title":"树结构","uri":"/posts/tree/"},{"categories":["笔记"],"content":"平衡二叉树 平衡二叉树是基于二分法的策略提高数据的查找速度的二叉树的数据结构。 特点 非叶子节点只能允许最多两个子节点存在。 每一个非叶子节点数据分布规则为左边的子节点小当前节点的值，右边的子节点大于当前节点的值 树的左右两边的层级数相差不会大于1; 没有值相等重复的节点; 优势 层级越深，查找速度越慢。 为了保证树的结构左右两端数据大致平衡降低二叉树的查询难度一般会采用一种算法机制实现节点数据结构的平衡，实现了这种算法的有比如AVL、Treap、红黑树，使用平衡二叉树能保证数据的左右两边的节点层级相差不会大于1，通过这样避免树形结构由于删除增加变成线性链表影响查询效率，保证数据平衡的情况下查找数据的速度近于二分法查找。 ","date":"2019-04-12","objectID":"/posts/tree/:0:1","tags":["数据结构"],"title":"树结构","uri":"/posts/tree/"},{"categories":["笔记"],"content":"红黑树 红黑树是一颗二叉树。它有自己的特定的规则来保证树的深度比普通的二叉排序树小，但它不是完全的平衡二叉树。 特点 每个结点或者为黑色或者为红色。 根结点为黑色。 每个叶结点(实际上就是NULL指针)都是黑色的。 如果一个结点是红色的，那么它的两个子节点都是黑色的（也就是说，不能有两个相邻的红色结点）。 对于每个结点，从该结点到其所有子孙叶结点的路径中所包含的黑色结点数量必须相同。 优势 红黑树通过上面这种限制来保证它大致是平衡的——因为红黑树的高度不会无限增高。 红黑树是一个更高效的检索二叉树，因此常常用来实现关联数组。典型地，JDK 提供的集合类 TreeMap 本身就是一个红黑树的实现。 红黑树左旋 红黑树插入或删除后，以不满足条件，要进行旋转并着色。 左旋： 红黑树右旋 ","date":"2019-04-12","objectID":"/posts/tree/:0:2","tags":["数据结构"],"title":"树结构","uri":"/posts/tree/"},{"categories":["笔记"],"content":"b树 B树和平衡二叉树稍有不同的是B树属于多叉树又名平衡多路查找树（查找路径不只两个）。 特点 所有节点关键字是按递增次序排列，并遵循左小右大原则； 非叶节点的子节点数\u003e1，且\u003c=M ，且M\u003e=2。M就是叉路树。 枝节点的关键字数量大于等于ceil(m/2)-1个且小于等于M-1个（注：ceil()是个朝正无穷方向取整的函数 如ceil(1.1)结果为2); 所有叶子节点均在同一层、叶子节点除了包含了关键字和关键字记录的指针外也有指向其子节点的指针只不过其指针地址都为null对应下图最后一层节点的空格子; 查找流程 如上图我要从上图中找到E字母，查找流程如下 获取根节点的关键字进行比较，当前根节点关键字为M，E\u003cM（26个字母顺序），所以往找到指向左边的子节点（二分法规则，左小右大，左边放小于当前节点值的子节点、右边放大于当前节点值的子节点）； 拿到关键字D和G，D\u003cE\u003cG 所以直接找到D和G中间的节点； 拿到E和F，因为E=E 所以直接返回关键字和指针信息（如果树结构里面没有包含所要查找的节点则返回null）； 优势 B树相对于平衡二叉树的不同是，每个节点包含的关键字增多了，把树的节点关键字增多后树的层级比原来的二叉树少了，减少数据查找的次数和复杂度。 ","date":"2019-04-12","objectID":"/posts/tree/:0:3","tags":["数据结构"],"title":"树结构","uri":"/posts/tree/"},{"categories":["笔记"],"content":"b+树 B+树是B树的一个升级版，相对于B树来说B+树更充分的利用了节点的空间，让查询速度更加稳定，其速度完全接近于二分法查找。 特点 B+跟B树不同B+树的非叶子节点不保存关键字记录的指针，只进行数据索引，这样使得B+树每个非叶子节点所能保存的关键字大大增加； B+树叶子节点保存了父节点的所有关键字记录的指针，所有数据地址必须要到叶子节点才能获取到。所以每次数据查询的次数都一样； B+树叶子节点的关键字从小到大有序排列，左边结尾数据都会保存右边节点开始数据的指针。 非叶子节点的子节点数=关键字数 优势 B+树的层级更少：相较于B树B+每个非叶子节点存储的关键字数更多，树的层级更少所以查询数据更快； B+树查询速度更稳定：B+所有关键字数据地址都存在叶子节点上，所以每次查找的次数都相同所以查询速度要比B树更稳定; B+树天然具备排序功能：B+树所有的叶子节点数据构成了一个有序链表，在查询大小区间的数据时候更方便，数据紧密性很高，缓存的命中率也会比B树高。 B+树全节点遍历更快：B+树遍历整棵树只需要遍历所有的叶子节点即可，，而不需要像B树一样需要对每一层进行遍历，这有利于数据库做全表扫描 ","date":"2019-04-12","objectID":"/posts/tree/:0:4","tags":["数据结构"],"title":"树结构","uri":"/posts/tree/"},{"categories":["笔记"],"content":"elasticsearch","date":"2019-03-27","objectID":"/posts/elasticsearch/","tags":["项目"],"title":"elasticsearch","uri":"/posts/elasticsearch/"},{"categories":["笔记"],"content":"安装启动 elasticsearch需要依赖jdk elastic默认是9200端口 ctrl+c elastic就会停止 默认情况下，Elastic 只允许本机访问，如果需要远程访问，可以修改 Elastic 安装目录的config/elasticsearch.yml文件，去掉network.host的注释，将它的值改成0.0.0.0，然后重新启动 Elastic。 基本概念 elastic可以看作是分布式数据库，每台elastic实例就是一个node，一组node构成一个集群cluster。 index会索引所有字段，elastic顶层单位就是index。类似于mysql的数据库。 document 就是index里面的单条记录。存储格式为json。类似于mysql的记录。 type是单条记录的分组，一个index里面可以有多个type，但是推荐一index里面一个type。Elastic 6.x 版只允许每个 Index 包含一个 Type，7.x 版将会彻底移除 Type。类似于msyql中的表。 库表操作 查看所有index curl -X GET ‘http://localhost:9200/_cat/indices?v’ 删除index curl -X DELETE ’localhost:9200/weather' 查看映射器（表结构） curl ’localhost:9200/_mapping?pretty=true' 新增index并指定表数据结构 $ curl -X PUT 'localhost:9200/accounts' -d ' { \"mappings\": { \"person\": { \"properties\": { \"user\": { \"type\": \"text\", \"analyzer\": \"ik_max_word\", \"search_analyzer\": \"ik_max_word\" }, \"title\": { \"type\": \"text\", \"analyzer\": \"ik_max_word\", \"search_analyzer\": \"ik_max_word\" }, \"desc\": { \"type\": \"text\", \"analyzer\": \"ik_max_word\", \"search_analyzer\": \"ik_max_word\" } } } } }' 这里有三个字段，而且类型都是文本（text），所以需要指定中文分词器，不能使用默认的英文分词器。分词器要安装插件。 数据curd ","date":"2019-03-27","objectID":"/posts/elasticsearch/:0:0","tags":["项目"],"title":"elasticsearch","uri":"/posts/elasticsearch/"},{"categories":["笔记"],"content":"新增 $ curl -X PUT 'localhost:9200/accounts/person/1' -d ' { \"user\": \"张三\", \"title\": \"工程师\", \"desc\": \"数据库管理\" }' 向accounts索引person类别（表）中插入数据，指定记录的id为1。如果不指定，那么会自动随机生成一个字符串作为id，新增记录的时候，也可以不指定 Id，这时要改成 POST 请求。 ","date":"2019-03-27","objectID":"/posts/elasticsearch/:1:0","tags":["项目"],"title":"elasticsearch","uri":"/posts/elasticsearch/"},{"categories":["笔记"],"content":"查看 curl ’localhost:9200/accounts/person/1?pretty=true' { \"_index\" : \"accounts\", \"_type\" : \"person\", \"_id\" : \"1\", \"_version\" : 1, \"found\" : true, \"_source\" : { \"user\" : \"张三\", \"title\" : \"工程师\", \"desc\" : \"数据库管理\" } } 我们还可以对字段完全相等查询 GET /accounts/person/_search?q=user:Smith ","date":"2019-03-27","objectID":"/posts/elasticsearch/:2:0","tags":["项目"],"title":"elasticsearch","uri":"/posts/elasticsearch/"},{"categories":["笔记"],"content":"删除 curl -X DELETE ’localhost:9200/accounts/person/1' ","date":"2019-03-27","objectID":"/posts/elasticsearch/:3:0","tags":["项目"],"title":"elasticsearch","uri":"/posts/elasticsearch/"},{"categories":["笔记"],"content":"更新 更新记录就是使用 PUT 请求，重新发送一次数据。 $ curl -X PUT 'localhost:9200/accounts/person/1' -d ' { \"user\" : \"张三\", \"title\" : \"工程师\", \"desc\" : \"数据库管理，软件开发\" }' { \"_index\":\"accounts\", \"_type\":\"person\", \"_id\":\"1\", \"_version\":2, \"result\":\"updated\", \"_shards\":{\"total\":2,\"successful\":1,\"failed\":0}, \"created\":false } 复杂查询 ","date":"2019-03-27","objectID":"/posts/elasticsearch/:4:0","tags":["项目"],"title":"elasticsearch","uri":"/posts/elasticsearch/"},{"categories":["笔记"],"content":"查询所有记录。 使用 GET 方法，直接请求/Index/Type/_search，就会返回所有记录。 $ curl 'localhost:9200/accounts/person/_search' { \"took\":2, \"timed_out\":false, \"_shards\":{\"total\":5,\"successful\":5,\"failed\":0}, \"hits\":{ \"total\":2, \"max_score\":1.0, \"hits\":[ { \"_index\":\"accounts\", \"_type\":\"person\", \"_id\":\"AV3qGfrC6jMbsbXb6k1p\", \"_score\":1.0, \"_source\": { \"user\": \"李四\", \"title\": \"工程师\", \"desc\": \"系统管理\" } }, { \"_index\":\"accounts\", \"_type\":\"person\", \"_id\":\"1\", \"_score\":1.0, \"_source\": { \"user\" : \"张三\", \"title\" : \"工程师\", \"desc\" : \"数据库管理，软件开发\" } } ] } } 上面代码中，返回结果的 took字段表示该操作的耗时（单位为毫秒），timed_out字段表示是否超时，hits字段表示命中的记录，里面子字段的含义如下。 total：返回记录数，本例是2条。 max_score：最高的匹配程度，本例是1.0。 hits：返回的记录组成的数组。 返回的记录中，每条记录都有一个_score字段，表示匹配的程序，默认是按照这个字段降序排列。 ","date":"2019-03-27","objectID":"/posts/elasticsearch/:5:0","tags":["项目"],"title":"elasticsearch","uri":"/posts/elasticsearch/"},{"categories":["笔记"],"content":"精准查询 精准查询用的是term { \"term\" : { \"price\" : 20 } } 精准查询的时候就不用再对结果项进行匹配度评分了，用constant_score ","date":"2019-03-27","objectID":"/posts/elasticsearch/:6:0","tags":["项目"],"title":"elasticsearch","uri":"/posts/elasticsearch/"},{"categories":["笔记"],"content":"模糊查询 match是模糊查询，只要包含就行。 Elastic 的查询非常特别，使用自己的查询语法，要求 GET 请求带有数据体。 $ curl 'localhost:9200/accounts/person/_search' -d '{ \"query\" : { \"match\" : { \"desc\" : \"软件\" }} }' 上面代码使用 Match 查询，指定的匹配条件是desc字段里面包含\"软件\"这个词。返回结果如下。 { \"took\":3, \"timed_out\":false, \"_shards\":{\"total\":5,\"successful\":5,\"failed\":0}, \"hits\":{ \"total\":1, \"max_score\":0.28582606, \"hits\":[ { \"_index\":\"accounts\", \"_type\":\"person\", \"_id\":\"1\", \"_score\":0.28582606, \"_source\": { \"user\" : \"张三\", \"title\" : \"工程师\", \"desc\" : \"数据库管理，软件开发\" } } ] } } 默认是10条记录,我们还可以分页 $ curl 'localhost:9200/accounts/person/_search' -d ' { \"query\" : { \"match\" : { \"desc\" : \"管理\" }}, \"from\": 1, //默认是从位置0开始 \"size\": 1 }' ","date":"2019-03-27","objectID":"/posts/elasticsearch/:7:0","tags":["项目"],"title":"elasticsearch","uri":"/posts/elasticsearch/"},{"categories":["笔记"],"content":"短语查询 上面的查询表达式中match是只要包含就行，那怕他们是不在一起的，但有时我们需要查询必须是在一起的，比如短语。 GET /megacorp/employee/_search { \"query\" : { \"match_phrase\" : { \"about\" : \"rock climbing\" } } } 这里match换成了match_phrase ","date":"2019-03-27","objectID":"/posts/elasticsearch/:8:0","tags":["项目"],"title":"elasticsearch","uri":"/posts/elasticsearch/"},{"categories":["笔记"],"content":"高亮搜索 就是把搜索的结果高亮显示 ","date":"2019-03-27","objectID":"/posts/elasticsearch/:9:0","tags":["项目"],"title":"elasticsearch","uri":"/posts/elasticsearch/"},{"categories":["笔记"],"content":"组合查询 有时候需要把多个查询组合起来，用bool { \"bool\" : { \"must\" : [],//and \"should\" : [],//or \"must_not\" : [],//非 } } must,should,must_not不必全出现。 集群 一个运行中的 Elasticsearch 实例称为一个 节点，而集群是由一个或者多个拥有相同 cluster.name 配置的节点组成， 它们共同承担数据和负载的压力。当有节点加入集群中或者从集群中移除节点时，集群将会重新平均分布所有的数据。 当一个节点被选举成为 主 节点时， 它将负责管理集群范围内的所有变更，例如增加、删除索引，或者增加、删除节点等。 而主节点并不需要涉及到文档级别的变更和搜索等操作，所以当集群只拥有一个主节点的情况下，即使流量的增加它也不会成为瓶颈。 任何节点都可以成为主节点。我们的示例集群就只有一个节点，所以它同时也成为了主节点。 作为用户，我们可以将请求发送到 集群中的任何节点 ，包括主节点。 每个节点都知道任意文档所处的位置，并且能够将我们的请求直接转发到存储我们所需文档的节点。 无论我们将请求发送到哪个节点，它都能负责从各个包含我们所需文档的节点收集回数据，并将最终结果返回給客户端。 Elasticsearch 对这一切的管理都是透明的。 GET /_cluster/health可以查看集群的状态。 golang使用elastic 使用第三方库github.com/olivere/elastic 插入 func main() { client, err := elastic.NewClient(elastic.SetURL(\"http://localhost:9200\")) if err != nil { fmt.Println(\"connect es error\") } //insertElastic(client) //delElastic(client) //queryElasticById(client) BulkAdd(client) } 查询 func queryDocument(client *elastic.Client) { defer client.Stop() boolQuery := elastic.NewBoolQuery() boolQuery =boolQuery.Should(elastic.NewMatchQuery(\"Name\",\"中\")) boolQuery =boolQuery.Should(elastic.NewMatchQuery(\"Age\",12)) //query = query.Must(elastic.NewTermQuery(\"Name\", \"a bc\")) // //query = query.Must(elastic.NewMatchQuery(\"Name\", \"ab\")) //query = query.Must(elastic.NewRangeQuery(\"update_timestamp\").Gte(criteria.UpdateTime)) src, err := boolQuery.Source() if err != nil { panic(err) } data, err := json.MarshalIndent(src, \"\", \" \") if err != nil { panic(err) } fmt.Println(string(data)) esResponse, err := client.Search().Index(\"user\").Type(\"user\"). Query(boolQuery). Sort(\"Age\", true). From(0).Size(10). Do(context.Background()) fmt.Println(esResponse, err) for _, value := range esResponse.Hits.Hits { var doc *model.User json.Unmarshal(*value.Source,\u0026doc) fmt.Println(doc) } } ","date":"2019-03-27","objectID":"/posts/elasticsearch/:10:0","tags":["项目"],"title":"elasticsearch","uri":"/posts/elasticsearch/"},{"categories":["笔记"],"content":"elasticsearch和mysql同步","date":"2019-03-27","objectID":"/posts/es_mysql%E5%90%8C%E6%AD%A5/","tags":["项目"],"title":"elasticsearch和mysql同步","uri":"/posts/es_mysql%E5%90%8C%E6%AD%A5/"},{"categories":["笔记"],"content":"es mysql同步 go-mysql-elasticsearch是用于同步mysql数据到ES集群的一个开源工具。 go-mysql-elasticsearch的基本原理是：如果是第一次启动该程序，首先使用mysqldump工具对源mysql数据库进行一次全量同步，通过elasticsearch client执行操作写入数据到ES；然后实现了一个mysql client,作为slave连接到源mysql,源mysql作为master会将所有数据的更新操作通过binlog event同步给slave， 通过解析binlog event就可以获取到数据的更新内容，之后写入到ES. 我们在启动时的配置的文件中指定数据库地址端口，用户密码，es的地址端口密码，还可以指定同步的库，表对应es的index，type，字段。 启动命令 ./bin/go-mysql-elasticsearch -config=./etc/river.toml ","date":"2019-03-27","objectID":"/posts/es_mysql%E5%90%8C%E6%AD%A5/:0:0","tags":["项目"],"title":"elasticsearch和mysql同步","uri":"/posts/es_mysql%E5%90%8C%E6%AD%A5/"},{"categories":["笔记"],"content":"问题 mysql binlog必须是ROW模式。 要同步的mysql数据表必须包含主键，否则直接忽略，这是因为如果数据表没有主键，UPDATE和DELETE操作就会因为在ES中找不到对应的document而无法进行同步。 ","date":"2019-03-27","objectID":"/posts/es_mysql%E5%90%8C%E6%AD%A5/:1:0","tags":["项目"],"title":"elasticsearch和mysql同步","uri":"/posts/es_mysql%E5%90%8C%E6%AD%A5/"},{"categories":["笔记"],"content":"golang性能分析","date":"2019-03-27","objectID":"/posts/go_pprof%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90/","tags":["golang"],"title":"golang性能分析","uri":"/posts/go_pprof%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90/"},{"categories":["笔记"],"content":"概述 我们要想分析数据，首先需要拿到prof文件或者叫概要文件。 Go 语言为程序开发者们提供了丰富的性能分析 API，和非常好用的标准工具。 这些 API 主要存在于： runtime/pprof； net/http/pprof； runtime/trace； 这三个代码包中。 另外，runtime代码包中还包含了一些更底层的 API。它们可以被用来收集或输出 Go 程序运行过程中的一些关键指标，并帮助我们生成相应的概要文件以供后续分析时使用。 至于标准工具，主要有go tool pprof和go tool trace这两个。 它们可以解析概要文件中的信息，并以人类易读的方式把这些信息展示出来。 此外，go test命令也可以在程序测试完成后生成概要文件。如此一来，我们就可以很方便地使用前面那两个工具读取概要文件，并对被测程序的性能加以分析。 在 Go 语言中，用于分析程序性能的概要文件有三种，分别是：CPU 概要文件（CPU Profile）、内存概要文件（Mem Profile）和阻塞概要文件（Block Profile）。 这些概要文件中包含的都是：在某一段时间内，对 Go 程序的相关指标进行多次采样后得到的概要信息。对于 CPU 概要文件来说，其中的每一段独立的概要信息都记录着，在进行某一次采样的那个时刻，CPU 上正在执行的 Go 代码。而对于内存概要文件，其中的每一段概要信息都记载着，在某个采样时刻，正在执行的 Go 代码以及堆内存的使用情况，这里包含已分配和已释放的字节数量和对象数量。至于阻塞概要文件，其中的每一段概要信息，都代表着 Go 程序中的一个 goroutine 阻塞事件。 在默认情况下，这些概要文件中的信息并不是普通的文本，它们都是以二进制的形式展现的（protocol buffers）。如果你使用一个常规的文本编辑器查看它们的话，那么肯定会看到一堆“乱码”。这时就可以显现出go tool pprof这个工具的作用了。我们可以通过它进入一个基于命令行的交互式界面，并对指定的概要文件进行查阅。 生成概要文件 ","date":"2019-03-27","objectID":"/posts/go_pprof%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90/:0:0","tags":["golang"],"title":"golang性能分析","uri":"/posts/go_pprof%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90/"},{"categories":["笔记"],"content":"通过runtime/pprof包 runtime/pprof：采集程序（非 Server）的运行数据进行分析。 手动调用runtime.StartCPUProfile或者runtime.StopCPUProfile等 API来生成和写入采样文件，灵活性高，通过 runtime/pprof 实现。 func genPprof() { cupOut, err := os.OpenFile(\"cpu.prof\", os.O_RDWR|os.O_CREATE, 0666) if err != nil { log.Fatal(err) } defer cupOut.Close() // memOut, err := os.OpenFile(\"mem.prof\", os.O_RDWR|os.O_CREATE, 0666) // if err != nil { // log.Fatal(err) // } // defer memOut.Close() pprof.StartCPUProfile(cupOut) defer pprof.StopCPUProfile() // pprof.WriteHeapProfile(memOut) // 统计内存信息 for i := 0; i \u003c 1000; i++ { getRand() } } func getRand() { rand.Seed(time.Now().UnixNano()) fmt.Println(rand.Int()) } ","date":"2019-03-27","objectID":"/posts/go_pprof%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90/:1:0","tags":["golang"],"title":"golang性能分析","uri":"/posts/go_pprof%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90/"},{"categories":["笔记"],"content":"net/http/pprof包 net/http/pprof对采集 HTTP Server 的运行时数据进行分析,runtime/pprof和runtime/trace的封装 import ( _ \"net/http/pprof\" ) log.Println(http.ListenAndServe(\"localhost:8082\", nil)) 我们就可以通过http://localhost:8082/debug/pprof来查看，这个域名和端口和我们启动服务的时候一致。点击里面的按钮就会开始相应的采样，采样结束就会自动下载概要文件。 这个路径下还有几个子页面： /debug/pprof/profile：访问这个链接会自动进行 CPU profiling，持续 30s，并生成一个文件供下载 /debug/pprof/heap： Memory Profiling 的路径，访问这个链接会得到一个内存 Profiling 结果的文件 /debug/pprof/block：block Profiling 的路径 /debug/pprof/goroutines：运行的 goroutines 列表，以及调用关系 ","date":"2019-03-27","objectID":"/posts/go_pprof%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90/:2:0","tags":["golang"],"title":"golang性能分析","uri":"/posts/go_pprof%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90/"},{"categories":["笔记"],"content":"通过go test获取 初了上面两种方法，go test: 通过 go test -bench . -cpuprofile prof.cpu生成采样文件 适用对函数进行针对性测试。通常使用上面两个进行整体分析，找出热点以后再go test进行分析 func TestTest(t *testing.T) { for i := 0; i \u003c 1000; i++ { getRand() } } go test -bench=. -cpuprofile=cpu.prof go test -bench . -memprofile=./mem.prof 分析概要文件 ","date":"2019-03-27","objectID":"/posts/go_pprof%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90/:3:0","tags":["golang"],"title":"golang性能分析","uri":"/posts/go_pprof%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90/"},{"categories":["笔记"],"content":"go tool pprof工具 有两种交互方式，一种是进入命令行，另一种可以指定http,通过web的形式更方便 命令行方式查看： go tool pprof最简单的使用方式为: go tool pprof [binary] [source] 其中： binary 是应用的二进制文件，用来解析各种符号； source 表示 profile 数据的来源，可以是本地的文件，也可以是 http 地址。 go tool pprof cpu.prof 如果是web应用也可以自己指定通过接口去拿，下载和分析合为一步： go tool pprof http://localhost:8082/debug/pprof/profile 它要去下载，然后进入命令行，在命令行里通过命令查看 常用的命令有top,可以输入help查看支持的所有命令。 我们可以在交互界面输入top3来查看程序中占用CPU前3位的函数： (pprof) top3 Showing nodes accounting for 100.37s, 87.68% of 114.47s total Dropped 17 nodes (cum \u003c= 0.57s) Showing top 3 nodes out of 4 flat flat% sum% cum cum% 42.52s 37.15% 37.15% 91.73s 80.13% runtime.selectnbrecv 35.21s 30.76% 67.90% 39.49s 34.50% runtime.chanrecv 22.64s 19.78% 87.68% 114.37s 99.91% main.logicCode 其中： flat：当前函数占用CPU的耗时 flat：:当前函数占用CPU的耗时百分比 sun%：函数占用CPU的耗时累计百分比 cum：当前函数加上调用当前函数的函数占用CPU的总耗时 cum%：当前函数加上调用当前函数的函数占用CPU的总耗时百分比 最后一列：函数名称 在大多数的情况下，我们可以通过分析这五列得出一个应用程序的运行情况，并对程序进行优化。 通过web形式查看 命令行查看不方便？只需要在上面的命令中加个参数即可 -http go tool pprof -http localhost:8088 cpu.prof 这个localhost:8088是自己随意指定的。 键入命令后自动打开浏览器http://localhost:8088/ui/ r如果没有安装graphviz,Could not execute dot; may need to install graphviz.Mac安装：brew install graphviz trace大杀器 单单使用 PProf 有时候不一定足够完整，因为在真实的程序中还包含许多的隐藏动作，例如 Goroutine 在执行时会做哪些操作？执行/阻塞了多长时间？在什么时候阻止？在哪里被阻止的？谁又锁/解锁了它们？GC 是怎么影响到 Goroutine 的执行的？这些东西用 PProf 是很难分析出来的。 其实web应用中我们import _ “net/http/pprof\"时通过web就能点击view-\u003etrace拿到trace文件。 不过官方还提供了如下方法 func main() { traceOut, err := os.OpenFile(\"trace.out\", os.O_RDWR|os.O_CREATE, 0666) if err != nil { log.Fatal(err) } defer traceOut.Close() trace.Start(traceOut) defer trace.Stop() go func() { fmt.Println(\"new go\") }() time.Sleep(time.Second) } 有了trace文件，就可以使用go tool trace trace.out自动打开浏览器。 View trace：查看跟踪 Goroutine analysis：Goroutine 分析 Network blocking profile：网络阻塞概况 Synchronization blocking profile：同步阻塞概况 Syscall blocking profile：系统调用阻塞概况 Scheduler latency profile：调度延迟概况 User defined tasks：用户自定义任务 User defined regions：用户自定义区域 Minimum mutator utilization：最低 Mutator 利用率 gin包性能分析 使用gin包开启web,如果再使用import _ “net/http/pprof\"是没有用的。可以使用第三方包。 go get https://github.com/gin-contrib/pprof 示例代码： package main import ( \"net/http\" \"github.com/gin-contrib/pprof\" \"github.com/gin-gonic/gin\" ) func main() { app := gin.Default() pprof.Register(app) // 性能 app.GET(\"/test\", func(c *gin.Context) { c.String(http.StatusOK, \"test\") }) app.Run(\":3000\") } 代码运行之后可以看到系统自动增加了很多/debug/pprof的API。通过这些API我们可以看到需要的数据。使用起来和官方的web没有什么差别。 ","date":"2019-03-27","objectID":"/posts/go_pprof%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90/:4:0","tags":["golang"],"title":"golang性能分析","uri":"/posts/go_pprof%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90/"},{"categories":["笔记"],"content":"golang zaplog使用","date":"2019-03-22","objectID":"/posts/go_zaplog/","tags":["golang"],"title":"golang zaplog使用","uri":"/posts/go_zaplog/"},{"categories":["笔记"],"content":"zap zap使用 uber开源，zap可以在控制台、文件甚至发送数据到其他系统中，以此来记录日志。我们可以指定日志的级别，支持json结构化，方便查询。 和logrus类似，简单来讲，日志有两个概念：字段和消息。字段用来结构化输出错误相关的上下文环境，而消息简明扼要的阐述错误本身。 开发模式下是普通文本的结构： package main import ( \"go.uber.org/zap\" \"time\" ) func main() { // zap.NewDevelopment 开发模式，格式化输出 logger, _ := zap.NewDevelopment() // zap.NewProduction 生产模式json序列化输出 // logger, _ := zap.NewProduction() defer logger.Sync() logger.Info(\"无法获取网址\", zap.String(\"url\", \"http://www.baidu.com\"), zap.Int(\"attempt\", 3), zap.Duration(\"backoff\", time.Second), ) } 自定义配置： func main() { encoderConfig := zapcore.EncoderConfig{ TimeKey: \"time\", LevelKey: \"level\", NameKey: \"logger\", CallerKey: \"caller\", MessageKey: \"msg\", StacktraceKey: \"stacktrace\", LineEnding: zapcore.DefaultLineEnding, EncodeLevel: zapcore.LowercaseLevelEncoder, // 小写编码器 EncodeTime: zapcore.ISO8601TimeEncoder, // ISO8601 UTC 时间格式 EncodeDuration: zapcore.SecondsDurationEncoder, EncodeCaller: zapcore.FullCallerEncoder, // 全路径编码器 } // 设置日志级别 atom := zap.NewAtomicLevelAt(zap.DebugLevel) config := zap.Config{ Level: atom, // 日志级别 Development: true, // 开发模式，堆栈跟踪 Encoding: \"json\", // 输出格式 console 或 json EncoderConfig: encoderConfig, // 编码器配置 InitialFields: map[string]interface{}{\"serviceName\": \"spikeProxy\"}, // 初始化字段，如：添加一个服务器名称 OutputPaths: []string{\"stdout\", \"./logs/spikeProxy.log\"}, // 输出到指定文件 stdout（标准输出，正常颜色） stderr（错误输出，红色） ErrorOutputPaths: []string{\"stderr\"}, } // 构建日志 logger, err := config.Build() if err != nil { panic(fmt.Sprintf(\"log 初始化失败: %v\", err)) } logger.Info(\"log 初始化成功\") logger.Info(\"无法获取网址\", zap.String(\"url\", \"http://www.baidu.com\"), zap.Int(\"attempt\", 3), zap.Duration(\"backoff\", time.Second), ) } lumberjack归档 日志文件越来越大，我们根据大小、日期进行归档。 zap可以写入文件，但是并没有归档的功能。借助于lumberjack第三方库，利用hook进行归档。 import ( \"go.uber.org/zap\" \"go.uber.org/zap/zapcore\" \"time\" \"gopkg.in/natefinch/lumberjack.v2\" \"os\" ) func main() { hook := lumberjack.Logger{ Filename: \"./logs/spikeProxy1.log\", // 日志文件路径 MaxSize: 128, // 每个日志文件保存的最大尺寸 单位：M MaxBackups: 30, // 日志文件最多保存多少个备份 MaxAge: 7, // 文件最多保存多少天 Compress: true, // 是否压缩 } encoderConfig := zapcore.EncoderConfig{ TimeKey: \"time\", LevelKey: \"level\", NameKey: \"logger\", CallerKey: \"linenum\", MessageKey: \"msg\", StacktraceKey: \"stacktrace\", LineEnding: zapcore.DefaultLineEnding, EncodeLevel: zapcore.LowercaseLevelEncoder, // 小写编码器 EncodeTime: zapcore.ISO8601TimeEncoder, // ISO8601 UTC 时间格式 EncodeDuration: zapcore.SecondsDurationEncoder, // EncodeCaller: zapcore.FullCallerEncoder, // 全路径编码器 EncodeName: zapcore.FullNameEncoder, } // 设置日志级别 atomicLevel := zap.NewAtomicLevel() atomicLevel.SetLevel(zap.InfoLevel) core := zapcore.NewCore( zapcore.NewJSONEncoder(encoderConfig), // 编码器配置 zapcore.NewMultiWriteSyncer(zapcore.AddSync(os.Stdout), zapcore.AddSync(\u0026hook)), // 打印到控制台和文件 atomicLevel, // 日志级别 ) // 开启开发模式，堆栈跟踪 caller := zap.AddCaller() // 开启文件及行号 development := zap.Development() // 设置初始化字段 filed := zap.Fields(zap.String(\"serviceName\", \"serviceName\")) // 构造日志 logger := zap.New(core, caller, development, filed) logger.Info(\"log 初始化成功\") logger.Info(\"无法获取网址\", zap.String(\"url\", \"http://www.baidu.com\"), zap.Int(\"attempt\", 3), zap.Duration(\"backoff\", time.Second)) } zap优势及原理 标准log没有日志分级。seelog可分级，支持归档，比较灵活，但是利用反射，效率低。 ","date":"2019-03-22","objectID":"/posts/go_zaplog/:0:0","tags":["golang"],"title":"golang zaplog使用","uri":"/posts/go_zaplog/"},{"categories":["笔记"],"content":"避免 GC: 对象复用 zap 通过 sync.Pool 提供的对象池，复用了大量可以复用的对象，避开了 gc 这个大麻烦。 ","date":"2019-03-22","objectID":"/posts/go_zaplog/:1:0","tags":["golang"],"title":"golang zaplog使用","uri":"/posts/go_zaplog/"},{"categories":["笔记"],"content":"内建的 Encoder: 避免反射 标准库中的 json.Marshaler 提供的是基于类型反射的拼接方式，代价是高昂的。 zap 选择了自己实现 json Encoder。 通过明确的类型调用，直接拼接字符串，最小化性能开销。 ","date":"2019-03-22","objectID":"/posts/go_zaplog/:2:0","tags":["golang"],"title":"golang zaplog使用","uri":"/posts/go_zaplog/"},{"categories":["笔记"],"content":"level handler level handler 是 zap 提供的一种 level 的处理方式，通过 http 请求动态改变日志组件级别。 ","date":"2019-03-22","objectID":"/posts/go_zaplog/:3:0","tags":["golang"],"title":"golang zaplog使用","uri":"/posts/go_zaplog/"},{"categories":["笔记"],"content":"zap logger 提供的 utils zap 还在 logger 这层提供了丰富的工具包，这让整个 zap 库更加的易用: grpc logger：封装 zap logger 可以直接提供给 grpc 使用，对于大多数的 Go 分布式程序，grpc 都是默认的 rpc 方案，grpc 提供了 SetLogger 的接口。 zap 提供了对这个接口的封装。 hook：作为 zap。Core 的实现，zap 提供了 hook。 使用方实现 hook 然后注册到 logger，zap在合适的时机将日志进行后续的处理，例如写 kafka，统计日志错误率 等等。 std Logger: zap 提供了将标准库提供的 logger 对象重定向到 zap logger 中的能力，也提供了封装 zap 作为标准库 logger 输出的能力。 整体上十分易用。 sublog: 通过创建 绑定了 field 的子logger，实现了更加易用的功能。 ","date":"2019-03-22","objectID":"/posts/go_zaplog/:4:0","tags":["golang"],"title":"golang zaplog使用","uri":"/posts/go_zaplog/"},{"categories":["笔记"],"content":"go值传递","date":"2019-03-22","objectID":"/posts/go_%E5%80%BC%E4%BC%A0%E9%80%92/","tags":["golang"],"title":"go值传递","uri":"/posts/go_%E5%80%BC%E4%BC%A0%E9%80%92/"},{"categories":["笔记"],"content":"参数值传递 函数传递的总是原来这个东西的一个副本，一副拷贝。比如我们传递一个int类型的参数，传递的其实是这个参数的一个副本；传递一个指针类型的参数，其实传递的是这个该指针的一份拷贝，而不是这个指针指向的值。 对于int这类基础类型我们可以很好的理解，它们就是一个拷贝，但是指针呢？我们觉得可以通过它修改原来的值，怎么会是一个拷贝呢？下面我们看个例子。 func main() { i:=10 ip:=\u0026i fmt.Printf(\"原始指针的内存地址是：%p\\n\",\u0026ip) modify(ip) fmt.Println(\"int值被修改了，新值为:\",i) } func modify(ip *int){ fmt.Printf(\"函数里接收到的指针的内存地址是：%p\\n\",\u0026ip) *ip=1 } 运行结果： 原始指针的内存地址是：0xc42000c028 函数里接收到的指针的内存地址是：0xc42000c038 int值被修改了，新值为: 1 首先我们要知道，任何存放在内存里的东西都有自己的地址，指针也不例外，它虽然指向别的数据，但是也有存放该指针的内存。 所以通过输出我们可以看到，这是一个指针的拷贝，因为存放这两个指针的内存地址是不同的，虽然指针的值相同，但是是两个不同的指针。 参数引用传递 Go语言(Golang)是没有引用传递的。 引用类型 func main() { persons:=make(map[string]int) persons[\"张三\"]=19 mp:=\u0026persons fmt.Printf(\"原始map的内存地址是：%p\\n\",mp) modify(persons) fmt.Println(\"map值被修改了，新值为:\",persons) } func modify(p map[string]int){ fmt.Printf(\"函数里接收到map的内存地址是：%p\\n\",\u0026p) p[\"张三\"]=20 } 输出： 原始map的内存地址是：0xc42000c028 函数里接收到map的内存地址是：0xc42000c038 map值被修改了，新值为: map[张三:20] map作为函数的参数时，也是值传递，因为地址是不一样的。但是通过函数我们却能改变map里的值，为何？ 通过查看src/runtime/hashmap.go源代码发现，make函数返回的是一个hmap类型的指针*hmap。也就是说map == *hmap。 现在看func modify(p map)这样的函数，其实就等于func modify(p *hmap)。 所以在这里，Go语言通过make函数，字面量的包装，为我们省去了指针的操作，让我们可以更容易的使用map。这里的map可以理解为引用类型，但是记住引用类型不是传引用。 channel类型与map类型同理。src/runtime/chan.go源码中可以看到make函数其实是返回的*hchan slice和map、chan都不太一样的，一样的是，它也是引用类型，它也可以在函数中修改对应的内容。 但是它并不是对指针的换个面具。而是它字段里有指针类型。 我们来看slice大概的结构： type slice struct { array unsafe.Pointer len int cap int } 所以修改类型的内容的办法有很多种，类型本身作为指针可以，类型里有指针类型的字段也可以。 单纯的从slice这个结构体看，我们可以通过modify修改存储元素的内容，但是永远修改不了len和cap，因为他们只是一个拷贝，如果要修改，那就要传递*slice作为参数才可以。 总结 最终我们可以确认的是Go语言中所有的传参都是值传递（传值），都是一个副本，一个拷贝。因为拷贝的内容有时候是非引用类型（int、string、struct等这些），这样就在函数中就无法修改原内容数据；有的是引用类型（指针、map、slice、chan、func等这些），这样就可以修改原内容数据。 是否可以修改原内容数据，和传值、传引用没有必然的关系。 在Go语言里，虽然只有传值，但是我们也可以修改原内容数据，因为参数是引用类型。 这里也要记住，引用类型和传引用是两个概念，go函数只有值传递。 ","date":"2019-03-22","objectID":"/posts/go_%E5%80%BC%E4%BC%A0%E9%80%92/:0:0","tags":["golang"],"title":"go值传递","uri":"/posts/go_%E5%80%BC%E4%BC%A0%E9%80%92/"},{"categories":["笔记"],"content":"hash一致性","date":"2019-03-22","objectID":"/posts/hash%E4%B8%80%E8%87%B4%E6%80%A7/","tags":["项目"],"title":"hash一致性","uri":"/posts/hash%E4%B8%80%E8%87%B4%E6%80%A7/"},{"categories":["笔记"],"content":"普通hash 假如现在需要向4个redis存取图片，以图片的名字为key，value为图片实际存放的路径，我们可以取到key的hash值，然后就hash值进行模运算，然后就定位到该条数据应该存储到哪个redis节点上了。 通过上面的方法提高了效率，我们在取数据的时候就不用遍历4个redis节点，直接到正确的节点去取数据即可，但是有个缺点：当增加或删除节点时，大量的数据就失效，因为再进行取余运算结果已不同。 一致性hash 其实一致性hash还是进行取模运算，只不过取模不在除以节点数，而是2^32。 有个虚拟的圆环，上面均匀分布0-2^32 -1，就像表盘一样，对key取余后就定位到该数据在圆盘的位置了。 我们同样对redis实例节点进行hash取余，也得到了节点的位置。 对数据顺时针查找最近的redis节点，该节点就是应该存取的节点。 根据一致性Hash算法，数据A会被定为到Node A上，B被定为到Node B上，C被定为到Node C上，D被定为到Node D上。 添加或删除节点 现假设Node C不幸宕机，可以看到此时对象A、B、D不会受到影响，只有C对象被重定位到Node D。一般的，在一致性Hash算法中，如果一台服务器不可用，则受影响的数据仅仅是此服务器到其环空间中前一台服务器（即沿着逆时针方向行走遇到的第一台服务器）之间数据，其它不会受到影响 新增节点X时，此时对象Object A、B、D不受影响，只有对象C需要重定位到新的Node X ！一般的，在一致性Hash算法中，如果增加一台服务器，则受影响的数据仅仅是新服务器到其环空间中前一台服务器（即沿着逆时针方向行走遇到的第一台服务器）之间数据，其它数据也不会受到影响。 综上所述，一致性Hash算法对于节点的增减都只需重定位环空间中的一小部分数据，具有较好的容错性和可扩展性。 Hash环的数据倾斜问题 一致性Hash算法在服务节点太少时，容易因为节点分部不均匀而造成数据倾斜（被缓存的对象大部分集中缓存在某一台服务器上）问题，例如系统中只有两台服务器，其环分布如下： 此时必然造成大量数据集中到Node A上，而只有极少量会定位到Node B上。 为了解决这种数据倾斜问题，一致性Hash算法引入了虚拟节点机制，即对每一个服务节点计算多个哈希，每个计算结果位置都放置一个此服务节点，称为虚拟节点。具体做法可以在服务器IP或主机名的后面增加编号来实现。 例如上面的情况，可以为每台服务器计算三个虚拟节点，于是可以分别计算 “Node A#1”、“Node A#2”、“Node A#3”、“Node B#1”、“Node B#2”、“Node B#3”的哈希值，于是形成六个虚拟节点： 例如定位到“Node A#1”、“Node A#2”、“Node A#3”三个虚拟节点的数据均定位到Node A上。这样就解决了服务节点少时数据倾斜的问题。在实际应用中，通常将虚拟节点数设置为32甚至更大，因此即使很少的服务节点也能做到相对均匀的数据分布。 总结 hash一致性当节点增加或减少的时候并不能完全做到无感，但是只有少量的数据进行迁移，这样比原来的hash好多了。 ","date":"2019-03-22","objectID":"/posts/hash%E4%B8%80%E8%87%B4%E6%80%A7/:0:0","tags":["项目"],"title":"hash一致性","uri":"/posts/hash%E4%B8%80%E8%87%B4%E6%80%A7/"},{"categories":["笔记"],"content":"linux","date":"2019-03-22","objectID":"/posts/linux/","tags":["linux"],"title":"linux","uri":"/posts/linux/"},{"categories":["笔记"],"content":"linux主要目录 /bin 放二进制文件，都是可以运行的 /lib 存放系统最基本的动态连接共享库 /dev 放置外接设备，光盘u判断等，外接设备是不能直接使用的，需要挂载（类似于windows的分配盘符） /etc 放置配置文件 /home 除了root用户以外的用户家目录 /proc linux运行时候的进程 /root root账户自己独有的家目录 /sbin 全称是super binary，就是super权限才能运行的二进制文件 /tmp 临时文件 /opt 可选的意思，放些大型软件服务，产生的所有文件也和改软件同目录，卸载的时候直接删除目录即可 /usr 用户自己安装的软件，类似windows的programfiles目录 /usr/local 主要放手动安装的软件，既不是软件管理软件安装的 /var 程序或系统的日志目录 /media 系统会自动识别一些设备，比如u盘 /mnt 用户临时外接设备挂载的时候，挂载到此目录 命令 指令格式： 指令 选项 操作对象 命令的查找顺序为：先在当前目录查找，如果没有再去PATH中查找。 ","date":"2019-03-22","objectID":"/posts/linux/:0:0","tags":["linux"],"title":"linux","uri":"/posts/linux/"},{"categories":["笔记"],"content":"常用 ls -a 所有文件 -l 多列信息 -t 按时间 -F 简单名称并显示文件类型 -h 文件大小带单位 ls -l共8列信息： 文件权限、链接数、用户名、组名、大小、最后修改日期、最后修改时间、文件名 mkdir -p 多级目录 cp cp src dst cp复制文件夹时，需要添加-r选项 mv 移动文件，或者重命名 mv src dst 输出重定向 “\u003e”会覆盖原内容 “»“追加内容 ","date":"2019-03-22","objectID":"/posts/linux/:1:0","tags":["linux"],"title":"linux","uri":"/posts/linux/"},{"categories":["笔记"],"content":"进阶指令 df 查看磁盘空间 df -h 一般看结果的第一行 free 查看内存 free -m以M为单位 tail 查看一个文件的末尾n行,默认是10行 tail -f还可以实时查看文件的内容 wc 文件内容信息统计 -l 行数 -c 字节数 -w单词数 -m字符数 date 读取、设置时间 时间格式： CET欧洲时间 GMT格林威治时间 UTC世界标准时间 CST中国标准时间 CET=UTC/GMT + 1小时 CST=UTC/GMT +8 小时 CST=CET+9 读取时间： date +%F 2016-01-02 date \"+%Y-%m-%d %H:%m:%S\" 2016-01-02 15:04:05 date -d \"-/+1 day/month/year\" \"+%Y-%m-%d %H:%m:%S\" 2016-01-02 15:04:05 id 查看用户的id，组id等 ps process status的缩写 -a 同一终端下的所有进程 -A 所有进程，等于-e top 实时显示进程的状态。 lsof list of file的缩写 可以查看端口占用lsof “:8080” netstat 查看网络连接状态 常用的是netstat -tnpl -t只列出tcp连接 -n显示IP地址和端口 -p显示程序名或pid -l显示作为监听的进程 du 查看文件夹的大小 -s 汇总大小 -h 带单位 du -sh 目录 find 在路径下搜索文件的文件 find 路径 选项 选项值 -name 按文件名来搜索 -type 按文件类型来搜索 f文件 d文件夹 例子 find /etc -name xx.conf service 启动关闭服务 service 服务名 start/stop/restart uname 查看系统信息 -a all详细的信息 uname -all 链接文件 为文件创建快捷方式（链接文件） ln -s 源文件 目标文件 vim 命令模式：通过快捷键进行删除行，粘贴、光标移动等操作 编辑模式：进行编辑 末尾模式：进行搜索、替换、保存、退出等 模式切换 命令\u003e末尾 ： 末尾\u003e命令 连续两次esc 命令\u003e编辑 i 编辑\u003e末尾 esc 命令模式 撤销上一次操作 u 光标移动到行首 ^按键，也就是shift + 6 光标移动到行尾 $按键，也就是shirft + 4 光标移动到首行 gg (good game)重新开始意思 光标移动到末行 G 向上翻屏 ctrl + b backwards 向下翻屏 ctrl + f forward 复制一行 yy 只是复制，并没有粘贴 粘贴 p 编辑模式 进行编辑即可 末尾模式 显示行号 set number 不显示行号 set nonumber 光标移动到特定行 行号+回车 w保存 q退出 wq保存退出 x智能退出，如果修改了就会保存后退出，没修改直接退出 搜索 /关键字 默认是大小写敏感的，在关键字后面加\\c即可不敏感 set hlsearch 搜索后的关键字就会高亮 set nohlsearch 替换所有的关键字 %s/搜索的关键字/替换字/g init进程 初始化进程init运行级别 0 关机 3 不带桌面的 5 带桌面的 6 重启 例子：init 0关机，通过init进程关机。 init 命令需要root权限 用户管理 主要涉及三个文件 /etc/passwd 用户信息 /etc/group 用户组的信息 /etc/shadow 用户的密码信息 useradd useradd 可选项 用户名 -g 主要用户组(一个用户可以有多个组) -G 附加组，非主要组 -u 用户ID 例子：useradd zhangsan usermod usermod 可选项 用户名 -g 主要用户组(一个用户可以有多个组) -G 附加组，非主要组 -u 用户ID -l 修改用户名 修改用户密码 passwd 用户名 注意：用户修改自己的密码，不用跟用户名 切换用户 su 用户名 普通用户名 su 切换到用户 删除用户 userdel 用户名 -r 删除用户的时候，家目录也删除 用户组管理 groupadd 添加用户组 groupmod 修改用户组 groupdel 删除用户组 用户权限 默认用户是没有suod命令，用户或用户组可以执行的sudo权限可以在/etc/sudoers配置 ssh ssh远程连接传输协议，默认端口22 注意：端口0-65535 文件权限 文件权限 d rwx rwx rwx 首位d代表文件夹，-代表文件，l代表链接文件。 第一组rwx是文件的拥有者权限，第二组是组内其他成员操作权限，第三组是组外其他人员。 文件可读：可以读取里面的数据。 文件可写：可以写数据，可以删除该文件。 文件可执行：是可执行文件。 ","date":"2019-03-22","objectID":"/posts/linux/:2:0","tags":["linux"],"title":"linux","uri":"/posts/linux/"},{"categories":["笔记"],"content":"特殊权限SUID、SGID、Sticky SUID(Set User ID) 该属性只对有执行权限的文件有效，对目录无效。执行具有SUID权限的程序时，引发的进程的所有者是程序文件的所有者，而不是启动程序的用户。 例子： ls -rwsr-xr-x 1 root root 47032 Feb 16 2014 /usr/bin/passwd 这样非root用户执行/usr/bin/passwd命令，就等于所有者也就是root在执行。 SGID(Set Group ID） 对于可执行文件，SGID与SUID类似，引发的进程的所有组是程序文件所属的组。 例子： drwxrwsr-x 2 root staff 4096 Apr 10 2014 local Sticky 仅对目录有效。 带sticky属性的目录下的文件或目录可以被其拥有者删除或改名。常利用sticky属性创建这样的目录：组用户可以在此目录中创建新文件、修改文件内容，但只有文件所有者才能对自己的文件进行删除或改名。 例子： drwxrwxrwt 8 root root 4096 Apr 4 23:57 tmp 修改特殊位 那么原来的执行标志x到哪里去了呢? 系统是这样规定的, 如果本来在该位上有x, 则这些特殊标志显示为小写字母 (s, s, t). 否则, 显示为大写字母 (S, S, T)。 chmod u+s temp 加setuid权限 chmod g+s temp 加setgid权限 chmod o+t temp 加sticky权限 ","date":"2019-03-22","objectID":"/posts/linux/:3:0","tags":["linux"],"title":"linux","uri":"/posts/linux/"},{"categories":["笔记"],"content":"修改文件属性 修改文件所有者 chown -R 属主名 文件名 -R是递归，所有子文件均修改 修改文件组 chgrp -R 组名 文件名 更改文件权限 修改文件权限一种是通过数字，一种是符号。 数字法：chmod [-R] 777 文件 符号法：chmod[-R] u/g/o/a =/+/- r/wx 目录权限 目录和常规文件一样使用相同的权限位进行标识，但是它们的翻译不同。 文件夹的读：允许用户使用该权限列出目录内容。 文件夹的写：写权限意味着用户使用该权限能够在目录中创建或者删除文件。 文件夹的执行：是否可以cd到改目录下 shell编程 变量 注意，变量名和等号之间不能有空格 声明、赋值变量是不需要$符的，引用的时候需要 使用 readonly 命令可以将变量定义为只读变量，只读变量的值不能被改变 使用 unset 命令可以删除变量 可以在定义时加上local命令，此时该变量就成了局部变量。 在 Shell 中定义的变量，默认就是全局变量。 全局变量只在当前 Shell 进程中有效，对其它 Shell 进程和子进程都无效。如果使用export命令将全局变量导出，那么它就在所有的子进程中也有效了，这称为“环境变量”。 读取输入 read -p '请输入' 变量 字符串 字符串单引号、双引号包括都可以，但是双引号里面可以有转义和变量 获取字符串长度，echo ${#str} 获取子串，echo ${string:1:4} 数组 bash支持一维数组（不支持多维数组），并且没有限定数组的大小。 数组名=(值1 值2 ... 值n) array_name[0]=value0 获取数组所有元素，echo ${array_name[@]} 获取数组长度，length=${#array_name[@]} 注释 多行注释： :«EOF … EOF 单行注释：一个井号 获取参数 $0获取程序文件名 $n第n个参数 算术运算符 bash原生不支持简单的算术运算的，可以利用expr，(()),[],bc等实现 前三者只能进行整数运算，后者可以小数。 expr v=expr 2 + 2 表达式和运算符之间要有空格，例如 2+2 是不对的，必须写成 2 + 2 完整的表达式要被 包含 乘号需要转义* $(()) echo $(( 2 + 5 )) 不需要转义乘号 $[] r=$[ 4 + 5 ] bc bc可以看做是linux计算器,支持小数。 浮点数的小数位数是由内建变量scale控制的。scala变量的默认值是0，在scala值被设置之前，bc计算结果不包含小数位。 在脚本中我们可以这样使用bc: variable=$(echo “options; expression” | bc) var1=$(echo “scale=4; 3.44 /5” | bc) 注意：小于1的时候，结果是没有整数的0的，可以这样解决： res1=$(printf “%.2f” echo \"scale=2;1/3\"|bc) 关系运算符 [a -eq b] 判断相等 相当于= [a -ne b] 判断不相等 相当与!= [a -lt b] 小于 [a -gt b] 大于 #!/bin/bash a=2 b=2 if [ ${a} = ${b} ] then echo \"yes\" fi if [ ${a} -eq ${b} ] then echo \"yes\" else echo \"no\" fi if [ ${a} -eq 34 ] then echo \"yes\" elif [ ${a} -lt 20 ] then echo \"no\" fi 非与或 -o 或 -a 与 ！非 #!/bin/bash a=2 b=3 if [ ${a} = ${b} -o ${a} -lt ${b} ] then echo \"yes\" fi 字符串运算符 # 判断是否相等 if [ $str = $str2 ] then echo \"yes\" fi # 判断是否为空 if [ -z $str ] then echo \"zero\" fi 文件运算符 文件测试运算符用于检测 Unix 文件的各种属性。 f=\"./hello\" if [ -f $f ] then echo \"file\" fi -f 是否为文件 -d是否为目录 -e文件是否存在 -x是否可执行 流程控制 if判断: num1=$[2*3] num2=$[1+5] if test $[num1] -eq $[num2] then echo '两个数字相等!' else echo '两个数字不相等!' fi for循环: for loop in 1 2 3 4 5 do echo \"The value is: $loop\" done #第二种： for (( i=0; i \u003c 3; ++i )) do echo $i done while循环： #!/bin/bash int=1 while(( $int\u003c=5 )) do echo $int let \"int++\" done 命令执行结果赋值 shell编程中把命令的执行结果赋值给变量： 第一种方法： v=$(ls ) 第二种： v=`ls` 压缩文件 ","date":"2019-03-22","objectID":"/posts/linux/:4:0","tags":["linux"],"title":"linux","uri":"/posts/linux/"},{"categories":["笔记"],"content":"gzip 压缩后的格式为：*.gz 这种压缩方式不能保存原文件；且不能压缩目录 命令举例： gzip buodo gunzip buodo.gz ","date":"2019-03-22","objectID":"/posts/linux/:5:0","tags":["linux"],"title":"linux","uri":"/posts/linux/"},{"categories":["笔记"],"content":"zip 与gzip相比：可以压缩目录； 可以保留原文件； 选项： -r(recursive) 递归压缩目录内的所有文件和目录 命令举例： zip boduo.zip boduo unzip boduo.zip ","date":"2019-03-22","objectID":"/posts/linux/:6:0","tags":["linux"],"title":"linux","uri":"/posts/linux/"},{"categories":["笔记"],"content":"tar 命令选项： -z(gzip) 用gzip来压缩/解压缩文件 -j(bzip2) 用bzip2来压缩/解压缩文件 -v(verbose) 详细报告tar处理的文件信息 -c(create) 创建新的档案文件 -x(extract) 解压缩文件或目录 -f(file) 使用档案文件或设备，这个选项通常是必选的。 命令举例： tar -zvcf buodo.tar.gz buodo tar -zvxf buodo.tar.gz awk awk是一个强大的文本分析工具，awk在其对数据分析并生成报告时，显得尤为强大。简单来说awk就是把文件逐行的读入，以空格为默认分隔符将每行切片，切开的部分再进行各种分析处理。 调用awk有多种方法，常用命令行方法： awk [-F field-separator] ‘commands’ input-file(s) 其中，commands 是真正awk命令，[-F域分隔符]是可选的。 input-file(s) 是待处理的文件。 在awk中，文件的每一行中，由域分隔符分开的每一项称为一个域。通常，在不指名-F域分隔符的情况下，默认的域分隔符是空格 命令举例： # 查看ls -l结果的第一列 ls -l ./ | awk '{print $1}' 案例 ","date":"2019-03-22","objectID":"/posts/linux/:7:0","tags":["linux"],"title":"linux","uri":"/posts/linux/"},{"categories":["笔记"],"content":"读取一个文件每行内容 利用管道 #!/bin/bash f=\"./hello_test.go\" cat $f | while read line do echo $line done ","date":"2019-03-22","objectID":"/posts/linux/:8:0","tags":["linux"],"title":"linux","uri":"/posts/linux/"},{"categories":["笔记"],"content":"linux io模型","date":"2019-03-22","objectID":"/posts/linux_io/","tags":["linux"],"title":"linux io模型","uri":"/posts/linux_io/"},{"categories":["笔记"],"content":"用户空间与内核空间 现在操作系统都是采用虚拟存储器，那么对32位操作系统而言，它的寻址空间（虚拟存储空间）为4G（2的32次方）。操作系统的核心是内核，独立于普通的应用程序，可以访问受保护的内存空间，也有访问底层硬件设备的所有权限。为了保证用户进程不能直接操作内核（kernel），保证内核的安全，操心系统将虚拟空间划分为两部分，一部分为内核空间，一部分为用户空间。针对linux操作系统而言，将最高的1G字节（从虚拟地址0xC0000000到0xFFFFFFFF），供内核使用，称为内核空间，而将较低的3G字节（从虚拟地址0x00000000到0xBFFFFFFF），供各个进程使用，称为用户空间。 进程切换 为了控制进程的执行，内核必须有能力挂起正在CPU上运行的进程，并恢复以前挂起的某个进程的执行。这种行为被称为进程切换。因此可以说，任何进程都是在操作系统内核的支持下运行的，是与内核紧密相关的。 进程的阻塞 正在执行的进程，由于期待的某些事件未发生，如请求系统资源失败、等待某种操作的完成、新数据尚未到达或无新工作做等，则由系统自动执行阻塞原语(Block)，使自己由运行状态变为阻塞状态。可见，进程的阻塞是进程自身的一种主动行为，也因此只有处于运行态的进程（获得CPU），才可能将其转为阻塞状态。当进程进入阻塞状态，是不占用CPU资源的。 文件描述符 我们都知道unix(like)世界里，一切皆文件，而文件是什么呢？文件就是一串二进制流而已，不管socket,还是FIFO、管道、终端，对我们来说，一切都是文件，一切都是流。在信息 交换的过程中，我们都是对这些流进行数据的收发操作，简称为I/O操作(input and output)，往流中读出数据，系统调用read，写入数据，系统调用write。 计算机里有这么多的流，我怎么知道要操作哪个流呢？对，就是文件描述符，即通常所说的fd，一个fd就是一个整数，所以，对这个整数的操作，就是对这个文件（流）的操作。我们创建一个socket,通过系统调用会返回一个文件描述符，那么剩下对socket的操作就会转化为对这个描述符的操作。 缓存 I/O 缓存 I/O 又被称作标准 I/O，大多数文件系统的默认 I/O 操作都是缓存 I/O。在 Linux 的缓存 I/O 机制中，操作系统会将 I/O 的数据缓存在文件系统的页缓存（ page cache ）中，也就是说，数据会先被拷贝到操作系统内核的缓冲区中，然后才会从操作系统内核的缓冲区拷贝到应用程序的地址空间。 缓存 I/O 的缺点： 数据在传输过程中需要在应用程序地址空间和内核进行多次数据拷贝操作，这些数据拷贝操作所带来的 CPU 以及内存开销是非常大的。 IO模式 刚才说了，对于一次IO访问（以read举例），数据会先被拷贝到操作系统内核的缓冲区中，然后才会从操作系统内核的缓冲区拷贝到应用程序的地址空间。所以说，当一个read操作发生时，它会经历两个阶段： 等待数据准备 (Waiting for the data to be ready) 将数据从内核拷贝到进程中 (Copying the data from the kernel to the process) 正式因为这两个阶段，linux系统产生了下面五种网络模式的方案。 阻塞 I/O（blocking IO） 非阻塞 I/O（nonblocking IO） I/O 多路复用（ IO multiplexing） 信号驱动 I/O（ signal driven IO） 异步 I/O（asynchronous IO） 注：由于signal driven IO在实际中并不常用，所以我这只提及剩下的四种IO Model。 阻塞IO 当用户进程调用了recvfrom这个系统调用，kernel就开始了IO的第一个阶段：准备数据（对于网络IO来说，很多时候数据在一开始还没有到达。比如，还没有收到一个完整的UDP包。这个时候kernel就要等待足够的数据到来）。这个过程需要等待，也就是说数据被拷贝到操作系统内核的缓冲区中是需要一个过程的。而在用户进程这边，整个进程会被阻塞（当然，是进程自己选择的阻塞）。当kernel一直等到数据准备好了，它就会将数据从kernel中拷贝到用户内存，然后kernel返回结果，用户进程才解除block的状态，重新运行起来。 所以，blocking IO的特点就是在IO执行的两个阶段都被block了。 非阻塞io 当用户进程发出read操作时，如果kernel中的数据还没有准备好，那么它并不会block用户进程，而是立刻返回一个error。从用户进程角度讲 ，它发起一个read操作后，并不需要等待，而是马上就得到了一个结果。用户进程判断结果是一个error时，它就知道数据还没有准备好，于是它可以再次发送read操作。一旦kernel中的数据准备好了，并且又再次收到了用户进程的system call，那么它马上就将数据拷贝到了用户内存，然后返回。 所以，nonblocking IO的特点是用户进程需要不断的主动询问kernel数据好了没有。 io多路复用 IO multiplexing就是我们说的select，poll，epoll，有些地方也称这种IO方式为event driven IO。select/epoll的好处就在于单个process就可以同时处理多个网络连接的IO。它的基本原理就是select，poll，epoll这个function会不断的轮询所负责的所有socket，当某个socket有数据到达了，就通知用户进程。 当用户进程调用了select，那么整个进程会被block，而同时，kernel会“监视”所有select负责的socket，当任何一个socket中的数据准备好了，select就会返回。这个时候用户进程再调用read操作，将数据从kernel拷贝到用户进程。 所以，I/O 多路复用的特点是通过一种机制一个进程能同时等待多个文件描述符，而这些文件描述符（套接字描述符）其中的任意一个进入读就绪状态，select()函数就可以返回。 I/O多路复用就通过一种机制，可以监视多个描述符，一旦某个描述符就绪（一般是读就绪或者写就绪），能够通知程序进行相应的读写操作。但select，poll，epoll本质上都是同步I/O，因为他们都需要在读写事件就绪后自己负责进行读写，也就是说这个读写过程是阻塞的，而异步I/O则无需自己负责进行读写，异步I/O的实现会负责把数据从内核拷贝到用户空间。 在IO multiplexing Model中，实际中，对于每一个socket，一般都设置成为non-blocking，但是，如上图所示，整个用户的process其实是一直被block的。只不过process是被select这个函数block，而不是被socket IO给block。 select，poll，epoll都是IO多路复用的机制。I/O多路复用就是通过一种机制，一个进程可以监视多个描述符，一旦某个描述符就绪（一般是读就绪或者写就绪），能够通知程序进行相应的读写操作。但select，poll，epoll本质上都是同步I/O，因为他们都需要在读写事件就绪后自己负责进行读写，也就是说这个读写过程是阻塞的，而异步I/O则无需自己负责进行读写，异步I/O的实现会负责把数据从内核拷贝到用户空间。 异步io linux下的asynchronous IO其实用得很少。 用户进程发起read操作之后，立刻就可以开始去做其它的事。而另一方面，从kernel的角度，当它受到一个asynchronous read之后，首先它会立刻返回，所以不会对用户进程产生任何block。然后，kernel会等待数据准备完成，然后将数据拷贝到用户内存，当这一切都完成之后，kernel会给用户进程发送一个signal，告诉它read操作完成了。 blocking和non-blocking的区别 调用blocking IO会一直block住对应的进程直到操作完成，而non-blocking IO在kernel还准备数据的情况下会立刻返回。 synchronous IO和asynchronous IO的区别 在说明synchronous IO和asynchronous IO的区别之前，需要先给出两者的定义。POSIX的定义是这样子的： A synchronous I/O operation causes the requesting process to be blocked until that I/O operation completes; An asynchronous I/O operation does not cause the requesting process to be blocked; 两者的区别就在于synchronous IO做”IO operation”的时候会将process阻塞。按照这个定义，之前所述的blocking IO，non-blocking IO，IO multiplexing都属于synchronous IO。 有人会说，non-blocking IO并没有被block啊。这里有个非常“狡猾”的地方，定义中所指的”IO operation”是指真实的IO操作，就是例子中的recvfrom这个system call。non-blocking IO在执行recvfrom这个system call的时候，如果kernel的数据没有准备好，这时候不会block进程。但是，当kernel中数据准备好的时候","date":"2019-03-22","objectID":"/posts/linux_io/:0:0","tags":["linux"],"title":"linux io模型","uri":"/posts/linux_io/"},{"categories":["笔记"],"content":"epoll工作模式 epoll对文件描述符的操作有两种模式：LT（水平触发，level trigger）和ET（边缘触发，edge trigger）。LT模式是默认模式，LT模式与ET模式的区别如下： LT模式：当epoll_wait检测到描述符事件发生并将此事件通知应用程序，应用程序可以不立即处理该事件。下次调用epoll_wait时，会再次响应应用程序并通知此事件。 ET模式：当epoll_wait检测到描述符事件发生并将此事件通知应用程序，应用程序必须立即处理该事件。如果不处理，下次调用epoll_wait时，不会再次响应应用程序并通知此事件。 1. LT模式 LT(level triggered)是缺省的工作方式，并且同时支持block和no-block socket.在这种做法中，内核告诉你一个文件描述符是否就绪了，然后你可以对这个就绪的fd进行IO操作。如果你不作任何操作，内核还是会继续通知你的。 2. ET模式 ET(edge-triggered)是高速工作方式，只支持no-block socket。在这种模式下，当描述符从未就绪变为就绪时，内核通过epoll告诉你。然后它会假设你知道文件描述符已经就绪，并且不会再为那个文件描述符发送更多的就绪通知，直到你做了某些操作导致那个文件描述符不再为就绪状态了(比如，你在发送，接收或者接收请求，或者发送接收的数据少于一定量时导致了一个EWOULDBLOCK 错误）。但是请注意，如果一直不对这个fd作IO操作(从而导致它再次变成未就绪)，内核不会发送更多的通知(only once) ET模式在很大程度上减少了epoll事件被重复触发的次数，因此效率要比LT模式高。epoll工作在ET模式的时候，必须使用非阻塞套接口，以避免由于一个文件句柄的阻塞读/阻塞写操作把处理多个文件描述符的任务饿死。 epoll总结 在 select/poll中，进程只有在调用一定的方法后，内核才对所有监视的文件描述符进行扫描，而epoll事先通过epoll_ctl()来注册一 个文件描述符，一旦基于某个文件描述符就绪时，内核会采用类似callback的回调机制，迅速激活这个文件描述符，当进程调用epoll_wait() 时便得到通知。(此处去掉了遍历文件描述符，而是通过监听回调的的机制。这正是epoll的魅力所在。) epoll比select高效 监视的描述符数量不受限制，它所支持的FD上限是最大可以打开文件的数目，这个数字一般远大于2048,举个例子,在1GB内存的机器上大约是10万左 右，具体数目可以cat /proc/sys/fs/file-max察看,一般来说这个数目和系统内存关系很大。select的最大缺点就是进程打开的fd是有数量限制的。这对 于连接数量比较大的服务器来说根本不能满足。虽然也可以选择多进程的解决方案( Apache就是这样实现的)，不过虽然linux上面创建进程的代价比较小，但仍旧是不可忽视的，加上进程间数据同步远比不上线程间同步的高效，所以也不是一种完美的方案。 IO的效率不会随着监视fd的数量的增长而下降。epoll不同于select和poll轮询的方式，而是通过每个fd定义的回调函数来实现的。只有就绪的fd才会执行回调函数。 如果没有大量的idle -connection或者dead-connection，epoll的效率并不会比select/poll高很多，但是当遇到大量的idle- connection，就会发现epoll的效率大大高于select/poll。 总结 形象的例子： 有多个公司快递要来，比如申通、中通、韵达等。 同步非阻塞：打电话给申通，问申通的来没有，如果没有，挂电话，再打过去，问来没有，一直这样。毫无疑问，这样会严重浪费快递员的时间，也就是CPU的资源。 同步阻塞：网购后在家等着快递（自己被阻塞了，被“定”住了），申通快递到了会给我打电话（唤醒），我在也不用一直打扰快递员了（CPU）。 同步阻塞虽然比非阻塞忙轮询好，但是同一时间只能接收一个人快递员的电话，如果同时给我打电话的话，我需要分身（创建多个线程），各分身负责等一个电话。 io多路复用：可以找一个快递接收点，接收员叫select,有快递到时候他会联系我，但是select只是告诉我有快递，具体哪个快递来了，我还要再依次跟各快递员打电话问一遍哪个快递到了。epoll接收员比较负责任，他直接就能告诉我是哪个快递来了。 异步：快递送到家并给我拆开，我什么都不用管。只需要给我发个短信说他干完了就行了。 参考资料： https://segmentfault.com/a/1190000003063859 ","date":"2019-03-22","objectID":"/posts/linux_io/:1:0","tags":["linux"],"title":"linux io模型","uri":"/posts/linux_io/"},{"categories":["笔记"],"content":"linux","date":"2019-03-22","objectID":"/posts/linux_%E5%8E%8B%E7%BC%A9/","tags":["linux"],"title":"linux-压缩命令","uri":"/posts/linux_%E5%8E%8B%E7%BC%A9/"},{"categories":["笔记"],"content":"gzip ","date":"2019-03-22","objectID":"/posts/linux_%E5%8E%8B%E7%BC%A9/:0:0","tags":["linux"],"title":"linux-压缩命令","uri":"/posts/linux_%E5%8E%8B%E7%BC%A9/"},{"categories":["笔记"],"content":"压缩单个文件 gzip命令的使用方式很简单，命令后直接跟输入文件即可，gzip命令压缩后默认会覆盖源文件，生成以.gz为后缀的文件。 命令加-k参数，表示keep保留源文件 gzip -k a.txt ","date":"2019-03-22","objectID":"/posts/linux_%E5%8E%8B%E7%BC%A9/:1:0","tags":["linux"],"title":"linux-压缩命令","uri":"/posts/linux_%E5%8E%8B%E7%BC%A9/"},{"categories":["笔记"],"content":"压缩目录下所有文件 -r参数表示递归压缩目录下每一个文件的作用，gzip命令只能压缩单个文件，即使压缩目录，也只是压缩目录下的每一个文件。(这里讲gzip只能压缩单个文件，并不是一次只能压缩一个文件，而是压缩的单位是单个文件，即并不能将多个文件压缩成为一个文件。) ls directory a b gzip -r directory ls directory a.gz b.gz ","date":"2019-03-22","objectID":"/posts/linux_%E5%8E%8B%E7%BC%A9/:2:0","tags":["linux"],"title":"linux-压缩命令","uri":"/posts/linux_%E5%8E%8B%E7%BC%A9/"},{"categories":["笔记"],"content":"解压缩单个文件 解压缩有两种方式，可以使用gzip -d或者gunzip完成解压缩操作。 同理，可以使用-k参数保留源文件 gunzip -k dd.txt.gz bzip2 相对于gzip，bzip2是一个压缩效率更高的命令，压缩后文件占据的空间更小，所以需要的压缩时间要比gzip更久，bzip2的使用方式与gzip基本相同。 bzip2 a // 压缩 bzip2 -dk a.bz2 // 解压 bunzip2 a.bz2 // 解压 bzip2命令的压缩和解压方式与gzip相同，且同样通过-k参数保留源文件。压缩后生成以.bz2为后缀的文件。 bzip2命令对目录的压缩同样是压缩目录下每一个文件，不过bzip2命令并没有提供-r参数，所以无法递归的对目录下文件进行压缩与解压操作。 zip ","date":"2019-03-22","objectID":"/posts/linux_%E5%8E%8B%E7%BC%A9/:3:0","tags":["linux"],"title":"linux-压缩命令","uri":"/posts/linux_%E5%8E%8B%E7%BC%A9/"},{"categories":["笔记"],"content":"zip压缩 zip命令的压缩率要低于bzip2和gzip，不过使用较为广泛，且兼容性较好。 zip dest.txt.zip src.txt unzip test.zip zip命令生成以.zip为后缀的压缩文件，使用-r参数完成对目录的递归压缩，且默认情况下不删除源文件。zip命令提供有-m参数，用于删除源文件，-m表示move移动源文件到压缩包中的意思。 ","date":"2019-03-22","objectID":"/posts/linux_%E5%8E%8B%E7%BC%A9/:4:0","tags":["linux"],"title":"linux-压缩命令","uri":"/posts/linux_%E5%8E%8B%E7%BC%A9/"},{"categories":["笔记"],"content":"unzip解压缩 与压缩操作相同，解压缩操作同样不删除源文件，所以若目录下存在同名文件时，会出现是否更新文件的提示。 使用-o参数忽略提示，直接更新同名文件，-o表示overwrite覆盖同名文件的意思。unzip同时提供-n参数，忽略提示并不更新同名文件，-n表示never覆盖同名文件的意思。 打包和压缩 打包是指将一大堆文件或目录什么的变成一个总的文件，压缩则是将一个大的文件通过一些压缩算法变成一个小文件。 Linux中的很多压缩程序只能针对一个文件进行压缩，这样当你想要压缩一大堆文件时，你就得先借助另它的工具将这一大堆文件先打成一个包，然后再就原来的压缩程序进行压缩。 Linux下最常用的打包程序就是tar了，使用tar程序打出来的包我们常称为tar包，tar包文件的命令通常都是以.tar结尾的。生成tar包后，就可以用其它的程序来进行压缩了，所以首先就来讲讲tar命令的基本用法： tar命令的选项有很多(用man tar可以查看到)，但常用的就那么几个选项，下面来举例说明一下： *tar -cf all.tar .jpg 这条命令是将所有.jpg的文件打成一个名为all.tar的包。-c是表示产生新的包，-f指定包的文件名。 tar -xf all.tar 这条命令是解出all.tar包中所有文件，-x是解开的意思。 以上就是tar的最基本的用法。为了方便用户在打包解包的同时可以压缩或解压文件，tar提供了一种特殊的功能。这就是tar可以在打包或解包的同时调用其它的压缩程序，比如调用gzip、bzip2等。 ","date":"2019-03-22","objectID":"/posts/linux_%E5%8E%8B%E7%BC%A9/:5:0","tags":["linux"],"title":"linux-压缩命令","uri":"/posts/linux_%E5%8E%8B%E7%BC%A9/"},{"categories":["笔记"],"content":"tar调用压缩程序 tar中使用-z这个参数来调用gzip。下面来举例说明一下： *tar -czf all.tar.gz .jpg 这条命令是将所有.jpg的文件打成一个tar包，并且将其用gzip压缩，生成一个gzip压缩过的包，包名为all.tar.gz tar -xzf all.tar.gz 这条命令是将上面产生的包解开。 tar中使用-j这个参数来调用gzip。下面来举例说明一下： *tar -cjf all.tar.bz2 .jpg 这条命令是将所有.jpg的文件打成一个tar包，并且将其用bzip2压缩，生成一个bzip2压缩过的包，包名为all.tar.bz2 tar -xjf all.tar.bz2 这条命令是将上面产生的包解开。 ","date":"2019-03-22","objectID":"/posts/linux_%E5%8E%8B%E7%BC%A9/:6:0","tags":["linux"],"title":"linux-压缩命令","uri":"/posts/linux_%E5%8E%8B%E7%BC%A9/"},{"categories":["笔记"],"content":"nsq消息队列","date":"2019-03-22","objectID":"/posts/mq_nsq/","tags":["mq"],"title":"nsq消息队列","uri":"/posts/mq_nsq/"},{"categories":["笔记"],"content":"nsq消息队列 NSQ是一个基于Go语言的分布式实时消息平台，它基于MIT开源协议发布，由bitly公司开源出来的一款简单易用的消息中间件。 ","date":"2019-03-22","objectID":"/posts/mq_nsq/:0:1","tags":["mq"],"title":"nsq消息队列","uri":"/posts/mq_nsq/"},{"categories":["笔记"],"content":"Features 特点 Distributed NSQ提供了分布式的，去中心化，且没有单点故障的拓扑结构，稳定的消息传输发布保障，能够具有高容错和HA（高可用）特性。 Scalable易于扩展 NSQ支持水平扩展，没有中心化的brokers。内置的发现服务简化了在集群中增加节点。同时支持pub-sub和load-balanced 的消息分发。 Ops Friendly NSQ非常容易配置和部署，生来就绑定了一个管理界面。二进制包没有运行时依赖。官方有Docker image。 Integrated高度集成 官方的 Go 和 Python库都有提供。而且为大多数语言提供了库。 可延时接收 ","date":"2019-03-22","objectID":"/posts/mq_nsq/:0:2","tags":["mq"],"title":"nsq消息队列","uri":"/posts/mq_nsq/"},{"categories":["笔记"],"content":"组件 Topic ：一个topic就是程序发布消息的一个逻辑键，当程序第一次发布消息时就会创建topic。 Channels ：channel与消费者相关，是消费者之间的负载均衡，channel在某种意义上来说是一个“队列”。每当一个发布者发送一条消息到一个topic，消息会被复制到所有消费者连接的channel上，消费者通过这个特殊的channel读取消息，实际上，在消费者第一次订阅时就会创建channel。Channel会将消息进行排列，如果没有消费者读取消息，消息首先会在内存中排队，当量太大时就会被保存到磁盘中。 Messages：消息构成了我们数据流的中坚力量，消费者可以选择结束消息，表明它们正在被正常处理，或者重新将他们排队待到后面再进行处理。每个消息包含传递尝试的次数，当消息传递超过一定的阀值次数时，我们应该放弃这些消息，或者作为额外消息进行处理。 nsqd：nsqd 是一个守护进程，负责接收，排队，投递消息给客户端。它可以独立运行，不过通常它是由 nsqlookupd 实例所在集群配置的（它在这能声明 topics 和 channels，以便大家能找到）。 nsqlookupd：nsqlookupd 是守护进程负责管理拓扑信息。客户端通过查询 nsqlookupd 来发现指定话题（topic）的生产者，并且 nsqd 节点广播话题（topic）和通道（channel）信息。有两个接口：TCP 接口，nsqd 用它来广播。HTTP 接口，客户端用它来发现和管理。 nsqadmin：nsqadmin 是一套 WEB UI，用来汇集集群的实时统计，并执行不同的管理任务。 常用工具类： nsq_to _file：消费指定的话题（topic）/通道（channel），并写到文件中，有选择的滚动和/或压缩文件。 nsq_to _http：消费指定的话题（topic）/通道（channel）和执行 HTTP requests (GET/POST) 到指定的端点。 nsq_to _nsq：消费者指定的话题/通道和重发布消息到目的地 nsqd 通过 TCP。 组件通讯 [文件] nsqd 分发数据 这是官方的图，第一个channel(meteics)因为有多个消费者，所以触发了负载均衡机制。后面两个channel由于没有消费者，所有的message均会被缓存在相应的队列里，直到消费者出现 ","date":"2019-03-22","objectID":"/posts/mq_nsq/:0:3","tags":["mq"],"title":"nsq消息队列","uri":"/posts/mq_nsq/"},{"categories":["笔记"],"content":"运行服务 首先启动nsqlookud nsqlookupd 启动nsqd，并接入刚刚启动的nsqlookud。这里为了方便接下来的测试，启动了两个nsqd nsqd –lookupd-tcp-address=127.0.0.1:4160 nsqd –lookupd-tcp-address=127.0.0.1:4160 -tcp-address=0.0.0.0:4152 -http-address=0.0.0.0:4153 启动nqsadmin（不是必须的） nsqadmin –lookupd-http-address=127.0.0.1:4161 go使用nsq nsq提供了go客户端库 生产者： var producer *nsq.Producer func main() { nsqd := \"127.0.0.1:4150\" producer, err := nsq.NewProducer(nsqd, nsq.NewConfig()) for i := 0; i \u003c 5; i++ { producer.Publish(\"test\", []byte(\"nihao\")) } if err != nil { panic(err) } } 消费者： import ( \"fmt\" \"github.com/nsqio/go-nsq\" \"sync\" ) type NSQHandler struct { id int } func (this *NSQHandler) HandleMessage(msg *nsq.Message) error { fmt.Println(this.id,\"receive\", msg.NSQDAddress, \"message:\", string(msg.Body)) return nil } func testNSQ() { waiter := sync.WaitGroup{} waiter.Add(1) go func() { defer waiter.Done() config:=nsq.NewConfig() config.MaxInFlight=9 //建立多个连接 for i := 0; i\u003c10; i++ { consumer, err := nsq.NewConsumer(\"test\", \"struggle\", config) if nil != err { fmt.Println(\"err\", err) return } consumer.AddHandler(\u0026NSQHandler{i}) err = consumer.ConnectToNSQD(\"127.0.0.1:4150\") if nil != err { fmt.Println(\"err\", err) return } } select{} }() waiter.Wait() } func main() { testNSQ() } ","date":"2019-03-22","objectID":"/posts/mq_nsq/:0:4","tags":["mq"],"title":"nsq消息队列","uri":"/posts/mq_nsq/"},{"categories":["笔记"],"content":"总结 事实上，简单性是我们决定使用NSQ的首要因素，这方便与我们的许多其他软件一起维护，通过引入队列使我们得到了堪称完美的表现，通过队列甚至让我们增加了几个数量级的吞吐量。越来越多的consumer需要一套严格可靠性和顺序性保障，这已经超过了NSQ提供的简单功能。 NSQ 的 TCP 协议是面向 push 的。另外它是无序的，可能有重复数据，这个要根据实际需求考虑。 ","date":"2019-03-22","objectID":"/posts/mq_nsq/:0:5","tags":["mq"],"title":"nsq消息队列","uri":"/posts/mq_nsq/"},{"categories":["笔记"],"content":"rabbitMQ","date":"2019-03-22","objectID":"/posts/mq_rabbitmq/","tags":["mq"],"title":"rabbitMQ","uri":"/posts/mq_rabbitmq/"},{"categories":["笔记"],"content":"简介 RabbitMQ是一个实现了AMQP（Advanced Message Queuing Protocol）高级消息队列协议的消息队列服务，用Erlang语言的。 在我们秒杀抢购商品的时候，系统会提醒我们稍等排队中，而不是像几年前一样页面卡死或报错给用户。 像这种排队结算就用到了消息队列机制，放入通道里面一个一个结算处理，而不是某个时间断突然涌入大批量的查询新增把数据库给搞宕机，所以RabbitMQ本质上起到的作用就是削峰填谷，为业务保驾护航。 为什么选择RabbitMQ 现在的市面上有很多MQ可以选择，比如ActiveMQ、ZeroMQ、Appche Qpid，那问题来了为什么要选择RabbitMQ？ 除了Qpid，RabbitMQ是唯一一个实现了AMQP标准的消息服务器； 可靠性，RabbitMQ的持久化支持，保证了消息的稳定性； 高并发，RabbitMQ使用了Erlang开发语言，Erlang是为电话交换机开发的语言，天生自带高并发光环，和高可用特性； 集群部署简单，正是应为Erlang使得RabbitMQ集群部署变的超级简单； 社区活跃度高，根据网上资料来看，RabbitMQ也是首选； 工作机制 生产者、消费者和代理 在了解消息通讯之前首先要了解3个概念：生产者、消费者和代理。 生产者：消息的创建者，负责创建和推送数据到消息服务器； 消费者：消息的接收方，用于处理数据和确认消息； 代理：就是RabbitMQ本身，用于扮演“快递”的角色，本身不生产消息，只是扮演“快递”的角色。 ","date":"2019-03-22","objectID":"/posts/mq_rabbitmq/:0:0","tags":["mq"],"title":"rabbitMQ","uri":"/posts/mq_rabbitmq/"},{"categories":["笔记"],"content":"消息发送原理 首先你必须连接到Rabbit才能发布和消费消息，那怎么连接和发送消息的呢？ 你的应用程序和Rabbit Server之间会创建一个TCP连接，一旦TCP打开，并通过了认证，认证就是你试图连接Rabbit之前发送的Rabbit服务器连接信息和用户名和密码，有点像程序连接数据库，一旦认证通过你的应用程序和Rabbit就创建了一条AMQP信道（Channel）。 信道是创建在“真实”TCP上的虚拟连接，AMQP命令都是通过信道发送出去的，每个信道都会有一个唯一的ID，不论是发布消息，订阅队列或者介绍消息都是通过信道完成的。 为什么不通过TCP直接发送命令？ 对于操作系统来说创建和销毁TCP会话是非常昂贵的开销，假设高峰期每秒有成千上万条连接，每个连接都要创建一条TCP会话，这就造成了TCP连接的巨大浪费，而且操作系统每秒能创建的TCP也是有限的，因此很快就会遇到系统瓶颈。 如果我们每个请求都使用一条TCP连接，既满足了性能的需要，又能确保每个连接的私密性，这就是引入信道概念的原因。 Rabbit的名词 想要真正的了解Rabbit有些名词是你必须知道的。 包括：ConnectionFactory（连接管理器）、Channel（信道）、Exchange（交换器）、Queue（队列）、RoutingKey（路由键）、BindingKey（绑定键）、message、Broker、虚拟主机 ConnectionFactory（连接管理器）：应用程序与Rabbit之间建立连接的管理器，程序代码中使用； Channel（信道）：消息推送使用的通道； Exchange（交换器）：用于接受、分配消息；可以将交换器理解成一个由绑定构成的路由表。 Queue（队列）：用于存储生产者的消息；一个消息可投入一个或多个队列。消息一直在队列里面，等待消费者连接到这个队列将其取走。 RoutingKey（路由键）：用于把生成者的数据分配到交换器上； BindingKey（绑定键）：用于把交换器的消息绑定到队列上； Message消息：消息是不具名的，它由消息头和消息体组成。消息体是不透明的，而消息头则由一系列的可选属性组成，这些属性包括routing-key（路由键）、priority（相对于其他消息的优先权）、delivery-mode（指出该消息可能需要持久性存储）等。 Broker 表示消息队列服务器实体。 虚拟主机，每个Rabbit都能创建很多vhost，我们称之为虚拟主机，每个虚拟主机其实都是mini版的RabbitMQ，拥有自己的队列，交换器和绑定，拥有自己的权限机制。 RabbitMQ默认的vhost是“/”开箱即用； 多个vhost是隔离的，多个vhost无法通讯，并且不用担心命名冲突（队列和交换器和绑定），实现了多层分离； 创建用户的时候必须指定vhost； Exchange 类型 Exchange分发消息时根据类型的不同分发策略有区别，目前共四种类型：direct、fanout、topic、headers 。headers 匹配 AMQP 消息的 header 而不是路由键，此外 headers 交换器和 direct 交换器完全一致，但性能差很多，目前几乎用不到了，所以直接看另外三种类型： ","date":"2019-03-22","objectID":"/posts/mq_rabbitmq/:1:0","tags":["mq"],"title":"rabbitMQ","uri":"/posts/mq_rabbitmq/"},{"categories":["笔记"],"content":"direct类型 消息中的路由键（routing key）如果和 Binding 中的 binding key 一致， 交换器就将消息发到对应的队列中。路由键与队列名完全匹配，如果一个队列绑定到交换机要求路由键为“dog”，则只转发 routing key 标记为“dog”的消息，不会转发“dog.puppy”，也不会转发“dog.guard”等等。它是完全匹配、单播的模式。 接收相关 当接收端订阅者有多个的时候，direct会轮询公平的分发给每个订阅者（订阅者消息确认正常 消息接收到之后必须使用channel.basicAck()方法手动确认（非自动确认删除模式下），那么问题来了。 消息收到未确认会怎么样？ 如果应用程序接收了消息，因为bug忘记确认接收的话，消息在队列的状态会从“Ready”变为“Unacked”。 如果消息收到却未确认，Rabbit将不会再给这个应用程序发送更多的消息了，这是因为Rabbit认为你没有准备好接收下一条消息。 此条消息会一直保持Unacked的状态，直到你确认了消息，或者断开与Rabbit的连接，Rabbit会自动把消息改完Ready状态，分发给其他订阅者。 消息拒绝 消息在确认之前，可以有两个选择： 选择1：断开与Rabbit的连接，这样Rabbit会重新把消息分派给另一个消费者； 选择2：拒绝Rabbit发送的消息使用channel.basicReject(long deliveryTag, boolean requeue)，参数1：消息的id；参数2：处理消息的方式，如果是true，Rabbib会重新分配这个消息给其他订阅者，如果设置成false的话，Rabbit会把消息发送到一个特殊的“死信”队列，用来存放被拒绝而不重新放入队列的消息。 ","date":"2019-03-22","objectID":"/posts/mq_rabbitmq/:2:0","tags":["mq"],"title":"rabbitMQ","uri":"/posts/mq_rabbitmq/"},{"categories":["笔记"],"content":"fanout类型 每个发到 fanout 类型交换器的消息都会分到所有绑定的队列上去。fanout 交换器不处理路由键，只是简单的将队列绑定到交换器上，每个发送到交换器的消息都会被转发到与该交换器绑定的所有队列上。很像子网广播，每台子网内的主机都获得了一份复制的消息。fanout 类型转发消息是最快的。 当你发送一条消息的时候，交换器会把消息广播到所有附加到这个交换器的队列上。 比如用户上传了自己的头像，这个时候图片需要清除缓存，同时用户应该得到积分奖励，你可以把这两个队列绑定到图片上传的交换器上，这样当有第三个、第四个上传完图片需要处理的需求的时候，原来的代码可以不变，只需要添加一个订阅消息即可，这样发送方和消费者的代码完全解耦，并可以轻而易举的添加新功能了。 接收相关 接受消息不同于direct，我们需要声明fanout路由器，并使用默认的队列绑定到fanout交换器上。 ","date":"2019-03-22","objectID":"/posts/mq_rabbitmq/:3:0","tags":["mq"],"title":"rabbitMQ","uri":"/posts/mq_rabbitmq/"},{"categories":["笔记"],"content":"topic类型 topic 交换器通过模式匹配分配消息的路由键属性，将路由键和某个模式进行匹配，此时队列需要绑定到一个模式上。它将路由键和绑定键的字符串切分成单词，这些单词之间用点隔开。 假设我们现在有一个日志系统，会把所有日志级别的日志发送到交换器，warning、log、error、fatal，但我们只想处理error以上的日志，要怎么处理？这就需要使用topic路由器了。 topic路由器的关键在于定义路由键，定义routingKey名称不能超过255字节，使用“.”作为分隔符，例如：com.mq.rabbit.error。 消费消息的时候routingKey可以使用下面字符匹配消息： “*“匹配一个分段(用“.”分割)的内容； “#“匹配0和多个字符； 例如发布了一个“com.mq.rabbit.error”的消息： 能匹配上的路由键： cn.mq.rabbit.* cn.mq.rabbit.# #.error cn.mq.# # 不能匹配上的路由键： cn.mq.* *.error * 所以如果想要订阅所有消息，可以使用“#”匹配。 fanout、topic交换器是没有历史数据的，也就是说对于中途创建的队列，获取不到之前的消息。 RabbitMQ 集群 RabbitMQ 最优秀的功能之一就是内建集群，这个功能设计的目的是允许消费者和生产者在节点崩溃的情况下继续运行，以及通过添加更多的节点来线性扩展消息通信吞吐量。RabbitMQ 内部利用 Erlang 提供的分布式通信框架 OTP 来满足上述需求，使客户端在失去一个 RabbitMQ 节点连接的情况下，还是能够重新连接到集群中的任何其他节点继续生产、消费消息。 ","date":"2019-03-22","objectID":"/posts/mq_rabbitmq/:4:0","tags":["mq"],"title":"rabbitMQ","uri":"/posts/mq_rabbitmq/"},{"categories":["笔记"],"content":"RabbitMQ 集群中的一些概念 RabbitMQ 会始终记录以下四种类型的内部元数据： 队列元数据 包括队列名称和它们的属性，比如是否可持久化，是否自动删除 交换器元数据 交换器名称、类型、属性 绑定元数据 内部是一张表格记录如何将消息路由到队列 vhost 元数据 为 vhost 内部的队列、交换器、绑定提供命名空间和安全属性 在单一节点中，RabbitMQ 会将所有这些信息存储在内存中，同时将标记为可持久化的队列、交换器、绑定存储到硬盘上。存到硬盘上可以确保队列和交换器在节点重启后能够重建。而在集群模式下同样也提供两种选择：存到硬盘上（独立节点的默认设置），存在内存中。 如果在集群中创建队列，集群只会在单个节点而不是所有节点上创建完整的队列信息（元数据、状态、内容）。结果是只有队列的所有者节点知道有关队列的所有信息，因此当集群节点崩溃时，该节点的队列和绑定就消失了，并且任何匹配该队列的绑定的新消息也丢失了。还好RabbitMQ 2.6.0之后提供了镜像队列以避免集群节点故障导致的队列内容不可用。 RabbitMQ 集群中可以共享 user、vhost、exchange等，所有的数据和状态都是必须在所有节点上复制的，例外就是上面所说的消息队列。RabbitMQ 节点可以动态的加入到集群中。 当在集群中声明队列、交换器、绑定的时候，这些操作会直到所有集群节点都成功提交元数据变更后才返回。集群中有内存节点和磁盘节点两种类型，内存节点虽然不写入磁盘，但是它的执行比磁盘节点要好。内存节点可以提供出色的性能，磁盘节点能保障配置信息在节点重启后仍然可用，那集群中如何平衡这两者呢？ RabbitMQ 只要求集群中至少有一个磁盘节点，所有其他节点可以是内存节点，当节点加入火离开集群时，它们必须要将该变更通知到至少一个磁盘节点。如果只有一个磁盘节点，刚好又是该节点崩溃了，那么集群可以继续路由消息，但不能创建队列、创建交换器、创建绑定、添加用户、更改权限、添加或删除集群节点。换句话说集群中的唯一磁盘节点崩溃的话，集群仍然可以运行，但知道该节点恢复，否则无法更改任何东西。 消息持久化 当你把消息发送到Rabbit服务器的时候，你需要选择你是否要进行持久化，但这并不能保证Rabbit能从崩溃中恢复，想要Rabbit消息能恢复必须满足条件： 投递消息的时候durable设置为true，消息持久化，代码：channel.queueDeclare(x, true, false, false, null)，参数2设置为true持久化； 设置投递模式deliveryMode设置为2（持久），代码：channel.basicPublish(x, x, MessageProperties.PERSISTENT_TEXT_PLAIN,x)，参数3设置为存储纯文本到磁盘； 消息已经到达持久化交换器上； 消息已经到达持久化的队列； Rabbit会将你的持久化消息写入磁盘上的持久化日志文件，等消息被消费之后，Rabbit会把这条消息标识为等待垃圾回收。 持久化的缺点 消息持久化的优点显而易见，但缺点也很明显，那就是性能，因为要写入硬盘要比写入内存性能较低很多，从而降低了服务器的吞吐量，尽管使用SSD硬盘可以使事情得到缓解，但他仍然吸干了Rabbit的性能，当消息成千上万条要写入磁盘的时候，性能是很低的。 如果要保证消息的可靠性，需要对消息进行持久化处理，然而消息持久化除了需要代码的设置之外，还有一个重要步骤是至关重要的，那就是保证你的消息顺利进入Broker（代理服务器），如图所示： 正常情况下，如果消息经过交换器进入队列就可以完成消息的持久化，但如果消息在没有到达broker之前出现意外，那就造成消息丢失，有没有办法可以解决这个问题？ RabbitMQ有两种方式来解决这个问题： 通过AMQP提供的事务机制实现； 使用发送者确认模式实现； 面试中的一些问题 ","date":"2019-03-22","objectID":"/posts/mq_rabbitmq/:5:0","tags":["mq"],"title":"rabbitMQ","uri":"/posts/mq_rabbitmq/"},{"categories":["笔记"],"content":"如何确保消息正确地发送至 RabbitMQ 发送方确认模式将信道设置成 confirm 模式（发送方确认模式），则所有在信道上发布的消息都会被指派一个唯一的 ID。一旦消息被投递到目的队列后，或者消息被写入磁盘后（可持久化的消息），信道会发送一个确认给生产者（包含消息唯一 ID）。如果 RabbitMQ 发生内部错误从而导致消息丢失，会发送一条 nack（notacknowledged，未确认）消息。发送方确认模式是异步的，生产者应用程序在等待确认的同时，可以继续发送消息。当确认消息到达生产者应用程序，生产者应用程序的回调方法就会被触发来处理确认消息。 ","date":"2019-03-22","objectID":"/posts/mq_rabbitmq/:6:0","tags":["mq"],"title":"rabbitMQ","uri":"/posts/mq_rabbitmq/"},{"categories":["笔记"],"content":"如何确保消息接收方消费了消息？ 消费者接收每一条消息后都必须进行确认（消息接收和消息确认是两个不同操作）。只有消费者确认了消息，RabbitMQ 才能安全地把消息从队列中删除。这里并没有用到超时机制，RabbitMQ 仅通过 Consumer 的连接中断来确认是否需要重新发送消息。也就是说，只要连接不中断，RabbitMQ 给了 Consumer 足够长的时间来处理消息。保证数据的最终一致性；下面罗列几种特殊情况（1）如果消费者接收到消息，在确认之前断开了连接或取消订阅，RabbitMQ 会认为消息没有被分发，然后重新分发给下一个订阅的消费者。（可能存在消息重复消费的隐患，需要去重）（1）2如果消费者接收到消息却没有确认消息，连接也未断开，则 RabbitMQ 认为该消费者繁忙，将不会给该消费者分发更多的消息。 ","date":"2019-03-22","objectID":"/posts/mq_rabbitmq/:7:0","tags":["mq"],"title":"rabbitMQ","uri":"/posts/mq_rabbitmq/"},{"categories":["笔记"],"content":"如何避免消息重复投递或重复消费？ 在消息生产时，MQ 内部针对每条生产者发送的消息生成一个 inner-msg-id，作为去重的依据（消息投递失败并重传），避免重复的消息进入队列；在消息消费时，要求消息体中必须要有一个 bizId（对于同一业务全局唯一，如支付 ID、订单 ID、帖子 ID 等）作为去重的依据，避免同一条消息被重复消费。 ","date":"2019-03-22","objectID":"/posts/mq_rabbitmq/:8:0","tags":["mq"],"title":"rabbitMQ","uri":"/posts/mq_rabbitmq/"},{"categories":["笔记"],"content":"消息基于什么传输？ 由于 TCP 连接的创建和销毁开销较大，且并发数受系统资源限制，会造成性能瓶颈。RabbitMQ 使用信道的方式来传输数据。信道是建立在真实的 TCP 连接内的虚拟连接，且每条 TCP 连接上的信道数量没有限制。 ","date":"2019-03-22","objectID":"/posts/mq_rabbitmq/:9:0","tags":["mq"],"title":"rabbitMQ","uri":"/posts/mq_rabbitmq/"},{"categories":["笔记"],"content":"消息如何分发？ 若该队列至少有一个消费者订阅，消息将以循环（round-robin）的方式发送给消费者。每条消息只会分发给一个订阅的消费者（前提是消费者能够正常处理消息并进行确认）。通过路由可实现多消费的功能 ","date":"2019-03-22","objectID":"/posts/mq_rabbitmq/:10:0","tags":["mq"],"title":"rabbitMQ","uri":"/posts/mq_rabbitmq/"},{"categories":["笔记"],"content":"消息怎么路由？ 消息提供方-\u003e路由-\u003e一至多个队列消息发布到交换器时，消息将拥有一个路由键（routing key），在消息创建时设定。通过队列路由键，可以把队列绑定到交换器上。消息到达交换器后，RabbitMQ 会将消息的路由键与队列的路由键进行匹配（针对不同的交换器有不同的路由规则）；常用的交换器主要分为一下三种：fanout：如果交换器收到消息，将会广播到所有绑定的队列上direct：如果路由键完全匹配，消息就被投递到相应的队列topic：可以使来自不同源头的消息能够到达同一个队列。 使用 topic 交换器时，可以使用通配符 ","date":"2019-03-22","objectID":"/posts/mq_rabbitmq/:11:0","tags":["mq"],"title":"rabbitMQ","uri":"/posts/mq_rabbitmq/"},{"categories":["笔记"],"content":"如何确保消息不丢失？ 消息持久化，当然前提是队列必须持久化RabbitMQ 确保持久性消息能从服务器重启中恢复的方式是，将它们写入磁盘上的一个持久化日志文件，当发布一条持久性消息到持久交换器上时，Rabbit 会在消息提交到日志文件后才发送响应。一旦消费者从持久队列中消费了一条持久化消息，RabbitMQ 会在持久化日志中把这条消息标记为等待垃圾收集。如果持久化消息在被消费之前 RabbitMQ 重启，那么 Rabbit 会自动重建交换器和队列（以及绑定），并重新发布持久化日志文件中的消息到合适的队列。 ","date":"2019-03-22","objectID":"/posts/mq_rabbitmq/:12:0","tags":["mq"],"title":"rabbitMQ","uri":"/posts/mq_rabbitmq/"},{"categories":["笔记"],"content":"实现支付超时 订单30分钟未支付,系统自动超时关闭 原理:当我们在下单的时候,往MQ投递一个消息设置有效期为30分钟,但该消息失效的时候(没有被消费的情况下), 执行我们客户端一个方法告诉我们该消息已经失效,这时候查询这笔订单是否有支付. ","date":"2019-03-22","objectID":"/posts/mq_rabbitmq/:13:0","tags":["mq"],"title":"rabbitMQ","uri":"/posts/mq_rabbitmq/"},{"categories":["笔记"],"content":"mysql分库分表分区","date":"2019-03-07","objectID":"/posts/mysql_%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8%E5%88%86%E5%8C%BA/","tags":["mysql"],"title":"mysql分库分表分区","uri":"/posts/mysql_%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8%E5%88%86%E5%8C%BA/"},{"categories":["笔记"],"content":"传统的分库分表都是通过应用层逻辑实现的，对于数据库层面来说，都是普通的表和库。分区是数据库层面的。 分库 database不是文件，只起到namespace的作用，所以MySQL对database大小当然也是没有限制的，而且对里面的表数量也没有限制。为什么要分库呢？ 为了解决单台服务器的性能问题，当单台数据库服务器无法支撑当前的数据量时，就需要根据业务逻辑紧密程度把表分成几撮，分别放在不同的数据库服务器中以降低单台服务器的负载。 比如一个论坛系统的数据库因当前服务器性能无法满足需要进行分库。先垂直切分，按业务逻辑把用户相关数据表比如用户信息、积分、用户间私信等放入user数据库；论坛相关数据表比如板块，帖子，回复等放入forum数据库，两个数据库放在不同服务器上。 分表 当表中的数据库巨大的时候就要考虑分表了，不然这将产生大量随机I/O，随之，数据库的响应时间将大到不可接受的程度。另外，索引维护（磁盘空间、I/O操作）的代价也非常高。 ","date":"2019-03-07","objectID":"/posts/mysql_%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8%E5%88%86%E5%8C%BA/:0:0","tags":["mysql"],"title":"mysql分库分表分区","uri":"/posts/mysql_%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8%E5%88%86%E5%8C%BA/"},{"categories":["笔记"],"content":"竖向分表 顾名思义，就是竖着开刀，把表中的一些字段分出去。 我们知道innodb主索引叶子节点存储着当前行的所有信息，减少字段可以让内存加载更多的数据。 受操作系统中文件大小限制。 ","date":"2019-03-07","objectID":"/posts/mysql_%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8%E5%88%86%E5%8C%BA/:1:0","tags":["mysql"],"title":"mysql分库分表分区","uri":"/posts/mysql_%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8%E5%88%86%E5%8C%BA/"},{"categories":["笔记"],"content":"横向分表 顾名思义，就是横着开刀，这样就把表一些行分出去。 随着数据量的增大，table行数巨大，查询的效率越来越低。 同样受限于操作系统中的文件大小限制，数据量不能无限增加，当到达一定容量时，需要水平切分以降低单表（文件）的大小。 至于怎么横着切要看需求了，可以根据主键范围，可以对主键进行hash等。 表分区 上面的分库、分表都是在应用层实现，拆分要对应用层有很大的调整，比如要实现全局主键生成器，无法再做关联查询等。 但是表分区完全不用考虑这些东西，因为它是在数据库层，对应用层是透明的。 表分区其实也是横向拿刀，切法有下面几种 ","date":"2019-03-07","objectID":"/posts/mysql_%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8%E5%88%86%E5%8C%BA/:2:0","tags":["mysql"],"title":"mysql分库分表分区","uri":"/posts/mysql_%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8%E5%88%86%E5%8C%BA/"},{"categories":["笔记"],"content":"range分区 根据范围分区，范围应该连续但是不重叠，使用PARTITION BY RANGE, VALUES LESS THAN关键字。 注意：不使用COLUMNS关键字时RANGE括号内必须为整数字段名或返回确定整数的函数。 CREATE TABLE employees ( id INT NOT NULL, fname VARCHAR(30), lname VARCHAR(30), hired DATE NOT NULL DEFAULT '1970-01-01', separated DATE NOT NULL DEFAULT '9999-12-31', job_code INT NOT NULL, store_id INT NOT NULL ) PARTITION BY RANGE (store_id) ( PARTITION p0 VALUES LESS THAN (6), PARTITION p1 VALUES LESS THAN (11), PARTITION p2 VALUES LESS THAN (16), PARTITION p3 VALUES LESS THAN MAXVALUE ); 根据TIMESTAMP范围： CREATE TABLE quarterly_report_status ( report_id INT NOT NULL, report_status VARCHAR(20) NOT NULL, report_updated TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP ) PARTITION BY RANGE ( UNIX_TIMESTAMP(report_updated) ) ( PARTITION p0 VALUES LESS THAN ( UNIX_TIMESTAMP('2008-01-01 00:00:00') ), PARTITION p1 VALUES LESS THAN ( UNIX_TIMESTAMP('2008-04-01 00:00:00') ), PARTITION p2 VALUES LESS THAN ( UNIX_TIMESTAMP('2008-07-01 00:00:00') ), PARTITION p3 VALUES LESS THAN ( UNIX_TIMESTAMP('2008-10-01 00:00:00') ), PARTITION p4 VALUES LESS THAN ( UNIX_TIMESTAMP('2009-01-01 00:00:00') ), PARTITION p5 VALUES LESS THAN ( UNIX_TIMESTAMP('2009-04-01 00:00:00') ), PARTITION p6 VALUES LESS THAN ( UNIX_TIMESTAMP('2009-07-01 00:00:00') ), PARTITION p7 VALUES LESS THAN ( UNIX_TIMESTAMP('2009-10-01 00:00:00') ), PARTITION p8 VALUES LESS THAN ( UNIX_TIMESTAMP('2010-01-01 00:00:00') ), PARTITION p9 VALUES LESS THAN (MAXVALUE) ); ","date":"2019-03-07","objectID":"/posts/mysql_%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8%E5%88%86%E5%8C%BA/:3:0","tags":["mysql"],"title":"mysql分库分表分区","uri":"/posts/mysql_%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8%E5%88%86%E5%8C%BA/"},{"categories":["笔记"],"content":"List分区 根据具体数值分区，每个分区数值不重叠，使用PARTITION BY LIST、VALUES IN关键字。跟Range分区类似，不使用COLUMNS关键字时List括号内必须为整数字段名或返回确定整数的函数。 数值必须被所有分区覆盖，否则插入一个不属于任何一个分区的数值会报错。 ","date":"2019-03-07","objectID":"/posts/mysql_%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8%E5%88%86%E5%8C%BA/:4:0","tags":["mysql"],"title":"mysql分库分表分区","uri":"/posts/mysql_%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8%E5%88%86%E5%8C%BA/"},{"categories":["笔记"],"content":"Hash分区 Hash分区主要用来确保数据在预先确定数目的分区中平均分布，Hash括号内只能是整数列或返回确定整数的函数，实际上就是使用返回的整数对分区数取模。 CREATE TABLE employees ( id INT NOT NULL, fname VARCHAR(30), lname VARCHAR(30), hired DATE NOT NULL DEFAULT '1970-01-01', separated DATE NOT NULL DEFAULT '9999-12-31', job_code INT, store_id INT ) PARTITION BY HASH(store_id) PARTITIONS 4; Hash分区也存在与传统Hash分表一样的问题，可扩展性差。MySQL也提供了一个类似于一致Hash的分区方法－线性Hash分区，只需要在定义分区时添加LINEAR关键字 ","date":"2019-03-07","objectID":"/posts/mysql_%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8%E5%88%86%E5%8C%BA/:5:0","tags":["mysql"],"title":"mysql分库分表分区","uri":"/posts/mysql_%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8%E5%88%86%E5%8C%BA/"},{"categories":["笔记"],"content":"Key分区 Key分区与Hash分区很相似，只是Hash函数不同，定义时把Hash关键字替换成Key即可 CREATE TABLE tk ( col1 INT NOT NULL, col2 CHAR(5), col3 DATE ) PARTITION BY LINEAR KEY (col1) PARTITIONS 3; ","date":"2019-03-07","objectID":"/posts/mysql_%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8%E5%88%86%E5%8C%BA/:6:0","tags":["mysql"],"title":"mysql分库分表分区","uri":"/posts/mysql_%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8%E5%88%86%E5%8C%BA/"},{"categories":["笔记"],"content":"子分区 子分区是分区表中每个分区的再次分割。创建子分区方法： CREATE TABLE ts (id INT, purchased DATE) PARTITION BY RANGE( YEAR(purchased) ) SUBPARTITION BY HASH( TO_DAYS(purchased) ) SUBPARTITIONS 2 ( PARTITION p0 VALUES LESS THAN (1990), PARTITION p1 VALUES LESS THAN (2000), PARTITION p2 VALUES LESS THAN MAXVALUE ); ","date":"2019-03-07","objectID":"/posts/mysql_%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8%E5%88%86%E5%8C%BA/:7:0","tags":["mysql"],"title":"mysql分库分表分区","uri":"/posts/mysql_%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8%E5%88%86%E5%8C%BA/"},{"categories":["笔记"],"content":"分区的使用 分区的目的是为了提高查询效率，如果查询范围是所有分区那么就说明分区没有起到作用，我们用explain partitions命令来查看SQL对于分区的使用情况。 一般来说，就是在where条件中加入分区列。 总结 分库分表是应用层面的，分区是数据库层面的。 分区对程序改动最小。 分表有横向、竖向，但是分区只有横向。 ","date":"2019-03-07","objectID":"/posts/mysql_%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8%E5%88%86%E5%8C%BA/:8:0","tags":["mysql"],"title":"mysql分库分表分区","uri":"/posts/mysql_%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8%E5%88%86%E5%8C%BA/"},{"categories":["笔记"],"content":"mysql索引原理","date":"2019-03-05","objectID":"/posts/mysql_%E7%B4%A2%E5%BC%95%E5%8E%9F%E7%90%86/","tags":["mysql"],"title":"mysql索引原理","uri":"/posts/mysql_%E7%B4%A2%E5%BC%95%E5%8E%9F%E7%90%86/"},{"categories":["笔记"],"content":"页 mysql中的页和操作系统的页有点类似，都是逻辑单位。都是假设数据在磁盘上是一起的，我们读取磁盘的时候一次读一页，而不是一条一条的数据来取，一页上可能会有多条数据，再取后面的数据的时候就先去已读取的页中查看，提高效率。操作系统的页是4KB，mysql的页默认是16KB。 innodb数据页结构及其中的目录 每条记录中有个next_record字段，这玩意儿非常重要，它表示从当前记录的真实数据到下一条记录的真实数据的地址偏移量，所以数据页每条记录是个单向链表。 既然是单向链表，如果页里数据比较多，用遍历效率是不高的。所以我们在数据页里再做个目录。 怎么做目录？分槽。 把16k数据分成若干槽，每个槽里面有若干条记录。由于记录都是主键递增的，每个槽都取的最后一条的偏移量，那么槽也是递增的。查找的时候先对槽进行二分查找，再对槽里面的数据进行遍历。 索引实现 不同的存储引擎采用不同的实现方式。 MyISAM 非聚集索引 MyISAM引擎使用B+Tree作为索引结构，叶节点的data域存放的是数据记录的地址。 我们看到，索引文件本身和数据表的文件是分离的，这也是非聚集索引的由来。叶子节点的data区域存放的是数据表每条记录的地址。 其实叶子节点都是页，里面又很多数据，当页数据满的时候在加一个页。 ","date":"2019-03-05","objectID":"/posts/mysql_%E7%B4%A2%E5%BC%95%E5%8E%9F%E7%90%86/:0:0","tags":["mysql"],"title":"mysql索引原理","uri":"/posts/mysql_%E7%B4%A2%E5%BC%95%E5%8E%9F%E7%90%86/"},{"categories":["笔记"],"content":"InnoDB聚集索引 InnoDB的主键索引也使用B+Tree作为索引结构，但这里表数据文件本身就是B+树的一个结构，也就是说叶子节点的data区域保存了完整的数据表的一条记录。索引的key就是表的主键，这就是聚集索引的由来。 由此看来，InnoDB的数据表必须要有一个主键，如果没有指定，mysql就会自动选择一个唯一标识记录的作为主键。那这样的也不存在怎么办？mysql就会生成一个隐含的6字节长整型作为主键。 现在看下如何定位一个Record： 1 通过根节点开始遍历一个索引的B+树，通过各层非叶子节点最终到达一个Page，这个Page里存放的都是叶子节点。 2 在Page内从”Infimum”节点开始遍历单链表（这种遍历往往会被优化），如果找到该键则成功返回。如果记录到达了”supremum”，说明当前Page里没有合适的键，这时要借助Page的Next Page指针，跳转到下一个Page继续从”Infimum”开始逐个查找。 B+树本身就是一个目录，或者说本身就是一个索引。 它有两个特点: 使用记录主键值的大小进行记录和页的排序，这包括三个方面的含义: 页内的记录是按照主键的大小顺序排成一个单向链表。 各个存放用户记录的页也是根据页中用户记录的主键大小顺序排成一个双向链表。 存放目录项记录的页分为不同的层次，在同一层次中的页也是根据页中目录项记录的主键大小顺序排成一个双向链表。 B+树的叶子节点存储的是完整的用户记录。 所谓完整的用户记录，就是指这个记录中存储了所有列的值(包括隐藏列)。 ","date":"2019-03-05","objectID":"/posts/mysql_%E7%B4%A2%E5%BC%95%E5%8E%9F%E7%90%86/:1:0","tags":["mysql"],"title":"mysql索引原理","uri":"/posts/mysql_%E7%B4%A2%E5%BC%95%E5%8E%9F%E7%90%86/"},{"categories":["笔记"],"content":"InnoDB的次级索引 InnoDB还有个地方与MyISAM不同，就是辅助索引data记录的是主键的值而不是数据表记录的地址。此时，索引文件和数据文件是分开的。 这样就要注意了，InnoDB主键不要太大，因为所有辅助索引都引用主索引，过长的主索引会令辅助索引变得过大。 当通过辅助索引来寻找数据时，Innodb存储引擎会遍历辅助索引并通过叶级别的指针获得指向主键索引的主键，然后再通过主键索引来找到一个完整的行记录。这种在二级索引中不能找到所有需要的数据列的现象，被称为非覆盖索引，反之称为覆盖索引。 这个B+树与上边介绍的聚簇索引有几处不同: 使用记录c2列的大小进行记录和页的排序，这包括三个方面的含义: 页内的记录是按照c2列的大小顺序排成一个单向链表。 各个存放用户记录的页也是根据页中记录的c2列大小顺序排成一个双向链表。 存放目录项记录的页分为不同的层次，在同一层次中的页也是根据页中目录项记录的c2列大小顺序排成一个双向链表。 B+树的叶子节点存储的并不是完整的用户记录，而只是c2列+主键这两个列的值。 目录项记录中不再是主键+页号的搭配，而变成了c2列+页号的搭配。 采用B+树的原因 B+树的特点决定的。查询一般为log(n)。选择B+树而不是其他数据结构的原因主要是因为数据是保存在硬盘上而不是内存中，所以减少磁盘IO次数才是提升效率的关键。 B+树的磁盘读写代价更低：B+树的内部节点并没有指向关键字具体信息的指针，因此其内部节点相对B树更小，如果把所有同一内部节点的关键字存放在同一盘块中，那么盘块所能容纳的关键字数量也越多，一次性读入内存的需要查找的关键字也就越多，相对IO读写次数就降低了。 B+树的查询效率更加稳定：由于非终结点并不是最终指向文件内容的结点，而只是叶子结点中关键字的索引。所以任何关键字的查找必须走一条从根结点到叶子结点的路。所有关键字查询的路径长度相同，导致每一个数据的查询效率相当。 由于B+树的数据都存储在叶子结点中，分支结点均为索引，方便扫库，只需要扫一遍叶子结点即可，但是B树因为其分支结点同样存储着数据，我们要找到具体的数据，需要进行一次中序遍历按序来扫，所以B+树更加适合在区间查询的情况，所以通常B+树用于数据库索引。 总结： Hash索引查询是O(1),应该是更快，但是为啥不用呢？因为大多数情况下并不是每次只查询一个，而是多个，比如前10条，b+树叶子节点有链接，所以能快速查询。另外，文件或数据库索引数据比较大，也不做不到一次加载到内存，但是B树可以一个一个节点的加载，进行查询。 ","date":"2019-03-05","objectID":"/posts/mysql_%E7%B4%A2%E5%BC%95%E5%8E%9F%E7%90%86/:1:1","tags":["mysql"],"title":"mysql索引原理","uri":"/posts/mysql_%E7%B4%A2%E5%BC%95%E5%8E%9F%E7%90%86/"},{"categories":["笔记"],"content":"mysql explain","date":"2019-03-03","objectID":"/posts/mysql_explain/","tags":["mysql"],"title":"mysql explain","uri":"/posts/mysql_explain/"},{"categories":["笔记"],"content":"一条查询语句在经过MySQL查询优化器的各种基于成本和规则的优化会后生成一个所谓的执行计划，这个执行计划展示了接下来具体执行查询的方式，比如多表连接的顺序是什么，对于每个表采用什么访 问方法来具体执行查询等等。 其实除了以SELECT开头的查询语句，其余的DELETE、INSERT、REPLACE以及UPDATE语句前边都可以加上EXPLAIN这个词儿。 列名 描述 id 在一个大的查询语句每个SELECT关键字都对应一个唯一的id select_type SELECT关键字对应的那个查询的类型 table 表名 partitions 匹配的分区信息 type 针对单表的访问方法 possible_keys 可能用到的索引 key 实际上使用的索引 key_len 实际使用到的索引长度 ref 当使用索引列等值查询时，与索引列进行等值匹配的对象信息 rows 预估的需要读取的记录条数 filtered 某个表经过搜索条件过滤后剩余记录条数的百分比 Extra 一些额外的信息 table 不论我们的查询语句有多复杂，里边儿包含了多少个表，到最后也是需要对每个表进行单表访问的，所以设计MySQL的大叔规定EXPLAIN语句输出的每条记录都对应着某个单表的访问方法，该条记录的 table列代表着该表的表名. id 查询语句中每出现一个SELECT关键字，设计MySQL的大叔就会为它分配一个唯一的id值。 一个SELECT关键字后边的FROM子句中可以跟随多个表，所以在连接查询的执行计划中，每个表都会对应一条记录，但是这些记录的id值都是相同的。出现在前边的表表示驱动表，出现在后边的表表示被驱动表。 查询优化器可能对涉及子查询的查询语句进行重写，从而转换为连接查询。所以如果我们想知道查询优化器对某个包含子查询的语句是否进行了重写，直接查看执行计划就好了。重写后两个ID是相同的。 select_type 一条大的查询语句里边可以包含若干个SELECT关键字，每个SELECT关键字代表着一个小的查询语句，而每个SELECT关键字的FROM子句中都可以包含若干张表(这些表用来做连 接查询)，每一张表都对应着执行计划输出中的一条记录，对于在同一个SELECT关键字中的表来说，它们的id值是相同的。 设计MySQL的大叔为每一个SELECT关键字代表的小查询都定义了一个称之为select_type的属性，意思是我们只要知道了某个小查询的select_type属性，就知道了这个小查询在整个大查询中扮演了一个什么角色。 SIMPLE (简单SELECT,不使用UNION或子查询等) PRIMARY 对于包含UNION、UNION ALL或者子查询的大查询来说，它是由几个小查询组成的，其中最左边的那个查询的select_type值就是PRIMARY。查询中若包含任何复杂的子部分,最外层的select被标记为PRIMARY。 EXPLAIN SELECT * FROM s1 UNION SELECT * FROM s2; 最左边的小查询SELECT * FROM s1对应的是执行计划中的第一条记录，它的select_type值就是PRIMARY。 UNION 对于包含UNION或者UNION ALL的大查询来说，它是由几个小查询组成的，其中除了最左边的那个小查询以外，其余的小查询的select_type值就是UNION UNION RESULT MySQL选择使用临时表来完成UNION查询的去重工作，针对该临时表的查询的select_type就是UNION RESULT。 SUBQUERY 如果包含子查询的查询语句不能够转为对应的semi-join的形式，并且该子查询是不相关子查询，并且查询优化器决定采用将该子查询物化的方案来执行该子查询时，该子查询的第一个SELECT关键 字代表的那个查询的select_type就是SUBQUERY。 mysql\u003e EXPLAIN SELECT * FROM s1 WHERE key1 IN (SELECT key1 FROM s2) OR key3 = 'a'; +----+-------------+-------+------------+-------+---------------+----------+---------+------+------+----------+-------------+ | id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra | +----+-------------+-------+------------+-------+---------------+----------+---------+------+------+----------+-------------+ | 1 | PRIMARY | s1 | NULL | ALL | idx_key3 | NULL | NULL | NULL | 9688 | 100.00 | Using where | | 2 | SUBQUERY | s2 | NULL | index | idx_key1 | idx_key1 | 303 | NULL | 9954 | 100.00 | Using index | +----+-------------+-------+------------+-------+---------------+----------+---------+------+------+----------+-------------+ 外层查询的select_type就是PRIMARY，子查询的select_type就是SUBQUERY。需要大家注意的是，由于select_type为SUBQUERY的子查询由于会被物化，所以只需要执行一遍。 DEPENDENT SUBQUERY 如果包含子查询的查询语句不能够转为对应的semi-join的形式，并且该子查询是相关子查询，则该子查询的第一个SELECT关键字代表的那个查询的select_type就是DEPENDENT SUBQUERY。 EXPLAIN SELECT * FROM s1 WHERE key1 IN (SELECT key1 FROM s2 WHERE s1.key2 = s2.key2) OR key3 = 'a'; select_type为DEPENDENT SUBQUERY的查询可能会被执行多次。 DEPENDENT UNION 在包含UNION或者UNION ALL的大查询中，如果各个小查询都依赖于外层查询的话，那除了最左边的那个小查询之外，其余的小查询的select_type的值就是DEPENDENT UNION。说的有些绕哈，比方说 下边这个查询: mysql\u003e EXPLAIN SELECT * FROM s1 WHERE key1 IN (SELECT key1 FROM s2 WHERE key1 = 'a' UNION SELECT key1 FROM s1 WHERE key1 = 'b'); DERIVED 对于采用物化的方式执行的包含派生表的查询，该派生表对应的子查询的select_type就是DERIVED， EXPLAIN SELECT * FROM (SELECT key1, count(*) as c FROM s1 GROUP BY key1) AS derived_s1 where c \u003e 1; +----+-------------+------------+------------+-------+---------------+----------+---------+------+------+----------+-------------+ | id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra | +----+-------------+------------+------------+-------+---------------+----------+---------+------+------+----------+-------------+ | 1 | PRIMARY | \u003cderived2\u003e | NULL | ALL | NULL | NULL | NULL | NULL | 9688 | 33.33 | Using where | | 2 | DERIVED | s1 | NULL | index | idx_key1 | idx_key1 | 303 | NULL | 9688 | 100.00 | Using index | +----+-------------+------------+------------+-------+---------------+----------+---------+------+------+----------+-------------+ 从执行计划中可以看出，id为2的记录就代表子查询的执行方式，它的select_type是DERIVED，说明该子查询是以物化的方式执行的。id为1的记录代表外层查询，大家注意看它的ta","date":"2019-03-03","objectID":"/posts/mysql_explain/:0:0","tags":["mysql"],"title":"mysql explain","uri":"/posts/mysql_explain/"},{"categories":["笔记"],"content":"mysql 索引下推","date":"2019-03-03","objectID":"/posts/mysql_%E7%B4%A2%E5%BC%95%E4%B8%8B%E6%8E%A8/","tags":["mysql"],"title":"mysql 索引下推","uri":"/posts/mysql_%E7%B4%A2%E5%BC%95%E4%B8%8B%E6%8E%A8/"},{"categories":["笔记"],"content":"索引下推（INDEX CONDITION PUSHDOWN，简称 ICP）是 MySQL 5.6 发布后针对扫描二级索引的一项优化改进。总的来说是通过把索引过滤条件下推到存储引擎，来减少 MySQL 存储引擎访问基表的次数以及 MySQL 服务层访问存储引擎的次数。ICP 适用于 MYISAM 和 INNODB，本篇的内容只基于 INNODB。ICP的实质就是通过二级索引尽可能的过滤不符合条件的记录，哪怕不符合最左匹配原则，减少回表，降低执行成本。 MySQL ICP 里涉及到的知识点如下： MySQL 服务层：也就是 SERVER 层，用来解析 SQL 的语法、语义、生成查询计划、接管从 MySQL 存储引擎层上推的数据进行二次过滤等等。 MySQL 存储引擎层：按照 MySQL 服务层下发的请求，通过索引或者全表扫描等方式把数据上传到 MySQL 服务层。 MySQL 索引扫描：根据指定索引过滤条件（比如 where id = 1) ，遍历索引找到索引键对应的主键值后回表过滤剩余过滤条件。 MySQL 索引过滤：通过索引扫描并且基于索引进行二次条件过滤后再回表。 ICP 就是把以上索引扫描和索引过滤合并在一起处理，过滤后的记录数据下推到存储引擎后的一种索引优化策略。这样做的优点如下： 减少了回表的操作次数。 减少了上传到 MySQL SERVER 层的数据。 ICP 默认开启，可通过优化器开关参数关闭 ICP：optimizer_switch=‘index_condition_pushdown=off’ 或者是在 SQL 层面通过 HINT 来关闭。 在不使用 ICP 索引扫描的过程： MySQL 存储引擎层只把满足索引键值对应的整行表记录一条一条取出，并且上传给 MySQL 服务层。 MySQL 服务层对接收到的数据，使用 SQL 语句后面的 where 条件过滤，直到处理完最后一行记录，再一起返回给客户端。 使用 ICP 扫描的过程： MySQL 存储引擎层，先根据过滤条件中包含的索引键确定索引记录区间，再在这个区间的记录上使用包含索引键的其他过滤条件进行过滤，之后规避掉不满足的索引记录，只根据满足条件的索引记录回表取回数据上传到 MySQL 服务层。 explain中查看 查看语句是否用了 ICP，只需要对语句进行 EXPLAIN，在 EXTRA 信息里可以看到 ICP 相关信息。其中 extra 里显示 “Using index condition” 就代表用了 ICP。 使用时限制条件 任何需要下推到底层存储层的操作一般都有诸多限制，MySQL ICP 也不例外，ICP 限制如下： ICP 仅用于需要访问基表所有记录时使用，适用的访问方法为：range、ref、eq_ref、ref_or_null。我上面举的例子即是 ref 类型，ICP 尤其是对联合索引的部分列模糊查找非常有效。 ICP 同样适用于分区表。 ICP 的目标是减少全行记录读取，从而减少 I/O 操作，仅用于二级索引。主键索引本身即是表数据，不存在下推操作。 ICP 不支持基于虚拟列上建立的索引，比如函数索引。 ICP 不支持引用子查询的条件。 ","date":"2019-03-03","objectID":"/posts/mysql_%E7%B4%A2%E5%BC%95%E4%B8%8B%E6%8E%A8/:0:0","tags":["mysql"],"title":"mysql 索引下推","uri":"/posts/mysql_%E7%B4%A2%E5%BC%95%E4%B8%8B%E6%8E%A8/"},{"categories":["笔记"],"content":"mysql 锁","date":"2019-03-03","objectID":"/posts/mysql_%E9%94%81/","tags":["mysql"],"title":"mysql 锁","uri":"/posts/mysql_%E9%94%81/"},{"categories":["笔记"],"content":"意向锁 当我们在对使用InnoDB存储引擎的表的某些记录加S锁之前，那就需要先在表级别加一个IS锁，当我们在对使用InnoDB存储引擎的表的某些记录加X锁之前，那就需要先在表级别加一个IX锁。 IS、IX锁是表级锁，它们的提出仅仅为了在之后加表级别的S锁和X锁时可以快速判断表中的记录是否被上锁，以避免用遍历的方式来查看表中有没有上锁的记录，也就是说其实IS锁和IX锁是 兼容的，IX锁和IX锁是兼容的。 表级别的S锁、X锁 在对某个表执行SELECT、INSERT、DELETE、UPDATE语句时，InnoDB存储引擎是不会为这个表添加表级别的S锁或者X锁的。 另外，在对某个表执行一些诸如ALTER TABLE、DROP TABLE这类的DDL语句时，其他事务对这个表并发执行诸如SELECT、INSERT、DELETE、UPDATE的语句会发生阻塞，同理，某个事务中对某个表执 行SELECT、INSERT、DELETE、UPDATE语句时，在其他会话中对这个表执行DDL语句也会发生阻塞。这个过程其实是通过在server层使用一种称之为元数据锁(英文名:Metadata Locks，简称MDL)东东 来实现的，一般情况下也不会使用InnoDB存储引擎自己提供的表级别的S锁和X锁。 其实这个InnoDB存储引擎提供的表级S锁或者X锁是相当鸡肋，只会在一些特殊情况下，比方说崩溃恢复过程中用到。 表级别的AUTO-INC锁 在使用MySQL过程中，我们可以为表的某个列添加AUTO_INCREMENT属性，之后在插入记录时，可以不指定该列的值，系统会自动为它赋上递增的值。 系统实现这种自动给AUTO_INCREMENT修饰的列递增赋值的原理主要是两个: 采用AUTO-INC锁 也就是在执行插入语句时就在表级别加一个AUTO-INC锁，然后为每条待插入记录的AUTO_INCREMENT修饰的列分配递增的值，在该语句执行结束后，再把AUTO-INC锁释放掉。 这样一个事务在持有AUTO-INC锁的过程中，其他事务的插入语句都要被阻塞，可以保证一个语句中分配的递增值是连续的。 如果我们的插入语句在执行前不可以确定具体要插入多少条记录(无法预计即将插入记录的数量)，比方说使用INSERT … SELECT、REPLACE … SELECT或者LOAD DATA这种插入语句，一般 是使用AUTO-INC锁为AUTO_INCREMENT修饰的列生成对应的值。 小贴士: 需要注意一下的是，这个AUTO-INC锁的作用范围只是单个插入语句，插入语句执行完成后，这个锁就被释放了，跟我们之前介绍的锁在事务结束时释放是不一样的。 采用一个轻量级的锁 在为插入语句生成AUTO_INCREMENT修饰的列的值时获取一下这个轻量级锁，然后生成本次插入语句需要用到的AUTO_INCREMENT列的值之后，就把该轻量级锁释放掉，并 不需要等到整个插入语句执行完才释放锁。 如果我们的插入语句在执行前就可以确定具体要插入多少条记录，比方说我们上边举的关于表t的例子中，在语句执行前就可以确定要插入2条记录，那么一般采用轻量级锁的方式 对AUTO_INCREMENT修饰的列进行赋值。这种方式可以避免锁定表，可以提升插入性能。 小贴士: 设计InnoDB的大叔提供了一个称之为innodb_autoinc_lock_mode的系统变量来控制到底使用上述两种方式中的哪种来为AUTO_INCREMENT修饰的列进行赋值，当 innodb_autoinc_lock_mode值为0时，一律采用AUTO-INC锁;当innodb_autoinc_lock_mode值为2时，一律采用轻量级锁;当innodb_autoinc_lock_mode值为1时，两种方式混着来(也就是在插入 记录数量确定时采用轻量级锁，不确定时使用AUTO-INC锁)。不过当innodb_autoinc_lock_mode值为2时，可能会造成不同事务中的插入语句为AUTO_INCREMENT修饰的列生成的值是交 叉的，在有主从复制的场景中是不安全的。 ","date":"2019-03-03","objectID":"/posts/mysql_%E9%94%81/:0:0","tags":["mysql"],"title":"mysql 锁","uri":"/posts/mysql_%E9%94%81/"},{"categories":["笔记"],"content":"mysql优化","date":"2019-03-03","objectID":"/posts/mysql_%E4%BC%98%E5%8C%96/","tags":["mysql"],"title":"mysql优化","uri":"/posts/mysql_%E4%BC%98%E5%8C%96/"},{"categories":["笔记"],"content":"分析效率 show status like “xxx” 查询一些msyql性能参数 connections: 连接数据库服务端的次数 slow_queries：慢查询次数 com_select：查询次数 com_insert：插入次数等 ","date":"2019-03-03","objectID":"/posts/mysql_%E4%BC%98%E5%8C%96/:0:0","tags":["mysql"],"title":"mysql优化","uri":"/posts/mysql_%E4%BC%98%E5%8C%96/"},{"categories":["笔记"],"content":"慢查询日志分析 MySQL的慢查询日志是MySQL提供的一种日志记录，用来记录在MySQL中响应时间超过阈值的语句，具体指运行时间超过long_query_time值的SQL，则会被记录到慢查询日志中（日志可以写入文件或者数据库表，如果对性能要求高的话，建议写文件）。默认情况下，MySQL数据库是不开启慢查询日志的，long_query_time的默认值为10（即10秒，通常设置为1秒），即运行10秒以上的语句是慢查询语句。 修改my.cnf文件，增加或修改参数slow_query_log 和slow_query_log_file后，然后重启MySQL服务器，如下所示 slow_query_log =1 slow_query_log_file=/tmp/mysql_slow.log 优化查询 ","date":"2019-03-03","objectID":"/posts/mysql_%E4%BC%98%E5%8C%96/:1:0","tags":["mysql"],"title":"mysql优化","uri":"/posts/mysql_%E4%BC%98%E5%8C%96/"},{"categories":["笔记"],"content":"联合索引最左前缀原则 复合索引遵守「最左前缀」原则，查询条件中，使用了复合索引前面的字段，索引才会被使用，如果不是按照索引的最左列开始查找，则无法使用索引。 比如在(a,b,c)三个字段上建立联合索引，那么它能够加快a|(a,b)|(a,b,c)三组查询的速度，而不能加快b|(b,a)这种查询顺序。 另外，建联合索引的时候，区分度最高的字段在最左边。 ","date":"2019-03-03","objectID":"/posts/mysql_%E4%BC%98%E5%8C%96/:2:0","tags":["mysql"],"title":"mysql优化","uri":"/posts/mysql_%E4%BC%98%E5%8C%96/"},{"categories":["笔记"],"content":"不要在列上使用函数和进行运算 不要在列上使用函数，这将导致索引失效而进行全表扫描。 例如下面的 SQL 语句： select * from artile where YEAR(create_time) \u003c= ‘2018’; 即使 date 上建立了索引，也会全表扫描，可以把计算放到业务层，这样做不仅可以节省数据库的 CPU，还可以起到查询缓存优化效果。 ","date":"2019-03-03","objectID":"/posts/mysql_%E4%BC%98%E5%8C%96/:3:0","tags":["mysql"],"title":"mysql优化","uri":"/posts/mysql_%E4%BC%98%E5%8C%96/"},{"categories":["笔记"],"content":"负向条件查询不能使用索引 负向条件有：!=、\u003c\u003e、not in、not exists、not like 等。 select * from artile where status != 1 and status != 2; 可以使用in进行优化： select * from artile where status in (0,3) ","date":"2019-03-03","objectID":"/posts/mysql_%E4%BC%98%E5%8C%96/:4:0","tags":["mysql"],"title":"mysql优化","uri":"/posts/mysql_%E4%BC%98%E5%8C%96/"},{"categories":["笔记"],"content":"使用覆盖索引 所谓覆盖索引，是指被查询的列，数据能从索引中取得，而不用通过行定位符再到数据表上获取，能够极大的提高性能。 可以定义一个让索引包含的额外的列，即使这个列对于索引而言是无用的。 ","date":"2019-03-03","objectID":"/posts/mysql_%E4%BC%98%E5%8C%96/:5:0","tags":["mysql"],"title":"mysql优化","uri":"/posts/mysql_%E4%BC%98%E5%8C%96/"},{"categories":["笔记"],"content":"避免强制类型转换 当查询条件左右两侧类型不匹配的时候会发生强制转换，强制转换可能导致索引失效而进行全表扫描。 如果phone字段是varchar类型，则下面的SQL不能命中索引： select * from user where phone=12345678901; 复制代码可以优化为： select * from user where phone=‘12345678901’; 并不是所有的类型转换都会让索引失效。 数字和字符串比对时，是把字符串转为数字了。 例子：user表name和age都分别有索引。 explain select * from user where age ='5';//字符串转为数字了，刚好age是数字，依然能用上索引 explain select * from user where name =8; //此时name索引失效 ","date":"2019-03-03","objectID":"/posts/mysql_%E4%BC%98%E5%8C%96/:6:0","tags":["mysql"],"title":"mysql优化","uri":"/posts/mysql_%E4%BC%98%E5%8C%96/"},{"categories":["笔记"],"content":"范围列可以用到索引 范围条件有：\u003c、\u003c=、\u003e、\u003e=、between等。 范围列可以用到索引，但是范围列后面的列无法用到索引，索引最多用于一个范围列，如果查询条件中有两个范围列则无法全用到索引 ","date":"2019-03-03","objectID":"/posts/mysql_%E4%BC%98%E5%8C%96/:7:0","tags":["mysql"],"title":"mysql优化","uri":"/posts/mysql_%E4%BC%98%E5%8C%96/"},{"categories":["笔记"],"content":"更新频繁、数据区分度不高的字段上不宜建立索引 更新会变更B+树，更新频繁的字段建立索引会大大降低数据库性能。 「性别」这种区分度不大的属性，建立索引没有意义，不能有效过滤数据，性能与全表扫描类似。 区分度可以使用 count(distinct(列名))/count(*) 来计算，在80%以上的时候就可以建立索引。 ","date":"2019-03-03","objectID":"/posts/mysql_%E4%BC%98%E5%8C%96/:8:0","tags":["mysql"],"title":"mysql优化","uri":"/posts/mysql_%E4%BC%98%E5%8C%96/"},{"categories":["笔记"],"content":"避免使用or来连接条件 应该尽量避免在 where 子句中使用 or 来连接条件，因为这会导致索引失效而进行全表扫描，虽然新版的MySQL能够命中索引，但查询优化耗费的 CPU比in多。 Or的时候只有or前后都是索引才有效。 ","date":"2019-03-03","objectID":"/posts/mysql_%E4%BC%98%E5%8C%96/:9:0","tags":["mysql"],"title":"mysql优化","uri":"/posts/mysql_%E4%BC%98%E5%8C%96/"},{"categories":["笔记"],"content":"模糊查询 前导模糊查询不能使用索引，非前导查询可以。 ","date":"2019-03-03","objectID":"/posts/mysql_%E4%BC%98%E5%8C%96/:10:0","tags":["mysql"],"title":"mysql优化","uri":"/posts/mysql_%E4%BC%98%E5%8C%96/"},{"categories":["笔记"],"content":"分页优化 大的分页数据效率比较差，可以使用子查询先获得对于的Id,然后再查。或者通过id的返回。不过这些都是要id是有序的才行。 如果明确知道只有一条结果返回，limit 1 能够提高效率 虽然自己知道只有一条结果，但数据库并不知道，明确告诉它，让它主动停止游标移动。 旧版本中可以使用limit n,-1;来获取偏移量到最后的数据，新版本中不能这样了，官方建议使用一个较大的数字来实现。 ","date":"2019-03-03","objectID":"/posts/mysql_%E4%BC%98%E5%8C%96/:11:0","tags":["mysql"],"title":"mysql优化","uri":"/posts/mysql_%E4%BC%98%E5%8C%96/"},{"categories":["笔记"],"content":"插入的优化 如果要插入大量的数据可以在插入前暂停索引、唯一校验、外键检查。不过看存储引擎是否支持。 load data infile比insert语句快。 ","date":"2019-03-03","objectID":"/posts/mysql_%E4%BC%98%E5%8C%96/:12:0","tags":["mysql"],"title":"mysql优化","uri":"/posts/mysql_%E4%BC%98%E5%8C%96/"},{"categories":["笔记"],"content":"mysql备份、主从","date":"2019-03-02","objectID":"/posts/mysql_%E5%A4%87%E4%BB%BD%E4%B8%BB%E4%BB%8E/","tags":["mysql"],"title":"mysql备份、主从","uri":"/posts/mysql_%E5%A4%87%E4%BB%BD%E4%B8%BB%E4%BB%8E/"},{"categories":["笔记"],"content":"备份和恢复 ","date":"2019-03-02","objectID":"/posts/mysql_%E5%A4%87%E4%BB%BD%E4%B8%BB%E4%BB%8E/:0:0","tags":["mysql"],"title":"mysql备份、主从","uri":"/posts/mysql_%E5%A4%87%E4%BB%BD%E4%B8%BB%E4%BB%8E/"},{"categories":["笔记"],"content":"mysqldump命令备份 mysqldump命令把数据库备份为一个文本文件，包含了很多create和insert语句，使用这些语句就可以重新插入和备份。 我们可以直接对数据库备份，也可以对具体某些表进行导出。 mysqldump -u user -h host -p password dbname [tablename…] \u003e filename.sql ","date":"2019-03-02","objectID":"/posts/mysql_%E5%A4%87%E4%BB%BD%E4%B8%BB%E4%BB%8E/:1:0","tags":["mysql"],"title":"mysql备份、主从","uri":"/posts/mysql_%E5%A4%87%E4%BB%BD%E4%B8%BB%E4%BB%8E/"},{"categories":["笔记"],"content":"数据恢复 mysql -u user -p [dbname] \u003c filename.sql 如果导出的语句中包含创建数据库的语句就不用指定数据库名了 ","date":"2019-03-02","objectID":"/posts/mysql_%E5%A4%87%E4%BB%BD%E4%B8%BB%E4%BB%8E/:2:0","tags":["mysql"],"title":"mysql备份、主从","uri":"/posts/mysql_%E5%A4%87%E4%BB%BD%E4%B8%BB%E4%BB%8E/"},{"categories":["笔记"],"content":"数据库迁移 我们可能需要安装新的数据库、mysql版本更新等原因而迁移数据库。 迁移其实就是导出和恢复的过程，当然如果主版本号相同我们还可以直接拷贝数据库文件（只适用于MyISAM引擎） Mysql Binlog格式介绍 Mysql binlog日志有三种格式，分别为Statement,MiXED,以及ROW！ Mysql默认是使用Statement日志格式，推荐使用MIXED。 由于一些特殊使用，可以考虑使用ROWED，如自己通过binlog日志来同步数据的修改，这样会节省很多相关操作。 ","date":"2019-03-02","objectID":"/posts/mysql_%E5%A4%87%E4%BB%BD%E4%B8%BB%E4%BB%8E/:3:0","tags":["mysql"],"title":"mysql备份、主从","uri":"/posts/mysql_%E5%A4%87%E4%BB%BD%E4%B8%BB%E4%BB%8E/"},{"categories":["笔记"],"content":"Statement 每一条会修改数据的sql都会记录在binlog中。 **优点：**不需要记录每一行的变化，减少了binlog日志量，节约了IO，提高性能。 **缺点：**由于记录的只是执行语句，为了这些语句能在slave上正确运行，因此还必须记录每条语句在执行的时候的一些相关信息，以保证所有语句能在slave得到和在master端执行时候相同 的结果。另外mysql 的复制,像一些特定函数功能，slave可与master上要保持一致会有很多相关问题。 ","date":"2019-03-02","objectID":"/posts/mysql_%E5%A4%87%E4%BB%BD%E4%B8%BB%E4%BB%8E/:4:0","tags":["mysql"],"title":"mysql备份、主从","uri":"/posts/mysql_%E5%A4%87%E4%BB%BD%E4%B8%BB%E4%BB%8E/"},{"categories":["笔记"],"content":"Row: 不记录sql语句上下文相关信息，仅保存哪条记录被修改。 优点： binlog中可以不记录执行的sql语句的上下文相关的信息，仅需要记录那一条记录被修改成什么了。所以rowlevel的日志内容会非常清楚的记录下每一行数据修改的细节。而且不会出现某些特定情况下的存储过程，或function，以及trigger的调用和触发无法被正确复制的问题。 **缺点：**所有的执行的语句当记录到日志中的时候，都将以每行记录的修改来记录，这样可能会产生大量的日志内容,比如一条update语句，修改多条记录，则binlog中每一条修改都会有记录，这样造成binlog日志量会很大，特别是当执行alter table之类的语句的时候，由于表结构修改，每条记录都发生改变，那么该表每一条记录都会记录到日志中。 ","date":"2019-03-02","objectID":"/posts/mysql_%E5%A4%87%E4%BB%BD%E4%B8%BB%E4%BB%8E/:5:0","tags":["mysql"],"title":"mysql备份、主从","uri":"/posts/mysql_%E5%A4%87%E4%BB%BD%E4%B8%BB%E4%BB%8E/"},{"categories":["笔记"],"content":"Mixedlevel: 是以上两种level的混合使用，一般的语句修改使用statment格式保存binlog，如一些函数，statement无法完成主从复制的操作，则采用row格式保存binlog 主从复制 主服务器master把数据复制到多台从服务器slaves。 主从一般是一个主，多个从。也可以是链式的a-\u003eb-\u003ec ,这样b既是主又是从。 主从复制主要分以下步骤： 主服务器 将数据的更新记录到 二进制日志（Binary log）中，用于记录二进制日志事件，这一步由 主库线程 完成； 从库 将 主库 的 二进制日志 复制到本地的 中继日志（Relay log），这一步由 从库 I/O 线程 完成； 从库 读取 中继日志 中的 事件，将其重放到数据中，这一步由 从库 SQL 线程 完成。 ","date":"2019-03-02","objectID":"/posts/mysql_%E5%A4%87%E4%BB%BD%E4%B8%BB%E4%BB%8E/:6:0","tags":["mysql"],"title":"mysql备份、主从","uri":"/posts/mysql_%E5%A4%87%E4%BB%BD%E4%B8%BB%E4%BB%8E/"},{"categories":["笔记"],"content":"流程 可以看到：主库接收到客户端的更新请求后，执行内部事务的更新逻辑，同时写 binlog。 备库 B 跟主库 A 之间维持了一个长连接。主库 A 内部有一个线程，专门用于服务备库 B 的这个长连接。一个事务日志同步的完整过程是这样的： 在备库 B 上通过 change master 命令，设置主库 A 的 IP、端口、用户名、密码，以及要从哪个位置开始请求 binlog，这个位置包含文件名和日志偏移量。 在备库 B 上执行 start slave 命令，这时候备库会启动两个线程，就是图中的 io_thread 和 sql_thread。其中 io_thread 负责与主库建立连接。 主库 A 校验完用户名、密码后，开始按照备库 B 传过来的位置，从本地读取 binlog，发给 B。 备库 B 拿到 binlog 后，写到本地文件，称为中转日志（relay log）。 sql_thread 读取中转日志，解析出日志里的命令，并执行。 备库设置成只读了，还怎么跟主库保持同步更新呢？ 这个问题，你不用担心。因为 readonly 设置对超级 (super) 权限用户是无效的，而用于同步更新的线程，就拥有超级权限。 ","date":"2019-03-02","objectID":"/posts/mysql_%E5%A4%87%E4%BB%BD%E4%B8%BB%E4%BB%8E/:7:0","tags":["mysql"],"title":"mysql备份、主从","uri":"/posts/mysql_%E5%A4%87%E4%BB%BD%E4%B8%BB%E4%BB%8E/"},{"categories":["笔记"],"content":"操作步骤 ","date":"2019-03-02","objectID":"/posts/mysql_%E5%A4%87%E4%BB%BD%E4%B8%BB%E4%BB%8E/:8:0","tags":["mysql"],"title":"mysql备份、主从","uri":"/posts/mysql_%E5%A4%87%E4%BB%BD%E4%B8%BB%E4%BB%8E/"},{"categories":["笔记"],"content":"master配置 [mysqld] ## 设置server_id，一般设置为IP,注意要唯一 server_id=100 ## 复制过滤：也就是指定哪个数据库不用同步（mysql库一般不同步） binlog-ignore-db=mysql ## 开启二进制日志功能，可以随便取，最好有含义（关键就是这里了） log-bin=edu-mysql-bin ## 为每个session 分配的内存，在事务过程中用来存储二进制日志的缓存 binlog_cache_size=1M ## 主从复制的格式（mixed,statement,row，默认格式是statement） binlog_format=mixed ## 二进制日志自动删除/过期的天数。默认值为0，表示不自动删除。 expire_logs_days=7 ## 跳过主从复制中遇到的所有错误或指定类型的错误，避免slave端复制中断。 ## 如：1062错误是指一些主键重复，1032错误是因为主从数据库数据不一致 slave_skip_errors=1062 ","date":"2019-03-02","objectID":"/posts/mysql_%E5%A4%87%E4%BB%BD%E4%B8%BB%E4%BB%8E/:8:1","tags":["mysql"],"title":"mysql备份、主从","uri":"/posts/mysql_%E5%A4%87%E4%BB%BD%E4%B8%BB%E4%BB%8E/"},{"categories":["笔记"],"content":"创建复制账户，授予权限 CREATE USER 'slave'@'%' IDENTIFIED BY '123456'; GRANT REPLICATION SLAVE, REPLICATION CLIENT ON *.* TO 'slave'@'%'; 这里主要是要授予用户REPLICATION SLAVE权限和REPLICATION CLIENT权限 ","date":"2019-03-02","objectID":"/posts/mysql_%E5%A4%87%E4%BB%BD%E4%B8%BB%E4%BB%8E/:8:2","tags":["mysql"],"title":"mysql备份、主从","uri":"/posts/mysql_%E5%A4%87%E4%BB%BD%E4%B8%BB%E4%BB%8E/"},{"categories":["笔记"],"content":"slave配置 [mysqld] ## 设置server_id，一般设置为IP,注意要唯一 server_id=101 ## 复制过滤：也就是指定哪个数据库不用同步（mysql库一般不同步） binlog-ignore-db=mysql ## 开启二进制日志功能，以备Slave作为其它Slave的Master时使用 log-bin=edu-mysql-slave1-bin ## 为每个session 分配的内存，在事务过程中用来存储二进制日志的缓存 binlog_cache_size=1M ## 主从复制的格式（mixed,statement,row，默认格式是statement） binlog_format=mixed ## 二进制日志自动删除/过期的天数。默认值为0，表示不自动删除。 expire_logs_days=7 ## 跳过主从复制中遇到的所有错误或指定类型的错误，避免slave端复制中断。 ## 如：1062错误是指一些主键重复，1032错误是因为主从数据库数据不一致 slave_skip_errors=1062 ## relay_log配置中继日志 relay_log=edu-mysql-relay-bin ## log_slave_updates表示slave将复制事件写进自己的二进制日志 log_slave_updates=1 ## 防止改变数据(除了特殊的线程) read_only=1 ","date":"2019-03-02","objectID":"/posts/mysql_%E5%A4%87%E4%BB%BD%E4%B8%BB%E4%BB%8E/:8:3","tags":["mysql"],"title":"mysql备份、主从","uri":"/posts/mysql_%E5%A4%87%E4%BB%BD%E4%B8%BB%E4%BB%8E/"},{"categories":["笔记"],"content":"查看master status show master status; 记录下返回结果的File列和Position列的值 ","date":"2019-03-02","objectID":"/posts/mysql_%E5%A4%87%E4%BB%BD%E4%B8%BB%E4%BB%8E/:8:4","tags":["mysql"],"title":"mysql备份、主从","uri":"/posts/mysql_%E5%A4%87%E4%BB%BD%E4%B8%BB%E4%BB%8E/"},{"categories":["笔记"],"content":"Slave中设置Master信息 change master to master_host='192.168.1.100', master_user='slave', master_password='123456', master_port=3306, master_log_file='edu-mysql-bin.000001', master_log_pos=1389, master_connect_retry=30; 上面执行的命令的解释： master_host=’192.168.1.100′ ## Master的IP地址 master_user=’slave’ ## 用于同步数据的用户（在Master中授权的用户） master_password=’123456′ ## 同步数据用户的密码 master_port=3306 ## Master数据库服务的端口 masterlogfile=’edu-mysql-bin.000001′ ##指定Slave从哪个日志文件开始读复制数据（Master上执行命令的结果的File字段） masterlogpos=429 ## 从哪个POSITION号开始读（Master上执行命令的结果的Position字段） masterconnectretry=30 ##当重新建立主从连接时，如果连接建立失败，间隔多久后重试。单位为秒，默认设置为60秒，同步延迟调优参数。 ","date":"2019-03-02","objectID":"/posts/mysql_%E5%A4%87%E4%BB%BD%E4%B8%BB%E4%BB%8E/:8:5","tags":["mysql"],"title":"mysql备份、主从","uri":"/posts/mysql_%E5%A4%87%E4%BB%BD%E4%B8%BB%E4%BB%8E/"},{"categories":["笔记"],"content":"开始同步 start slave; show slave status; 查询查看主从同步状态，会发现SlaveIORunning和SlaveSQLRunning是Yes了，表明开启成功 读写分离 主服务只负责写，从服务器只负责读，可以达到负载均衡的效果。 我们可以在程序中指定连接地址，区分读写数据库。 这里用第三方mysql proxy来读写分离。它是位于客户端与mysql数据库之间的程序。 我们可以指定主、从的地址，它能自动的做负载均衡，对程序是透明的。 ","date":"2019-03-02","objectID":"/posts/mysql_%E5%A4%87%E4%BB%BD%E4%B8%BB%E4%BB%8E/:8:6","tags":["mysql"],"title":"mysql备份、主从","uri":"/posts/mysql_%E5%A4%87%E4%BB%BD%E4%B8%BB%E4%BB%8E/"},{"categories":["笔记"],"content":"mysql基础","date":"2019-03-01","objectID":"/posts/mysql_%E5%9F%BA%E7%A1%80/","tags":["mysql"],"title":"mysql基础","uri":"/posts/mysql_%E5%9F%BA%E7%A1%80/"},{"categories":["笔记"],"content":"概述 sql包含4个部分： 数据定义语言DDL:create,drop,alter 数据操作语言DML:insert,update,delete 数据查询语言DQL:select 数据控制语言DCL:grant,revoke,commit,rollback 数据引擎 mysql有InnoDB,MyISAM,Merge等存储引擎，我们可以针对每一个表指定引擎。 CREATE TABLE parent ( id INT NOT NULL, PRIMARY KEY (id) ) ENGINE=INNODB; 查看支持的引擎。show engines InnoDB和MyISAM对比： InnoDB 支持事务，MyISAM 不支持事务。这是 MySQL 将默认存储引擎从 MyISAM 变成 InnoDB 的重要原因之一； InnoDB 支持外键，而 MyISAM 不支持。对一个包含外键的 InnoDB 表转为 MYISAM 会失败； InnoDB 是聚集索引，MyISAM 是非聚集索引。聚簇索引的文件存放在主键索引的叶子节点上，因此 InnoDB 必须要有主键，通过主键索引效率很高。但是辅助索引需要两次查询，先查询到主键，然后再通过主键查询到数据。因此，主键不应该过大，因为主键太大，其他索引也都会很大。而 MyISAM 是非聚集索引，数据文件是分离的，索引保存的是数据文件的指针。主键索引和辅助索引是独立的。 InnoDB 不保存表的具体行数，执行 select count(*) from table 时需要全表扫描。而MyISAM 用一个变量保存了整个表的行数，执行上述语句时只需要读出该变量即可，速度很快； InnoDB 最小的锁粒度是行锁，MyISAM 最小的锁粒度是表锁。一个更新语句会锁住整张表，导致其他查询和更新都会被阻塞，因此并发访问受限。这也是 MySQL 将默认存储引擎从 MyISAM 变成 InnoDB 的重要原因之一； 如何选择： 是否要支持事务，如果要请选择 InnoDB，如果不需要可以考虑 MyISAM； 如果表中绝大多数都只是读查询，可以考虑 MyISAM，如果既有读写也挺频繁，请使用InnoDB。 系统奔溃后，MyISAM恢复起来更困难，能否接受，不能接受就选 InnoDB； MySQL5.5版本开始Innodb已经成为Mysql的默认引擎(之前是MyISAM) 字符集 字符集可以针对全局、数据库、表、列进行设置 show variables like ‘character%’; 查看当前字符集设置 /etc/mysql/mysql.conf.d/mysqld.cnf 文件[musqld]下增加：character-set-server =utf8 重启服务 sudo service mysql restart 数据类型 mysql数据类型主要包含整数、浮点数、日期、时间及字符串。 ","date":"2019-03-01","objectID":"/posts/mysql_%E5%9F%BA%E7%A1%80/:0:0","tags":["mysql"],"title":"mysql基础","uri":"/posts/mysql_%E5%9F%BA%E7%A1%80/"},{"categories":["笔记"],"content":"整数 ","date":"2019-03-01","objectID":"/posts/mysql_%E5%9F%BA%E7%A1%80/:1:0","tags":["mysql"],"title":"mysql基础","uri":"/posts/mysql_%E5%9F%BA%E7%A1%80/"},{"categories":["笔记"],"content":"小数 字段名 float(M,N)，M代表总共位数；N代表小数位数。若不知道M，N则有硬件和操作系统决定。 ","date":"2019-03-01","objectID":"/posts/mysql_%E5%9F%BA%E7%A1%80/:2:0","tags":["mysql"],"title":"mysql基础","uri":"/posts/mysql_%E5%9F%BA%E7%A1%80/"},{"categories":["笔记"],"content":"日期时间 datetime输入什么时间取出就是什么时间，但是timestamp就是会根据数据库的时区变化。 我们还可以利用timestamp来指定字段插入或更新的时候自动生成/修改值 alter table user add last_time timestamp default current_timestamp on update current_timestamp; ","date":"2019-03-01","objectID":"/posts/mysql_%E5%9F%BA%E7%A1%80/:3:0","tags":["mysql"],"title":"mysql基础","uri":"/posts/mysql_%E5%9F%BA%E7%A1%80/"},{"categories":["笔记"],"content":"字符串 CHAR(M), VARCHAR(M)不同之处 CHAR(M)定义的列的长度为固定的，M取值可以为0～255之间，当保存CHAR值时，在它们的右边填充空格以达到指定的长度。当检 索到CHAR值时，尾部的空格被删除掉。在存储或检索过程中不进行大小写转换。CHAR存储定长数据很方便，CHAR字段上的索引效率比较高。 VARCHAR(M)定义的列的长度为可变长字符串，M取值可以为0~65535之间(旧版本255），(VARCHAR的最大有效长度由最大行大小和使用 的字符集确定。整体最大长度是65,532字节）。VARCHAR值保存时只保存需要的字符数，另加一个字节来记录长度(如果列声明的长度超过255，则 使用两个字节)。VARCHAR值保存时不进行填充。 enum只能选用其中的一个，而set可以是其中几个的联合，set会把值的末尾空格去除。 ","date":"2019-03-01","objectID":"/posts/mysql_%E5%9F%BA%E7%A1%80/:4:0","tags":["mysql"],"title":"mysql基础","uri":"/posts/mysql_%E5%9F%BA%E7%A1%80/"},{"categories":["笔记"],"content":"二进制 ","date":"2019-03-01","objectID":"/posts/mysql_%E5%9F%BA%E7%A1%80/:5:0","tags":["mysql"],"title":"mysql基础","uri":"/posts/mysql_%E5%9F%BA%E7%A1%80/"},{"categories":["笔记"],"content":"数据类型的选择 在长度一定的情况下，浮点数float能表示更大的范围，但是定点数decimal更准确 char固定长度，自动删除尾部空格，varchar是可变长度，不会删除尾部空格。 数据库基本操作 showdatabases查看所有数据库,其中MySQL是必须的，描述用户访问权限 创建数据库。create database db1; 查看数据库创建信息。show create database db1; 删除数据库。drop database db1; 切换数据库。use db1; 数据表的基本操作 ","date":"2019-03-01","objectID":"/posts/mysql_%E5%9F%BA%E7%A1%80/:6:0","tags":["mysql"],"title":"mysql基础","uri":"/posts/mysql_%E5%9F%BA%E7%A1%80/"},{"categories":["笔记"],"content":"创建数据表。 create table tb_name ( 字段名1，数据类型[列级别约束条件][默认值]， 字段名2，数据类型[列级别约束条件][默认值] ) ","date":"2019-03-01","objectID":"/posts/mysql_%E5%9F%BA%E7%A1%80/:7:0","tags":["mysql"],"title":"mysql基础","uri":"/posts/mysql_%E5%9F%BA%E7%A1%80/"},{"categories":["笔记"],"content":"约束 主键约束 主键约束。要求唯一，不能为空。 可以直接在类型后指定，也可以定义完所有的列之后指定。 create table tb1 ( id int primary key, name varchar(25) ) create table tb2 ( id int, name varchar(25), primary key(id) ) 联合主键约束。 联合主键要用第二种声明方式了。 create table tb2 ( id int, name varchar(25), primary key(id,name) ) 外键约束 只有InnoDB引擎才能使用外键 外键约束用在表之间建立链接。可以是一列或多列。外键可以为空值，如果不为空则必须等于另一个表中的主键。 外键不一定必须是另一个表的主键，只要满足唯一性就好。 外键中主键所在的那个表是主表，想关联外表的表是从表。 create table if not exists user ( id int, name varchar(25), classid int, cityid int, constraint fk_user_class foreign key (classid) references class(id) on delete cascade on update cascade, constraint fk_user_city foreign key (cityid) references city(id) ) user表classid，cityid分别链接class，city表中的id。 另外我们还自定义了一个on，当我们没指定on的时候，删除和修改主表中的主键都是不可以的。 on delete，on update后面都可以跟参数，有4种参数： restrict方式：严格模式，同no action，都是立即检查外键约束；不能删除和改 cascade方式：也叫级联方式，在父表上update/delete记录时，同步update/delete子表的匹配记录 No action方式：如果子表中有匹配的记录,则不允许对父表对应候选键进行update/delete set null方式：在父表上update/delete记录时，将子表上匹配记录的列设为null 要注意子表的外键列不能为not null 外键默认是严格模式。 非空约束 create table class ( id int primary key, name varchar(20) not null ) 唯一约束 create table person ( id int primary key, name varchar(20) unique ) 要求表中该值唯一，但只能出现一个空值。但是mysql是可以多个null的。 默认约束 字段名 数据类型 default 默认值 比如用户表男性较多，那么性别默认值可以设置为男 自增约束 自增类型可以是任何整数类型 字段名 数据类型 auto_increment ","date":"2019-03-01","objectID":"/posts/mysql_%E5%9F%BA%E7%A1%80/:7:1","tags":["mysql"],"title":"mysql基础","uri":"/posts/mysql_%E5%9F%BA%E7%A1%80/"},{"categories":["笔记"],"content":"查看数据表结构 desc tb1 查看表的结构 show create table tb1 查看表的创建语句 ","date":"2019-03-01","objectID":"/posts/mysql_%E5%9F%BA%E7%A1%80/:8:0","tags":["mysql"],"title":"mysql基础","uri":"/posts/mysql_%E5%9F%BA%E7%A1%80/"},{"categories":["笔记"],"content":"修改表 重命名表 alter table 旧表名 rename 新表名 修改表引擎 alter table 表名 engine=“InnoDB” 修改字段类型 alter table 表名 change 旧字段 新字段 数据类型 添加字段 alter table 表名 add 字段名 类型（这里我们可以指定字段的位置） 删除字段 alter table 表名 drop 字段名 添加外键 alter table 表名 add constraint 外键名 foreign key （字段名）references 主表名（字段名） 删除外键 alter table 表名 drop foreign key 外键名 ","date":"2019-03-01","objectID":"/posts/mysql_%E5%9F%BA%E7%A1%80/:9:0","tags":["mysql"],"title":"mysql基础","uri":"/posts/mysql_%E5%9F%BA%E7%A1%80/"},{"categories":["笔记"],"content":"删除表 删除表 drop table 表名1，表名2… 如果有字段被其他表关联，要先删除外键 数据库函数 ","date":"2019-03-01","objectID":"/posts/mysql_%E5%9F%BA%E7%A1%80/:10:0","tags":["mysql"],"title":"mysql基础","uri":"/posts/mysql_%E5%9F%BA%E7%A1%80/"},{"categories":["笔记"],"content":"数学函数 常见的有: 绝对值函数abs(x) 三角函数 随机函数 rand(),rand(x) 0\u003c=结果\u003c=1，x是种子 向上取整 ceil(x) 向下取整 floor(x) 平均值 avg(x) select avg(price) from user; ","date":"2019-03-01","objectID":"/posts/mysql_%E5%9F%BA%E7%A1%80/:11:0","tags":["mysql"],"title":"mysql基础","uri":"/posts/mysql_%E5%9F%BA%E7%A1%80/"},{"categories":["笔记"],"content":"字符串函数 长度 char_length(“xxx”) 小写 lcase(“XXX”) 大写 ucase(“xxx”) 去除空格 trim(”x\")，ltrim(“xx”)，rtrim(“xx”) 取子串 substring(“str”,start,len) start从1开始，负数表示从后面开始 子串位置 instr(“abcd”,“c”) 字符串翻转 reverse(“abc”) ","date":"2019-03-01","objectID":"/posts/mysql_%E5%9F%BA%E7%A1%80/:12:0","tags":["mysql"],"title":"mysql基础","uri":"/posts/mysql_%E5%9F%BA%E7%A1%80/"},{"categories":["笔记"],"content":"日期时间函数 当前时间 now() 当前日期 current_date() 当前时间 current_time() 获取年月日year(now())，month(now())，day(now()) 格式化 date_format(date,format) 日期时间加减 date_add()，addtime()等 ","date":"2019-03-01","objectID":"/posts/mysql_%E5%9F%BA%E7%A1%80/:13:0","tags":["mysql"],"title":"mysql基础","uri":"/posts/mysql_%E5%9F%BA%E7%A1%80/"},{"categories":["笔记"],"content":"加密函数 md加密 md5(str) ","date":"2019-03-01","objectID":"/posts/mysql_%E5%9F%BA%E7%A1%80/:14:0","tags":["mysql"],"title":"mysql基础","uri":"/posts/mysql_%E5%9F%BA%E7%A1%80/"},{"categories":["笔记"],"content":"聚合函数 AVG()函数忽略列值为NULL的行。 MAX()函数忽略列值为NULL的行。 MIN()函数忽略列值为NULL的行。 SUM()函数忽略列值为NULL的行。 COUNT()函数有两种使用方式： 使用COUNT(*)对表中行的数目进行计数，不管表列中包含的是空值（NULL）还是非空值。 使用COUNT(column)对特定列中具有值的行进行计数，忽略NULL值。 ","date":"2019-03-01","objectID":"/posts/mysql_%E5%9F%BA%E7%A1%80/:15:0","tags":["mysql"],"title":"mysql基础","uri":"/posts/mysql_%E5%9F%BA%E7%A1%80/"},{"categories":["笔记"],"content":"其他函数 返回最近一次插入的数据的id last_insert_id() 这里需要注意，last_insert_id是与表无关的，向a插入，再向b插入，获取的是b表的最新id 增、删，改表中的数据 插入数据 insert into 表名（字段名，字段名）values(值，值)； 将查询结果插入数据表 insert into 表名 （字段名）select …. 修改数据 update 表名 set 字段名=value，字段名=value [条件] 删除数据 delete from 表名 [条件] 单表的查询 select {* |字段列表} [ from 表1，表2 [where] [group by] [having] [order by] [limit [offet,]rowcount] ] ","date":"2019-03-01","objectID":"/posts/mysql_%E5%9F%BA%E7%A1%80/:16:0","tags":["mysql"],"title":"mysql基础","uri":"/posts/mysql_%E5%9F%BA%E7%A1%80/"},{"categories":["笔记"],"content":"where in操作 where id in (1,2,3)或者where id not in(1,2,3) 位于两者之间 where price between 1 and 2或者where price not between 1 and 2 通配符 where name like ‘%xxx%’。_只能通配一个字符 is null判断 where name is null或者where name not is null 多个条件 用and连接 条件或用or连接 去重 select distinct 字段名 ","date":"2019-03-01","objectID":"/posts/mysql_%E5%9F%BA%E7%A1%80/:17:0","tags":["mysql"],"title":"mysql基础","uri":"/posts/mysql_%E5%9F%BA%E7%A1%80/"},{"categories":["笔记"],"content":"分组 分组通常和count(),max()等函数配合使用 比如： select s_id,count(*) from fruits group by s_id 另外group_concat(字段名)可以把分组中想要的各字段名显示出来 ","date":"2019-03-01","objectID":"/posts/mysql_%E5%9F%BA%E7%A1%80/:18:0","tags":["mysql"],"title":"mysql基础","uri":"/posts/mysql_%E5%9F%BA%E7%A1%80/"},{"categories":["笔记"],"content":"多字段分组 分组可以指定多个字段，先按第一个字段分组，再按第二个分组，一次类推。 ","date":"2019-03-01","objectID":"/posts/mysql_%E5%9F%BA%E7%A1%80/:19:0","tags":["mysql"],"title":"mysql基础","uri":"/posts/mysql_%E5%9F%BA%E7%A1%80/"},{"categories":["笔记"],"content":"having过滤分组 group by 和having配合，只有满足条件的分组才能被显示。 select s_id,count(*) from fruits group by s_id having count(*)\u003e2 ","date":"2019-03-01","objectID":"/posts/mysql_%E5%9F%BA%E7%A1%80/:20:0","tags":["mysql"],"title":"mysql基础","uri":"/posts/mysql_%E5%9F%BA%E7%A1%80/"},{"categories":["笔记"],"content":"排序 order by 字段名，字段名 [asc/desc] 默认升序 如果有分组是对分组的排序 ","date":"2019-03-01","objectID":"/posts/mysql_%E5%9F%BA%E7%A1%80/:21:0","tags":["mysql"],"title":"mysql基础","uri":"/posts/mysql_%E5%9F%BA%E7%A1%80/"},{"categories":["笔记"],"content":"limit limit[offset] rows 如果想跳过n行取所有，可以指定一个很大的数字 连接查询 连接查询就是对2个或2个以上的表连接为一个表进行查询。 from 表1 [链接方式] join 表2 [on 条件] 链接方式常有交叉链接cross,内连接inner，左外连接left,右外连接right ","date":"2019-03-01","objectID":"/posts/mysql_%E5%9F%BA%E7%A1%80/:22:0","tags":["mysql"],"title":"mysql基础","uri":"/posts/mysql_%E5%9F%BA%E7%A1%80/"},{"categories":["笔记"],"content":"交叉连接 select * from user cross join class; 这样的话，左表每条记录分别和右表每条记录连接，造成很大的无用的数据，实际意义不大。 ","date":"2019-03-01","objectID":"/posts/mysql_%E5%9F%BA%E7%A1%80/:23:0","tags":["mysql"],"title":"mysql基础","uri":"/posts/mysql_%E5%9F%BA%E7%A1%80/"},{"categories":["笔记"],"content":"内连接 select * from user inner join class on user.classid=class.id; 保留的都是满足条件的记录，使用最多。 除此之外，内连接还有where形式的： select * from user,class where user.classid=class.id; 但是where相比inner join on效率低 ","date":"2019-03-01","objectID":"/posts/mysql_%E5%9F%BA%E7%A1%80/:24:0","tags":["mysql"],"title":"mysql基础","uri":"/posts/mysql_%E5%9F%BA%E7%A1%80/"},{"categories":["笔记"],"content":"左外连接 select * from user left join class on user.classid=class.id; 在进行连接后，以左边的表为主，左边表所有的数据都会显示在结果中，哪怕他们不符合on后边的条件，此时右边表的数据显示null ","date":"2019-03-01","objectID":"/posts/mysql_%E5%9F%BA%E7%A1%80/:25:0","tags":["mysql"],"title":"mysql基础","uri":"/posts/mysql_%E5%9F%BA%E7%A1%80/"},{"categories":["笔记"],"content":"右连接 右连接和左连接类似，只不过是以右边为主 ","date":"2019-03-01","objectID":"/posts/mysql_%E5%9F%BA%E7%A1%80/:26:0","tags":["mysql"],"title":"mysql基础","uri":"/posts/mysql_%E5%9F%BA%E7%A1%80/"},{"categories":["笔记"],"content":"联合查询 联合查询就是把多个查询语句结果集中在一起。 SELECT column_name(s) FROM table_name1 UNION SELECT column_name(s) FROM table_name2 联合的时候两个结果集的字段名不要求一样，但是数量和类型要求一样。 union联合的时候不包括重复行 union all联合的时候包括重复行 子查询 子查询是指一个查询语句嵌套在另一个查询语句中，首先内部子查询的结果作为外部查询的输入。 any 只要满足子查询随意的一个结果即可 all 满足所有子查询的结果 exists 子查询如果查到了一行，那么exists就是true,否则为false外查询不进行查询 in 存在和子查询中的结果一样的数据 not in 参考in 起别名 字段 [as] 别名 表 [as] 别名 利用正则查询 利用正则匹配查询 select * from user where name regexp \"^h\"; select * from user where name regexp \"a$\" 查询name以h开头的数据，name以a结尾的数据。 索引 索引是一个独立的、存在在磁盘上的数据库结构，它包含着表里所有记录的引用指针。有了索引查询的时候就不用从头到尾一行一行的判断了，速度更快。比如两万条数据，select * from user where id =30必须遍历表。有了索引直接找到id=30的行。 mysql的索引类型有btree和hash，具体和存储引擎相关。 索引优点： 通过创建唯一索引，保证每一条数据的唯一性 大大提升查询速度，这是主要原因 索引缺点 创建和维护需要时间，尤其是数据量大以后，每次插入要排序 占用空间 ","date":"2019-03-01","objectID":"/posts/mysql_%E5%9F%BA%E7%A1%80/:27:0","tags":["mysql"],"title":"mysql基础","uri":"/posts/mysql_%E5%9F%BA%E7%A1%80/"},{"categories":["笔记"],"content":"索引分类 ","date":"2019-03-01","objectID":"/posts/mysql_%E5%9F%BA%E7%A1%80/:28:0","tags":["mysql"],"title":"mysql基础","uri":"/posts/mysql_%E5%9F%BA%E7%A1%80/"},{"categories":["笔记"],"content":"普通索引和唯一索引 普通索引允许索引列有重复值和空值。 唯一索引必须唯一，可以为空。主键索引是特殊的唯一索引，不能为空。 mysql创建外键的时候会自动为该列创建普通索引 ","date":"2019-03-01","objectID":"/posts/mysql_%E5%9F%BA%E7%A1%80/:28:1","tags":["mysql"],"title":"mysql基础","uri":"/posts/mysql_%E5%9F%BA%E7%A1%80/"},{"categories":["笔记"],"content":"单列索引和组合索引 单列索引顾名思义就是只包含一个列。 组合索引包含多个列，联合索引又叫复合索引。对于复合索引:Mysql从左到右的使用索引中的字段，一个查询可以只使用索引中的一部份，但只能是最左侧部分。例如索引是key index (a,b,c). 可以支持a | a,b| a,b,c 3种组合进行查找，但不支持 b,c进行查找 .当最左侧字段是常量引用时，索引就十分有效。 ","date":"2019-03-01","objectID":"/posts/mysql_%E5%9F%BA%E7%A1%80/:28:2","tags":["mysql"],"title":"mysql基础","uri":"/posts/mysql_%E5%9F%BA%E7%A1%80/"},{"categories":["笔记"],"content":"全文索引 全文索引可以在char、varchar或者text类型上创建。 常用的全文检索模式有两种： 自然语言的全文索引 自然语言模式是MySQL 默认的全文检索模式。自然语言模式不能使用操作符，不能指定关键词必须出现或者必须不能出现等复杂查询。 默认情况下，或者使用 in natural language mode 修饰符时，match() 函数对文本集合执行自然语言搜索，上面的例子都是自然语言的全文索引。 自然语言搜索引擎将计算每一个文档对象和查询的相关度。这里，相关度是基于匹配的关键词的个数，以及关键词在文档中出现的次数。在整个索引中出现次数越少的词语，匹配时的相关度就越高。相反，非常常见的单词将不会被搜索，如果一个词语的在超过 50% 的记录中都出现了，那么自然语言的搜索将不会搜索这类词语。上面提到的，测试表中必须有 4 条以上的记录，就是这个原因。 这个机制也比较好理解，比如说，一个数据表存储的是一篇篇的文章，文章中的常见词、语气词等等，出现的肯定比较多，搜索这些词语就没什么意义了，需要搜索的是那些文章中有特殊意义的词，这样才能把文章区分开。 布尔全文索引 在布尔搜索中，我们可以在查询中自定义某个被搜索的词语的相关性，当编写一个布尔搜索查询时，可以通过一些前缀修饰符来定制搜索。 MySQL 内置的修饰符，上面查询最小搜索长度时，搜索结果 ft_boolean_syntax 变量的值就是内置的修饰符，下面简单解释几个，更多修饰符的作用可以查手册 必须包含该词 必须不包含该词 提高该词的相关性，查询的结果靠前 \u003c 降低该词的相关性，查询的结果靠后 (*)星号 通配符，只能接在词后面 对于上面提到的问题，可以使用布尔全文索引查询来解决，使用下面的命令，a、aa、aaa、aaaa 就都被查询出来了。 select * test where match(content) against(‘a*’ in boolean mode); MySQL 中的全文索引，有两个变量，最小搜索长度和最大搜索长度，对于长度小于最小搜索长度和大于最大搜索长度的词语，都不会被索引。通俗点就是说，想对一个词语使用全文索引搜索，那么这个词语的长度必须在以上两个变量的区间内。 最小搜索长度 MyISAM 引擎下默认是 4，InnoDB 引擎下是 3，也即，MySQL 的全文索引只会对长度大于等于 4 或者 3 的词语建立索引。 ","date":"2019-03-01","objectID":"/posts/mysql_%E5%9F%BA%E7%A1%80/:28:3","tags":["mysql"],"title":"mysql基础","uri":"/posts/mysql_%E5%9F%BA%E7%A1%80/"},{"categories":["笔记"],"content":"创建索引 创建表的时候，修改表的时候或者直接create index都可以创建索引。 一个列上可以创建多个索引 ","date":"2019-03-01","objectID":"/posts/mysql_%E5%9F%BA%E7%A1%80/:29:0","tags":["mysql"],"title":"mysql基础","uri":"/posts/mysql_%E5%9F%BA%E7%A1%80/"},{"categories":["笔记"],"content":"创建表时建索引 create table 表名 [字段名 类型] [unionque|fulltext|spatial] [index|key] [索引名] (字段名[length]) [asc|desc] 其中unique、fulltext、spatial分别代表唯一、全文、空间索引，不指定时为普通索引；index和key意义一样；索引名可以为空，此时索引名默认为列名；asc或desc代表升序或降序的索引。 **explain select * from user where id=30;**利用explain可以查看执行的时候是不是用到了索引。 ","date":"2019-03-01","objectID":"/posts/mysql_%E5%9F%BA%E7%A1%80/:29:1","tags":["mysql"],"title":"mysql基础","uri":"/posts/mysql_%E5%9F%BA%E7%A1%80/"},{"categories":["笔记"],"content":"修改表时增索引 alter table 表名 add [unionque|fulltext|spatial] [index|key] [索引名] (字段名[length]) [asc|desc] ","date":"2019-03-01","objectID":"/posts/mysql_%E5%9F%BA%E7%A1%80/:29:2","tags":["mysql"],"title":"mysql基础","uri":"/posts/mysql_%E5%9F%BA%E7%A1%80/"},{"categories":["笔记"],"content":"直接increate index create [unionque|fulltext|spatial] [index|key] [索引名] on 表名 (字段名[length]) [asc|desc] ","date":"2019-03-01","objectID":"/posts/mysql_%E5%9F%BA%E7%A1%80/:29:3","tags":["mysql"],"title":"mysql基础","uri":"/posts/mysql_%E5%9F%BA%E7%A1%80/"},{"categories":["笔记"],"content":"删除索引 drop index 索引名 on 表名 alter table 表名 drop index 索引名 ","date":"2019-03-01","objectID":"/posts/mysql_%E5%9F%BA%E7%A1%80/:30:0","tags":["mysql"],"title":"mysql基础","uri":"/posts/mysql_%E5%9F%BA%E7%A1%80/"},{"categories":["笔记"],"content":"索引涉及原则 索引并不是越多越好 避免经常更新的表进行索引，并且索引尽可能少。经常查询的表可以建索引。 数据量少的话不用建索引，不然适得其反。 如果列上相同的值很多，不适合建索引，比如性别列 频繁进行排序order by和分组group by的列上适合创建索引 尽量使用短索引，比如有的字段0-255，实际上前10个字符就能判断出唯一，可指定索引长度10 ","date":"2019-03-01","objectID":"/posts/mysql_%E5%9F%BA%E7%A1%80/:31:0","tags":["mysql"],"title":"mysql基础","uri":"/posts/mysql_%E5%9F%BA%E7%A1%80/"},{"categories":["笔记"],"content":"索引无用的情况 利用索引可以直接定位，免去了遍历，是最有效的优化查询方案。但是有些情况虽然字段带索引，是不起作用的： 利用like，而且%在第一个位置，索引无效。 一个索引可以有16个字段，只有查询条件是索引的第一个字段时才有效。 查询条件有or时，or前后条件都是索引才有效。 视图 视图（view）是一种虚拟存在的表，是一个逻辑表，本身并不包含数据。作为一个select语句保存在数据字典中的。 通过视图，可以展现基表的部分数据；视图数据来自定义视图的查询中使用的表，使用视图动态生成。 视图本身没有数据，数据的变化是修改的数据表。 不是所有的视图都可以做DML操作。 有下列内容之一，视图不能做DML操作： ①select子句中包含distinct ②select子句中包含组函数 ③select语句中包含group by子句 ④select语句中包含order by子句 ⑤select语句中包含union 、union all等集合运算符 ⑥where子句中包含相关子查询 ⑦from子句中包含多个表 ⑧如果视图中有计算列，则不能更新 ⑨如果基表中有某个具有非空约束的列未出现在视图定义中，则不能做insert操作 存储过程和函数 存储过程可以简单的看成一条或多条sql语句的集合。 ","date":"2019-03-01","objectID":"/posts/mysql_%E5%9F%BA%E7%A1%80/:32:0","tags":["mysql"],"title":"mysql基础","uri":"/posts/mysql_%E5%9F%BA%E7%A1%80/"},{"categories":["笔记"],"content":"创建存储过程 create procedure sp_name ([param]) [characteristics ...] routine_body 其中参数可以是 IN OUT INOUT三种类型 routine_body是sql代码的内容，可以用begin…end开始和结束。 示例： delimiter // create procedure pdtest() begin select * from user; end // delimiter //指定语句用//来结束，防止与sql语句冲突，定义完存储过程后，在回到原来的delimiter ; ","date":"2019-03-01","objectID":"/posts/mysql_%E5%9F%BA%E7%A1%80/:33:0","tags":["mysql"],"title":"mysql基础","uri":"/posts/mysql_%E5%9F%BA%E7%A1%80/"},{"categories":["笔记"],"content":"创建函数 create function func_name([param]) returns type [characteristic...] routine_body 参数和存储过程一样，也是那三种，默认是IN。 returns types是必须要的。不需要begin end 示例： delimiter // create function functest() returns varchar(10) return (select name from user where id=2); // ","date":"2019-03-01","objectID":"/posts/mysql_%E5%9F%BA%E7%A1%80/:34:0","tags":["mysql"],"title":"mysql基础","uri":"/posts/mysql_%E5%9F%BA%E7%A1%80/"},{"categories":["笔记"],"content":"变量及流程控制 我们可以声明变量、使用流程控制、光标等。 ","date":"2019-03-01","objectID":"/posts/mysql_%E5%9F%BA%E7%A1%80/:35:0","tags":["mysql"],"title":"mysql基础","uri":"/posts/mysql_%E5%9F%BA%E7%A1%80/"},{"categories":["笔记"],"content":"调用存储过程或函数 call pd_name() 调用存储过程 select func_name() 调用函数 ","date":"2019-03-01","objectID":"/posts/mysql_%E5%9F%BA%E7%A1%80/:36:0","tags":["mysql"],"title":"mysql基础","uri":"/posts/mysql_%E5%9F%BA%E7%A1%80/"},{"categories":["笔记"],"content":"查看存储过程和函数 show {procedure | function} status [like “pattern”] show create {procedure | function} name ","date":"2019-03-01","objectID":"/posts/mysql_%E5%9F%BA%E7%A1%80/:37:0","tags":["mysql"],"title":"mysql基础","uri":"/posts/mysql_%E5%9F%BA%E7%A1%80/"},{"categories":["笔记"],"content":"修改、删除存储过程和函数 alter {procedure | function} name… drop procedure name; drop function name; 触发器 触发器和存储过程类似，都是嵌入到mysql的一段程序。 触发器是由事件触发某个操作，包括insert、update、delete。 ","date":"2019-03-01","objectID":"/posts/mysql_%E5%9F%BA%E7%A1%80/:38:0","tags":["mysql"],"title":"mysql基础","uri":"/posts/mysql_%E5%9F%BA%E7%A1%80/"},{"categories":["笔记"],"content":"创建触发器 create trigger trigger_name trigger_time trigger_event on ta_name for each row trigger_stmt 当有多条sql语句时，用begin end create trigger trigger_name trigger_time trigger_event on ta_name for each row begin trigger_stmt... end 示例： create trigger tgtest after update on user for each row begin ...//此处小心插入死循环 end ","date":"2019-03-01","objectID":"/posts/mysql_%E5%9F%BA%E7%A1%80/:39:0","tags":["mysql"],"title":"mysql基础","uri":"/posts/mysql_%E5%9F%BA%E7%A1%80/"},{"categories":["笔记"],"content":"查看、删除触发器 show triggers; 直接从information_schema数据库triggers表中查，另外还有存储过程和函数表 drop trigger name 事务 MyISAM 不支持事务，InnoDB支持事务。 在innodb里面， 所有的活动都是运行在事务里面的。innodb默认autocommit=1的，意思就是MySQL会在每个语句执行的时候自动提交事务，当然是语句没有报错，如果报错了，那就会自动回滚rollback。 查看当前autocommit模式 show variables like 'autocommit'; ","date":"2019-03-01","objectID":"/posts/mysql_%E5%9F%BA%E7%A1%80/:39:1","tags":["mysql"],"title":"mysql基础","uri":"/posts/mysql_%E5%9F%BA%E7%A1%80/"},{"categories":["笔记"],"content":"设置事务级别 开启autocommit模式 SET [SESSION | GLOBAL] TRANSACTION ISOLATION LEVEL {READ UNCOMMITTED | READ COMMITTED | REPEATABLE READ | SERIALIZABLE} 这里session是指对当前连接上执行的事务设置默认事务级别。默认是对下一个（未开始）事务设置隔离级别 查看事务隔离级别 SELECT @@global.tx_isolation; SELECT @@session.tx_isolation; SELECT @@tx_isolation; ","date":"2019-03-01","objectID":"/posts/mysql_%E5%9F%BA%E7%A1%80/:40:0","tags":["mysql"],"title":"mysql基础","uri":"/posts/mysql_%E5%9F%BA%E7%A1%80/"},{"categories":["笔记"],"content":"各种读问题 未提交读（Read uncommitted）就是能读取到其他回话中未提交事务的修改，造成脏读问题。 脏读。就是一个事务读取到了另一个事务修改还未提交的数据变化，因为这个变化可能被恢复。 提交读(Read Committed) 只能读取到别人已经提提交的数据。避免了脏读的问题。但是在自己的事务中多次读取，数据可能不一致也就是不可重复读。 不可重复读。就是在同一事务中多次读一个数据，但是不一致，因为期间哟其他事务修改并提交了。 可重复读(Repeated Read) InnoDB默认级别，同一个事务中多次读取都是一致的，解决了不可重复读的问题。但是有幻读问题。 幻读。可重复读级别虽然外部的修改和插入不会影响本事务查看数据，但是可能影响本事务修改或插入。 串行读(Serializable) 现在好了，完全串行化的读，每次读都需要获得表级共享锁，读写相互都会阻塞。 ","date":"2019-03-01","objectID":"/posts/mysql_%E5%9F%BA%E7%A1%80/:41:0","tags":["mysql"],"title":"mysql基础","uri":"/posts/mysql_%E5%9F%BA%E7%A1%80/"},{"categories":["笔记"],"content":"事务使用 start transaction; ... ... commit;//rollback; 三大范式 第一范式:确保每列的原子性. 第一范式是最基本的范式。 数据库表中的字段都是单一属性的，不可再分。 只要是关系数据库都满足第一范式 如果每列(或者每个属性)都是不可再分的最小数据单元(也称为最小的原子单元),则满足第一范式. 例如:顾客表(姓名、编号、地址、……)其中\"地址\"列还可以细分为国家、省、市、区等。 第二范式(确保表中的每列都和主键相关). 如果一个关系满足第一范式,并且除了主键以外的其它列,都依赖于该主键,则满足第二范式. 例如:订单表(订单编号、产品编号、定购日期、价格、……)，“订单编号\"为主键，“产品编号\"和主键列没有直接的关系，即\"产品编号\"列不依赖于主键列，应删除该列。 第三范式(确保每列都和主键列直接相关,而不是间接相关). 如果一个关系满足第二范式,并且除了主键以外的其它列都不依赖于主键列,则满足第三范式. 为了理解第三范式，需要根据Armstrong公里之一定义传递依赖。假设A、B和C是关系R的三个属性，如果A-〉B且B-〉C，则从这些函数依赖中，可以得出A-〉C，如上所述，依赖A-〉C是传递依赖。 例如:订单表(订单编号，定购日期，顾客编号，顾客姓名，……)，初看该表没有问题，满足第二范式，每列都和主键列\"订单编号\"相关，再细看你会发现\"顾客姓名\"和\"顾客编号\"相关，“顾客编号\"和\"订单编号\"又相关，最后经过传递依赖，“顾客姓名\"也和\"订单编号\"相关。为了满足第三范式，应去掉\"顾客姓名\"列，放入客户表中。 案例 ","date":"2019-03-01","objectID":"/posts/mysql_%E5%9F%BA%E7%A1%80/:42:0","tags":["mysql"],"title":"mysql基础","uri":"/posts/mysql_%E5%9F%BA%E7%A1%80/"},{"categories":["笔记"],"content":"先分组取分组内的n条 查询男、女生前两名 select a.* from student a where (select count(distinct b.score) from student b where a.sex=b.sex and b.score\u003ea.score)\u003c2; 其中子查询的select取决于外面的查询，每次拿外面的一个数据带入到里面进行判断，如果比他分数大的人少于两个，那么不用说他在前两名，满足条件。 为啥要distinct？因为分数可能有重复！ 看比他大的在两个以内那就是前两名，注意条件中\u003c=和\u003c的区别 ","date":"2019-03-01","objectID":"/posts/mysql_%E5%9F%BA%E7%A1%80/:43:0","tags":["mysql"],"title":"mysql基础","uri":"/posts/mysql_%E5%9F%BA%E7%A1%80/"},{"categories":["笔记"],"content":"defer语句","date":"2019-01-17","objectID":"/posts/go_defer/","tags":["golang"],"title":"defer语句","uri":"/posts/go_defer/"},{"categories":["笔记"],"content":"defer语句 defer及defer函数的执行顺序分2步： 执行defer语句，计算函数的入参的值，并传递给函数，但不执行函数，而是将函数压入栈。 函数return语句后，或panic后，执行压入栈的函数，函数中变量的值，此时会被计算。 Each time a “defer” statement executes, the function value and parameters to the call are evaluated as usual and saved anew but the actual function is not invoked. defer语句的引用问题 注意： defer 后面常跟匿名函数，函数内用到外面的变量，属于闭包，变量是引用 func main() { fmt.Println(test()) //5 fmt.Println(test2()) //4 } func test()(x int){ x=2 defer func() { x=5 fmt.Println(x)//5 }() return 4 } func test2()(x int){ x=2 defer func(i int) { i=5 }(x) return 4 } defer与return 编译器将 defer 处理成两个函数调用，deferproc 定义一个延迟调用对象，然后在函数结束 前通过 deferreturn 完成最终调用。 大致表达为： step 1 : 在defer表达式的地方，会调用runtime.deferproc(size int32, fn *funcval)保存延时调用，注意这里保存了延时调用的参数 step 2 : 在return时，先将返回值保存起来 step 3 : 按FILO顺序调用runtime.deferreturn，即延时调用 step 4 : RET指令 func namedReturn() (r int) { defer func() { r++ fmt.Println(\"defer in namedReturn : r = \", r) }() return } func unnamedReturn() int { var r int defer func() { r++ fmt.Println(\"defer in unnamedReturn : r = \", r) }() return r } func main() { //1 fmt.Println(\"namedReturn : r = \", namedReturn()) //0 fmt.Println(\"unnamedReturn : r = \", unnamedReturn()) } 原因就是return会将返回值先保存起来，对于无名返回值来说，保存在一个临时对象中，defer是看不到这个临时对象的；而对于有名返回值来说，就保存在已命名的变量中。 匿名返回值是在return执行时被声明，有名返回值则是在函数声明的同时被声明，因此在defer语句中只能访问有名返回值，而不能直接访问匿名返回值； ","date":"2019-01-17","objectID":"/posts/go_defer/:0:0","tags":["golang"],"title":"defer语句","uri":"/posts/go_defer/"},{"categories":["笔记"],"content":"认证鉴权","date":"2018-12-17","objectID":"/posts/%E8%AE%A4%E8%AF%81%E9%89%B4%E6%9D%83/","tags":["项目"],"title":"认证鉴权","uri":"/posts/%E8%AE%A4%E8%AF%81%E9%89%B4%E6%9D%83/"},{"categories":["笔记"],"content":"认证鉴权 HTTP协议是无状态的和Connection: keep-alive的区别： 无状态是指协议对于事务处理没有记忆能力，服务器不知道客户端是什么状态。从另一方面讲，打开一个服务器上的网页和你之前打开这个服务器上的网页之间没有任何联系。 HTTP是一个无状态的面向连接的协议，无状态不代表HTTP不能保持TCP连接，更不能代表HTTP使用的是UDP协议（无连接）。 从HTTP/1.1起，默认都开启了Keep-Alive，保持连接特性，简单地说，当一个网页打开完成后，客户端和服务器之间用于传输HTTP数据的TCP连接不会关闭，如果客户端再次访问这个服务器上的网页，会继续使用这一条已经建立的连接。 Keep-Alive不会永久保持连接，它有一个保持时间，可以在不同的服务器软件（如Apache）中设定这个时间。 http协议本身是无状态的，所以服务端不能知道用户的状态，需要借助些技术手段来实现。 ","date":"2018-12-17","objectID":"/posts/%E8%AE%A4%E8%AF%81%E9%89%B4%E6%9D%83/:0:0","tags":["项目"],"title":"认证鉴权","uri":"/posts/%E8%AE%A4%E8%AF%81%E9%89%B4%E6%9D%83/"},{"categories":["笔记"],"content":"basic认证 basic认证是基于http协议。 访问受保护的资源，没有登录 服务端返回401，并响应头设置WWW-Authenticate：Basic realm=”localhost 浏览器弹出用户名密码框，输入后再次访问资源通过，后续的访问都会请求头带上用户名和密码，格式为Authorization：Basic base64(name:password) http.HandleFunc(\"/user\", func(writer http.ResponseWriter, request *http.Request) { authHeader := request.Header.Get(\"Authorization\") fmt.Println(authHeader) //Basic emhhbmdzYW46MTIz if authHeader == \"\" { //请求没有头信息，返回401 writer.Header().Add(\"WWW-Authenticate\", \"Basic realm=”localhost\") writer.WriteHeader(401) } else { //请求有头信息，base64解码，并校验用户名密码 authHeader = authHeader[6:] bytes, _ := base64.StdEncoding.DecodeString(authHeader) fmt.Println(string(bytes)) //zhangsan:123 nameAndPwd := strings.Split(string(bytes), \":\") if nameAndPwd[0] == \"zhangsan\" \u0026\u0026 nameAndPwd[1] == \"123\" { writer.Write([]byte(\"login success\")) writer.WriteHeader(200) } else { writer.Header().Add(\"WWW-Authenticate\", \"Basic realm=”localhost\") writer.WriteHeader(401) } } }) 这种basic认证是不安全的，因为都知道是base64编码，相当于铭文。 另外，登录后每次都带上密码，要想注销很麻烦，一般采用约定用户名密码表明要注销，比如：logout:logout cookie session cookie是http协议中的，在请求头和响应头中，但是session并不是http协议中的一部分。 如果不设置Expires的属性那么Cookie的存活时间就是在关闭浏览器的时候。默认cookies失效时间是直到关闭浏览器，cookies失效，也可以指定cookies时间。 服务端通过Set-Cookie头来指导浏览器保存cookie，指导过期时间、域名 cookie保存在浏览器是不安全的，通过嵌入脚本就能拿到cookie，比如xss（跨网站脚本），可以通过设置cookie的httponly属性 + https来避免。 session保存在服务端，但是容易被劫持，也就是通过监听网络拿到sessionid，有效的手段就是https；另外如果session生成规则比较简单，也是可以暴力尝试的。 ","date":"2018-12-17","objectID":"/posts/%E8%AE%A4%E8%AF%81%E9%89%B4%E6%9D%83/:1:0","tags":["项目"],"title":"认证鉴权","uri":"/posts/%E8%AE%A4%E8%AF%81%E9%89%B4%E6%9D%83/"},{"categories":["笔记"],"content":"jwt jwt全称是json web token，它是保存在客户端的。传统的session是保存在服务端，每次拿到sessionid后还要查询数据库等来获取用户信息并校验，服务端压力比较大。jwt通过把用户信息通过一定的格式并加密后返回给客户端，每次请求传给服务端（一般放在header中），服务端进行解密校验即可自动用户的登录态和数据，避免了查询数据库。 jwt有三部分：base64url(header) +\".\" +base64url(payload) +\".\"+ 加密(base64url(header) +\".\" +base64url(payload) +盐) header： { \"alg\": \"HS256\", \"typ\": \"JWT\" } 它指定了加密方法 Claims 或者叫payload，指定了用户的信息及jwt的其他信息，比如userId、过期时间等 { \"sub\": \"1234567890\", \"name\": \"John Doe\", \"admin\": true } golang中有jwt-go等框架，方便我们生成校验jwt 生成jwt type Person struct { Name string Age int } type MyCustomClaims struct { Person Person jwt.StandardClaims } func jwtTest() string { mySigningKey := []byte(\"AllYourBase\") // Create the Claims claims := MyCustomClaims{ Person: Person{ Name: \"zhang san\", Age: 12, }, StandardClaims: jwt.StandardClaims{ NotBefore: int64(time.Now().Unix()), ExpiresAt: time.Now().Unix() + 3, Issuer: \"test\", Id: \"12345\", }, } token := jwt.NewWithClaims(jwt.SigningMethodHS256, claims) ss, err := token.SignedString(mySigningKey) fmt.Printf(\"%v %v\\n\", ss, err) return ss } 校验jwt func jwtCheck(str string) { token, err := jwt.Parse(str, func(token *jwt.Token) (interface{}, error) { return []byte(\"AllYourBase\"), nil }) if token.Valid { fmt.Println(\"You look nice today\") } else if ve, ok := err.(*jwt.ValidationError); ok { if ve.Errors\u0026jwt.ValidationErrorMalformed != 0 { fmt.Println(\"That's not even a token\") } else if ve.Errors\u0026(jwt.ValidationErrorExpired|jwt.ValidationErrorNotValidYet) != 0 { // Token is either expired or not active yet fmt.Println(\"Timing is everything\") } else { fmt.Println(\"Couldn't handle this token:\", err) } } else { fmt.Println(\"Couldn't handle this token:\", err) } c, ok := token.Claims.(jwt.MapClaims) fmt.Println(c[\"Person\"], ok) } ","date":"2018-12-17","objectID":"/posts/%E8%AE%A4%E8%AF%81%E9%89%B4%E6%9D%83/:2:0","tags":["项目"],"title":"认证鉴权","uri":"/posts/%E8%AE%A4%E8%AF%81%E9%89%B4%E6%9D%83/"},{"categories":["笔记"],"content":"godoc","date":"2018-01-01","objectID":"/posts/go_doc/","tags":["golang"],"title":"godoc","uri":"/posts/go_doc/"},{"categories":["笔记"],"content":"生成文档 生成文档是很简单的，主要在package,方法等想要注释的地方写上注释即可。可以使用//或者/**/ 举例： // this is model comment package model import \"fmt\" // user struct comment type User struct { } func (u User) Run() { fmt.Println(\" user run\") } // return the maxvalue between a and b func GetMax(a, b int) int { if a \u003e b { return a } return b } 上面的代码中我们为包、结构体，函数都写了注释，这样即可。 查看文档 查看文档可以使用 godoc -http=:8080生产一个web服务，在localhost:8080即可查看文档 生产示例文档 除了以上的注释文档，我们还可以在文档中查看示例； 创建 example_test.go文件 为函数创建示例代码。创建ExampleGetMax函数，规则是Example+示例函数名 为方法创建示例代码。创建ExampleUser_Run函数，规则是Example + 结构体 + _方法名 在创建示例代码的时候还可以指定Output:标签 example_test.go func ExampleUser_Run() { user := User{} user.Run() } func ExampleGetMax() { max:=GetMax(1,2) fmt.Println(max) //Output: //2 } 文档如下： ","date":"2018-01-01","objectID":"/posts/go_doc/:0:0","tags":["golang"],"title":"godoc","uri":"/posts/go_doc/"},{"categories":["笔记"],"content":"go fmt","date":"2017-07-15","objectID":"/posts/go_fmt/","tags":["golang"],"title":"go fmt","uri":"/posts/go_fmt/"},{"categories":["笔记"],"content":"布尔 %t 单词true或false 整型 %b 表示为二进制 %c 该值对应的unicode码值 %d 表示为十进制 %o 表示为八进制 %q 该值对应的单引号括起来的go语法字符字面值，必要时会采用安全的转义表示 %x 表示为十六进制，使用a-f %X 表示为十六进制，使用A-F %U 表示为Unicode格式：U+1234，等价于\"U+%04X\" %g 根据情况选择 %e 或 %f 以产生更紧凑的（无末尾的0）输出 %G 根据情况选择 %E 或 %f 以产生更紧凑的（无末尾的0）输出 浮点数 %f: 默认宽度，默认精度 %9f 宽度9，默认精度 %.2f 默认宽度，精度2 %9.2f 宽度9，精度2 %9.f 宽度9，精度0 宽度是在％之后的值，如果没有指定，则使用该值的默认值，精度是跟在宽度之后的值，如果没有指定，也是使用要打印的值的默认精度．例如：％９.２f，宽度９，精度２,如果宽度不足9就补0。 对数值而言，宽度为该数值占用区域的最小宽度；精度为小数点之后的位数。但对于 %g/%G 而言，精度为所有数字的总数。例如，对于123.45，格式 %6.2f会打印123.45，而 %.4g 会打印123.5。%e 和 %f 的默认精度为6；但对于 %g 而言，它的默认精度为确定该值所必须的最小位数 ","date":"2017-07-15","objectID":"/posts/go_fmt/:0:0","tags":["golang"],"title":"go fmt","uri":"/posts/go_fmt/"},{"categories":["笔记"],"content":"golang 单例和once详解","date":"2017-05-15","objectID":"/posts/go_%E5%8D%95%E4%BE%8B%E5%8F%8Aonce%E8%AF%A6%E8%A7%A3/","tags":["golang"],"title":"golang 单例和once详解","uri":"/posts/go_%E5%8D%95%E4%BE%8B%E5%8F%8Aonce%E8%AF%A6%E8%A7%A3/"},{"categories":["笔记"],"content":"饿汉式 type singleton struct { } var instance = new(singleton) func GetInstance() *singleton{ return instance } 或者 type singleton struct { } var instance *singleton func init() { instance = new(singleton) } func GetInstance() *singleton{ return instance } 这两种方法都可以，第一种我们采用创建一个全局变量的方式来实现，第二种我们使用init包加载的时候创建实例，这里两个都可以，不过根据golang的执行顺序，全局变量的初始化函数会比包的init函数先执行，没有特别的差距。 懒汉式v1 type singleton struct { } var instance *singleton func GetInstance() *singleton { if instance == nil{ instance = new(singleton) } return instance } 在高并发的时候会有多个线程同时掉这个方法，那么都会检测instance为nil，这样就会导致创建多个对象,完全不可取。 懒汉式v2 type singleton struct { } var instance *singleton var lock sync.Mutex func GetInstance() *singleton { lock.Lock() defer lock.Unlock() if instance == nil{ instance = new(singleton) } return instance } 这里对整个方法进行了加锁，这种可以解决并发安全的问题，但是效率就会降下来，每一个对象创建时都是进行加锁解锁判断，这样就拖慢了速度。 懒汉式v3 type singleton struct { } var instance *singleton var lock sync.Mutex func GetInstance() *singleton { if instance == nil{ // 代码1 lock.Lock() // 代码2 instance = new(singleton) lock.Unlock() } return instance } 这种方法也是线程不安全的，虽然我们加了锁，多个线程同样会导致创建多个实例，执行完代码1后，执行代码2前，这小段时间别的协程可能已经调用完了GetInstance，已经生成一个实例。 完全不可取。 懒汉式v4 type singleton struct { } var instance *singleton var lock sync.Mutex func GetInstance() *singleton { if instance == nil{ lock.Lock() if instance == nil{ instance = new(singleton) } lock.Unlock() } return instance } 双重检查，安全 懒汉式v5 type singleton struct { } var instance *singleton var once sync.Once func GetInstance() *singleton { once.Do(func() { instance = new(singleton) }) return instance } 利用golang sync.once只执行一次的特性。 once内部原理 我们要自己实现这么一个功能如何做呢？ 定义一个status变量用来描述是否已经执行过了 使用sync.Mutex 或者sync.Atomic实现线程安全的获取status状态， 根据状态判断是否执行特定的函数。 看看官方是怎么实现的： type Once struct { done uint32 m Mutex } func (o *Once) Do(f func()) { if atomic.LoadUint32(\u0026o.done) == 0 { // 代码1 o.doSlow(f) } } func (o *Once) doSlow(f func()) { o.m.Lock() defer o.m.Unlock() if o.done == 0 { // 代码2 defer atomic.StoreUint32(\u0026o.done, 1) // 代码3 f() } } done就是标志位，表示是否已经执行过Do内部的函数。 代码1和代码2是双检查，不然代码1和加锁之前可能已经实例了。 问题1：代码1处为啥用原子操作，直接判断o.done不行吗？ 不用原子操作是可以的，因为后面代码2处还有一个check,但是通过atomic可以保证在o.done设置为1之后能看到这个设置的结果，避免总是落入到doSlow逻辑中。 问题2：代码2处为啥不用原子操作？ 已经在锁中间了，别人现在也改不了o.done，不用原子操作。 问题3：为啥要用锁？ 为了执行f()和修改done在一个锁范围内 问题4：这样优化为啥不行？ func (o *Once) Do(f func()) { if !atomic.CompareAndSwapUint32(\u0026o.done, 0, 1) { return } f() } 有人这样提议优化，A协程修改了done,f函数还没执行完，如果是用来单例的话，那么单例还没初始化完，但是B已经能读到done是1了，他自己用单例就会出问题了。 ","date":"2017-05-15","objectID":"/posts/go_%E5%8D%95%E4%BE%8B%E5%8F%8Aonce%E8%AF%A6%E8%A7%A3/:0:0","tags":["golang"],"title":"golang 单例和once详解","uri":"/posts/go_%E5%8D%95%E4%BE%8B%E5%8F%8Aonce%E8%AF%A6%E8%A7%A3/"},{"categories":["笔记"],"content":"sync包","date":"2017-05-15","objectID":"/posts/go_sync/","tags":["golang"],"title":"sync包","uri":"/posts/go_sync/"},{"categories":["笔记"],"content":"锁 ","date":"2017-05-15","objectID":"/posts/go_sync/:0:0","tags":["golang"],"title":"sync包","uri":"/posts/go_sync/"},{"categories":["笔记"],"content":"共享内存的并发问题 在很多语言中并发编程都有对一个内存进行读写的情况，比如下面的测试： func test() { count++ println(count) } var count int func main() { for i := 0; i \u003c 30; i++ { go test() } time.Sleep(3 * time.Second) } 很简单，我们开启30个协程对count变量进行增加写操作，而后又进行读取打印操作。 结果并不是顺序的打出1-30，因为协程的执行时机是随机的。但是有重复的结果打印出，那就是多协程并发读写的原因了。 java语言有锁，golang中也有，有了锁，就可以安全的读写了。 ","date":"2017-05-15","objectID":"/posts/go_sync/:1:0","tags":["golang"],"title":"sync包","uri":"/posts/go_sync/"},{"categories":["笔记"],"content":"互斥锁 sync.Mutex sync.Mutex是一个结构体，它是开箱即用的。 它实现了Locker接口： type Locker interface { Lock() Unlock() } 通过sync.Mutex来对上面示例进行改造： func test(mutex *sync.Mutex) { mutex.Lock() defer mutex.Unlock()//lock unlock要成对出现，直接使用defer更安全 count++ println(count) } var count int func main() { mutex := sync.Mutex{} for i := 0; i \u003c 30; i++ { go test(\u0026mutex)//这里要使用指针，若发生复制便不再是一个锁了 } time.Sleep(3 * time.Second) } 注意： 未lock直接unlock报错 ","date":"2017-05-15","objectID":"/posts/go_sync/:2:0","tags":["golang"],"title":"sync包","uri":"/posts/go_sync/"},{"categories":["笔记"],"content":"读写锁 sync.RWMutex sync.RWMutex也是一个结构体，内部嵌入sync.Mutex，也是开箱即用的。 读写锁顾名思义就是可以加只读锁和写锁 注意： RWMutex 是单写多读锁，该锁可以加多个读锁或者一个写锁 读锁占用的情况下会阻止写，不会阻止读，多个 goroutine 可以同时获取读锁 写锁会阻止其他 goroutine（无论读和写）进来，整个锁由该 goroutine 独占适用于读多写少的场景 总结起来就是读和写是互斥的，但是读之间是不互斥的。 读写锁使用的例子： func testRead(group *sync.WaitGroup, mutex *sync.RWMutex) { mutex.RLock() defer func() { mutex.RUnlock() group.Done() }() fmt.Println(\"读取开始\") time.Sleep(time.Second) println(count) time.Sleep(time.Second) fmt.Println(\"读取结束\") } func testWrite(group *sync.WaitGroup, mutex *sync.RWMutex) { mutex.Lock() defer func() { mutex.Unlock() group.Done() }() fmt.Println(\"写开始\") time.Sleep(time.Second) count++ time.Sleep(time.Second) fmt.Println(\"写结束\") } var count int func main() { mutex := sync.RWMutex{} group := sync.WaitGroup{} group.Add(10) for i := 0; i \u003c 5; i++ { go testRead(\u0026group, \u0026mutex) //这里要使用指针，若发生复制便不再是一个锁了 go testWrite(\u0026group, \u0026mutex) } group.Wait() } 打印结果： 可以看到，读操作是有重叠的，即他们之间不互斥，但是写操作之间，读写操作之间都没有交叉，他们是互斥的。 条件 sync.Cond 条件变量的作用并不是保证在同一时刻仅有一个线程访问某一个共享数据，而是在对应的共享数据的状态发生变化时，通知其他因此而被阻塞的线程。 条件变量要和配合锁来使用 unc testWait(group *sync.WaitGroup, mutex *sync.Cond) { mutex.L.Lock()//wait前必须先加锁 defer func() { mutex.L.Unlock() group.Done() }() fmt.Println(\"读取开始\") mutex.Wait()//内部先解锁，再加锁 time.Sleep(time.Second) fmt.Println(\"读取结束\") } func testSignal(group *sync.WaitGroup, mutex *sync.Cond) { fmt.Println(\"唤醒其他协程\") mutex.Signal()//唤醒一个 mutex.Broadcast()//唤醒所有等待该条件变量的协程 group.Done() } func main() { mutex := sync.Mutex{} cond := sync.NewCond(\u0026mutex) group := sync.WaitGroup{} group.Add(2) go testWait(\u0026group, cond) time.Sleep(time.Second) go testSignal(\u0026group, cond) group.Wait() } wait会阻塞当前goroutine，内部会先解锁，所在协程进入阻塞等待状态，等到有人发通知恢复，再加锁，所以在wait之前要先加锁，否则会报错。 sync.Once 这个相对就比较简单了，直接看示例： func testOnce() { once := sync.Once{} for i := 0; i \u003c 5; i++ { once.Do(func() { fmt.Println(\"once do\") }) } time.Sleep(3 * time.Second) } once.do中的函数只会执行一次，其实原理也很简单，once结构体中有个互斥锁及标签位 type Once struct { m Mutex done uint32 } 原子操作 通过对互斥锁的合理使用，我们可以使一个 goroutine 在执行临界区中的代码时，不被其他的goroutine 打扰。不过，虽然不会被打扰，但是它仍然可能会被中断（interruption）。 当协程正执行临界区的时候还是会可能失去CPU的。换句话说，互斥锁可以保证串行执行，但是不能保证原子执行。 在底层，由 CPU 提供芯片级别的支持，原子操作可以实现。这使得原子操作可以完全地消除竞态条件，并能够绝对地保证并发安全性。并且，它的执行速度要比其他的同步工具快得多，通常会高出好几个数量级。不过，它的缺点也很明显。 更具体地说，正是因为原子操作不能被中断，所以它需要足够简单，并且要求快速。 Go 语言的原子操作当然是基于 CPU 和操作系统的，所以它也只针对少数数据类型的值提供了原子 操作函数。这些函数都存在于标准库代码包sync/atomic中。 sync/atomic包中的函数可以做的原子操作有： 加法（add） 比较并交换（compare andswap，简称 CAS） 加载（load） 存储（store） 交换（swap） 这些函数针对的数据类型并不多。但是，对这些类型中的每一个，sync/atomic包都会有一套函 数给予支持。这些数据类型有：int32、int64、uint32、uint64、uintptr，以及unsafe包中的Pointer。 示例： func testAtmic() { atomic.AddInt32(\u0026count, 10) val := atomic.LoadInt32(\u0026count) fmt.Println(val) atomic.AddInt32(\u0026count, -1) fmt.Println(count) atomic.SwapInt32(\u0026count, 33) fmt.Println(count) atomic.StoreInt32(\u0026count, 11) fmt.Println(count) atomic.CompareAndSwapInt32(\u0026count, 11, 22) fmt.Println(count) } ","date":"2017-05-15","objectID":"/posts/go_sync/:3:0","tags":["golang"],"title":"sync包","uri":"/posts/go_sync/"},{"categories":["笔记"],"content":"原子操作任意类型 此外，sync/atomic包还提供了一个名为Value的类型，它可以被用来存储任意类型的值 func testAtmicValue() { value := atomic.Value{} u := user{ name: \"tom\", age: 12, } value.Store(u) u2 := value.Load().(user) fmt.Println(u2) } 对象缓存池 sync.Pool func testPool() { pool := sync.Pool{ New: func() interface{} {//生成新的对象 return 0 }, } value1 := pool.Get()//取 fmt.Println(value1) pool.Put(1)//存 value2 := pool.Get() fmt.Println(value2) } 对象缓存池和普通意义的缓存池作用一样，我们可以指定生成新对象的方式，get时，缓存池中没有数据便调用改函数生成。 缓存池并没有数量的限制，每次gc时都会清除缓存池中的数据，所以要看清使用场景。比如当做数据连接池显然就不合适。 安全的map sync.Map 普通的map并不是并发安全的，于是提供了一个并发安全的。 简单使用示例： func testMap(){ m := sync.Map{} u := user{ name: \"tom\", age: 12, } m.Store(\"zhangsan\",u) value, _ := m.Load(\"zhangsan\") u2 := value.(user) fmt.Println(u2) } sync.Map主要使用方法： m.Load() m.Store() m.Delete() m.LoadOrStore() m.Range() sync.map的操作方法key，value都是interface{}格式的，所以要自己进行类型判断，这显然有点麻烦。这里有个方案： type mapWrapper struct { M sync.Map keyType reflect.Type valType reflect.Type } func (m *mapWrapper) load(key interface{}) (value interface{}, ok bool) { if reflect.TypeOf(key) != m.keyType { return nil, false } return m.M.Load(key) } func (m *mapWrapper) store(key interface{}, value interface{}) (ok bool) { if reflect.TypeOf(key) != m.keyType { //校验key和value类型 return false } if reflect.TypeOf(value) != m.valType { return false } m.M.Store(key, value) return true } func testMap() { m := mapWrapper{ keyType: reflect.TypeOf(\"\"), valType: reflect.TypeOf(user{}), } u := user{ name: \"tom\", age: 12, } ok := m.store(1, 2) fmt.Println(ok) ok = m.store(\"zhangsan\", u) fmt.Println(ok) value, ok := m.load(\"zhangsan\") fmt.Println(value, ok) } 主要思路是对sync.Map封装，操作的时候校验类型，类型由初始化的时候传入。 注意： 和原来的map一样，虽然key是interface{}类型，但是一定是可以比较类型的，func,map等作为key就不别想了 总结 互斥锁是一个很有用的同步工具，它可以保证每一时刻进入临界区的 goroutine 只有一个。读写锁对共享资源的写操作和读操作则区别看待，并消除了读操作之间的互斥。 条件变量主要是用于协调想要访问共享资源的那些线程。 当共享资源的状态发生变化时，它可以被用来通知被互斥锁阻塞的线程，它既可以基于互斥锁，也可以基于读写锁。当然了，读写锁也是一种互斥锁，前者是对后者的扩展。 原子操作不光能保证并发安全(不使用锁)，还能保证临界区操作不被中断而原子执行，但是只支持少数的数据类型。当然也有atomic.Value支持任意类型 缓存池提供了一个api，可以提高重复使用率，但是要注意gc时会清空。 ","date":"2017-05-15","objectID":"/posts/go_sync/:4:0","tags":["golang"],"title":"sync包","uri":"/posts/go_sync/"},{"categories":["笔记"],"content":"排序算法","date":"2017-05-10","objectID":"/posts/sort/","tags":["算法"],"title":"排序算法","uri":"/posts/sort/"},{"categories":["笔记"],"content":"排序 我们通常所说的排序算法往往指的是内部排序算法，即数据记录在内存中进行排序。 排序算法大体可分为两种： 一种是比较排序，时间复杂度O(nlogn) ~ O(n^2)，主要有：冒泡排序，选择排序，插入排序，归并排序，堆排序，快速排序等。 另一种是非比较排序，时间复杂度可以达到O(n)，主要有：计数排序，基数排序，桶排序等。 稳定性 稳定性的简单形式化定义为：如果Ai = Aj，排序前Ai在Aj之前，排序后Ai还在Aj之前，则称这种排序算法是稳定的。通俗地讲就是保证排序前后两个相等的数的相对顺序不变。 需要注意的是，排序算法是否为稳定的是由具体算法决定的，不稳定的算法在某种条件下可以变为稳定的算法，而稳定的算法在某种条件下也可以变为不稳定的算法。 例如，对于冒泡排序，原本是稳定的排序算法，如果将记录交换的条件改成A[i] \u003e= A[i + 1]，则两个相等的记录就会交换位置，从而变成不稳定的排序算法。 ","date":"2017-05-10","objectID":"/posts/sort/:0:0","tags":["算法"],"title":"排序算法","uri":"/posts/sort/"},{"categories":["笔记"],"content":"冒泡排序 它重复地走访过要排序的元素，依次比较相邻两个元素，如果他们的顺序错误就把他们调换过来，直到没有元素再需要交换，排序完成。这个算法的名字由来是因为越小(或越大)的元素会经由交换慢慢“浮”到数列的顶端。 冒泡排序算法的运作如下： 比较相邻的元素，如果前一个比后一个大，就把它们两个调换位置。 对每一对相邻元素作同样的工作，从开始第一对到结尾的最后一对。这步做完后，最后的元素会是最大的数。 针对所有的元素重复以上的步骤，除了最后一个。 持续每次对越来越少的元素重复上面的步骤，直到没有任何一对数字需要比较。 // 分类 -------------- 内部比较排序 // 数据结构 ---------- 数组 // 最差时间复杂度 ---- O(n^2) // 最优时间复杂度 ---- 如果能在内部循环第一次运行时,使用一个旗标来表示有无需要交换的可能,可以把最优时间复杂度降低到O(n) // 平均时间复杂度 ---- O(n^2) // 所需辅助空间 ------ O(1) // 稳定性 ------------ 稳定 func bubbleSort(arr []int) { for i := 0; i \u003c len(arr); i++ { for j := 0; j \u003c len(arr)-1-i; j++ { if arr[j] \u003e arr[j+1] { arr[j], arr[j+1] = arr[j+1], arr[j] } } } } ","date":"2017-05-10","objectID":"/posts/sort/:1:0","tags":["算法"],"title":"排序算法","uri":"/posts/sort/"},{"categories":["笔记"],"content":"冒泡改进（鸡尾酒排序） 鸡尾酒排序，也叫定向冒泡排序，是冒泡排序的一种改进。此算法与冒泡排序的不同处在于从低到高然后从高到低，而冒泡排序则仅从低到高去比较序列里的每个元素。他可以得到比冒泡排序稍微好一点的效能。 func bubbleSort2(arr []int) { l, r := 0, len(arr)-1 for ; l \u003c r; { for i := l; i \u003c r; i++ { if arr[i] \u003e arr[i+1] { arr[i], arr[i+1] = arr[i+1], arr[i] } } r-- for i := r; i \u003e l; i-- { if arr[i] \u003c arr[i-1] { arr[i], arr[i-1] = arr[i-1], arr[i] } } l++ } } ","date":"2017-05-10","objectID":"/posts/sort/:2:0","tags":["算法"],"title":"排序算法","uri":"/posts/sort/"},{"categories":["笔记"],"content":"快速选择排序 它的工作原理很容易理解：初始时在序列中找到最小（大）元素，放到序列的起始位置作为已排序序列；然后，再从剩余未排序元素中继续寻找最小（大）元素，放到已排序序列的末尾。以此类推，直到所有元素均排序完毕。 // 分类 -------------- 内部比较排序 // 数据结构 ---------- 数组 // 最差时间复杂度 ---- O(n^2) // 最优时间复杂度 ---- O(n^2) // 平均时间复杂度 ---- O(n^2) // 所需辅助空间 ------ O(1) // 稳定性 ------------ 不稳定 func selectionSort(arr []int) { for i := 0; i \u003c len(arr); i++ { minValeIndex := i for j := i + 1; j \u003c len(arr); j++ { if arr[j] \u003c arr[minValeIndex] { minValeIndex = j } } arr[i], arr[minValeIndex] = arr[minValeIndex], arr[i] } } 选择排序是不稳定的排序算法，不稳定发生在最小元素与A[i]交换的时刻。 比如序列：{ 5, 8, 5, 2, 9 }，一次选择的最小元素是2，然后把2和第一个5进行交换，从而改变了两个元素5的相对次序。 ","date":"2017-05-10","objectID":"/posts/sort/:3:0","tags":["算法"],"title":"排序算法","uri":"/posts/sort/"},{"categories":["笔记"],"content":"插入排序 插入排序是一种简单直观的排序算法。它的工作原理非常类似于我们抓扑克牌。 对于未排序数据(右手抓到的牌)，在已排序序列(左手已经排好序的手牌)中从后向前扫描，找到相应位置并插入。 插入排序在实现上，通常采用in-place排序（即只需用到O(1)的额外空间的排序），因而在从后向前扫描过程中，需要反复把已排序元素逐步向后挪位，为最新元素提供插入空间。 具体算法描述如下： 从第一个元素开始，该元素可以认为已经被排序 取出下一个元素，在已经排序的元素序列中从后向前扫描 如果该元素（已排序）大于新元素，将该元素移到下一位置 重复步骤3，直到找到已排序的元素小于或者等于新元素的位置 将新元素插入到该位置后 重复步骤2~5 // 分类 ------------- 内部比较排序 // 数据结构 ---------- 数组 // 最差时间复杂度 ---- 最坏情况为输入序列是降序排列的,此时时间复杂度O(n^2) // 最优时间复杂度 ---- 最好情况为输入序列是升序排列的,此时时间复杂度O(n) // 平均时间复杂度 ---- O(n^2) // 所需辅助空间 ------ O(1) // 稳定性 ------------ 稳定 func insertSort(arr []int) { for i := 1; i \u003c len(arr); i++ { toInsertValue := arr[i] var j int for j = i - 1; j \u003e= 0; j-- { if arr[j] \u003c= toInsertValue { break } arr[j+1] = arr[j] } arr[j+1] = toInsertValue } } ","date":"2017-05-10","objectID":"/posts/sort/:4:0","tags":["算法"],"title":"排序算法","uri":"/posts/sort/"},{"categories":["笔记"],"content":"插入排序升级（二分查找插入） 对于插入排序，如果比较操作的代价比交换操作大的话，可以采用二分查找法来减少比较操作的次数，我们称为二分插入排序，换句话说：原来从右往左老老实实的找，现在通过二分查找来找到要插入的位置。 // 分类 -------------- 内部比较排序 // 数据结构 ---------- 数组 // 最差时间复杂度 ---- O(n^2) // 最优时间复杂度 ---- O(nlogn) // 平均时间复杂度 ---- O(n^2) // 所需辅助空间 ------ O(1) // 稳定性 ------------ 稳定 func insertSort2(arr []int) { for i := 1; i \u003c len(arr); i++ { toInsertValue := arr[i] l, r := 0, i-1 for ; l \u003c= r; { mid := l + (r-l)/2 if arr[mid] \u003c toInsertValue { l = mid + 1 } else { r = mid - 1 } } var j int for j = i - 1; j \u003e= l; j-- { arr[j+1] = arr[j] } arr[j+1] = toInsertValue } } ","date":"2017-05-10","objectID":"/posts/sort/:5:0","tags":["算法"],"title":"排序算法","uri":"/posts/sort/"},{"categories":["笔记"],"content":"插入排序升级（希尔排序） 回想一下直接插入排序过程，排序过程中，我们可以设置一条线，左边是排好序的，右边则是一个一个等待排序， 如果最小的那个值在最右边，那么排这个最小值的时候，需要将所有元素向右边移动一位。 是否能够减少这样的移位呢？ 我们不希望它是一步一步的移动，而是大步大步的移动。刚开始的时候步长比较大，逐渐缩小至1（这时就是普通的插入排序了）。最后还是插入排序，那快在哪里？意义何在？其实最后一遍已经基本有序了，几乎就是O(n)，别忘了，插入排序最适合的场景就是排基本有序的数据。 分组，步长incre初始化为len/2(当然也可以设为其他) 从incre处开始依次进行插入 和普通的插入排序一样，把插入的值一一和左边已经排序好的进行比较，只不过步长不再为1，而是incre 一遍排完，缩小步长incre=incre/2,循环进行步骤1-3，知道incre=1 func insertSort3(arr []int) { for incre := len(arr) / 2; incre \u003e= 1; incre = incre / 2 { for i := incre; i \u003c len(arr); i++ { toInsertValue := arr[i] var j int for j = i - incre; j \u003e= 0; j -= incre { if arr[j] \u003c= toInsertValue { break } arr[j+incre] = arr[j] } arr[j+incre] = toInsertValue } } } ","date":"2017-05-10","objectID":"/posts/sort/:6:0","tags":["算法"],"title":"排序算法","uri":"/posts/sort/"},{"categories":["笔记"],"content":"归并排序 使用分而治之的思想，先把数组分成两个子数组，把两个子数组分别排序后再整合成一个有序的数组。 把两个有序的数组合成一个有序的数组： 两个指针了l,r分别指向两个有序的子数组头 分别对比指针所对应的值，小的那个存到新数组，指针向后移一位，直到把任意一个子数组比空 把剩余的那个子数组元素追加到合成的新数组后面 我们可以利用递归，先把两个子数组排序，再合并 func guibingSort(arr []int, l, r int) { if l \u003e= r { return } mid := l + (r-l)/2 guibingSort(arr, l, mid) guibingSort(arr, mid+1, r) merge(arr, l, r) } func merge(arr []int, l, r int) { if l \u003e= r { return } len := r - l + 1 newArr := make([]int, len) mid := l + (r-l)/2 i, j := l, mid+1 index := 0 for ; i \u003c= mid \u0026\u0026 j \u003c= r; index++ { if arr[i] \u003c arr[j] { newArr[index] = arr[i] i++ } else { newArr[index] = arr[j] j++ } } if i \u003c= mid {//这里注意是\u003c= for ; i \u003c= mid; i++ { newArr[index] = arr[i] index++ } } if j \u003c= r {//这里注意是\u003c= for ; j \u003c= r; j++ { newArr[index] = arr[j] index++ } } for index := 0; l \u003c= r; l++ { arr[l] = newArr[index] index++ } } ","date":"2017-05-10","objectID":"/posts/sort/:7:0","tags":["算法"],"title":"排序算法","uri":"/posts/sort/"},{"categories":["笔记"],"content":"快速排序 快速排序使用分治策略(Divide and Conquer)，是冒泡排序的升级版，步骤为： 从序列中挑出一个元素，作为\"基准\"(pivot). 把所有比基准值小的元素放在基准前面，所有比基准值大的元素放在基准的后面（相同的数可以到任一边），这个称为分区(partition)操作。 对每个分区递归地进行步骤1~2，递归的结束条件是序列的大小是0或1，这时整体已经被排好序了。 这里实例采用挖坑法，把第一个元素拿出来作为基准保存起来，那么第一位置就是坑了，从右边开始找到比k小的，放入坑中，那么右边的位置就成了新的坑，再从左边开始找比k大的元素，放入右边的坑中，一遍下来，把k放到最后的坑中，那么就达到了左边比k都笑，右边的都比k大。 //先进行一遍排序，第一个做基准，左边的比基准小，右边的比基准大;进行一遍排序后对剩余的进行分组再进行重复 func QuickSort(arr []int, start, end int) { if start \u003e= end { return } l, r := start, end //[l,r]为待排序的，l左边和r右边的都是已经排好的 k := arr[l] //基准,那么左边第一个就是坑，任何比k小的都可以放进去 for ; r \u003e l; { //从右边往左，找到比基准小的，填到坑里，那么右边的就变成了新的坑，任何比k大的都可以放进去 for ; r \u003e l \u0026\u0026 arr[r] \u003e= k; r-- { } arr[l] = arr[r] //从左往右，找到比基准大的 for ; r \u003e l \u0026\u0026 arr[l] \u003c= k; l++ { } arr[r] = arr[l] } //当for循环结束，说明l和r相遇了，那么l=r处就是最后的坑，直接把k放进去即可，这样一遍下来，k的左边比k小，右边比k大 arr[l]=k //剩余的分两组，进行重复操作 QuickSort(arr, start, l-1) QuickSort(arr, l+1, end) } 快速排序是不稳定的排序算法，不稳定发生在基准元素与A[tail+1]交换的时刻。 比如序列：{ 1, 3, 4, 2, 8, 9, 8, 7, 5 }，基准元素是5，一次划分操作后5要和第一个8进行交换，从而改变了两个元素8的相对次序。 ","date":"2017-05-10","objectID":"/posts/sort/:8:0","tags":["算法"],"title":"排序算法","uri":"/posts/sort/"},{"categories":["笔记"],"content":"三路快排 三路快排是快速排序的升级版，解决了在一般快速排序中，重复的元素会放到下个分区间中再进行排序。适用于有大量重复元素的排序。 这里我们通过一次排序分为左边小于k的，中间等于k的，右边大于k的。然后再对两头的两个小区间再进行排序即可。 通过维持三个指针来控制[left, lt )小于主元(pivot)，[lt, i)等于主元，[i, gt]未知，(gt, right]大于主元。 一开始，lt指向主元的位置left，gt指向right，而i从left右边接下来的第一个索引开始遍历，每当遇到一个数，就判断它与主元之间的大小关系，有三种情况： 小于主元就把这个数与lt指向的数交换，然后lt,i都自增1，然后继续遍历 大于主元就把这个数与gt指向的数交换，gt自减1，此时i还得不能自增，因为它不知道gt用一个什么样的元素跟它交换，所以留到下一次循环判断交换过来的这个元素的去留 等于主元就不用跟谁进行交换，直接自增1就可以 func QuickSort3(arr []int, start, end int) { if start \u003e= end { return } l, r := start, end //[start,l)为小于基准，(r,end]大于基准。都是已经排好的 i := l + 1 //[l,i)等于基准，[i,r]等待排序 k := arr[l] for ; r \u003e l \u0026\u0026 i \u003c= end; { if arr[i]\u003ck{ arr[l],arr[i]=arr[i],arr[l] l++ i++ }else if arr[i]\u003ek{ arr[r],arr[i]=arr[i],arr[r] r-- }else { i++ } } QuickSort3(arr, start, l-1) QuickSort3(arr, r+1, end) } ","date":"2017-05-10","objectID":"/posts/sort/:8:1","tags":["算法"],"title":"排序算法","uri":"/posts/sort/"},{"categories":["笔记"],"content":"堆排序 堆排序是指利用堆这种数据结构所设计的一种选择排序算法。堆是一种近似完全二叉树的结构（通常堆是通过一维数组来实现的），并满足性质：以最大堆（也叫大根堆、大顶堆）为例，其中父结点的值总是大于它的孩子节点。 ","date":"2017-05-10","objectID":"/posts/sort/:9:0","tags":["算法"],"title":"排序算法","uri":"/posts/sort/"},{"categories":["笔记"],"content":"计数排序 计数排序不是基于比较的排序算法，其核心在于将输入的数据值转化为键存储在额外开辟的数组空间中。 作为一种线性时间复杂度的排序，计数排序要求输入的数据必须是有确定范围的整数。 比如:一个数组中公共3种值：1,2,3。我们只要统计出1个个数，2的个数，3的个数然后反向填充进数组。 算法描述 找出待排序的数组中最大和最小的元素； 统计数组中每个值为i的元素出现的次数，存入数组C的第i项； 对所有的计数累加（从C中的第一个元素开始，每一项和前一项相加）； 反向填充目标数组：将每个元素i放在新数组的第C(i)项，每放一个元素就将C(i)减去1 ","date":"2017-05-10","objectID":"/posts/sort/:10:0","tags":["算法"],"title":"排序算法","uri":"/posts/sort/"},{"categories":["笔记"],"content":"桶排序 桶排序是计数排序的升级版。它利用了函数的映射关系，高效与否的关键就在于这个映射函数的确定。桶排序 (Bucket sort)的工作的原理：假设输入数据服从均匀分布，将数据分到有限数量的桶里，每个桶再分别排序（有可能再使用别的排序算法或是以递归方式继续使用桶排序进行排）。 算法描述 设置一个定量的数组当作空桶； 遍历输入数据，并且把数据一个一个放到对应的桶里去； 对每个不是空的桶进行排序； 从不是空的桶里把排好序的数据拼接起来 ","date":"2017-05-10","objectID":"/posts/sort/:11:0","tags":["算法"],"title":"排序算法","uri":"/posts/sort/"},{"categories":["笔记"],"content":"算法选择 各种排序方法比较: 简单排序中直接插入最好，快速排序最快。 当文件为正序时，直接插入和冒泡均最佳。 若要求排序稳定，则可选用归并排序。 当记录的规模较大时，为避免耗费大量的时间去移动记录，可以用链表作为存储结构。譬如插入排序、归并排序、基数排序都易于在链表上实现，使之减少记录的移动次数。但有的排序方法，如快速排序和堆排序，在链表上却难于实现 ","date":"2017-05-10","objectID":"/posts/sort/:12:0","tags":["算法"],"title":"排序算法","uri":"/posts/sort/"},{"categories":["笔记"],"content":"标准库排序所选用算法 java Java系统提供的Arrays.sort函数。对于基础类型，底层使用快速排序。对于非基础类型，底层使用归并排序。请问是为什么？ 答：这是考虑到排序算法的稳定性。对于基础类型，相同值是无差别的，排序前后相同值的相对位置并不重要，所以选择更为高效的快速排序，尽管它是不稳定的排序算法；而对于非基础类型，排序前后相等实例的相对位置不宜改变，所以选择稳定的归并排序。 go go语言中对基本类型排序是sort.Sort(data Interface)方法，自定义类型用的是sort.Slice方法，他们是“快排”、“堆排序”和“希尔排序的组合使用。是不安全的，具体如下： 总体是快排把数组不断的分为子数组，但是子数组内使用的排序就可能会变了：当子数组是元素\u003c=12时用希尔排序。当元素个数大于12时又要通过n获取一个阈值（2*ceil(lg(n+1))），如果阈值==0则使用堆排序，否则继续使用快排。 要想对自定义的类型稳定的排序可以用sort.SliceStable方法。它用的是插入排序。 ","date":"2017-05-10","objectID":"/posts/sort/:12:1","tags":["算法"],"title":"排序算法","uri":"/posts/sort/"},{"categories":["笔记"],"content":"channel","date":"2017-05-05","objectID":"/posts/go_channel/","tags":["golang"],"title":"go channel","uri":"/posts/go_channel/"},{"categories":["笔记"],"content":"底层结构 type hchan struct { qcount uint // total data in the queue dataqsiz uint // size of the circular queue buf unsafe.Pointer // points to an array of dataqsiz elements elemsize uint16 closed uint32 elemtype *_type // element type sendx uint // send index recvx uint // receive index recvq waitq // list of recv waiters sendq waitq // list of send waiters // lock protects all fields in hchan, as well as several // fields in sudogs blocked on this channel. // // Do not change another G's status while holding this lock // (in particular, do not ready a G), as this can deadlock // with stack shrinking. lock mutex } type waitq struct { first *sudog last *sudog } qcount uint // 当前队列中剩余元素个数 dataqsiz uint // 环形队列长度，即缓冲区的大小，即 make（chan T，N），N. buf unsafe.Pointer // 环形队列指针 单向循环链表 elemsize uint16 // 每个元素的大小 closed uint32 // 表示当前通道是否处于关闭状态。创建通道后，该字段设置为 0，即通道打开; 通过调用 close 将其设置为 1，通道关闭。 elemtype *_type // 元素类型，用于数据传递过程中的赋值； sendx uint 和 recvx uint 是环形缓冲区的状态字段，它指示缓冲区的当前索引 - 支持数组，它可以从中发送数据和接收数据。 recvq waitq // 等待读消息的 goroutine 队列,双向链表 sendq waitq // 等待写消息的 goroutine 队列 lock mutex // 互斥锁，为每个读写操作锁定通道，因为发送和接收必须是互斥操作。 各个状态下操作结果 发送接收流程 向 channel 写数据的流程：如果等待接收队列 recvq 不为空，说明缓冲区中没有数据或者没有缓冲区，此时直接从 recvq 取出 G,并把数据写入，最后把该 G 唤醒，结束发送过程；如果缓冲区中有空余位置，将数据写入缓冲区，结束发送过程；如果缓冲区中没有空余位置，将待发送数据写入 G，将当前 G 加入 sendq，进入睡眠，等待被读 goroutine 唤醒； 向 channel 读数据的流程：如果等待发送队列 sendq 不为空，且没有缓冲区，直接从 sendq 中取出 G，把 G 中数据读出，最后把 G 唤醒，结束读取过程；如果等待发送队列 sendq 不为空，此时说明缓冲区已满，从缓冲区中首部读出数据，把 G 中数据写入缓冲区尾部，把 G 唤醒，结束读取过程；如果缓冲区中有数据，则从缓冲区取出数据，结束读取过程；将当前 goroutine 加入 recvq，进入睡眠，等待被写 goroutine 唤醒； 其他 关闭不再需要使用的 channel 并不是必须的。跟其他资源比如打开的文件、socket 连接不一样，这类资源使用完后不关闭后会造成句柄泄露，channel 使用完后不关闭也没有关系，channel 没有被任何协程用到后最终会被 GC 回收。关闭 channel 一般是用来通知其他协程某个任务已经完成了。golang 也没有直接提供判断 channel 是否已经关闭的接口。 ","date":"2017-05-05","objectID":"/posts/go_channel/:0:0","tags":["golang"],"title":"go channel","uri":"/posts/go_channel/"},{"categories":["笔记"],"content":"go map","date":"2017-05-05","objectID":"/posts/go_map/","tags":["golang"],"title":"go map","uri":"/posts/go_map/"},{"categories":["笔记"],"content":"hash冲突 哈希查找表一般会存在“碰撞”的问题，就是说不同的 key 被哈希到了同一个 bucket。一般有两种应对方法：链表法和开放地址法。链表法将一个 bucket 实现成一个链表，落在同一个 bucket 中的 key 都会插入这个链表。开放地址法则是碰撞发生后，通过一定的规律，在数组的后面挑选“空位”，用来放置新的 key。 go使用的是链表法来解决hash冲突。 map内存模型 // A header for a Go map. type hmap struct { // 元素个数，调用 len(map) 时，直接返回此值 count int flags uint8 // buckets 的对数 log_2 B uint8 // overflow 的 bucket 近似数 noverflow uint16 // 计算 key 的哈希的时候会传入哈希函数 hash0 uint32 // 指向 buckets 数组，大小为 2^B // 如果元素个数为0，就为 nil buckets unsafe.Pointer // 扩容的时候，buckets 长度会是 oldbuckets 的两倍 oldbuckets unsafe.Pointer // 指示扩容进度，小于此地址的 buckets 迁移完成 nevacuate uintptr extra *mapextra // optional fields } B就是buckets数组的长度的对数，即有2^B个桶。 bucket桶的运行时数据结构为： type bmap struct { topbits [8]uint8 keys [8]keytype values [8]valuetype pad uintptr overflow uintptr } bmap 就是我们常说的“桶”，桶里面会最多装 8 个 key，这些 key 之所以会落入同一个桶，是因为它们经过哈希计算后，哈希结果是“一类”的。在桶内，又会根据 key 计算出来的 hash 值的高 8 位来决定 key 到底落入桶内的哪个位置（一个桶内最多有8个位置）。 我们可以看到桶里面有个数组是存放各key的hash值的高8位，key是放一起，value又是放一起。 key和value为什么要分开？ 源码里说明这样的好处是在某些情况下可以省略掉 padding 字段，节省内存空间。 map[int64]int8 如果按照 key/value/key/value/… 这样的模式存储，那在每一个 key/value 对之后都要额外 padding 7 个字节；而将所有的 key，value 分别绑定到一起，这种形式 key/key/…/value/value/…，则只需要在最后添加 padding。 使用map 使用map很简单，利用make内建函数，通过汇编语言可以看到，实际上底层调用的是 makemap 函数，主要做的工作就是初始化 hmap 结构体的各种字段，例如计算 B 的大小，设置哈希种子 hash0 等等。 注意，这个函数返回的结果：*hmap，它是一个指针，所以map看起来像是引用。 key定位过程 key 经过哈希计算后得到哈希值，共 64 个 bit 位（64位机，32位机就不讨论了，现在主流都是64位机），计算它到底要落在哪个桶时，只会用到最后 B 个 bit 位。还记得前面提到过的 B 吗？如果 B = 5，那么桶的数量，也就是 buckets 数组的长度是 2^5 = 32。 例如，现在有一个 key 经过哈希函数计算后，得到的哈希结果是： 10010111 | 000011110110110010001111001010100010010110010101010 │ 01010 用最后的 5 个 bit 位，也就是 01010，值为 10，也就是 10 号桶。这个操作实际上就是取余操作，但是取余开销太大，所以代码实现上用的位操作代替。 再用哈希值的高 8 位，找到此 key 在 bucket 中的位置，这是在寻找已有的 key。最开始桶内还没有 key，新加入的 key 会找到第一个空位，放入。 当两个不同的 key 落在同一个桶中，也就是发生了哈希冲突。冲突的解决手段是用链表法：在 bucket 中，从前往后找到第一个空位。这样，在查找某个 key 时，先找到对应的桶，再去遍历 bucket 中的 key。 如果在 bucket 中没找到，并且 overflow 不为空，还要继续去 overflow bucket 中寻找，直到找到或是所有的 key 槽位都找遍了，包括所有的 overflow bucket。 总结：对key进行hash运算取到值，低位也是就hash的右边（具体几位看B）来确定是在哪个桶中，高位也就是hash的左边8位来确定key在桶中的位置。 map的初始化 根据传入的 bucket 类型，获取其类型能够申请的最大容量大小。并对其长度 make(map[k]v, hint) 进行边界值检验 初始化 hmap 初始化哈希因子 根据传入的 hint，计算一个可以放下 hint 个元素的桶 B 的最小值 分配并初始化 hash table。如果 B 为 0 将在后续懒惰分配桶，大于 0 则会马上进行分配 返回初始化完毕的 hmap 当hint\u003c8时，最少一个bucket就可以了，否则，至少需要两个bucket，就需要立刻分配hash table。 扩容 在向 map 插入新 key 的时候，会进行条件检测，符合条件就会触发扩容。 扩容的方式 溢出桶太多，相同容量扩容 达到加载因子，2倍容量扩容 除了满足两个条件之一外，还要满足“不在扩容中”。 if (不是正在扩容 \u0026\u0026 (元素个数/bucket数超过某个值 || 太多overflow bucket)) { 进行扩容 } 啥意思呢？第一种出现的情况是：因为map不断的put和delete，出现了很多空格，这些空格会导致bmap很长，但是中间有很多空的地方，扫描时间变长。所以第一种扩容实际是一种整理，将数据整理到前面一起。第二种呢：就是真的不够用了，扩容两倍。 ","date":"2017-05-05","objectID":"/posts/go_map/:0:0","tags":["golang"],"title":"go map","uri":"/posts/go_map/"},{"categories":["笔记"],"content":"2倍扩容 理想中每个bucket里面只放一个元素，这样最快，但是空间太大。go采用链表解决冲突，但是，如果所有的key都在一个bucket里面，那就退化成了链表，因此需要衡量。 装载因子就是衡量标准， loadFactor := count / (2^B) 当loadFactor为6.5的时候就要扩容。 ","date":"2017-05-05","objectID":"/posts/go_map/:1:0","tags":["golang"],"title":"go map","uri":"/posts/go_map/"},{"categories":["笔记"],"content":"相同容量扩容 所谓的相同容量扩容，说白了就是不增加bucket的数量，只是整理现在的数据分布。 相同容量扩容的原因是overflow 的 bucket 数量过多： 当 B 小于 15，如果 overflow 的 bucket 数量超过 2^B ；当 B \u003e= 15，如果 overflow 的 bucket 数量超过 2^15 。 其实就是map元素本身不多（达不到加载因子的条件），但是有的bucket的溢出桶overflow太多，这样就造成效率低下。 这样的场景好理解：先添加一些元素，对前面的bucket元素删除，这样就造成大量的bucket不满，整理后可以提高效率。 ","date":"2017-05-05","objectID":"/posts/go_map/:2:0","tags":["golang"],"title":"go map","uri":"/posts/go_map/"},{"categories":["笔记"],"content":"扩容流程 ","date":"2017-05-05","objectID":"/posts/go_map/:3:0","tags":["golang"],"title":"go map","uri":"/posts/go_map/"},{"categories":["笔记"],"content":"等量扩容流程 其实元素没那么多，但是 overflow bucket 数特别多，说明很多 bucket 都没装满。解决办法就是开辟一个新 bucket 空间，将老 bucket 中的元素移动到新 bucket，使得同一个 bucket 中的 key 排列地更紧密。 ","date":"2017-05-05","objectID":"/posts/go_map/:3:1","tags":["golang"],"title":"go map","uri":"/posts/go_map/"},{"categories":["笔记"],"content":"2倍的扩容流程 元素太多，而 bucket 数量太少，很简单：将 B 加 1，bucket 最大数量（2^B）直接变成原来 bucket 数量的 2 倍。于是，就有新老 bucket 了。注意，这时候元素都在老 bucket 里，还没迁移到新的 bucket 来。而且，新 bucket 只是最大数量变为原来最大数量（2^B）的 2 倍（2^B * 2）。 2倍扩容流程，原来的一个bucket会裂变成两个bucket，理由很简单，多看了一位，该位0或1。 不管哪种方式，确定扩容的数量后，扩容本身是慢慢的过程。真正搬迁 buckets 的动作在 growWork() 函数中，而调用 growWork() 函数的动作是在 mapassign 和 mapdelete 函数中。**也就是插入或修改、删除 key 的时候，都会尝试进行搬迁 buckets 的工作。**先检查 oldbuckets 是否搬迁完毕，具体来说就是检查 oldbuckets 是否为 nil。 put hash表如果正在扩容，并且这次要操作的bucket还没搬到新hash表中，那么先进行搬迁（扩容细节下面细说）。 在buck中寻找key，同时记录下第一个空位置，如果找不到，那么就在空位置中插入数据；如果找到了，那么就更新对应的value； 找不到key就看下需不需要扩容，需要扩容并且没有正在扩容，那么就进行扩容，然后回到第一步。 找不到key，不需要扩容，但是没有空slot，那么就分配一个overflow bucket挂在链表结尾，用新bucket的第一个slot放存放数据。 向map插入数据，第一步还是先找bucket，对key取hash，低位找桶，再遍历桶中的数据，如果已经满了就新增一个溢出桶挂上去，如果不满就插入即可。 有一个细节注意，只有进行完了这个搬迁操作后，我们才能放心地在新 bucket 里定位 key 要安置的地址，再进行之后的操作。 找到桶后，应该先看下该桶对应的原始桶是否已经迁移完毕，如果还没迁移完毕，就应该先迁移，然后插入之前再看是否需要扩容（不用担心迁移过程中又要扩容，前面有条件），不需要的话再插。 get 先定位出bucket，如果正在扩容，并且这个bucket还没搬到新的hash表中，那么就从老的hash表中查找。 在bucket中进行顺序查找，使用高八位进行快速过滤，高八位相等，再比较key是否相等，找到就返回value。如果当前bucket找不到，就往下找overflow bucket，都没有就返回零值。 这里我们可以看到，访问的时候，并不进行扩容的数据搬迁。 delete 如果正在扩容，并且操作的bucket还没搬迁完，那么搬迁bucket。 找出对应的key，如果key、value是包含指针的那么会清理指针指向的内存，否则不会回收内存。 其他 bucket中key为何不直接和value放一起？ 之所以把所有k1k2放一起而不是k1v1是因为key和value的数据类型内存大小可能差距很大，比如map[int64]int8，考虑到字节对齐，kv存在一起会浪费很多空间。 map遍历为何无序？ map里的数据如果不进行操作，每次遍历应该是一样的，但是扩容以后就会变化。为了统一记忆，所有遍历都是无序。 实现方案就是每次遍历随机指定bucket和bucket中的key offset，这样遍历的位置就是随机的了。 map是否线程安全？ 不是的，对同一map进行写，会抛异常。 map的key限制 map的key不能是切片、map、函数。可以为interface{},但是运行时还是不能放这三种；key可以为数组，同样数组元素也不能为这三种。总之，key一定可以是“可比较”类型即可以使用==判断，nil==nil是不合法的，所以map不支持key为nil。另外 map的key占用空间越小，hash的速度越快，操作起来也是更快，尽量别用自定义类型 历史版本 在Go 1.6之前， 内置的map类型是部分goroutine安全的，并发的读没有问题，并发的写可能有问题。自go 1.6之后，并发地读写map会报错。牵涉到并发，应该用sync.map map桶的数量是2^N，为何一定要是2的指数次幂？ 在定位桶即tab的index时，一般是取余，hashCode % length,但是取余是复杂的操作，当length为2^N时，hashCode % length == hashCode \u0026 (length - 1)，这样就转为了更快的与运算。 总结 map的赋值（增和改）会造成扩容。 map扩容和迁移是分开的，迁移是渐进的，map的增，删，改都会进行迁移操作，查找并不能进行数据的搬迁。 ","date":"2017-05-05","objectID":"/posts/go_map/:3:2","tags":["golang"],"title":"go map","uri":"/posts/go_map/"},{"categories":["笔记"],"content":"golang slice","date":"2017-05-05","objectID":"/posts/go_slice/","tags":["golang"],"title":"golang slice","uri":"/posts/go_slice/"},{"categories":["笔记"],"content":"slice slice是一个数组某个部分的引用，非协程安全的。 ","date":"2017-05-05","objectID":"/posts/go_slice/:0:0","tags":["golang"],"title":"golang slice","uri":"/posts/go_slice/"},{"categories":["笔记"],"content":"创建 通过数组创建 切片的初始化格式是：var slice1 []type = arr1[start:end]，start和end都是角标，含头不含尾 s:=arr[0:3] s:=arr[:3] s:=arr[0:]都是合法的 make声明 make([]type,len,cap)cap为非必填 ","date":"2017-05-05","objectID":"/posts/go_slice/:1:0","tags":["golang"],"title":"golang slice","uri":"/posts/go_slice/"},{"categories":["笔记"],"content":"数据结构 type slice struct { array unsafe.Pointer len int cap int } 其中array就是指向数组的指针 从Go1.2开始slice支持了三个参数的slice，之前我们一直采用这种方式在slice或者array基础上来获取一个slice var array [10]int slice := array[2:4] 这个例子里面slice的容量是8，新版本里面可以指定这个容量 slice = array[2:4:7] 上面这个的容量就是7-2，即5。这样这个产生的新的slice就没办法访问最后的三个元素 ","date":"2017-05-05","objectID":"/posts/go_slice/:2:0","tags":["golang"],"title":"golang slice","uri":"/posts/go_slice/"},{"categories":["笔记"],"content":"扩容 先看几个示例： s := make([]int, 2) s2 := make([]int, 1) fmt.Println(len(s), cap(s), s) s = append(s, s2...) fmt.Println(len(s), cap(s), s) 打印： 2 2 [0 0] 3 4 [0 0 0] s := make([]int, 2) s2 := make([]int, 3) fmt.Println(len(s), cap(s), s) s = append(s, s2...) fmt.Println(len(s), cap(s), s) 打印： 2 2 [0 0] 5 6 [0 0 0 0 0] s := make([]int, 1024) s2 := make([]int, 3) fmt.Println(len(s), cap(s)) s = append(s, s2...) fmt.Println(len(s), cap(s)) 打印： 1024 1024 1027 1280 扩容策略 If 当前长度\u003c1024{ If 当追加后的len \u003c 当前的2倍{ 直接翻倍 }else{ 新的cap=追加后的长度 } }else{ 每次扩容为当前1.25倍，循环，直到装下所有元素 } 计算出了新容量之后，还没有完，出于内存的高效利用考虑，还要进行内存对齐。通过内存对齐后才能知道最终的cap值。 ","date":"2017-05-05","objectID":"/posts/go_slice/:3:0","tags":["golang"],"title":"golang slice","uri":"/posts/go_slice/"},{"categories":["笔记"],"content":"slice删除 arr[low:high] For arrays or strings, the indices are in range if 0 \u003c= low \u003c= high \u003c= len(a), otherwise they are out of range. For slices, the upper index bound is the slice capacity cap(a) rather than the length. 补充 slice的最大cap是多少？ 源码中slice的cap 为int类型，int和平台有关。其次cap也和能申请的内存大小有关，简单来说max = maxmemory / element.size slice没有缩容，必须自己注意内存泄漏 小心slice用的是同一个数组的坑 func main() { s := []int{1, 2, 3, 4, 5} s2 := append(s[0:1], 6) fmt.Printf(\"%p %v %d \\n\", s, s, cap(s))// 0xc04200a240 [1 6 3 4 5] 5 fmt.Printf(\"%p %v %d\", s2, s2, cap(s)) // 0xc04200a240 [1 6] 5 } ","date":"2017-05-05","objectID":"/posts/go_slice/:4:0","tags":["golang"],"title":"golang slice","uri":"/posts/go_slice/"},{"categories":["笔记"],"content":"range语句","date":"2017-05-05","objectID":"/posts/go_range/","tags":["golang"],"title":"range语句","uri":"/posts/go_range/"},{"categories":["笔记"],"content":"range slice func main() { s := []int{1, 2, 3} for i := 0; i \u003c len(s); i++ { if i == 1 { s = append(s, 4) } fmt.Println(i, s[i]) } } 上面的代码，遍历中新增元素是没有问题的。 但是下面的range遍历，结果是不一样的： func main() { s := []int{1, 2, 3} fmt.Println(s) //[1 2 3] for i, v := range s { if i == 0 { s = append(s, 4) } fmt.Println(s) //[1 2 3 4] fmt.Println(i, v) } fmt.Println(s) //[1 2 3 4] } 这里在range中增加元素，并没有改变range的次数。 其实这里只是一个语法糖。 对于切片的for…range底层源码是这样的： // for_temp := range // len_temp := len(for_temp) // for index_temp = 0; index_temp \u003c len_temp; index_temp++ { // value_temp = for_temp[index_temp] // index = index_temp // value = value_temp // original body // } 可以看到，在遍历之前，就取到了切片的长度。后面增删元素，并不影响遍历的次数。 仅仅不会改变遍历的次数，但是切片里的数据是会改动的，所以不要在遍历的时候删除数据。 range map // Lower a for range over a map. // The loop we generate: // var hiter map_iteration_struct // for mapiterinit(type, range, \u0026hiter); hiter.key != nil; mapiternext(\u0026hiter) { // index_temp = *hiter.key // value_temp = *hiter.val // index = index_temp // value = value_temp // original body // } func mtest() { m := make(map[int]int) m[0] = 1 m[1] = 2 var count int for k, v := range m { fmt.Println(k, v) count++ if count == 1 { m[3] = 3 fmt.Println(\"add\") } } } 输出： 0 1 add 1 2 3 3 可以看出，range过程中增加元素是会改变range次数的。实验证明，删除元素也有同样的效果。 range channel // Lower a for range over a channel. // The loop we generate: // for { // index_temp, ok_temp = \u003c-range // if !ok_temp { // break // } // index = index_temp // original body // } range array // Lower a for range over an array. // The loop we generate: // len_temp := len(range) // range_temp := range // for index_temp = 0; index_temp \u003c len_temp; index_temp++ { // value_temp = range_temp[index_temp] // index = index_temp // value = value_temp // original body // } 可以看到输入值和slice类似，都是提前取到长度。 range string // Lower a for range over a string. // The loop we generate: // len_temp := len(range) // var next_index_temp int // for index_temp = 0; index_temp \u003c len_temp; index_temp = next_index_temp { // value_temp = rune(range[index_temp]) // if value_temp \u003c utf8.RuneSelf { // next_index_temp = index_temp + 1 // } else { // value_temp, next_index_temp = decoderune(range, index_temp) // } // index = index_temp // value = value_temp // // original body // } func stringtest() { s := \"abcd\" var count int for _, v := range s { fmt.Printf(\"%c\\n\", v) count++ if count == 1 { s = s + \"ss\" fmt.Println(\"add\") } } fmt.Println(s) } 验证的结果为range过程中对字符串的追加及截取，都不影响range。 ","date":"2017-05-05","objectID":"/posts/go_range/:0:0","tags":["golang"],"title":"range语句","uri":"/posts/go_range/"},{"categories":["笔记"],"content":"Redis学习-redis底层数据结构","date":"2017-04-16","objectID":"/posts/redis_%E5%BA%95%E5%B1%82%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/","tags":["redis"],"title":"Redis学习-redis底层数据结构","uri":"/posts/redis_%E5%BA%95%E5%B1%82%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"},{"categories":["笔记"],"content":"动态字符串 redis是C语言编写，但是字符串并不是简单地的C字符串。它是SDS（simple dynamic string）。 /* * 保存字符串对象的结构 */ struct sdshdr { // buf 中已占用空间的长度 int len; // buf 中剩余可用空间的长度 int free; // 数据空间 char buf[]; }; 1、len 变量，用于记录buf 中已经使用的空间长度（这里指出Redis 的长度为5） 2、free 变量，用于记录buf 中还空余的空间（初次分配空间，一般没有空余，在对字符串修改的时候，会有剩余空间出现） 3、buf 字符数组，用于记录我们的字符串（记录Redis） ","date":"2017-04-16","objectID":"/posts/redis_%E5%BA%95%E5%B1%82%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/:0:0","tags":["redis"],"title":"Redis学习-redis底层数据结构","uri":"/posts/redis_%E5%BA%95%E5%B1%82%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"},{"categories":["笔记"],"content":"SDS 与 C 字符串的区别 ","date":"2017-04-16","objectID":"/posts/redis_%E5%BA%95%E5%B1%82%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/:1:0","tags":["redis"],"title":"Redis学习-redis底层数据结构","uri":"/posts/redis_%E5%BA%95%E5%B1%82%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"},{"categories":["笔记"],"content":"获取字符串长度（SDS O（1）/C 字符串 O(n)） 传统的C 字符串 使用长度为N+1 的字符串数组来表示长度为N 的字符串，所以为了获取一个长度为C字符串的长度，必须遍历整个字符串。 和C 字符串不同，SDS 的数据结构中，有专门用于保存字符串长度的变量，我们可以通过获取len 属性的值，直接知道字符串长度。 ","date":"2017-04-16","objectID":"/posts/redis_%E5%BA%95%E5%B1%82%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/:1:1","tags":["redis"],"title":"Redis学习-redis底层数据结构","uri":"/posts/redis_%E5%BA%95%E5%B1%82%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"},{"categories":["笔记"],"content":"杜绝缓冲区溢出 C 字符串 不记录字符串长度，除了获取的时候复杂度高以外，还容易导致缓冲区溢出。 假设程序中有两个在内存中紧邻着的 字符串 s1 和 s2，其中s1 保存了字符串“redis”，二s2 则保存了字符串“MongoDb” 如果我们现在将s1 的内容修改为redis cluster，但是又忘了重新为s1 分配足够的空间，这时候就会出现以下问题： 原本s2 中的内容已经被S1的内容给占领了，s2 现在为 cluster，而不是“Mongodb”。 Redis 中SDS 的空间分配策略完全杜绝了发生缓冲区溢出的可能性： 当我们需要对一个SDS 进行修改的时候，redis 会在执行拼接操作之前，预先检查给定SDS 空间是否足够，如果不够，会先拓展SDS 的空间，然后再执行拼接操作。 ","date":"2017-04-16","objectID":"/posts/redis_%E5%BA%95%E5%B1%82%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/:1:2","tags":["redis"],"title":"Redis学习-redis底层数据结构","uri":"/posts/redis_%E5%BA%95%E5%B1%82%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"},{"categories":["笔记"],"content":"减少修改字符串时带来的内存重分配次数　预分配内存，提前申请空间。 C语言字符串在进行字符串的扩充和收缩的时候，都会面临着内存空间的重新分配问题。 字符串拼接会产生字符串的内存空间的扩充，在拼接的过程中，原来的字符串的大小很可能小于拼接后的字符串的大小，那么这样的话，就会导致一旦忘记申请分配空间，就会导致内存的溢出。 字符串在进行收缩的时候，内存空间会相应的收缩，而如果在进行字符串的切割的时候，没有对内存的空间进行一个重新分配，那么这部分多出来的空间就成为了内存泄露。 举个例子：我们需要对下面的SDS进行拓展，则需要进行空间的拓展，这时候redis 会将SDS的长度修改为13字节，并且将未使用空间同样修改为13字节 。这样下次扩展字符串的时候可能就不需要申请了，因为上一次多申请了13个。 ","date":"2017-04-16","objectID":"/posts/redis_%E5%BA%95%E5%B1%82%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/:1:3","tags":["redis"],"title":"Redis学习-redis底层数据结构","uri":"/posts/redis_%E5%BA%95%E5%B1%82%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"},{"categories":["笔记"],"content":"惰性空间释放 我们在观察SDS 的结构的时候可以看到里面的free 属性，是用于记录空余空间的。我们除了在拓展字符串的时候会使用到free 来进行记录空余空间以外，在对字符串进行收缩的时候，我们也可以使用free 属性来进行记录剩余空间，这样做的好处就是避免下次对字符串进行再次修改的时候，需要对字符串的空间进行拓展。 然而，我们并不是说不能释放SDS 中空余的空间，SDS 提供了相应的API，让我们可以在有需要的时候，自行释放SDS 的空余空间。 通过惰性空间释放，SDS 避免了缩短字符串时所需的内存重分配操作，并未将来可能有的增长操作提供了优化 二进制安全 C 字符串中的字符必须符合某种编码，并且除了字符串的末尾之外，字符串里面不能包含空字符，否则最先被程序读入的空字符将被误认为是字符串结尾，这些限制使得C字符串只能保存文本数据，而不能保存想图片，音频，视频，压缩文件这样的二进制数据。 但是在Redis中，不是靠空字符来判断字符串的结束的，而是通过len这个属性。那么，即便是中间出现了空字符对于SDS来说，读取该字符仍然是可以的。 ","date":"2017-04-16","objectID":"/posts/redis_%E5%BA%95%E5%B1%82%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/:1:4","tags":["redis"],"title":"Redis学习-redis底层数据结构","uri":"/posts/redis_%E5%BA%95%E5%B1%82%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"},{"categories":["笔记"],"content":"总结 SDS相比C字符串好处多多。 获取字符串长度不用再遍历。 字符串变长会校验空间，不会溢出。 扩容的时候会多申请空间，减少内存分配次数。 可以保持二进制数据。图片、视频、空格都不怕了。 链表 redis的list是一个双向链表。 链表的特性 双端：链表节点带有prev 和next 指针，获取某个节点的前置节点和后置节点的时间复杂度都是O（N） 无环：表头节点的 prev 指针和表尾节点的next 都指向NULL，对立案表的访问时以NULL为截止 表头和表尾：因为链表带有head指针和tail 指针，程序获取链表头结点和尾节点的时间复杂度为O(1) 长度计数器：链表中存有记录链表长度的属性 len 多态：链表节点使用 void* 指针来保存节点值，并且可以通过list 结构的dup 、 free、 match三个属性为节点值设置类型特定函数。 字典 在字典中，一个键（key）可以和一个值（value）进行关联，字典中的每个键都是独一无二的。在C语言中，并没有这种数据结构，但是Redis 中构建了自己的字典实现。 SET msg “hello world” 创建这样的键值对（“msg”，“hello world”）在数据库中就是以字典的形式存储. 如果出现hash 值相同的情况怎么办？Redis 采用了链地址法： 当k1 和k0 的hash 值相同时，将k1中的next 指向k0 想成一个链表。 ","date":"2017-04-16","objectID":"/posts/redis_%E5%BA%95%E5%B1%82%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/:2:0","tags":["redis"],"title":"Redis学习-redis底层数据结构","uri":"/posts/redis_%E5%BA%95%E5%B1%82%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"},{"categories":["笔记"],"content":"Rehash rehash就是扩容和缩容。 加载因子（load factor） = ht[0].used / ht[0].size 也就是所有的元素个数 / 桶的个数 扩容： 没有执行BGSAVE和BGREWRITEAOF指令的情况下，哈希表的加载因子大于等于1。 正在执行BGSAVE和BGREWRITEAOF指令的情况下，哈希表的加载因子大于等于5。 收缩: 加载因子小于0.1时，程序自动开始对哈希表进行收缩操作。 扩容和收缩的数量 扩容： 第一个大于等于ht[0].used * 2的2^n(2的n次方幂)。 收缩： 第一个大于等于ht[0].used的2^n(2的n次方幂)。 hash的扩容和缩容都不是一下就完成的。而是·渐进式的，这样可以避免暂时的卡住。 甚至在进行期间，每次对哈希表的增删改查操作，除了正常执行之外，还会顺带将ht[0]哈希表相关键值对rehash到ht[1]。 ziplist 压缩列表。 redis的列表键和哈希键的底层实现之一。此数据结构是为了节约内存而开发的。和各种语言的数组类似，它是由连续的内存块组成的，这样一来，由于内存是连续的，就减少了很多内存碎片和指针的内存占用，进而节约了内存。 然后文中的entry的结构是这样的： 元素的遍历 先找到列表尾部元素, 然后再根据ziplist节点元素中的previous_entry_length属性，来逐个遍历。 整数集合 整数集合（intset）是Redis用于保存整数值的集合抽象数据结构，可以保存类型为int16_t、int32_t、int64_t的整数值，并且保证集合中不会出现重复元素 整数集合是集合（Set）的底层实现之一，如果一个集合只包含整数值元素，且元素数量不多时，会使用整数集合作为底层实现 typedef struct intset { // 编码方式 uint32_t encoding; // 集合包含的元素数量 uint32_t length; // 保存元素的数组 int8_t contents[]; } intset; contents数组：整数集合的每个元素在数组中按值的大小从小到大排序，且不包含重复项 length记录整数集合的元素数量，即contents数组长度 encoding决定contents数组的真正类型，如INTSET_ENC_INT16、INTSET_ENC_INT32、INTSET_ENC_INT64 跳跃表 个人理解有点像mysql索引的B+树，为的就是zset快速查找。 一个普通的单链表查询一个元素的时间复杂度为O(N)，即便该单链表是有序的。使用跳跃表（SkipList）是来解决查找问题的，它是一种有序的数据结构，不属于平衡树结构，也不属于Hash结构，它通过在每个节点维持多个指向其他节点的指针，而达到快速访问节点的目的。 跳跃表是有序集合（Sorted Set）的底层实现之一，如果有序集合包含的元素比较多，或者元素的成员是比较长的字符串时，Redis会使用跳跃表做有序集合的底层实现 跳跃表其实可以把它理解为多层的链表，它有如下的性质 多层的结构组成，每层是一个有序的链表 最底层（level 1）的链表包含所有的元素 跳跃表的查找次数近似于层数，时间复杂度为O(logn)，插入、删除也为 O(logn) 跳跃表是一种随机化的数据结构(通过抛硬币来决定层数) 那么如何来理解跳跃表呢，我们从最底层的包含所有元素的链表开始，给定如下的链表 然后我们每隔一个元素，把它放到上一层的链表当中，这里我把它叫做上浮（注意，科学的办法是抛硬币的方式，来决定元素是否上浮到上一层链表，我这里先简单每隔一个元素上浮到上一层链表，便于理解），操作完成之后的结构如下 查找元素的方法是这样，从上层开始查找，大数向右找到头，小数向左找到头，例如我要查找17，查询的顺序是：13 -\u003e 46 -\u003e 22 -\u003e 17；如果是查找35，则是 13 -\u003e 46 -\u003e 22 -\u003e 46 -\u003e 35；如果是54，则是 13 -\u003e 46 -\u003e 54 跳跃表节点的删除和添加都是不可预测的，很难保证跳表的索引是始终均匀的，抛硬币的方式可以让大体上是趋于均匀的 redis五种数据结构的实现 redis对象 redis中并没有直接使用以上所说的各种数据结构来实现键值数据库，而是基于一种对象，对象底层再间接的引用上文所说的具体的数据结构。 根据对象的类型可以判断一个对象是否可以执行给定的命令，也可针对不同的使用场景，对象设置有多种不同的数据结构实现，从而优化对象在不同场景下的使用效率 ","date":"2017-04-16","objectID":"/posts/redis_%E5%BA%95%E5%B1%82%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/:3:0","tags":["redis"],"title":"Redis学习-redis底层数据结构","uri":"/posts/redis_%E5%BA%95%E5%B1%82%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"},{"categories":["笔记"],"content":"字符串 其中：embstr和raw都是由SDS动态字符串构成的。唯一区别是：raw是分配内存的时候，redisobject和 sds 各分配一块内存，而embstr是redisobject和raw在一块儿内存中。 ","date":"2017-04-16","objectID":"/posts/redis_%E5%BA%95%E5%B1%82%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/:4:0","tags":["redis"],"title":"Redis学习-redis底层数据结构","uri":"/posts/redis_%E5%BA%95%E5%B1%82%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"},{"categories":["笔记"],"content":"列表 ","date":"2017-04-16","objectID":"/posts/redis_%E5%BA%95%E5%B1%82%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/:5:0","tags":["redis"],"title":"Redis学习-redis底层数据结构","uri":"/posts/redis_%E5%BA%95%E5%B1%82%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"},{"categories":["笔记"],"content":"hash ","date":"2017-04-16","objectID":"/posts/redis_%E5%BA%95%E5%B1%82%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/:6:0","tags":["redis"],"title":"Redis学习-redis底层数据结构","uri":"/posts/redis_%E5%BA%95%E5%B1%82%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"},{"categories":["笔记"],"content":"set ","date":"2017-04-16","objectID":"/posts/redis_%E5%BA%95%E5%B1%82%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/:7:0","tags":["redis"],"title":"Redis学习-redis底层数据结构","uri":"/posts/redis_%E5%BA%95%E5%B1%82%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"},{"categories":["笔记"],"content":"zset 第一种使用ziplist。 前提：保存元素数量小于128，并且每个元素长度小于64字节 （这两个参数可以通过zset-max-ziplist-entries 选项和 zset-max-ziplist-value 进行修改） ziplist原理： 压缩列表（ziplist）是Redis为了节省内存而开发的，是由一系列特殊编码的连续内存块组成的顺序型数据结构，一个压缩列表可以包含任意多个节点（entry），每个节点可以保存一个字节数组或者一个整数值。 使用字典和跳跃表 使用跳表的时候并不是完全是跳表，是hash表和跳表的组合。 typedef struct zset{ //跳跃表 zskiplist *zsl; //字典 dict *dice; } zset; 字典的键保存元素的值，字典的值则保存元素的分值；跳跃表节点的 object 属性保存元素的值，跳跃表节点的 score 属性保存元素的分值。 为什么不直接用跳跃表？ 假如我们单独使用 字典，虽然能以 O(1) 的时间复杂度查找成员的分值，但是因为字典是以无序的方式来保存集合元素，所以每次进行范围操作的时候都要进行排序；假如我们单独使用跳跃表来实现，虽然能执行范围操作，但是查找操作有 O(1)的复杂度变为了O(logN)。因此Redis使用了两种数据结构来共同实现有序集合。 ","date":"2017-04-16","objectID":"/posts/redis_%E5%BA%95%E5%B1%82%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/:8:0","tags":["redis"],"title":"Redis学习-redis底层数据结构","uri":"/posts/redis_%E5%BA%95%E5%B1%82%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"},{"categories":["笔记"],"content":"Redis学习-主从、哨兵、集群","date":"2017-04-16","objectID":"/posts/redis_%E4%B8%BB%E4%BB%8E%E5%93%A8%E5%85%B5%E9%9B%86%E7%BE%A4/","tags":["redis"],"title":"Redis学习-主从、哨兵、集群","uri":"/posts/redis_%E4%B8%BB%E4%BB%8E%E5%93%A8%E5%85%B5%E9%9B%86%E7%BE%A4/"},{"categories":["笔记"],"content":"主从 需要注意，主从复制的开启，完全是在从节点发起的；不需要我们在主节点做任何事情。 从节点开启主从复制，有3种方式： （1）配置文件 在从服务器的配置文件中加入：slaveof \u003cmasterip\u003e \u003cmasterport\u003e （2）启动命令 redis-server启动命令后加入 --slaveof \u003cmasterip\u003e \u003cmasterport\u003e （3）客户端命令 Redis服务器启动后，直接通过客户端执行命令：slaveof \u003cmasterip\u003e \u003cmasterport\u003e，则该Redis实例成为从节点。 ","date":"2017-04-16","objectID":"/posts/redis_%E4%B8%BB%E4%BB%8E%E5%93%A8%E5%85%B5%E9%9B%86%E7%BE%A4/:0:0","tags":["redis"],"title":"Redis学习-主从、哨兵、集群","uri":"/posts/redis_%E4%B8%BB%E4%BB%8E%E5%93%A8%E5%85%B5%E9%9B%86%E7%BE%A4/"},{"categories":["笔记"],"content":"复制 psync命令的执行会执行全量/增量的复制。 ","date":"2017-04-16","objectID":"/posts/redis_%E4%B8%BB%E4%BB%8E%E5%93%A8%E5%85%B5%E9%9B%86%E7%BE%A4/:1:0","tags":["redis"],"title":"Redis学习-主从、哨兵、集群","uri":"/posts/redis_%E4%B8%BB%E4%BB%8E%E5%93%A8%E5%85%B5%E9%9B%86%E7%BE%A4/"},{"categories":["笔记"],"content":"主从复制原理 当从数据库启动时，会向主数据库发送sync命令，主数据库接收到sync后开始在后台保存快照rdb，在保存快照期间受到的命令缓存起来，当快照完成时，主数据库会将快照和缓存的命令一块发送给从。复制初始化结束。 之后，主每受到1个命令就同步发送给从。 当出现断开重连后，2.8之后的版本会将断线期间的命令传给重数据库。增量复制 主从复制是乐观复制，当客户端发送写执行给主，主执行完立即将结果返回客户端，并异步的把命令发送给从，从而不影响性能。也可以设置至少同步给多少个从主才可写。 哨兵 当主数据库遇到异常中断服务后，开发者可以通过手动的方式选择一个从数据库来升格为主数据库，以使得系统能够继续提供服务。然而整个过程相对麻烦且需要人工介入，难以实现自动化。 为此，Redis 2.8中提供了哨兵工具来实现自动化的系统监控和故障恢复功能。 哨兵的作用就是监控redis主、从数据库是否正常运行，主出现故障自动将从数据库转换为主数据库。 哨兵节点本质还是一个redis实例，与主从节点不同的是sentinel节点作用是用于监控redis数据节点的，可以有多个哨兵节点。 上图就是一主二从多个哨兵。 ","date":"2017-04-16","objectID":"/posts/redis_%E4%B8%BB%E4%BB%8E%E5%93%A8%E5%85%B5%E9%9B%86%E7%BE%A4/:2:0","tags":["redis"],"title":"Redis学习-主从、哨兵、集群","uri":"/posts/redis_%E4%B8%BB%E4%BB%8E%E5%93%A8%E5%85%B5%E9%9B%86%E7%BE%A4/"},{"categories":["笔记"],"content":"哨兵原理 哨兵节点之间相互发消息，来检测其余哨兵是否正常工作。 哨兵也会向主从节点发送消息来检测主从节点是否在正常工作。如果主节点发送故障，那么某一个哨兵向它发送数据后就收不到回复，该哨兵会再向其他哨兵发消息来询问是否也认为该主节点异常，如果有一半的哨兵认为主节点异常，那么就进行主从节点的故障转移。 故障转移的基本思路是在从节点中选取某个从节点向其发送slaveof no one（假设选取的从节点为127.0.0.1:6380），使其称为独立的节点（也就是新的主节点），然后sentinel向其余的从节点发送slaveof 127.0.0.1 6380命令使它们重新成为新的主节点的从节点。重新分配之后sentinel节点集合还会继续监控已经下线的主节点（假设为127.0.0.1:6379），如果其重新上线，那么sentinel会向其发送slaveof命令，使其成为新的主机点的从节点，如此故障转移工作完成。 ","date":"2017-04-16","objectID":"/posts/redis_%E4%B8%BB%E4%BB%8E%E5%93%A8%E5%85%B5%E9%9B%86%E7%BE%A4/:3:0","tags":["redis"],"title":"Redis学习-主从、哨兵、集群","uri":"/posts/redis_%E4%B8%BB%E4%BB%8E%E5%93%A8%E5%85%B5%E9%9B%86%E7%BE%A4/"},{"categories":["笔记"],"content":"哨兵配置 在redis安装目录下有个默认的sentinel配置文件sentinel.conf。 每个sentinel的myid参数也要进行修改，因为sentinel之间是通过该属性来唯一区分其他sentinel节点的。 参数中sentinel monitor mymaster 127.0.0.1 6379 2，因为sentinel是通过检测主节点的状态来得知当前主节点的从节点有哪些的，因而设置为主节点的端口号即可 配置完成后我们首先启动三个主从节点，然后分别使用三个配置文件使用如下命令启用sentinel： ./src/redis-sentinel sentinel-26379.conf 哨兵节点本身也是redis实例，我们可以连接上，并查看主从节点的状态。 集群 主从模式可以解决读写分离，哨兵可以保证主从问题的容错切换，但是他们还是在单机，主从节点都要保存所有的数据。 当单机内存、并发和流量瓶颈的时候，需要使用集群方法来解决。 集群可以多个节点，每个节点可以有一个master，当master异常时，它的从节点切换。如果从节点也失败了，那么集群也就失败了。 ","date":"2017-04-16","objectID":"/posts/redis_%E4%B8%BB%E4%BB%8E%E5%93%A8%E5%85%B5%E9%9B%86%E7%BE%A4/:4:0","tags":["redis"],"title":"Redis学习-主从、哨兵、集群","uri":"/posts/redis_%E4%B8%BB%E4%BB%8E%E5%93%A8%E5%85%B5%E9%9B%86%E7%BE%A4/"},{"categories":["笔记"],"content":"Keys分布模型 redis集群中数据是和槽（slot）挂钩的，其总共定义了16384个槽，所有的数据根据一致哈希算法会被映射到这16384个槽中的某个槽中；另一方面，这16384个槽是按照设置被分配到不同的redis节点上的，比如启动了三个redis实例：cluster-A，cluster-B和cluster-C，这里将0-5460号槽分配给cluster-A，将5461-10922号槽分配给cluster-B，将10923-16383号槽分配给cluster-C。 ","date":"2017-04-16","objectID":"/posts/redis_%E4%B8%BB%E4%BB%8E%E5%93%A8%E5%85%B5%E9%9B%86%E7%BE%A4/:5:0","tags":["redis"],"title":"Redis学习-主从、哨兵、集群","uri":"/posts/redis_%E4%B8%BB%E4%BB%8E%E5%93%A8%E5%85%B5%E9%9B%86%E7%BE%A4/"},{"categories":["笔记"],"content":"redis集群投票机制 redis集群中有多台redis服务器不可避免会有服务器挂掉。redis集群服务器之间通过互相的ping-pong判断是否节点可以连接上。如果有一半以上的节点去ping一个节点的时候没有回应，集群就认为这个节点宕机了。 ","date":"2017-04-16","objectID":"/posts/redis_%E4%B8%BB%E4%BB%8E%E5%93%A8%E5%85%B5%E9%9B%86%E7%BE%A4/:6:0","tags":["redis"],"title":"Redis学习-主从、哨兵、集群","uri":"/posts/redis_%E4%B8%BB%E4%BB%8E%E5%93%A8%E5%85%B5%E9%9B%86%E7%BE%A4/"},{"categories":["笔记"],"content":"其他 在redis的官方文档中，对redis-cluster架构上，有这样的说明：在cluster架构下，默认的，一般redis-master用于接收读写，而redis-slave则用于备份，当有请求是在向slave发起时，会直接重定向到对应key所在的master来处理。但如果不介意读取的是redis-cluster中有可能过期的数据并且对写请求不感兴趣时，则亦可通过readonly命令，将slave设置成可读，然后通过slave获取相关的key，达到读写分离。 redigo库本身不支持redis哨兵和集群，redis-go-cluster库在此基础上在本地缓存了slot，并自动更新，为每一个节点管理连接池。 go-sentinel库也是支持redigo库来实现哨兵模式 ","date":"2017-04-16","objectID":"/posts/redis_%E4%B8%BB%E4%BB%8E%E5%93%A8%E5%85%B5%E9%9B%86%E7%BE%A4/:7:0","tags":["redis"],"title":"Redis学习-主从、哨兵、集群","uri":"/posts/redis_%E4%B8%BB%E4%BB%8E%E5%93%A8%E5%85%B5%E9%9B%86%E7%BE%A4/"},{"categories":["笔记"],"content":"Redis学习（二）-redis进阶","date":"2017-04-16","objectID":"/posts/redis_%E8%BF%9B%E9%98%B6/","tags":["redis"],"title":"Redis学习（二）-redis进阶","uri":"/posts/redis_%E8%BF%9B%E9%98%B6/"},{"categories":["笔记"],"content":"上篇主要讲到redis的基础命令，这篇涉及到redis的配置、持久化、主从、事务、内存淘汰策略 redis的配置 启动redis服务器的时候会指定配置文件，主要参数如下： daemonize： #是否以后台守护进程方式运行 pidfile： #pid 文件位置 port： #监听的端口号 timeout： #请求超时时间 loglevel： #log 信息级别，总共支持四个级别：debug、verbose、notice、warning ， 默认为 verbose logfile： #默认为标准输出（stdout），如果配置为守护进程方式运行，而这里又配 置为日志记录方式为标准输出，则日志将会发送给/dev/null databases： #开启数据库的数量。使用“SELECT 库 ID”方式切换操作各个数据库 save：保存快照的频率，第一个表示多长时间，第二个表示执行多少次写操作。在一定时间内执行一定数量的写操作时，自动保存快照。可设置多个条件。 rdbcompression：#保存快照是否使用压缩 dbfilename： #数据快照文件名（只是文件名，不包括目录）。默认值为 dump.rdb dir： #数据快照的保存目录（这个是目录） requirepass： #设置 Redis 连接密码，如果配置了连接密码，客户端在连接 Redis 时需 要通过 AUTH 命令提供密码，默认关闭。 redis持久化 redis在内存中存储数据，但重启的时候我们还是希望能恢复数据，那么可以把数据持久化到数据库中。你也可以同时开启两种持久化方式， 在这种情况下, 当redis重启的时候会优先载入AOF文件来恢复原始的数据,因为在通常情况下AOF文件保存的数据集要比RDB文件保存的数据集要完整. ","date":"2017-04-16","objectID":"/posts/redis_%E8%BF%9B%E9%98%B6/:0:0","tags":["redis"],"title":"Redis学习（二）-redis进阶","uri":"/posts/redis_%E8%BF%9B%E9%98%B6/"},{"categories":["笔记"],"content":"快照方式RDB (Redis Database) 快照方式也是redis默认使用方式，这种方式就是将内存中数据以快照的方式写入到二进制文件中，默认的文件名为dump.rdb。当然我们也可以主动调用命令来手动持久化，使用save 或者bgsave 命令通知 redis 做一次快照持久化。 配置参数 save 900 1 save 300 10 save 60 10000 分别表示900秒（15分钟）内有1个更改，300秒（5分钟）内有10个更改以及60秒内有10000个更改。 RDB文件通过两个命令来生成： SAVE:阻塞redis的服务器进程，直到RDB文件被创建完毕。在主线程中保存快照，redis是用一个主线程来处理所有请求的，这种方式会阻塞所有客户端的请求。 BGSAVE:Fork出一个子进程来创建RDB文件，不阻塞服务器进程，记录接收BGSAVE当时的数据库状态，父进程继续处理接收到的命令，子进程完成文件的创建之后，会发送信号给父进程。 自动化触发RDB持久化的方式 1\u003e根据配置redis.conf的save就可以(用的bgsave) 2\u003e主从复制时，主节点自动触发 3\u003e执行Debug Reload 4\u003e执行shutdown且没有开启AOF持久化 注意： save 操作是在主线程中保存快照的，由于 redis 是用一个主线程来处理所有客户端的请求，这种方式会阻塞所有客户端请求。所以不推荐使用。 每次快照持久化都是将内存数据完整写入到磁盘一次，并不是增量的只同步增量数据。如果数据量大的话，写操作会比较多，必然会引起大量的磁盘 IO 操作，可能会严重影响性能。 ","date":"2017-04-16","objectID":"/posts/redis_%E8%BF%9B%E9%98%B6/:1:0","tags":["redis"],"title":"Redis学习（二）-redis进阶","uri":"/posts/redis_%E8%BF%9B%E9%98%B6/"},{"categories":["笔记"],"content":"日志追加方式AOF(Append Only File) 这种方式 redis 会将每一个收到的写命令都通过 write 函数追加到文件中(默认appendonly.aof)。当 redis 重启时会通过重新执行文件中保存的写命令来在内存中重建整个数据库的内容。当然由于操作系统会在内核中缓存 write 做的修改，所以可能不是立即写到磁盘上。这样的持久化还是有可能会丢失部分修改。不过我们可以通过配置文件告诉redis 我们想要通过 fsync 函数强制操作系统写入到磁盘的时机。有三种方式如下（默认是 ：每秒 fsync 一次） appendonly yes //启用日志追加持久化方式 appendfsync always //每次收到写命令就立即强制写入磁盘，最慢的，但是保证完全 的持久化，不推荐使用 appendfsync everysec //每秒钟强制写入磁盘一次，在性能和持久化方面做了很好的折 中，推荐 appendfsync no //完全依赖操作系统，性能最好,持久化没保证 日志追加方式同时带来了另一个问题。持久化文件会变的越来越大。为了压缩这种持久化方式的日志文件 。redis 提供了bgrewriteaof 命令。收到此命令 redis 将使用与快照类似的方式将内存中的数据以命令的方式保存到临时文件中，而不是把原来那种所有的操作记录，最后替换原来的持久化日志文件。 我们可以配置aof文件的大小扩张倍数即重写。 比如说redis现在做一个定时器，轮询100下，那其实我们想要的结果是最后的数据，但是AOF会把整个过程记录下来，所以AOF文件大小会不断增大。怎么办呢？ BGREWRITEAOF命令来重写 1\u003e调用fork()，创建一个子进程 2\u003e子进程把新的AOF写到一个临时文件里，不依赖原来的AOF文件 3\u003e主进程持续将新的变动同时写到内存和原来的AOF里 4\u003e主进程获取子进程重写AOF的信号之后，往新的AOF同步增量变动 5\u003e使用新的AOF文件替换旧的AOF文件 ","date":"2017-04-16","objectID":"/posts/redis_%E8%BF%9B%E9%98%B6/:2:0","tags":["redis"],"title":"Redis学习（二）-redis进阶","uri":"/posts/redis_%E8%BF%9B%E9%98%B6/"},{"categories":["笔记"],"content":"RDB和AOF的优缺点比较 1\u003e RDB优点：全量数据快照，文件小，恢复快 RDB缺点：无法保存最近一次快照之后的数据 2\u003e AOF优点：可读性高，适合保存增量数据，数据不易丢失 AOF缺点：文件体积大，恢复时间长 RDB-AOF混合 redis4.0之后推出RDB-AOF混合持久化方式，并且是默认配置。 BGSAVE做镜像全量持久化，AOF做增量持久化。 在redis实例重启时，会使用BGSAVE持久化文件重新构建内容，再使用AOF重放近期的操作指令。 redis主从简介 Redis 支持将数据同步到多台从库上，这种特性对提高 读取性能非常有益。 master 可以有多个 slave。 除了多个 slave 连到相同的 master 外，slave 也可以连接其它 slave 形成图状结构。 主从复制不会阻塞 master。也就是说当一个或多个 slave 与 master 进行初次同步数据 时，master 可以继续处理客户端发来的请求。 主从复制可以用来提高系统的可伸缩性,我们可以用多个 slave 专门用于客户端的读请求，比如 sort 操作可以使用 slave 来处理。也可以用来做简单的数据冗余。 可以在 master 禁用数据持久化，只需要注释掉 master 配置文件中的所有 save 配置，然 后只在 slave 上配置数据持久化 ","date":"2017-04-16","objectID":"/posts/redis_%E8%BF%9B%E9%98%B6/:3:0","tags":["redis"],"title":"Redis学习（二）-redis进阶","uri":"/posts/redis_%E8%BF%9B%E9%98%B6/"},{"categories":["笔记"],"content":"redis主从原理 当设置好 slave 服务器后，slave 会建立和 master 的连接，然后发送 sync 命令。无论是第一次同步建立的连接还是连接断开后的重新连接，master 都会启动一个后台进程，将数据库快照保存到文件中，同时 master 主进程会开始收集新的写命令并缓存起来。后台进程完成写文件后，master 就发送文件给 slave，slave 将文件保存到磁盘上，然后加载到内存恢复数据库快照到 slave 上。接着 master 就会把缓存的命令转发给 slave。而且后续 master 收到的写命令都会通过开始建立的连接发送给slave。从master到slave的同步数据的命令和从客户端发送的命令使用相同的协议格式。当 master 和 slave 的连接断开时 slave 可以自动重新建立连接。如果 master 同时收到多个 slave 发来的同步连接命令，只会启动一个进程来写数据库镜像，然后发送给所有 slave。 配置 slave 服务器很简单，只需要在配置文件中加入如下配置： slaveof 192.168.1.1 6379 #指定 master 的 ip 和端口 redis事务 如果是在入队时报错，那么都不会执行；比如入队时一个命令参数错误，就会把队列清空，回到正常模式。 exec命令后，开始执行队列中的命令，执行中发生错误,并不会把原来的回滚，而是继续执行后面的命令。 Redis 通过 MULTI 、DISCARD 、EXEC 和 WATCH 四个命令来实现事务功能。 事务提供了一种“将多个命令打包，然后一次性、按顺序地执行”的机制，并且事务在执行的期间不会主动中断——服务器在执行完事务中的所有命令之后，才会继续处理其他客户端的其他命令。 另外，Redis的事务是不可嵌套的，当客户端已经处于事务状态，而客户端又再向服务器发送 MULTI 时，服务器只是简单地向客户端发送一个错误，然后继续等待其他命令的入队。 ","date":"2017-04-16","objectID":"/posts/redis_%E8%BF%9B%E9%98%B6/:4:0","tags":["redis"],"title":"Redis学习（二）-redis进阶","uri":"/posts/redis_%E8%BF%9B%E9%98%B6/"},{"categories":["笔记"],"content":"redis事务流程 一个事务从开始到执行会经历以下三个阶段： 开始事务。//MUTI命令，这个命令唯一做的就是，将客户端的 REDIS_MULTI 选项打开，让客户端从非事务状态切换到事务状态。 命令入队。//…要执行的命令，当客户端处于非事务状态下时，所有发送给服务器端的命令都会立即被服务器执行，但是当客户端进入事务状态之后，服务器在收到来自客户端的命令时，不会立即执行命令，而是将这些命令全部放进一个事务队列里，然后返回 QUEUED ，表示命令已入队。 执行事务。//EXEC命令，前面说到，当客户端进入事务状态之后，客户端发送的命令就会被放进事务队列里。但其实并不是所有的命令都会被放进事务队列，其中的例外就是 EXEC 、DISCARD 、MULTI 和 WATCH 这四个命令——当这四个命令从客户端发送到服务器时，它们会像客户端处于非事务状态一样，直接被服务器执行 ","date":"2017-04-16","objectID":"/posts/redis_%E8%BF%9B%E9%98%B6/:5:0","tags":["redis"],"title":"Redis学习（二）-redis进阶","uri":"/posts/redis_%E8%BF%9B%E9%98%B6/"},{"categories":["笔记"],"content":"DISCARD、WATCH命令 DISCARD代表取消事务，它清空客户端的整个事务队列，然后将客户端从事务状态调整回非事务状态，最后返回字符串 OK 给客户端，说明事务已被取消。 WATCH 命令用于在事务开始之前监视任意数量的键：当调用 EXEC 命令执行事务时，如果任意一个被监视的键已经被其他客户端修改了，那么整个事务不再执行，直接返回失败。(不是单线程吗？怎么还有别的客户端改？其实你打开multi开始入队，并不耽误别的客户端改） redis\u003e WATCH name OK redis\u003e MULTI OK redis\u003e SET name peter QUEUED redis\u003e EXEC 当一个客户端结束它的事务时，无论事务是成功执行，还是失败,watch内容都会被清空。 ","date":"2017-04-16","objectID":"/posts/redis_%E8%BF%9B%E9%98%B6/:6:0","tags":["redis"],"title":"Redis学习（二）-redis进阶","uri":"/posts/redis_%E8%BF%9B%E9%98%B6/"},{"categories":["笔记"],"content":"事务的 ACID 性质 Redis 事务保证了其中的一致性（C）和隔离性（I），但并不保证原子性（A）和持久性（D）。 原子性（Atomicity 单个 Redis 命令的执行是原子性的，但 Redis 没有在事务上增加任何维持原子性的机制，所以 Redis 事务的执行并不是原子性的。 如果一个事务队列中的所有命令都被成功地执行，那么称这个事务执行成功。另一方面，如果 Redis 服务器进程在执行事务的过程中被停止——比如接到 KILL 信号、宿主机器停机，等等，那么事务执行失败。当事务失败时，Redis 也不会进行任何的重试或者回滚动作。 一致性（Consistency Redis 的一致性问题可以分为三部分来讨论：入队错误、执行错误、Redis 进程被终结。 入队错误 在命令入队的过程中，如果客户端向服务器发送了错误的命令，比如命令的参数数量不对，等等，那么服务器将向客户端返回一个出错信息，并且将客户端的事务状态设为REDIS_DIRTY_EXEC，因此，带有不正确入队命令的事务不会被执行，也不会影响数据库的一致性。 执行错误 如果命令在事务执行的过程中发生错误，比如说，对一个不同类型的 key 执行了错误的操作， 那么 Redis 只会将错误包含在事务的结果中，这不会引起事务中断或整个失败，不会影响已执 行事务命令的结果，也不会影响后面要执行的事务命令，所以它对事务的一致性也没有影响。 Redis 进程被终结 如果 Redis 服务器进程在执行事务的过程中被其他进程终结，或者被管理员强制杀死，那么根 据 Redis 所使用的持久化模式，可能有以下情况出现： • 内存模式：如果 Redis 没有采取任何持久化机制，那么重启之后的数据库总是空白的，所 以数据总是一致的。 • RDB 模式：在执行事务时，Redis 不会中断事务去执行保存 RDB 的工作，只有在事务执 行之后，保存 RDB 的工作才有可能开始。所以当 RDB 模式下的 Redis 服务器进程在事 务中途被杀死时，事务内执行的命令，不管成功了多少，都不会被保存到 RDB 文件里。 恢复数据库需要使用现有的 RDB 文件，而这个 RDB 文件的数据保存的是最近一次的数 据库快照（snapshot），所以它的数据可能不是最新的，但只要 RDB 文件本身没有因为 其他问题而出错，那么还原后的数据库就是一致的。 • AOF 模式：因为保存 AOF 文件的工作在后台线程进行，所以即使是在事务执行的中途， 保存 AOF 文件的工作也可以继续进行，因此，根据事务语句是否被写入并保存到 AOF 文件，有以下两种情况发生： 如果事务语句未写入到 AOF 文件，或 AOF 未被 SYNC 调用保存到磁盘，那么当进 程被杀死之后，Redis 可以根据最近一次成功保存到磁盘的 AOF 文件来还原数据库，只 要 AOF 文件本身没有因为其他问题而出错，那么还原后的数据库总是一致的，但其中的 数据不一定是最新的。 如果事务的部分语句被写入到 AOF 文件，并且 AOF 文件被成功保存，那么不完整的 事务执行信息就会遗留在 AOF 文件里，当重启 Redis 时，程序会检测到 AOF 文件并不 完整，Redis 会退出，并报告错误。需要使用 redis-check-aof 工具将部分成功的事务命令 移除之后，才能再次启动服务器。还原之后的数据总是一致的，而且数据也是最新的（直 到事务执行之前为止）。 隔离性（Isolation） Redis 是单进程程序，并且它保证在执行事务时，不会对事务进行中断，事务可以运行直到执 行完所有事务队列中的命令为止。因此，Redis 的事务是总是带有隔离性的。 持久性（Durability） 因为事务不过是用队列包裹起了一组 Redis 命令，并没有提供任何额外的持久性功能，所以事 务的持久性由 Redis 所使用的持久化模式决定 redis淘汰策略 相关知识：redis 内存数据集大小上升到一定大小的时候，就会施行数据淘汰策略（回收策略）。redis 提供 6种数据淘汰策略： volatile-lru：从已设置过期时间的数据集（server.db[i].expires）中挑选最近最少使用的数据淘汰 volatile-ttl：从已设置过期时间的数据集（server.db[i].expires）中挑选将要过期的数据淘汰 volatile-random：从已设置过期时间的数据集（server.db[i].expires）中任意选择数据淘汰 allkeys-lru：从数据集（server.db[i].dict）中挑选最近最少使用的数据淘汰 allkeys-random：从数据集（server.db[i].dict）中任意选择数据淘汰 no-enviction（驱逐）：禁止驱逐数据，内存不足就报错（默认的策略） ","date":"2017-04-16","objectID":"/posts/redis_%E8%BF%9B%E9%98%B6/:7:0","tags":["redis"],"title":"Redis学习（二）-redis进阶","uri":"/posts/redis_%E8%BF%9B%E9%98%B6/"},{"categories":["笔记"],"content":"Redis学习（三）-redis过期机制","date":"2017-04-16","objectID":"/posts/redis_%E8%BF%87%E6%9C%9F%E6%9C%BA%E5%88%B6/","tags":["redis"],"title":"Redis学习（三）-redis过期机制","uri":"/posts/redis_%E8%BF%87%E6%9C%9F%E6%9C%BA%E5%88%B6/"},{"categories":["笔记"],"content":"此篇主要介绍redis的一些机制 expire删除机制 大家有没有想过redis是怎么实现key的过期删除的，作为有经验的开发人员，也许大家很容易想到以下三个方案： 每设置一个过期时间就启动一个定时器，时间到的时候删除即可，但是当有大量的key需要过期的时候就显得效率低下。 懒汉式检查，当我们取值的时候再进行判断，此种方案节约了不少检查的开支，但是如果长期没有进行取值那就行造成存储资源的浪费，而且同一时间进行大量的取值要进行的判断次数很多，cpu的压力陡增。 定期检查并删除，这样可以解决懒汉式的缺点，但是检查的周期和每次执行的时间要合适，不然会给cpu过多的压力 这里我们看下redis官网上的介绍 Redis keys are expired in two ways: a passive way, and an active way. A key is passively expired simply when some client tries to access it, and the key is found to be timed out. Of course this is not enough as there are expired keys that will never be accessed again. These keys should be expired anyway, so periodically Redis tests a few keys at random among keys with an expire set. All the keys that are already expired are deleted from the keyspace. Specifically this is what Redis does 10 times per second: Test 20 random keys from the set of keys with an associated expire. Delete all the keys found expired. If more than 25% of keys were expired, start again from step 1. 大概意思是说有两个方案可以选择。消极的方法就是客户端请求访问数据的时候再检查是否超时，当然这是不够的，因为有些keys可能再也不会访问到。redis从设置过期的keys中随机的选择一些进行检查。所有过期的key都要从空间中删除。 具体内容，每秒做10次以下步骤： 随机选择20个key 删除过期的key 如果超过25%的key是过期的，继续循环，执行第一步 ","date":"2017-04-16","objectID":"/posts/redis_%E8%BF%87%E6%9C%9F%E6%9C%BA%E5%88%B6/:0:0","tags":["redis"],"title":"Redis学习（三）-redis过期机制","uri":"/posts/redis_%E8%BF%87%E6%9C%9F%E6%9C%BA%E5%88%B6/"},{"categories":["笔记"],"content":"Redis学习（五）-缓存三大问题","date":"2017-04-16","objectID":"/posts/redis_%E7%BC%93%E5%AD%98%E4%B8%89%E5%A4%A7%E9%97%AE%E9%A2%98/","tags":["redis"],"title":"Redis学习（五）-缓存三大问题","uri":"/posts/redis_%E7%BC%93%E5%AD%98%E4%B8%89%E5%A4%A7%E9%97%AE%E9%A2%98/"},{"categories":["笔记"],"content":"缓存穿透 概念 访问一个不存在的key，缓存不起作用，请求会穿透到DB，流量大时DB会挂掉。 解决方案 第一种：采用布隆过滤器，使用一个足够大的bitmap，用于存储可能访问的key，不存在的key直接被过滤； 第二种：访问key未在DB查询到值，也将空值写进缓存，但可以设置较短过期时间。 缓存雪崩 概念 大量的key设置了相同的过期时间，导致在缓存在同一时刻全部失效，造成瞬时DB请求量大、压力骤增，引起雪崩。 解决方案 可以给缓存设置过期时间时加上一个随机值时间，使得每个key的过期时间分布开来，不会集中在同一时刻失效。 缓存击穿 概念 一个存在的key，在缓存过期的一刻，同时有大量的请求，这些请求都会击穿到DB，造成瞬时DB请求量大、压力骤增。 解决方案 当失效的时候有一个去读数据即可，其他的先等着。 在访问key之前，采用SETNX（set if not exists）来设置另一个短期key，设置成功去读数据库。如果没有设置成功就先等等（比如1s）再重试读缓存，读数据的成功后回设缓存，并访问结束再删除该短期key。 ","date":"2017-04-16","objectID":"/posts/redis_%E7%BC%93%E5%AD%98%E4%B8%89%E5%A4%A7%E9%97%AE%E9%A2%98/:0:0","tags":["redis"],"title":"Redis学习（五）-缓存三大问题","uri":"/posts/redis_%E7%BC%93%E5%AD%98%E4%B8%89%E5%A4%A7%E9%97%AE%E9%A2%98/"},{"categories":["笔记"],"content":"Redis学习（一）-redis基础","date":"2017-04-16","objectID":"/posts/redis_%E5%9F%BA%E7%A1%80/","tags":["redis"],"title":"Redis学习（一）-redis基础","uri":"/posts/redis_%E5%9F%BA%E7%A1%80/"},{"categories":["笔记"],"content":"redis简介 Redis 是一个开源的使用 ANSI C 语言编写、支持网络、可基于内存亦可持久化的日志 型、Key-Value 数据库。 redis会周期性的把更新的数据写入磁盘或者把修改操作写入追加的记录文件，并且在此基础上实现了master-slave(主从)同步，redis在3.0版本推出集群模式。 redis优势 1.k、v键值存储以及数据结构存储（如列表、字典） 2.所有数据(包括数据的存储)操作均在内存中完成 3.单线程服务(这意味着会有较多的阻塞情况)，采用epoll模型进行请求响应，对比nginx 4.支持主从复制模式，更提供高可用主从复制模式（哨兵） 5.去中心化分布式集群 6.丰富的编程接口支持，如Python、Golang、Java、php、Ruby、Lua、Node.js 7.功能丰富，除了支持多种数据结构之外，还支持事务、发布/订阅、消息队列等功能 8.支持数据持久化(AOF、RDB) redis的服务端启动与客户端连接 启动服务器 redis-server redis.windows.conf 关闭服务器 redis-cli shutdown 客户端连接 redis-cli 参数： -h \u003chostname\u003e Server hostname (default: 127.0.0.1). -p \u003cport\u003e Server port (default: 6379). -a \u003cpassword\u003e Password to use when connecting to the server. --version Output version and exit. redis的key操作 redis的key均为字符串，而且不能包含有空格（通过命令行客户端不能，但是通过redigo却是可以的，建议大家还是不要使用空格了），区分大小写。 ","date":"2017-04-16","objectID":"/posts/redis_%E5%9F%BA%E7%A1%80/:0:1","tags":["redis"],"title":"Redis学习（一）-redis基础","uri":"/posts/redis_%E5%9F%BA%E7%A1%80/"},{"categories":["笔记"],"content":"redis key常用命令 exists key 检测指定 key 是否存在，返回 1 表示存在，0 不存在 时间复杂度：O(1) del key1 key2 …… keyN 删除给定 key,返回删除 key 的数目，0 表示给定 key 都不存在。 时间复杂度： 当删除一个key的时候，若value为string复杂度为O(1)，否则为O(m),m为value的元素个数 当删除的value为string的时候,复杂度为O(n)，n为key的个数 type key 返回给定 key 值的类型。返回 none 表示 key 不存在 类型 set 无序集合类型…… keys pattern 返回匹配指定模式的所有 key,keys * 即可返回所有的key randomkey 返回从当前数据库中随机选择的一个 key,如果当前数据库是空的，返回空串,个人感觉用处不大 rename oldkey newkey 重命名一个 key,如果 newkey 存在，将会被覆盖，返回 1 表示成功，0 失败。可能是 oldkey 不存在或者和 newkey 相同。 renamenx oldkey newkey 同上，但是如果 newkey 存在返回失败。nx后缀为可作not exist理解，即新key不存在才成功 expire key seconds 为 key 指定过期时间，单位是秒。返回 1 成功，0 表示失败；设置-1则删除key;如果已经设置了过期时间，则覆盖原来的值（这里部分手册上说重复设置时间返回0，亲测证明可以重复设置） 时间复杂度O(1) persist key 去除key的有效期 ttl key 返回设置过过期时间key的剩余过期秒数。-2表示key不存在，-1未设置过期时间。（这里部分手册上说重复设置时间返回0，亲测证明可以重复设置） expireat key timestamp和上面的expire类似，这里传的是unix timestamp，也就是1970年至今的秒数，设置一个已经过去的时间，相当于删除key pexpire key milliseconds和expire类似，但是是毫秒，与之对应的是pttl,pexpireat命令（2.6.0以后可用） select db-index 通过索引选择数据库，默认连接的数据库是 0,默认数据库数是 16 个。返回 1表示成功，0 失败。 move key db-index 将 key 从当前数据库移动到指定数据库。返回 1 表示成功。0 表示 key不存在或者已经在指定数据库中。 **SORT key [BY pattern] [LIMIT offset count] [GET pattern [GET pattern …]] [ASC|DESC] [ALPHA] [STORE destination]**对list,set,sorted set的元素进行排序，可返回排序后的结果或把结果进行保存到指定的key中 时间复杂度为O(N+M*log(M))，其中N为集合中的个数，M为返回的元素个数 默认是把元素按照双精度的小数进行排序，如果我们想按字典顺序排序字符串可以使用ALPHA 默认是把排序后的结果返回给客户端，但是集合本身数据位置是没有变的 redis的value操作 redis的value共有5中类型：string，list，hash，set和sorted set ","date":"2017-04-16","objectID":"/posts/redis_%E5%9F%BA%E7%A1%80/:1:0","tags":["redis"],"title":"Redis学习（一）-redis基础","uri":"/posts/redis_%E5%9F%BA%E7%A1%80/"},{"categories":["笔记"],"content":"string类型相关命令 string字符串最多可以存512M字节。 string 是最基本的类型，而且 string 类型是二进制安全的。意思是 redis 的 string 可以 包含任何数据。比如 jpg 图片或者序列化的对象。从内部实现来看其实 string 可以看作 byte数组，最大上限是 1G 字节。 set key value 设置 key 对应 string 类型的值，返回 1 表示成功，0 失败。 setnx key value 如果 key 不存在，设置 key 对应 string 类型的值。如果 key 已经存在，返回 0。 setex key seconds value 设置值并设置过期时间 get key 获取 key 对应的 string 值,如果 key 不存在返回 nil getset key value 先获取 key 的值，再设置 key 的值。如果 key 不存在返回 nil。 mget key1 key2 …… keyN 一次获取多个 key 的值，如果对应 key 不存在，则对应返回 nil，m可以做multy理解 mset key1 value1 …… keyN valueN 一次设置多个 key 的值，成功返回 1 表示所有的值都设置了，失败返回 0 表示没有任何值被设置。 msetnx key1 value1 …… keyN valueN 一次设置多个 key 的值，但是不会覆盖已经存在的 key incr key 对 key 的值做加1操作，并返回新的值。注意 incr 一个不是 int 的 value 会返回错误，incr 一个不存在的 key，则设置 key 值为 1，当超出范围时也会返回错误 decr key 对 key 的值做减1操作，decr 一个不存在 key，则设置 key 值为-1。 incrby key integer 对 key 加上指定值 ，key 不存在时候会设置 key，并认为原来的 value是 0。 decrby key integer 对 key 减去指定值。decrby 完全是为了可读性，我们完全可以通过 incrby一个负值来实现同样效果，反之一样。 从 Redis 2.6.12 版本开始， SET 命令的行为可以通过一系列参数来修改： EX seconds ： 将键的过期时间设置为 seconds 秒。 执行 SET key value EX seconds 的效果等同于执行 SETEX key seconds value 。 PX milliseconds ： 将键的过期时间设置为 milliseconds 毫秒。 执行 SET key value PX milliseconds 的效果等同于执行 PSETEX key milliseconds value 。 NX ： 只在键不存在时， 才对键进行设置操作。 执行 * SET key value NX 的效果等同于执行 SETNX key value 。 XX ： 只在键已经存在时， 才对键进行设置操作。 我们在用redis实现分布式锁的时候可以保证setnx 和key过期时间的原子性，就是利用set key value EX time NX。 ","date":"2017-04-16","objectID":"/posts/redis_%E5%9F%BA%E7%A1%80/:2:0","tags":["redis"],"title":"Redis学习（一）-redis基础","uri":"/posts/redis_%E5%9F%BA%E7%A1%80/"},{"categories":["笔记"],"content":"list类型相关命令 ist的元素个数最多为2^32-1个，也就是4294967295个。 这里的list可看做双向链表，key就是一个list的名字，不通过的key即为不同的list lpush key value [value…] 在 key 对应 list 的左边添加字符串元素，返回 list的长度 表示成功，0 表示 key 存在且不是 list 类型。(2.4版本以后可以一次添加多个value) lpushx key value 和上面的类似，但是要求key已经存在且里面含有元素 rpush key string 在 key 对应 list 的右边添加字符串元素。 llen key 返回 key 对应 list 的长度，如果 key 不存在返回 0，如果 key 对应类型不是 list返回错误。 lrange key start end 返回指定区间内的元素，下标从 0 开始，包含end，负值表示从后面计算，-1 表示倒数第一个元素 ，key 不存在返回空。 ltrim key start end 截取 list 指定区间内元素，成功返回 1，key 不存在返回错误。 时间复杂度O(N)，N为要删除的元素个数 lset key index value 设置 list 中指定下标的元素值，成功返回 1，key 或者下标不存在返回错误。 lindex key index 返回index位置的值 lrem key count value从 List 的头部（count正数）或尾部（count负数）删除一定数量（count ）匹配 value 的元素，返回删除的元素数量。count 为 0 时候删除全部；count\u003e0从头部开始；count\u003c0从尾部开始 lpop key 从 list 的头部删除并返回删除元素。如果 key 对应 list 不存在或者是空返回 nil ，如果 key 对应值不是 list 返回错误。rpop key 从 list 的尾部删除并返回删除元素。 rpop key从list的右边开始移除并返回元素。 **BLPOP key [key …] timeout ** 阻塞式弹出，设置 timeout参数为0表示永远阻塞。但把它用在 MULTI / EXEC 块当中没有意义。 rpoplpush key1 key2把key1右边的元素删除，并放到key2的左边。key1 key2可以为同一个。 linsert key BEFORE|AFTER pivot value在指定的value前/后插入新的value LINSERT mylist BEFORE “World” “There” ","date":"2017-04-16","objectID":"/posts/redis_%E5%9F%BA%E7%A1%80/:3:0","tags":["redis"],"title":"Redis学习（一）-redis基础","uri":"/posts/redis_%E5%9F%BA%E7%A1%80/"},{"categories":["笔记"],"content":"hash类型相关命令 键值对个数最多为2^32-1个，也就是4294967295个。 Redis Hash对应Value内部实际就是一个HashMap，实际这里会有2种不同实现，这个Hash的成员比较少时Redis为了节省内存会采用类似一维数组的方式来紧凑存储，而不会采用真正的HashMap结构，对应的value redisObject的encoding为zipmap,当成员数量增大时会自动转成真正的HashMap hset key field value 设置hash字段的值 hget key field 根据字段key获取hash的值 hdel key field [field …] 根据key删除hash容器的值，2.4版本以后支持多个key hexists key field 判断是否存在key及hash字段，key或字段不存在返回0 hgetall key 获取所有的字段key和value，value紧跟key一同返回 hkeys key返回hash集合的所有字段key hvals key返回包含所有字段value的list ","date":"2017-04-16","objectID":"/posts/redis_%E5%9F%BA%E7%A1%80/:4:0","tags":["redis"],"title":"Redis学习（一）-redis基础","uri":"/posts/redis_%E5%9F%BA%E7%A1%80/"},{"categories":["笔记"],"content":"set类型相关命令 元素个数最多为2^32-1个，也就是4294967295个。 set 是无序集合，最大可以包含(2 的 32 次方-1)个元素。set 的是通过 hash table 实现的 ，所以添加，删除，查找的复杂度都是 O(1)。关于 set 集合类型除了基本的添加删除操作，其它有用的操作还包含集合的取并集(union)，交集(intersection) ，差集(difference)。 sadd key member 添加一个 string 元素到 key 对应 set 集合中，成功返回 1,如果元素以及在集合中则返回 0，key 对应的 set 不存在则返回错误。（2.4版本以后支持多个元素） srem key member 从 key 对应 set 中移除指定元素，成功返回 1，如果 member 在集合中不存在或者 key 不存在返回 0，如果 key 对应的不是 set 类型的值返回错误。 spop key 删除并返回 key 对应 set 中随机的一个元素,如果 set 是空或者 key 不存在返回nil。 srandmember随机获取一个元素，但是不从集合中移除。 scard key 返回 set 的元素个数，如果 set 是空或者 key 不存在返回 0。 sismember key member 判断 member 是否在 set 中，存在返回 1，0 表示不存在或者 key 不存在。 sinter key1 key2 …… keyN 返回所有给定 key 的交集。 sinterstore dstkey key1 ……. keyN 返回所有给定 key 的交集，并保存交集存到 dstkey 下。 sunion key1 key2 …… keyN 返回所有给定 key 的并集。 sunionstore dstkey key1 …… keyN 返回所有给定 key 的并集，并保存并集到 dstkey 下。 sdiff key1 key2 …… keyN 返回所有给定 key 的差集。 sdiffstore dstkey key1 …… keyN 返回所有给定 key 的差集，并保存差集到 dstkey 下。 smembers key 返回 key 对应 set 的所有元素，结果是无序的 ","date":"2017-04-16","objectID":"/posts/redis_%E5%9F%BA%E7%A1%80/:5:0","tags":["redis"],"title":"Redis学习（一）-redis基础","uri":"/posts/redis_%E5%9F%BA%E7%A1%80/"},{"categories":["笔记"],"content":"sorted set相关命令 元素个数最多为2^32-1个，也就是4294967295个。 sorted t set 是有序集合，它在 set 的基础上增加了一个顺序属性，这一属性在添加修改元素的时候可以指定，每次指定后，会自动重新按新的值调整顺序。 zadd key score member 添加元素到集合，元素在集合中存在则更新对应 score。set的元素是唯一的，但是他们可以有相同的score，此时是按元素的字典顺序进行排序（2.4版本可以支持多参数） zrem key member 删除指定元素，1 表示成功，如果元素不存在返回 0 zincrby key incr member 增加对应 member 的 score 值，返回修改后score的值 zrank key member 返回指定元素在集合中的排名（下标），集合中元素是按 score 从小到大排序的。 zrevrank key member 同上,但是集合中元素是按 score 从大到小排序。 zrange key start end 类似 lrange 操作从集合中去指定区间的元素。返回的是有序结果 zrevrange key start end返回指定区间成员，从大到小顺序。 zrangebyscore key min max 返回集合中 score 在给定区间的元素。 zcount key min max 返回集合中 score 在给定区间的数量。 [min,max]闭区间。 zcard key 返回集合中元素个数。 zscore key element 返回给定元素对应的 score。 zremrangebyrank key min max 删除集合中排名在给定区间的元素。 zremrangebyscore key min max 删除集合中 score 在给定区间的元素 其他命令 清除redis的expire时间： 对key的内容进行修改，包括set,del,lpush,getset等都可以 对一个key进行rename，则expire会转移到新的key PERSIST key(2.2.0以后可用) list没有提供by index删除元素的方法，可以使用 下面的方法迂回删除 lset listkey 1 del lrem listkey 0 del ","date":"2017-04-16","objectID":"/posts/redis_%E5%9F%BA%E7%A1%80/:6:0","tags":["redis"],"title":"Redis学习（一）-redis基础","uri":"/posts/redis_%E5%9F%BA%E7%A1%80/"},{"categories":["笔记"],"content":"Redis 内存过高","date":"2017-04-15","objectID":"/posts/redis_%E5%86%85%E5%AD%98%E8%BF%87%E9%AB%98/","tags":["redis"],"title":"Redis 内存过高","uri":"/posts/redis_%E5%86%85%E5%AD%98%E8%BF%87%E9%AB%98/"},{"categories":["笔记"],"content":"查找bigkey redis-cli --bigkeys 统计每种数据类型的键值对个数以及平均大小。此外，这个命令执行后，会输出每种数据类型中最大的 bigkey 的信息，对于 String 类型来说，会输出最大 bigkey 的字节长度，对于集合类型来说，会输出最大 bigkey 的元素个数。 内存使用情况 ","date":"2017-04-15","objectID":"/posts/redis_%E5%86%85%E5%AD%98%E8%BF%87%E9%AB%98/:0:0","tags":["redis"],"title":"Redis 内存过高","uri":"/posts/redis_%E5%86%85%E5%AD%98%E8%BF%87%E9%AB%98/"},{"categories":["笔记"],"content":"MEMORY STATS命令 在Redis命令行中，执行MEMORY STATS命令查询内存使用详情。 Redis实例的内存开销主要由两部分组成： 业务数据的内存开销，该部分一般作为重点分析对象。 非业务数据的内存开销，例如主备复制的积压缓冲区、Redis进程初始化消耗的内存等。 返回示例及各参数对应的解释如下： 1) \"peak.allocated\" //Redis进程自启动以来消耗内存的峰值。 2) (integer) 79492312 3) \"total.allocated\" //Redis使用其分配器分配的总字节数，即当前的总内存使用量。 4) (integer) 79307776 5) \"startup.allocated\" //Redis启动时消耗的初始内存量。 6) (integer) 45582592 7) \"replication.backlog\" //复制积压缓冲区的大小。 8) (integer) 33554432 9) \"clients.slaves\" //主从复制中所有从节点的读写缓冲区大小。 10) (integer) 17266 11) \"clients.normal\" //除从节点外，所有其他客户端的读写缓冲区大小。 12) (integer) 119102 13) \"aof.buffer\" //AOF持久化使用的缓存和AOF重写时产生的缓存。 14) (integer) 0 15) \"db.0\" //业务数据库的数量。 16) 1) \"overhead.hashtable.main\" //当前数据库的hash链表开销内存总和，即元数据内存。 2) (integer) 144 3) \"overhead.hashtable.expires\" //用于存储key的过期时间所消耗的内存。 4) (integer) 0 17) \"overhead.total\" //数值=startup.allocated+replication.backlog+clients.slaves+clients.normal+aof.buffer+db.X。 18) (integer) 79273616 19) \"keys.count\" //当前Redis实例的key总数 20) (integer) 2 21) \"keys.bytes-per-key\" //当前Redis实例每个key的平均大小，计算公式：(total.allocated-startup.allocated)/keys.count。 22) (integer) 16862592 23) \"dataset.bytes\" //纯业务数据占用的内存大小。 24) (integer) 34160 25) \"dataset.percentage\" //纯业务数据占用的内存比例，计算公式：dataset.bytes*100/(total.allocated-startup.allocated)。 26) \"0.1012892946600914\" 27) \"peak.percentage\" //当前总内存与历史峰值的比例，计算公式：total.allocated*100/peak.allocated。 28) \"99.767860412597656\" 29) \"fragmentation\" //内存的碎片率。 30) \"0.45836541056632996\" ","date":"2017-04-15","objectID":"/posts/redis_%E5%86%85%E5%AD%98%E8%BF%87%E9%AB%98/:1:0","tags":["redis"],"title":"Redis 内存过高","uri":"/posts/redis_%E5%86%85%E5%AD%98%E8%BF%87%E9%AB%98/"},{"categories":["笔记"],"content":"MEMORY USAGE命令 在Redis命令行中，执行MEMORY USAGE命令查询指定Key消耗的内存（单位为字节）。 命令执行示例： MEMORY USAGE Key0089393003 (integer) 1000072 ","date":"2017-04-15","objectID":"/posts/redis_%E5%86%85%E5%AD%98%E8%BF%87%E9%AB%98/:2:0","tags":["redis"],"title":"Redis 内存过高","uri":"/posts/redis_%E5%86%85%E5%AD%98%E8%BF%87%E9%AB%98/"},{"categories":["笔记"],"content":"MEMORY DOCTOR命令 在Redis命令行中，执行MEMORY DOCTOR命令获取内存诊断建议。 ","date":"2017-04-15","objectID":"/posts/redis_%E5%86%85%E5%AD%98%E8%BF%87%E9%AB%98/:3:0","tags":["redis"],"title":"Redis 内存过高","uri":"/posts/redis_%E5%86%85%E5%AD%98%E8%BF%87%E9%AB%98/"},{"categories":["笔记"],"content":"内存碎片查看 INFO memory \"# Memory used_memory:3986048 used_memory_human:3.80M used_memory_rss:14991360 used_memory_rss_human:14.30M used_memory_peak:4330880 used_memory_lua:36864 mem_fragmentation_ratio:3.80 mem_allocator:jemalloc-3.6.0 ","date":"2017-04-15","objectID":"/posts/redis_%E5%86%85%E5%AD%98%E8%BF%87%E9%AB%98/:4:0","tags":["redis"],"title":"Redis 内存过高","uri":"/posts/redis_%E5%86%85%E5%AD%98%E8%BF%87%E9%AB%98/"},{"categories":["笔记"],"content":"内存碎片清理 那要如何清除内存碎片呢？ 处理内存碎片是Redis调优的一种方法之一。 解决方法一： 重启Redis 没有什么问题是重启无法解决的，yyds！ 但，在生产环境不能这么玩啊，如果Redis恰好没有持久化，这会导致数据丢失的，即使持久化了，万一数据量大，重启恢复时间长，期间不可用对业务影响也大。 执行 memory purge 命令 手动暴力整理内存碎片，会阻塞主进程，生产环境慎用。 解决方法二： Redis 4.0-RC3版本之后，Redis提供了一种自动清理内存碎片的参数activedefrag 开启自动内存碎片整理(总开关) activedefrag yes 只需要设置开启即可： 127.0.0.1:6379\u003e config get activedefrag 1) \"activedefrag\" 2) \"no\" 127.0.0.1:6379\u003e config set activedefrag yes OK Redis开启了自动清理内存碎片参数，那要达到什么条件才会清理呢？ Redis提供了一下触发机制，下面4个参数都是满足任意一条件后就可以进行清理： active-defrag-ignore-bytes 100mb 默认值，碎片达到100MB时，开启清理。 active-defrag-threshold-lower 10 默认值，当碎片超过 10% 时，开启清理。 active-defrag-threshold-upper 100 默认值，内存碎片超过 100%，则尽最大努力整理。 只需要进入redis客户端或者在conf配置文件设置即可 ","date":"2017-04-15","objectID":"/posts/redis_%E5%86%85%E5%AD%98%E8%BF%87%E9%AB%98/:5:0","tags":["redis"],"title":"Redis 内存过高","uri":"/posts/redis_%E5%86%85%E5%AD%98%E8%BF%87%E9%AB%98/"},{"categories":["笔记"],"content":"Redis学习（四）-golang使用redis","date":"2017-04-15","objectID":"/posts/redis_go/","tags":["redis"],"title":"Redis学习（四）-golang使用redis","uri":"/posts/redis_go/"},{"categories":["笔记"],"content":"该文章讲解redigo的使用。 talk is cheap show me the code ","date":"2017-04-15","objectID":"/posts/redis_go/:0:0","tags":["redis"],"title":"Redis学习（四）-golang使用redis","uri":"/posts/redis_go/"},{"categories":["笔记"],"content":"redigo基本使用 opt := redis.DialPassword(\"root\") c, err := redis.Dial(\"tcp\", \"127.0.0.1:6379\", opt) if err != nil { fmt.Println(\"Connect to redis error\", err) return } defer c.Close() userName, err := redis.String(c.Do(\"GET\", \"userName\")) if err != nil { fmt.Println(\"redis get failed:\", err) } else { fmt.Printf(\"Get mykey: %v \\n\", userName) } 获取连接的时候可以提供配置文件，上面代码中指定了密码，此外还有redis.DialConnectTimeout()等。 Do方法执行命令，与客户端使用类似。 reids提供了一系列方法从返回值interface{}转换为我们需要的类型。 ","date":"2017-04-15","objectID":"/posts/redis_go/:1:0","tags":["redis"],"title":"Redis学习（四）-golang使用redis","uri":"/posts/redis_go/"},{"categories":["笔记"],"content":"redigo事务 redis还支持管道 c.Send(\"SET\", \"foo\", \"bar\") c.Send(\"GET\", \"foo\") c.Flush() c.Receive() // reply from SET v, err = c.Receive() // reply from GET 命令写在连接的缓冲区中，当flush后发送到服务器，receive方法从服务器读取一行命令的结果。 其实Do方法是上面系列方法的组合，Do以send命令并flush至缓冲区，后面接收所有的结果，其中一个命令的发生错误，那么Do方法就返回该error，所有的命令正常执行完， 则返回最后一条命令的结果。 我们使用该方案实现redis的事务： c.Send(\"MULTI\") c.Send(\"INCR\", \"foo\") c.Send(\"INCR\", \"bar\") r, err := c.Do(\"EXEC\") fmt.Println(r) // prints [1, 1] ","date":"2017-04-15","objectID":"/posts/redis_go/:2:0","tags":["redis"],"title":"Redis学习（四）-golang使用redis","uri":"/posts/redis_go/"},{"categories":["笔记"],"content":"redigo订阅、发布 redigo中pub/sub用法 psc := redis.PubSubConn{Conn: c} psc.Subscribe(\"example\") for { switch v := psc.Receive().(type) { case redis.Message: fmt.Printf(\"%s: message: %s\\n\", v.Channel, v.Data) case redis.Subscription: fmt.Printf(\"%s: %s %d\\n\", v.Channel, v.Kind, v.Count) case error: return } } ","date":"2017-04-15","objectID":"/posts/redis_go/:3:0","tags":["redis"],"title":"Redis学习（四）-golang使用redis","uri":"/posts/redis_go/"},{"categories":["笔记"],"content":"redigo连接池 在实际的项目中要频繁的使用redis，如果每次连接，使用完毕断开连接，势必会造成效率低下。推荐使用redigo自带的连接池。 我们先来看下线程池结构体的信息： type Pool struct { //Dial 是创建链接的方法 Dial func() (Conn, error) //TestOnBorrow 是一个测试链接可用性的方法 TestOnBorrow func(c Conn, t time.Time) error // 最大的空闲连接数，表示即使没有redis连接时依然可以保持N个空闲的连接，而不被清除，随时处于待命状态 MaxIdle int // 最大的激活连接数，表示同时最多有N个连接 ，为0事表示没有限制 MaxActive int //最大的空闲连接等待时间，超过此时间后，空闲连接将被关闭 IdleTimeout time.Duration // 当链接数达到最大后是否阻塞，如果不的话，达到最大后返回错误 Wait bool } 如果使用： RedisClient = \u0026redis.Pool{ // 从配置文件获取maxidle以及maxactive，取不到则用后面的默认值 IdleTimeout: 180 * time.Second, Dial: func() (redis.Conn, error) { c, err := redis.Dial(\"tcp\", REDIS_HOST) if err != nil { return nil, err } return c, nil }, } // 从池里获取连接 rc := RedisClient.Get() // 用完后将连接放回连接池 defer rc.Close() ","date":"2017-04-15","objectID":"/posts/redis_go/:4:0","tags":["redis"],"title":"Redis学习（四）-golang使用redis","uri":"/posts/redis_go/"},{"categories":["笔记"],"content":"golang包管理","date":"2017-03-10","objectID":"/posts/go_package_manage/","tags":["golang"],"title":"golang包管理","uri":"/posts/go_package_manage/"},{"categories":["笔记"],"content":"什么是vendor go vendor 是go 1.5 官方引入管理包依赖的方式，1.6正式引入。其基本思路是，将引用的外部包的源代码放在当前工程的vendor目录下面，go 1.6以后编译go代码会优先从vendor目录先寻找依赖包。 在Go 1.6之前，你需要手动的设置环境变量GO15VENDOREXPERIMENT=1才可以使Go找到Vendor目录，然而在Go 1.6之后，这个功能已经不需要配置环境变量就可以实现了。 引入vendor机制后查找依赖包路径的解决方案如下： 当前包下的vendor目录。 向上级目录查找，直到找到src下的vendor目录。 在GOPATH下面查找依赖包。 在GOROOT目录下查找。 为什么要引入vendor golang通过go get命令获取的代码是在gopath/src下的，如果我们有多个项目都引用同一个package就会出现一些问题，比如：项目迁移或新同事加入时，多次go get引入的package版本不一致，导致项目编译不通过。 dep的使用 vendor 机制虽然解决了上面的一些问题，但是无法精确的引用外部包进行版本控制，不能指定引用某个特定版本的外部包；只是在开发时，将其拷贝过来，但是一旦外部包升级,vendor下的代码不会跟着升级，而且vendor下面并没有文件记录引用包的版本信息。 目前为止，golang官方并没有指定的工具，一些第三方开源帮我们解决这个问题。比如：govendor、glide、godep，dep等。dep和godep其实作者是同一人，只不过dep是golang官方非正式版。 dep is a prototype dependency management tool for Go. It requires Go 1.9 or newer to compile. dep is safe for production use. dep is the official experiment, but not yet the official tool. 这是dep官网上的简介，它要求至少1.9的版本。 ","date":"2017-03-10","objectID":"/posts/go_package_manage/:0:0","tags":["golang"],"title":"golang包管理","uri":"/posts/go_package_manage/"},{"categories":["笔记"],"content":"安装dep: go get -u github.com/golang/dep/cmd/dep ","date":"2017-03-10","objectID":"/posts/go_package_manage/:1:0","tags":["golang"],"title":"golang包管理","uri":"/posts/go_package_manage/"},{"categories":["笔记"],"content":"dep命令： dep init set up a new project dep ensure install the project's dependencies dep ensure -update update the locked versions of all dependencies dep ensure -add github.com/pkg/errors add a dependency to the project 执行init命令后，项目根目录中新增了Gopkg.lock、Gopkg.toml和一个目录vendor。 Gopkg.toml可以灵活地描述用户的意图，包括依赖的 source、branch、version等。Gopkg.lock仅仅描述依赖的具体状态，例如各依赖的revision。Gopkg.toml可以通过命令生产，也可以被用户根据 需要手动修改，Gopkg.lock是自动生成的，不可以修改。 要想添加新的依赖可以使用命令 dep ensure -add github.com/bitly/go-simplejson@=0.4.3 也可以在代码中import，然后dep ensure命令即可。 gomod 另外使用 vendor 后，每个项目都完整拷贝一份依赖包，既不方便管理又浪费了本地空间。 此外，Go 项目中的 import 指令后面的 package 路径与项目代码的存放路径相关，项目目录不能随意移动，必须安分守己地趴在 $GOPATH/src 中，否则 import 会找不到项目中的 package，虽然可以通过在容器中编译或者为每个项目准备一套 Go 环境的方式解决，但是麻烦且有额外开销。 Go1.11 和 Go1.12 引入的 Go Modules 机制，提供了统一的依赖包管理工具 go mod，依赖包统一下载在 GOPATH/pkg/mod 中进行集中管理，统一包后面有版本号，所以不会出现问题。 GO111MODULE 有三个值：off, on和auto（默认值）。 GO111MODULE=off，go命令行将不会支持module功能，寻找依赖包的方式将会沿用旧版本那种通过vendor目录或者GOPATH模式来查找。 GO111MODULE=on，go命令行会使用modules，而一点也不会去GOPATH目录下查找。 GO111MODULE=auto，默认值，go命令行将会根据当前目录来决定是否启用module功能。这种情况下可以分为两种情形： 当前目录在GOPATH/src之外且该目录包含go.mod文件 当前文件在包含go.mod文件的目录下面。 gomod 依赖包下载存放目录在GOPATH/pkg/mod Go Modules 将成为 Go1.13 默认的依赖包管理方法，在 Go1.11 和 Go1.12 中， Go Modules 只能在 $GOPATH 外部使用。 ","date":"2017-03-10","objectID":"/posts/go_package_manage/:2:0","tags":["golang"],"title":"golang包管理","uri":"/posts/go_package_manage/"},{"categories":["笔记"],"content":"gomod初始化 在 GOPATH 外部创建一个目录，然后初始化，项目的路径设置为 exampe.com/hello： go mod init example.com/hello # 该项目代码的引用路径是 example.com/hello 增加了 go.mod 和 go.sum 文件。 go.mod 提供了module, require、replace和exclude 四个命令 module 语句指定包的名字（路径） require 语句指定的依赖项模块 replace 语句可以替换依赖项模块 exclude 语句可以忽略依赖项模块 ","date":"2017-03-10","objectID":"/posts/go_package_manage/:3:0","tags":["golang"],"title":"golang包管理","uri":"/posts/go_package_manage/"},{"categories":["笔记"],"content":"依赖包添加 我们可以直接在代码中import包，然后go build时就会自动更新go.mod文件。这个时候下载的是最新的版本。 go get命令会下载指定版本，并更新到go.mod，不过它是indirect，因为它还没有用到。 go get github.com/lijiaocn/glib@v0.0.2 goproxy 默认情况下，go 命令直接从 VCS 下载模块。GOPROXY 环境变量允许进一步控制下载源。环境变量将 go 命令配置为使用 Go 模 块代理。 不怕依赖的库被作者删除了。 您不再需要任何 VSC 工具来下载依赖项，因为依赖项是通过 HTTP 提供的 (Go 代理在后台使用 HTTP)。 下载和构建 Go 模块的速度明显加快，因为 Go 代理通过 HTTP 分别提供了源代码 ( .zip 存档) 和 go.mod。与从 VCS 进行提取相比，这导致下载花费更少的时间和更快的时间 (由于更少的开销)。 国内不用梯子了。 ","date":"2017-03-10","objectID":"/posts/go_package_manage/:4:0","tags":["golang"],"title":"golang包管理","uri":"/posts/go_package_manage/"},{"categories":["笔记"],"content":"在 Go 1.13 中如何使用 goproxy.cn？ 答：一条 go env -w GOPROXY=https://goproxy.cn,direct 即可。之所以在后面拼接一个 ,direct，是因为通过这样做我们可以在一定程度上解决私有库的问题（当然， goproxy.cn 无法访问你的私有库）。这个 GOPROXY 设定的工作原理是：当 go 在抓取目标模块时，若遇见了 404 错误，那么就回退到 direct 也就是直接去目标模块的源头（比如 GitHub） 去抓取。而恰好，GitHub 等类似的代码托管网站的原则基本都是“你无权访问的你来说就是不存在的”，所以我才说通过这样设定可以在一定程度上解决私有库无法通过模块代理访问的问题。 ","date":"2017-03-10","objectID":"/posts/go_package_manage/:5:0","tags":["golang"],"title":"golang包管理","uri":"/posts/go_package_manage/"},{"categories":["笔记"],"content":"在 Go 1.13 之前如何使用 goproxy.cn？ 答：同样也是设置环境变量即可，但是得你手动配置，而且还不能使用上述的那个 ,direct 后缀，因为那是 Go 1.13 刚加的特性。详细配置方法可以参见 goproxy.cn 的 README 文件。 ","date":"2017-03-10","objectID":"/posts/go_package_manage/:6:0","tags":["golang"],"title":"golang包管理","uri":"/posts/go_package_manage/"},{"categories":["笔记"],"content":"GOPRIVATE 前面也说到对于一些内部的 package，GoProxy 并不能很好的处理，Go 1.13 推出了 GOPRIVATE 机制。只需要设置这个环境变量，然后标识出哪些 package 是 private 的，那么对于这个 package 的处理将不会从 proxy 下载。GOPRIVATE 的值是一个以逗号分隔的列表，支持正则（正则语法遵守 Golang 的 包 path.Match） GOPRIVATE=*.corp.example.com,rsc.io/private ","date":"2017-03-10","objectID":"/posts/go_package_manage/:7:0","tags":["golang"],"title":"golang包管理","uri":"/posts/go_package_manage/"},{"categories":["笔记"],"content":"context","date":"2017-03-01","objectID":"/posts/go_context/","tags":["golang"],"title":"context","uri":"/posts/go_context/"},{"categories":["笔记"],"content":"为什么要有context 为了在协程之间传递信息，比如网络服务，每个请求都是一个协程，在协程中还可能开启新的子协程，子协程中还有新的协程，我们如果要取消请求的计算，所有的子协程都应该停止。 Go 1.7 标准库引入 context，中文译作“上下文”，准确说它是 goroutine 的上下文，包含 goroutine 的运行状态、环境、现场等信息。 context 主要用来在 goroutine 之间传递上下文信息，包括：取消信号、超时时间、截止时间、k-v 等。最重要的是它是并发安全的。 context使用 我们知道context是可以形成一个树形结构，context是可以有父和多个子的。 我们一般用context.Background()和context.TODO()返回值来作为根root。 //一般用于取消子协程 func WithCancel(parent Context) (ctx Context, cancel CancelFunc){} //一般用于给定时间点取消子协程，当然也可以在deadline前主动去取消 WithDeadline(parent Context, d time.Time) (Context, CancelFunc){} //和上面的类似，只不过不是特定的时间点，而是一段时间后取消 func WithTimeout(parent Context, timeout time.Duration) (Context, CancelFunc){} //一般用于在协程间传递数据 func WithValue(parent Context, key, val interface{}) Context{} 上面的4个方法返回context，主要分为取消用和传值用，context一般用于参数传给协程 func main() { deadline, cancelFunc := context.WithDeadline(context.Background(), time.Now().Add(2*time.Second)) go test1(deadline) cancelFunc() time.Sleep(7 * time.Second) } func test1(ctx context.Context) { for { select { case \u003c-ctx.Done(): //context到期或主动取消时，channel就被关闭，零值被取出，语句得到执行 fmt.Println(ctx.Err() == context.DeadlineExceeded) fmt.Println(\"test1 stop\") return default: fmt.Println(\"test1\") time.Sleep(time.Second) } } } 这个演示固定时间点自动取消子协程的代码。 context原理 ","date":"2017-03-01","objectID":"/posts/go_context/:0:0","tags":["golang"],"title":"context","uri":"/posts/go_context/"},{"categories":["笔记"],"content":"相关接口 context.Context是一个接口，比较简单： type Context interface { //获取设置的截止时间 Deadline() (deadline time.Time, ok bool) //返回只读通道，取消后即可从通道中读数据 Done() \u003c-chan struct{} //返回取消原因 Err() error //根据key取到conetext中设置的值 Value(key interface{}) interface{} } Context接口并不需要我们实现，Go内置emptyCtx结构体已经帮我们实现了，我们代码中最开始都是以这两个内置的作为最顶层的partent context，衍生出更多的子Context。 context.Background()和context.TODO()返回的都是emptyCtx。 Done()方法返回的是一个只读的channel，被取消的时候，channel会被父关闭，这样\u003c-channel会返回零值，在此之前一直被阻塞。 canceler 再来看另外一个接口： type canceler interface { cancel(removeFromParent bool, err error) Done() \u003c-chan struct{} } 实现了上面定义的两个方法的 Context，就表明该 Context 是可取消的。源码中有两个类型实现了 canceler 接口：*cancelCtx 和 *timerCtx。注意是加了 * 号的，是这两个结构体的指针实现了 canceler 接口。 父被取消的时候会调用所有子的取消方法，哪怕子还没有到预定的时间。 ","date":"2017-03-01","objectID":"/posts/go_context/:1:0","tags":["golang"],"title":"context","uri":"/posts/go_context/"},{"categories":["笔记"],"content":"相关结构体 emptyCtx 源码中定义了 Context 接口后，并且给出了一个实现： type emptyCtx int func (*emptyCtx) Deadline() (deadline time.Time, ok bool) { return } func (*emptyCtx) Done() \u003c-chan struct{} { return nil } func (*emptyCtx) Err() error { return nil } func (*emptyCtx) Value(key interface{}) interface{} { return nil } 这实际上是一个空的 context，永远不会被 cancel，没有存储值，也没有 deadline。 cancelCtx 再来看一个重要的 context： type cancelCtx struct { Context // 保护之后的字段 mu sync.Mutex done chan struct{} children map[canceler]struct{} err error } 这是一个可以取消的 Context，实现了 canceler 接口。它直接将接口 Context 作为它的一个匿名字段，这样，它就可以被看成一个 Context。 我们调用context.WithCancel(parent)方法时，生成一个cancelCtx,里面的Context字段就是context,然后判断父parent是不是也是cancelCtx类型，如果是，就把自己加到父的children中，这样子能找到父，父能找到子。 先来看 Done() 方法的实现： func (c *cancelCtx) Done() \u003c-chan struct{} { c.mu.Lock() if c.done == nil { c.done = make(chan struct{}) } d := c.done c.mu.Unlock() return d } c.done 是“懒汉式”创建 接下来，我们重点关注 cancel() 方法的实现： func (c *cancelCtx) cancel(removeFromParent bool, err error) { // 必须要传 err if err == nil { panic(\"context: internal error: missing cancel error\") } c.mu.Lock() if c.err != nil { c.mu.Unlock() return // 已经被其他协程取消 } // 给 err 字段赋值 c.err = err // 关闭 channel，通知其他协程 if c.done == nil { c.done = closedchan } else { close(c.done) } // 遍历它的所有子节点 for child := range c.children { // 递归地取消所有子节点 child.cancel(false, err) } // 将子节点置空 c.children = nil c.mu.Unlock() if removeFromParent { // 从父节点中移除自己 removeChild(c.Context, c) } } cancel() 方法的功能就是关闭 channel：c.done；递归地取消它的所有子节点；从父节点从删除自己。达到的效果是通过关闭 channel，将取消信号传递给了它的所有子节点。 timerCtx timerCtx 基于 cancelCtx，只是多了一个 time.Timer 和一个 deadline。Timer 会在 deadline 到来时，自动取消 context。 type timerCtx struct { cancelCtx timer *time.Timer // Under cancelCtx.mu. deadline time.Time } timerCtx 首先是一个 cancelCtx，所以它能取消 创建 timerCtx 的方法： func WithTimeout(parent Context, timeout time.Duration) (Context, CancelFunc) { return WithDeadline(parent, time.Now().Add(timeout)) } WithTimeout 函数直接调用了 WithDeadline，传入的 deadline 是当前时间加上 timeout 的时间，也就是从现在开始再经过 timeout 时间就算超时。也就是说，WithDeadline 需要用的是绝对时间。重点来看它： func WithDeadline(parent Context, deadline time.Time) (Context, CancelFunc) { if cur, ok := parent.Deadline(); ok \u0026\u0026 cur.Before(deadline) { // 如果父节点 context 的 deadline 早于指定时间。直接构建一个可取消的 context。 // 原因是一旦父节点超时，自动调用 cancel 函数，子节点也会随之取消。 // 所以不用单独处理子节点的计时器时间到了之后，自动调用 cancel 函数 return WithCancel(parent) } // 构建 timerCtx c := \u0026timerCtx{ cancelCtx: newCancelCtx(parent), deadline: deadline, } // 挂靠到父节点上 propagateCancel(parent, c) // 计算当前距离 deadline 的时间 d := time.Until(deadline) if d \u003c= 0 { // 直接取消 c.cancel(true, DeadlineExceeded) // deadline has already passed return c, func() { c.cancel(true, Canceled) } } c.mu.Lock() defer c.mu.Unlock() if c.err == nil { // d 时间后，timer 会自动调用 cancel 函数。自动取消 c.timer = time.AfterFunc(d, func() { c.cancel(true, DeadlineExceeded) }) } return c, func() { c.cancel(true, Canceled) } } 也就是说仍然要把子节点挂靠到父节点，一旦父节点取消了，会把取消信号向下传递到子节点，子节点随之取消。 如果要创建的这个子节点的 deadline 比父节点要晚，也就是说如果父节点是时间到自动取消，那么一定会取消这个子节点，导致子节点的 deadline 根本不起作用，因为子节点在 deadline 到来之前就已经被父节点取消了。 个函数的最核心的一句是： c.timer = time.AfterFunc(d, func() { c.cancel(true, DeadlineExceeded) }) c.timer 会在 d 时间间隔后，自动调用 cancel 函数，并且传入的错误就是 DeadlineExceeded： context通过withX操作可以嵌套，形成一个树，新生成的context继承父的属性和特点，对父进行取消，父及子context均取消。 valueCtx type valueCtx struct { Context key, val interface{} } 由于它直接将 Context 作为匿名字段，因此仅管它只实现了 2 个方法，其他方法继承自父 context。但它仍然是一个 Context。 它实现了两个方法： func (c *valueCtx) String() string { return fmt.Sprintf(\"%v.WithValue(%#v, %#v)\", c.Context, c.key, c.val) } func (c *valueCtx) Value(key interface{}) interface{} { if c.key == key { return c.val } return c.Context.Value(key) } 创建 valueCtx 的函数： func WithValue(parent Context, key, val interface{}) Context { if key == nil { panic(\"nil key\") } if !reflect.TypeOf(key).Comparable() { panic(\"key is not comparable\") } return \u0026valueCtx{parent, key, val} } 从创建函数可以看出，valueCtx和cancelCtx不同，他可以找到父亲，父亲并不能找到他。 取值的时候先在当前context中查询，若没有找到，则依次向上查找，通过查看代码得知，是采用深度遍历。 ","date":"2017-03-01","objectID":"/posts/go_context/:2:0","tags":["golang"],"title":"context","uri":"/posts/go_context/"},{"categories":["笔记"],"content":"golang byte和string互转","date":"2017-02-10","objectID":"/posts/go_byte%E5%92%8Cstring%E4%BA%92%E8%BD%AC/","tags":["golang"],"title":"golang byte和string互转","uri":"/posts/go_byte%E5%92%8Cstring%E4%BA%92%E8%BD%AC/"},{"categories":["笔记"],"content":"两种转换方式 标准转换 go中string与[]byte的互换，相信每一位gopher都能立刻想到以下的转换方式，我们将之称为标准转换。 // string to []byte s1 := \"hello\" b := []byte(s1) // []byte to string s2 := string(b) 强转换 通过unsafe和reflect包，可以实现另外一种转换方式，我们将之称为强转换（也常常被人称作黑魔法）。 func String2Bytes(s string) []byte { sh := (*reflect.StringHeader)(unsafe.Pointer(\u0026s)) bh := reflect.SliceHeader{ Data: sh.Data, Len: sh.Len, Cap: sh.Len, } return *(*[]byte)(unsafe.Pointer(\u0026bh)) } func Bytes2String(b []byte) string { return *(*string)(unsafe.Pointer(\u0026b)) } 强转原理 在go的源码中src/runtime/slice.go，slice的定义如下： type slice struct { array unsafe.Pointer len int cap int } 在go的源码中src/runtime/string.go，string的定义如下： type stringStruct struct { str unsafe.Pointer len int } string与[]byte在底层结构上是非常的相近,对于[]byte与string而言，两者之间最大的区别就是string的值不能改变。 string在底层都是结构体stringStruct{str: str_point, len: str_len}，string结构体的str指针指向的是一个字符常量的地址， 这个地址里面的内容是不可以被改变的，因为它是只读的，但是这个指针可以指向不同的地址。 string的指针指向的内容是不可以更改的，所以每更改一次字符串，就得重新分配一次内存，之前分配的空间还需要gc回收，这是导致string相较于[]byte操作低效的根本原因。 强转换的实现细节 万能的unsafe.Pointer指针 在go中，任何类型的指针\\T都可以转换为unsafe.Pointer类型的指针，它可以存储任何变量的地址。同时，unsafe.Pointer类型的指针也可以转换回普通指针，而且可以不必和之前的类型T相同。另外，unsafe.Pointer类型还可以转换为uintptr类型，该类型保存了指针所指向地址的数值，从而可以使我们对地址进行数值计算。以上就是强转换方式的实现依据。 Q\u0026A Q1. 为啥强转换性能会比标准转换好？ 对于标准转换，无论是从[]byte转string还是string转[]byte都会涉及底层数组的拷贝。而强转换是直接替换指针的指向，从而使得string和[]byte指向同一个底层数组。这样，当然后者的性能会更好。 Q2. 既然强转换方式性能这么好，为啥go语言提供给我们使用的是标准转换方式？ 首先，我们需要知道Go是一门类型安全的语言，而安全的代价就是性能的妥协。但是，性能的对比是相对的，这点性能的妥协对于现在的机器而言微乎其微。另外强转换的方式，会给我们的程序带来极大的安全隐患。 如下示例 a := “hello” b := String2Bytes(a) b[0] = ‘H’ a是string类型，前面我们讲到它的值是不可修改的。通过强转换将a的底层数组赋给b，而b是一个[]byte类型，它的值是可以修改的，所以这时对底层数组的值进行修改，将会造成严重的错误（通过defer+recover也不能捕获）。 Q3. 为啥string要设计为不可修改的？ 我认为有必要思考一下该问题。string不可修改，意味它是只读属性，这样的好处就是：在并发场景下，我们可以在不加锁的控制下，多次使用同一字符串，在保证高效共享的情况下而不用担心安全问题。 ","date":"2017-02-10","objectID":"/posts/go_byte%E5%92%8Cstring%E4%BA%92%E8%BD%AC/:0:0","tags":["golang"],"title":"golang byte和string互转","uri":"/posts/go_byte%E5%92%8Cstring%E4%BA%92%E8%BD%AC/"},{"categories":["笔记"],"content":"golang文件操作大比拼","date":"2017-02-10","objectID":"/posts/go_%E6%96%87%E4%BB%B6%E6%93%8D%E4%BD%9C%E5%A4%A7%E6%AF%94%E6%8B%BC/","tags":["golang"],"title":"golang文件操作大比拼","uri":"/posts/go_%E6%96%87%E4%BB%B6%E6%93%8D%E4%BD%9C%E5%A4%A7%E6%AF%94%E6%8B%BC/"},{"categories":["笔记"],"content":"bufio bufio包原理 bufio 是通过缓冲来提高效率。读的时候一次多读一些，默认 io操作本身的效率并不低，低的是频繁的访问本地磁盘的文件。所以bufio就提供了缓冲区(分配一块内存)，读和写都先在缓冲区中，最后再读写文件，来降低访问本地磁盘的次数，从而提高效率。 简单的说就是，把文件读取进缓冲（内存）之后再读取的时候就可以避免文件系统的io 从而提高速度。同理，在进行写操作时，先把文件写入缓冲（内存），然后由缓冲写入文件系统。看完以上解释有人可能会表示困惑了，直接把 内容-\u003e文件 和 内容-\u003e缓冲-\u003e文件相比， 缓冲区好像没有起到作用嘛。其实缓冲区的设计是为了存储多次的写入，最后一口气把缓冲区内容写入文件。 ","date":"2017-02-10","objectID":"/posts/go_%E6%96%87%E4%BB%B6%E6%93%8D%E4%BD%9C%E5%A4%A7%E6%AF%94%E6%8B%BC/:0:0","tags":["golang"],"title":"golang文件操作大比拼","uri":"/posts/go_%E6%96%87%E4%BB%B6%E6%93%8D%E4%BD%9C%E5%A4%A7%E6%AF%94%E6%8B%BC/"},{"categories":["笔记"],"content":"Reader对象 bufio.Reader 是bufio中对io.Reader 的封装 // Reader implements buffering for an io.Reader object. type Reader struct { buf []byte rd io.Reader // reader provided by the client r, w int // buf read and write positions err error lastByte int // last byte read for UnreadByte; -1 means invalid lastRuneSize int // size of last rune read for UnreadRune; -1 means invalid } bufio.Read(p []byte) 相当于读取大小len(p)的内容，思路如下： 当缓存区有内容的时，将缓存区内容全部填入p并清空缓存区 当缓存区没有内容的时候且len(p)\u003elen(buf),即要读取的内容比缓存区还要大，直接去文件读取即可 当缓存区没有内容的时候且len(p)\u003clen(buf),即要读取的内容比缓存区小，缓存区从文件读取内容充满缓存区，并将p填满（此时缓存区有剩余内容） ","date":"2017-02-10","objectID":"/posts/go_%E6%96%87%E4%BB%B6%E6%93%8D%E4%BD%9C%E5%A4%A7%E6%AF%94%E6%8B%BC/:1:0","tags":["golang"],"title":"golang文件操作大比拼","uri":"/posts/go_%E6%96%87%E4%BB%B6%E6%93%8D%E4%BD%9C%E5%A4%A7%E6%AF%94%E6%8B%BC/"},{"categories":["笔记"],"content":"Writer对象 bufio.Writer 是bufio中对io.Writer 的封装 // Writer implements buffering for an io.Writer object. // If an error occurs writing to a Writer, no more data will be // accepted and all subsequent writes, and Flush, will return the error. // After all data has been written, the client should call the // Flush method to guarantee all data has been forwarded to // the underlying io.Writer. type Writer struct { err error buf []byte n int wr io.Writer } bufio.Write(p []byte) 的思路如下 判断buf中可用容量是否可以放下 p 如果能放下，直接把p拼接到buf后面，即把内容放到缓冲区 如果缓冲区的可用容量不足以放下，且此时缓冲区是空的，直接把p写入文件即可 如果缓冲区的可用容量不足以放下，且此时缓冲区有内容，则用p把缓冲区填满，把缓冲区所有内容写入文件，并清空缓冲区 判断p的剩余内容大小能否放到缓冲区，如果能放下（此时和步骤1情况一样）则把内容放到缓冲区 如果p的剩余内容依旧大于缓冲区，（注意此时缓冲区是空的，情况和步骤3一样）则把p的剩余内容直接写入文件 以后再次读取时缓存区有内容，将缓存区内容全部填入p并清空缓存区（此时和情况1一样） 我们可以看到reader和writer都有一个buf字节切片，默认大小是4096，我们也可以自己指定大小。 文件读取比拼 ","date":"2017-02-10","objectID":"/posts/go_%E6%96%87%E4%BB%B6%E6%93%8D%E4%BD%9C%E5%A4%A7%E6%AF%94%E6%8B%BC/:2:0","tags":["golang"],"title":"golang文件操作大比拼","uri":"/posts/go_%E6%96%87%E4%BB%B6%E6%93%8D%E4%BD%9C%E5%A4%A7%E6%AF%94%E6%8B%BC/"},{"categories":["笔记"],"content":"os包 //使用File自带的Read func read1(filename string) int { fi, err := os.Open(filename) if err != nil { panic(err) } defer fi.Close() buf := make([]byte, 4096) var nbytes int for { n, err := fi.Read(buf) if err != nil \u0026\u0026 err != io.EOF { panic(err) } if n == 0 { break } nbytes += n } return nbytes } ","date":"2017-02-10","objectID":"/posts/go_%E6%96%87%E4%BB%B6%E6%93%8D%E4%BD%9C%E5%A4%A7%E6%AF%94%E6%8B%BC/:3:0","tags":["golang"],"title":"golang文件操作大比拼","uri":"/posts/go_%E6%96%87%E4%BB%B6%E6%93%8D%E4%BD%9C%E5%A4%A7%E6%AF%94%E6%8B%BC/"},{"categories":["笔记"],"content":"bufio包 /使用bufio func read2(filename string) int { fi, err := os.Open(filename) if err != nil { panic(err) } defer fi.Close() buf := make([]byte, 4096) var nbytes int rd := bufio.NewReader(fi) for { n, err := rd.Read(buf) if err != nil \u0026\u0026 err != io.EOF { panic(err) } if n == 0 { break } nbytes += n } return nbytes } ","date":"2017-02-10","objectID":"/posts/go_%E6%96%87%E4%BB%B6%E6%93%8D%E4%BD%9C%E5%A4%A7%E6%AF%94%E6%8B%BC/:4:0","tags":["golang"],"title":"golang文件操作大比拼","uri":"/posts/go_%E6%96%87%E4%BB%B6%E6%93%8D%E4%BD%9C%E5%A4%A7%E6%AF%94%E6%8B%BC/"},{"categories":["笔记"],"content":"ioutil //使用ioutil func read3(filename string) int { fi, err := os.Open(filename) if err != nil { panic(err) } defer fi.Close() fd, err := ioutil.ReadAll(fi) nbytes := len(fd) return nbytes } ","date":"2017-02-10","objectID":"/posts/go_%E6%96%87%E4%BB%B6%E6%93%8D%E4%BD%9C%E5%A4%A7%E6%AF%94%E6%8B%BC/:5:0","tags":["golang"],"title":"golang文件操作大比拼","uri":"/posts/go_%E6%96%87%E4%BB%B6%E6%93%8D%E4%BD%9C%E5%A4%A7%E6%AF%94%E6%8B%BC/"},{"categories":["笔记"],"content":"比拼结果 当文件较小（KB 级别）时，ioutil \u003e bufio \u003e os。 当文件大小比较常规（MB 级别）时，三者差别不大，但 bufio 又是已经显现出来。 当文件较大（GB 级别）时，bufio \u003e os \u003e ioutil。 其实ioutil最好理解，当文件较小时，ioutil使用ReadAll函数将文件中所有内容直接读入内存，只进行了一次 io 操作，但是os和bufio都是进行了多次读取，才将文件处理完，所以ioutil肯定要快于os和bufio的。 但是随着文件的增大，达到接近 GB 级别时，ioutil直接读入内存的弊端就显现出来，要将 GB 级别的文件内容全部读入内存，也就意味着要开辟一块 GB 大小的内存用来存放文件数据，这对内存的消耗是非常大的，因此效率就慢了下来。如果文件继续增大，达到 3GB 甚至以上，ioutil这种读取方式就完全无能为力了。而os为什么在面对大文件时，效率会低于bufio？通过查看bufio的NewReader源码不难发现，在NewReader里，默认为我们提供了一个大小为 4096 的缓冲区，所以系统调用会每次先读取 4096 字节到缓冲区，然后rd.Read会从缓冲区去读取 os因为少了这一层缓冲区，每次读取，都会执行系统调用，因此内核频繁的在用户态和内核态之间切换，而这种切换，也是需要消耗的，故而会慢于bufio的读取方式。 ","date":"2017-02-10","objectID":"/posts/go_%E6%96%87%E4%BB%B6%E6%93%8D%E4%BD%9C%E5%A4%A7%E6%AF%94%E6%8B%BC/:6:0","tags":["golang"],"title":"golang文件操作大比拼","uri":"/posts/go_%E6%96%87%E4%BB%B6%E6%93%8D%E4%BD%9C%E5%A4%A7%E6%AF%94%E6%8B%BC/"},{"categories":["笔记"],"content":"golang字符串拼接大比拼","date":"2017-02-10","objectID":"/posts/go_%E5%AD%97%E7%AC%A6%E4%B8%B2%E6%8B%BC%E6%8E%A5%E5%A4%A7%E6%AF%94%E6%8B%BC/","tags":["golang"],"title":"golang字符串拼接大比拼","uri":"/posts/go_%E5%AD%97%E7%AC%A6%E4%B8%B2%E6%8B%BC%E6%8E%A5%E5%A4%A7%E6%AF%94%E6%8B%BC/"},{"categories":["笔记"],"content":"拼接字符串大对比 直接+拼接 func StringsAdd() string { var s string for _, v := range StrData { s += v } return s } 使用fmt包进行组装 func StringsFmt() string { var s string = fmt.Sprint(StrData) return s } 使用strings包的join方法 func StringsJoin() string { var s string = strings.Join(StrData, \"\") return s } strings.Join方法是实现效果最好的方法，耗时是最低的，内存占用也最低，额外内存分配次数也只有1次，我们查看strings.Join的方法内部的实现代码。 func Join(elems []string, sep string) string { switch len(elems) { case 0: return \"\" case 1: return elems[0] } n := len(sep) * (len(elems) - 1) for i := 0; i \u003c len(elems); i++ { n += len(elems[i]) } var b Builder b.Grow(n) b.WriteString(elems[0]) for _, s := range elems[1:] { b.WriteString(sep) b.WriteString(s) } return b.String() } 使用bytes.Buffer拼接 func StringsBuffer() string { var s bytes.Buffer for _, v := range StrData { s.WriteString(v) } return s.String() } 使用strings.Builder拼接 func StringsBuilder() string { var b strings.Builder for _, v := range StrData { b.WriteString(v) } return b.String() } 来看下builder内部细节 type Builder struct { addr *Builder // of receiver, to detect copies by value buf []byte } func (b *Builder) copyCheck() { if b.addr == nil { // This hack works around a failing of Go's escape analysis // that was causing b to escape and be heap allocated. // See issue 23382. // TODO: once issue 7921 is fixed, this should be reverted to // just \"b.addr = b\". b.addr = (*Builder)(noescape(unsafe.Pointer(b))) } else if b.addr != b { panic(\"strings: illegal use of non-zero Builder copied by value\") } } // String returns the accumulated string. func (b *Builder) String() string { return *(*string)(unsafe.Pointer(\u0026b.buf)) } // WriteString appends the contents of s to b's buffer. // It returns the length of s and a nil error. func (b *Builder) WriteString(s string) (int, error) { b.copyCheck() b.buf = append(b.buf, s...) return len(s), nil } // grow copies the buffer to a new, larger buffer so that there are at least n // bytes of capacity beyond len(b.buf). func (b *Builder) grow(n int) { buf := make([]byte, len(b.buf), 2*cap(b.buf)+n) copy(buf, b.buf) b.buf = buf } +拼接会造成大量的临时字符串，效率最低。 fmt.Sprintf内部的逻辑比较复杂，有很多额外的判断，还用到了 interface，所以性能也不是很好 bytes.buffer有缓冲，效果好一点，如果能预估字符串的长度，还可以用 buffer.Grow() 接口来设置 capacity strings.join内部其实使用的strings.builder,并且对builder使用了grow(n)方法，效率比较高 在Go 1.10开始，Go官方将strings.Builder作为一个feature引入，其能较大程度的提高字符串拼接的效率，为了解决bytes.Buffer.String()存在的[]byte -\u003e string类型转换和内存拷贝问题，这里使用了一个unsafe.Pointer的内存指针转换操作，实现了直接将buf []byte转换为 string类型，同时避免了内存充分配的问题。如果我们也像strings.join那样使用 b.Grow(n)方法，效率也很快 ","date":"2017-02-10","objectID":"/posts/go_%E5%AD%97%E7%AC%A6%E4%B8%B2%E6%8B%BC%E6%8E%A5%E5%A4%A7%E6%AF%94%E6%8B%BC/:0:0","tags":["golang"],"title":"golang字符串拼接大比拼","uri":"/posts/go_%E5%AD%97%E7%AC%A6%E4%B8%B2%E6%8B%BC%E6%8E%A5%E5%A4%A7%E6%AF%94%E6%8B%BC/"},{"categories":["笔记"],"content":"总结 官方是建议使用strings.Builder的方式，其实strings.join也是使用的它，如果提前知道总字节的大小，设置容量效率很快的。 较大的字符串拼接时，五种方式的拼接效率由高到低排序是： strings.Builder ≈ strings.Join \u003e strings.Buffer \u003e “+” \u003e fmt 总结 1.io库属于底层接口定义库，其作用是是定义一些基本接口和一些基本常量，并对这些接口的作用给出说明，常见的接口有Reader、Writer等。一般用这个库只是为了调用它的一些常量，比如io.EOF。 2.ioutil库包含在io目录下，它的主要作用是作为一个工具包，里面有一些比较实用的函数，比如 ReadAll(从某个源读取数据)、ReadFile（读取文件内容）、WriteFile（将数据写入文件）、ReadDir（获取目录） 3.os库主要是跟操作系统打交道，所以文件操作基本都会跟os库挂钩，比如创建文件、打开一个文件等。这个库往往会和ioutil库、bufio库等配合使用 4.bufio库可以理解为在io库上再封装一层，加上了缓存功能。它可能会和ioutil库和bytes.Buffer搞混。 4.1 bufio VS ioutil库：两者都提供了对文件的读写功能，唯一的不同就是bufio多了一层缓存的功能，这个优势主要体现读取大文件的时候（ioutil.ReadFile是一次性将内容加载到内存，如果内容过大，很容易爆内存） 4.2 bufio VS bytes.Buffer：两者都提供一层缓存功能，它们的不同主要在于 bufio 针对的是文件到内存的缓存，而 bytes.Buffer 的针对的是内存到内存的缓存（个人感觉有点像channel，你也可以发现 bytes.Buffer 并没有提供接口将数据写到文件）。 5.bytes和strings库：这两个库有点迷，首先它们都实现了Reader接口，所以它们的不同主要在于针对的对象不同，bytes针对的是字节，strings针对的是字符串（它们的方法实现原理很相似）。另一个区别就是 bytes还带有Buffer的功能，但是 strings没提供。 ","date":"2017-02-10","objectID":"/posts/go_%E5%AD%97%E7%AC%A6%E4%B8%B2%E6%8B%BC%E6%8E%A5%E5%A4%A7%E6%AF%94%E6%8B%BC/:1:0","tags":["golang"],"title":"golang字符串拼接大比拼","uri":"/posts/go_%E5%AD%97%E7%AC%A6%E4%B8%B2%E6%8B%BC%E6%8E%A5%E5%A4%A7%E6%AF%94%E6%8B%BC/"},{"categories":["笔记"],"content":"IP，子网掩码，网关","date":"2017-02-01","objectID":"/posts/network_ip_%E6%8E%A9%E7%A0%81_%E7%BD%91%E5%85%B3/","tags":["网络"],"title":"IP，子网掩码，网关","uri":"/posts/network_ip_%E6%8E%A9%E7%A0%81_%E7%BD%91%E5%85%B3/"},{"categories":["笔记"],"content":"经常看到下图，有些概念还是要弄清楚的。 IP 电脑之间要实现网络通信，就必须要有一个合法的ip地址。IP地址 = 网络地址 + 主机地址，（又称：主机号和网络号组成）ip地址的结构使我们可以在Internet上很方便的寻址。ip地址通常用更直观的，以圆点分隔号的四个十进制数字表示，每个数字从0到255，如某一台主机的ip地址为：128.20.4.1在局域网里，同样也需要ip地址，一般内网的ip地址是以192.168开头的，这样很容易区分公网和内网的ip地址。 ip对于一台计算记得的意义可以举个形象的例子： “幸福小区”有若干住户，每个住户都有门牌号，范围是0-255，我们若在小区里要找5号，显然很简单就找到了。但是如果我们在大街上找“幸福小区”肯定就蒙了，因为我们压根不知道去哪个小区的地址。所以我们给小区找个地址，一般就要第一个住户的地址。 网络地址：小区地址，一般是第一个用户的地址。比如192.168.0.0 主机地址：门牌号，比如0.0.0.1 IP地址：网络地址+主机地址，192.168.0.1 Ip地址的分级 主机号为啥要减去2呢？主机号全0和全1的不让用。 子网 IP的有了分级，但是会造成IP地址的浪费和不灵活。比如有的会申请一个B类IP地址，但是主机目前并没有这么多。再比如IP不够了，还需要再申请。 子网就是把主机地址的一部分再分割。 申请到IP地址后要不要划分子网及怎么划分是你自己的事情。但是要注意，主机号至少要留2位。 外网并不知道你子网的情况，他们看到的只是网络号。 子网掩码 用来知道子网是多少的。 最为简单的理解就是两台计算机各自的ip地址与子网掩码进行\u0026运算后，得出的结果是相同的，则说明这两台计算机是处于同一个子网络上的，可以进行直接的通讯。 举例： A的ip为192.168.0.1，子网掩码为255.255.255.0 B的ip为192.168.0.200，子网掩码为255.255.255.0 ip，子网掩码分别换算为2进制，进行\u0026运算，结果一致则在同一网络，可以进行互连。 网关 上面讲到只有在同一个网络才能进行通信，利用的是子网掩码。那就是需要不在同一个网络的计算器进行互连可怎么办？网关的作用就来了。 ","date":"2017-02-01","objectID":"/posts/network_ip_%E6%8E%A9%E7%A0%81_%E7%BD%91%E5%85%B3/:0:0","tags":["网络"],"title":"IP，子网掩码，网关","uri":"/posts/network_ip_%E6%8E%A9%E7%A0%81_%E7%BD%91%E5%85%B3/"},{"categories":["笔记"],"content":"那么网关到底是什么呢？ 网关实质上是一个网络通向其他网络的IP地址。比如有网络A和网络B，网络A的IP地址范围为“192.168.1.1~192. 168.1.254”，子网掩码为255.255.255.0；网络B的IP地址范围为“192.168.2.1~192.168.2.254”，子网掩码为255.255.255.0。在没有路由器的情况下，两个网络之间是不能进行TCP/IP通信的，即使是两个网络连接在同一台交换机（或集线器）上，TCP/IP协议也会根据子网掩码（255.255.255.0）判定两个网络中的主机处在不同的网络里。而要实现这两个网络之间的通信，则必须通过网关。如果网络A中的主机发现数据包的目的主机不在本地网络中，就把数据包转发给它自己的网关，再由网关转发给网络B的网关，网络B的网关再转发给网络B的某个主机。网络B向网络A转发数据包的过程也是如此。所以说，只有设置好网关的IP地址，TCP/IP协议才能实现不同网络之间的相互通信。那么这个IP地址是哪台机器的IP地址呢？网关的IP地址是具有路由功能的设备的IP地址，具有路由功能的设备有路由器、启用了路由协议的服务器（实质上相当于一台路由器）、代理服务器（也相当于一台路由器）。 简单来说，tcp/ip发现两台计算机不在同一个网络，别怕，先发给自己所在网络的网关，利用网关传到另一网络中去，进而传到目标主机。 一台主机可以有多个网关，还会有一个默认网关，当不知道利用那个网关传数据的时候就用这个默认网关。 ","date":"2017-02-01","objectID":"/posts/network_ip_%E6%8E%A9%E7%A0%81_%E7%BD%91%E5%85%B3/:1:0","tags":["网络"],"title":"IP，子网掩码，网关","uri":"/posts/network_ip_%E6%8E%A9%E7%A0%81_%E7%BD%91%E5%85%B3/"},{"categories":["笔记"],"content":"设置默认网关 设置默认网关可以手动和自动设置 手动设置不利于迁移，因为迁移时ip地址会改变 自动设置就是利用DHCP服务器来自动给网络中的电脑分配IP地址、子网掩码和默认网关。这样做的好处是一旦网络的默认网关发生了变化时，只要更改了DHCP服务器中默认网关的设置，那么网络中所有的电脑均获得了新的默认网关的IP地址。 私有地址 绝大部分计算机都是在一个内网中，而不是直接分配一个公网ipv4地址，我们可以用ipconfig查看一下本地的吧ip地址，然后对比一下公网ip地址： ipconfig查出来的是你本机的IP地址，也就是内网私有地址，此类地址仅在局域网使用，不能联通外网。百度在线查出来的地址是你上网的公网地址。 NAT技术 公有IP地址有限，私有IP地址网络又不认识，私有地址怎么和公有地址电脑传数据呢？ net address translation技术解决，主要原理是就是在经过路由器时把目的IP或源IP，目的端口或源端口修改下。这里有一个地址和端口的转换表。 总结： 网络通信就好比送快递，商品外面的一层层包裹就是各种协议，协议包含了商品信息、收货地址、收件人、联系方式等，然后还需要配送车、配送站、快递员，商品才能最终到达用户手中。 一般情况下，快递是不能直达的，需要先转发到对应的配送站，然后由配送站再进行派件。 配送车就是物理介质，配送站就是网关， 快递员就是路由器，收货地址就是IP地址，联系方式就是MAC地址。 快递员负责把包裹转发到各个配送站，配送站根据收获地址里的省市区，确认是否需要继续转发到其他配送站，当包裹到达了目标配送站以后，配送站再根据联系方式找到收件人进行派件。 参考文章： https://www.cnblogs.com/songQQ/archive/2009/05/27/1490612.html ","date":"2017-02-01","objectID":"/posts/network_ip_%E6%8E%A9%E7%A0%81_%E7%BD%91%E5%85%B3/:2:0","tags":["网络"],"title":"IP，子网掩码，网关","uri":"/posts/network_ip_%E6%8E%A9%E7%A0%81_%E7%BD%91%E5%85%B3/"},{"categories":["笔记"],"content":"TCP UDP HTTP SOCKET","date":"2017-02-01","objectID":"/posts/network_socket/","tags":["网络"],"title":"TCP UDP HTTP SOCKET","uri":"/posts/network_socket/"},{"categories":["笔记"],"content":"SOCKET 套接字，它并不是协议，而是TCP，UDP网络的API，每个语言基本都有对应的实现。Socket是应用层与TCP/IP协议族通信的中间软件抽象层，它是一组接口。socket是在应用层和传输层之间的一个抽象层，它把TCP/IP层复杂的操作抽象为几个简单的接口供应用层调用已实现进程在网络中通信。 Websocket Websocket协议解决了服务器与客户端全双工通信的问题。 **注:什么是单工、半双工、全工通信？ ** 信息只能单向传送为单工； 信息能双向传送但不能同时双向传送称为半双工； 信息能够同时双向传送则称为全双工。 websocket协议解析 wensocket协议包含两部分:一部分是“握手”，一部分是“数据传输”。 WebSocket和Socket区别 可以把WebSocket想象成HTTP(应用层)，HTTP和Socket什么关系，WebSocket和Socket就是什么关系。 HTTP 协议有一个缺陷：通信只能由客户端发起，做不到服务器主动向客户端推送信息。 WebSocket 协议在2008年诞生，2011年成为国际标准。所有浏览器都已经支持了。 它的最大特点就是，服务器可以主动向客户端推送信息，客户端也可以主动向服务器发送信息，是真正的双向平等对话，属于服务器推送技术的一种。 ","date":"2017-02-01","objectID":"/posts/network_socket/:0:0","tags":["网络"],"title":"TCP UDP HTTP SOCKET","uri":"/posts/network_socket/"},{"categories":["笔记"],"content":"网络 DNS","date":"2017-02-01","objectID":"/posts/network_dns/","tags":["网络"],"title":"网络 DNS","uri":"/posts/network_dns/"},{"categories":["笔记"],"content":"域名分级 DNS查询过程 总结 我们知道了DNS的两种查询方法，但实际上，在DNS查询过程中，客户端和服务器也都会加入缓存的机制，这样可以减少查询的次数，加快域名解析过程。当我们在浏览器中输入一个网站时，会发生如下过程 1、浏览器中输入想要访问的网站的域名，操作系统会先检查本地的hosts文件是否有这个网址映射关系，如果有，就先调用这个IP地址映射，完成域名解析。 2、如果hosts里没有这个域名的映射，客户端会向本地DNS服务器发起查询。本地DNS服务器收到查询时，如果要查询的域名包含在本地配置区域资源中，则返回解析结果给客户机，完成域名解析。 3、如果本地DNS服务器本地区域文件与缓存解析都失效，则根据本地DNS服务器的设置，采用递归或者迭代查询，直至解析完成。 ","date":"2017-02-01","objectID":"/posts/network_dns/:0:0","tags":["网络"],"title":"网络 DNS","uri":"/posts/network_dns/"},{"categories":["笔记"],"content":"网络_分层","date":"2017-02-01","objectID":"/posts/network_%E5%88%86%E5%B1%82/","tags":["网络"],"title":"网络_分层","uri":"/posts/network_%E5%88%86%E5%B1%82/"},{"categories":["笔记"],"content":"网络分层 在OSI七层模型中，每一层的作用和对应的协议如下： 我们都知道数据从应用层发下来，会在每一层都会加上头部信息，进行封装，然后再发送到数据接收端。接收端的操作就刚好与此相反，它是一层一层剥数据的过程。 每个数据都会经过数据的封装和解封装的过程。 第7层 应用层 应用层（Application Layer）提供为应用软件而设的接口，以设置与另一应用软件之间的通信。例如: HTTP，HTTPS，FTP，TELNET，SSH，SMTP，POP3.HTML.等。 第6层 表示层 表示层（Presentation Layer）把数据转换为能与接收者的系统格式兼容并适合传输的格式。在传输之前是否进行加密或压缩处理。 第5层 会话层 会话层（Session Layer）负责在数据传输中设置和维护计算机网络中两台计算机之间的通信连接。查木马。 第4层 传输层 传输层（Transport Layer）把传输表头（TH）加至数据以形成数据包。传输表头包含了所使用的协议等发送信息。例如:传输控制协议（TCP）等。udp并不能保证可靠传输，这个时候就要依赖应用层来保证了。 第3层 网络层 网络层（Network Layer）决定数据的路径选择和转寄，将网络表头（NH）加至数据包，以形成报文。网络表头包含了网络数据。例如:互联网协议（IP）等。负责规划最佳路径，规划IP地址。网络层并不能保证成功传输。 整个包最大65535字节。 第2层 数据链路层 数据链路层（Data Link Layer）负责网络寻址、错误侦测和改错。当表头和表尾被加至数据包时，会形成帧。帧的开始和结束，透明传输，差错校验（但不能纠错，纠错由传输层）。 分为两个子层：逻辑链路控制（logical link control，LLC）子层和介质访问控制（Medium access control，MAC）子层。 这个帧最大1500字节。网络层数据过大就分片。网络层头部分有个“标识”字段，在链路层分片后是可以看到的，相同的“标识”值是同一个ip数据包分割而来的。 第1层 物理层 物理层（Physical Layer）在局部局域网上传送数据帧（data frame），它负责管理计算机通信设备和网络媒体之间的互通。包括了针脚、电压、线缆规范、集线器、中继器、网卡、主机接口卡等。 链路层 ","date":"2017-02-01","objectID":"/posts/network_%E5%88%86%E5%B1%82/:0:0","tags":["网络"],"title":"网络_分层","uri":"/posts/network_%E5%88%86%E5%B1%82/"},{"categories":["笔记"],"content":"数据链路层的主要功能 封装成帧 差错控制 流量控制 链路控制 MAC寻址 区分数据和控制信息 透明传输 成帧(帧同步)—将数据组合成数据块，封装成帧 为了向网络层提供服务，数据链路层必须使用物理层提供的服务。而物理层是以比特流进行传输的，这种比特流并不保证在数据传输过程中没有错误，接收到的位数量可能少于、等于或者多于发送的位数量。而且它们还可能有不同的值，这时数据链路层为了能实现数据有效的差错控制，就采用了一种”帧”的数据块进行传输。而要采帧格式传输，就必须有相应的帧同步技术，这就是数据链路层的”成帧”（也称为”帧同步”）功能。 采用帧传输方式的好处是：在发现有数据传送错误时，只需将有差错的帧再次传送，而不需要将全部数据的比特流进行重传，这就在传送效率上将大大提高。 采用帧传输方式的好处是带来了两方面的问题： (1)如何识别帧的开始与结束； (2)在夹杂着重传的数据帧中，接收方在接收到重传的数据帧时是识别成新的数据帧，还是识别成重传帧呢？这就要靠数据链路层的各种”帧同步”技术来识别了。”帧同步”技术既可使接收方能从并不是完全有序的比特流中准确地区分出每一帧的开始和结束，同时还可识别重传帧。 差错控制 在数据通信过程中可能会因物理链路性能和网络通信环境等因素，难免会出现一些传送错误，但为了确保数据通信的准确，又必须使得这些错误发生的几率尽可能低。这一功能也是在数据链路层实现的，就是它的”差错控制”功能。 在数字或数据通信系统中，通常利用抗干扰编码进行差错控制。一般分为4类：前向纠错（FEC）、反馈检测（ARQ）、混合纠错（HEC）和信息反馈（IRQ）。 FEC方式是在信息码序列中，以特定结构加入足够的冗余位–称为”监督元”（或”校验元”）。接收端解码器可以按照双方约定的这种特定的监督规则，自动识别出少量差错，并能予以纠正。FEC最适合于实时的高速数据传输的情况。 在非实时数据传输中，常用ARQ差错控制方式。解码器对接收码组逐一按编码规则检测其错误。如果无误，向发送端反馈”确认”ACK信息；如果有错，则反馈回ANK信息，以表示请求发送端重复发送刚刚发送过的这一信息。ARQ方式的优点在于编码冗余位较少，可以有较强的检错能力，同时编解码简单。由于检错与信道特征关系不大，在非实时通信中具有普遍应用价值。 HEC方式是上述两种方式的有机结合，即在纠错能力内，实行自动纠错；而当超出纠错能力的错误位数时，可以通过检测而发现错码，不论错码多少都可以利用ARQ方式进行纠错。 IRQ方式是一种全回执式最简单差错控制方式。在该检错方式中，接收端将收到的信码原样转发回发送端，并与原发送信码相比较，若发现错误，则发送端再进行重发。只适于低速非实时数据通信，是一种较原始的做法。 流量控制 在双方的数据通信中，如何控制数据通信的流量同样非常重要。它既可以确保数据通信的有序进行，还可避免通信过程中不会出现因为接收方来不及接收而造成的数据丢失。这就是数据链路层的”流量控制”功能。 数据的发送与接收必须遵循一定的传送速率规则，可以使得接收方能及时地接收发送方发送的数据。并且当接收方来不及接收时，就必须及时控制发送方数据的发送速率，使两方面的速率基本匹配。 链路控制 数据链路层的”链路管理”功能包括数据链路的建立、维持和释放三个主要方面。 当网络中的两个节点要进行通信时，数据的发送方必须确知接收方是否已处在准备接收的状态。为此通信双方必须先要交换一些必要的信息，以建立一条基本的数据链路。在传输数据时要维持数据链路，而在通信完毕时要释放数据链路。 MAC寻址 这是数据链路层中的MAC子层主要功能。这里所说的”寻址”与下一章将要介绍的”IP地址寻址”是完全不一样的，因为此处所寻找的地址是计算机网卡的MAC地址，也称”物理地址”、”硬件地址”，而不是IP地址。 在以太网中，采用媒体访问控制（Media Access Control, MAC）地址进行寻址，MAC地址被烧入每个以太网网卡中。这在多点连接的情况下非常必需，因为在这种多点连接的网络通信中，必须保证每一帧都能准确地送到正确的地址，接收方也应当知道发送方是哪一个站。 区分数据和控制信息 由于数据和控制信息都是在同一信道中传输，在许多情况下，数据和控制信息处于同一帧中，因此一定要有相应的措施使接收方能够将它们区分开来，以便向上传送仅是真正需要的数据信息。 透明传输 这里所说的”透明传输”是指可以让无论是哪种比特组合的数据，都可以在数据链路上进行有效传输。这就需要在所传数据中的比特组合恰巧与某一个控制信息完全一样时，能采取相应的技术措施，使接收方不会将这样的数据误认为是某种控制信息。只有这样，才能保证数据链路层的传输是透明的。 注：在以上七大链路层功能中，主要的还是前面的五项，后面两项功能是在前五项功能中附带实现的，无需另外的技术，所以在此仅介绍前面五项功能。 链路层向网络层提供的服务 数据链路层的设计目标就是为网络层提供各种需要的服务。实际的服务随系统的不同而不同，但是一般情况下，数据链路层会向网络层提供以下三种类型的服务： 无确认的无连接服务 “无确认的无连接服务”是指源计算机向目标计算机发送独立的帧，目标计算机并不对这些帧进行确认。这种服务，事先无需建立逻辑连接，事后也不用解释逻辑连接。正因如此，如果由于线路上的原因造成某一帧的数据丢失，则数据链路层并不会检测到这样的丢失帧，也不会恢复这些帧。出现这种情况的后果是可想而知的，当然在错误率很低，或者对数据的完整性要求不高的情况下（如话音数据），这样的服务还是非常有用的，因为这样简单的错误可以交给OSI上面的各层来恢复。如大多数局域网在数据链路层所采用的服务也是无确认的无连接服务。 有确认的无连接服务 为了解决以上“无确认的无连接服务”的不足，提高数据传输的可靠性，引入了“有确认的无连接服务”。在这种连接服务中，源主机数据链路层必须对每个发送的数据帧进行编号，目的主机数据链路层也必须对每个接收的数据帧进行确认。如果源主机数据链路层在规定的时间内未接收到所发送的数据帧的确认，那么它需要重发该帧。 这样发送方知道每一帧是否正确地到达对方。这类服务主要用于不可靠信道，如无线通信系统。它与下面将要介绍的“有确认的面向连接服务”的不同之处在于它不需要在帧传输之前建立数据链路，也不要在在帧传输结束后释放数据链路。 有确认的面向连接服务 大多数数据链路层都采用向网络层提供面向连接确认服务。利用这种服务，源计算机和目标计算机在传输数据之前需要先建立一个连接，该连接上发送的每一帧也都被编号，数据链路层保证每一帧都会被接收到。而且它还保证每一帧只被按正常顺序接收一次。这也正是面向连接服务与前面介绍的“有确认无连接服务”的区别，在无连接有确认的服务中，在没有检测到确认时，系统会认为对方没收到，于是会重发数据，而由于是无连接的，所以这样的数据可能会复发多次，对方也可能接收多次，造成数据错误。这种服务类型存在3个阶段，即：数据链路建立、数据传输、数据链路释放阶段。每个被传输的帧都被编号，以确保帧传输的内容与顺序的正确性。大多数广域网的通信子网的数据链路层采用面向连接确认服务。 以太网采用无连接的工作方式，读发送的数据帧不进行编号，也不要求对方发回确认。目的站收到有差错的帧就把他丢弃，不采取其他行为。 ","date":"2017-02-01","objectID":"/posts/network_%E5%88%86%E5%B1%82/:1:0","tags":["网络"],"title":"网络_分层","uri":"/posts/network_%E5%88%86%E5%B1%82/"},{"categories":["笔记"],"content":"帧格式 其中末尾的crc生成fcs是校验位，校验数据传输中是否出错，类似于数字签名。 ","date":"2017-02-01","objectID":"/posts/network_%E5%88%86%E5%B1%82/:2:0","tags":["网络"],"title":"网络_分层","uri":"/posts/network_%E5%88%86%E5%B1%82/"},{"categories":["笔记"],"content":"链路层协议 以太网（Ethernet）协议；PPPoE（ADSL）协议；等 mac地址 以太网规协议定，接入网络的设备都必须安装网络适配器，即网卡， 数据包必须是从一块网卡传送到另一块网卡。而网卡地址就是数据包的发送地址和接收地址，也就是帧首部所包含的MAC地址，MAC地址是每块网卡的身份标识，就如同我们身份证上的身份证号码，具有全球唯一性。MAC地址采用十六进制标识，共6个字节， 前三个字节是厂商编号，后三个字节是网卡流水号，例如 4C-0F-6E-12-D2-19 有了MAC地址以后，以太网采用广播形式，把数据包发给该子网内所有主机，子网内每台主机在接收到这个包以后，都会读取首部里的目标MAC地址，然后和自己的MAC地址进行对比，如果相同就做下一步处理，如果不同，就丢弃这个包。 网络层 网络层负责在不同网络之间尽力转发数据包，基于数据包的ip地址转发。 不负责数据的丢失重传，丢就丢了，也不负责顺序，这些都是上层传输层该干的事。 ip协议 静态路由：路由器判断ip数据包要想到达目的地下一步需要去哪个路由器，可以有静态的路由表。 动态路由：网段过多，静态路由显然不合适，而且网络变化静态路由不会跟着变化。动态路由可以自己学习。 动态路由有RIP协议，可以判断最佳路由路径。 ARP协议 网络层中为ip协议服务，主要作用是在同一网段中通过ip找到对应的计算机的mac地址。实现方式是通过广播，当计算机中收到数据时和自己的ip比对，如果一致就返回自己的mac地址。 有恶意的计算机会假装满足所有的ip，返回自己的mac就能收到后面的数据，或者返回一个不存在的mac的地址，造成别人无法正常通讯。 ICMP协议 ICMP协议是网络层在ip上层的协议，主要用于检测网络层是否畅通。 ping（网络包嗅探器）命令就是其协议的应用。 ping命令结果中有ttl，其含义是每经过一个路由器ttl就减少1，当减少到0的时候就不往下传了，这样可以避免路由循环占用流量。ttl的开始数据根据操作系统不同而不同。 IGMP协议 多播组播 ","date":"2017-02-01","objectID":"/posts/network_%E5%88%86%E5%B1%82/:3:0","tags":["网络"],"title":"网络_分层","uri":"/posts/network_%E5%88%86%E5%B1%82/"},{"categories":["笔记"],"content":"ip数据报格式 由首部和数据两部分组成.首部的前一部分是固定长度,共 20 字节,是所有IP数据报必须具有的.在首部的固定部分的后面是一些可选字段,其长度是可变的。ipv6已经将首部长度改为固定的，去掉可选部分。 版本:占4位,指IP协议的版本.通信双方使用的IP协议版本必须一致.日前广泛使用的 IP协议版本号为 4 (即 IPv4).IPv6 目前还处于起步阶段. 首部长度:占 4 位。 总长度:总长度指首都及数据之和的长度,单位为字节.因为总长度字段为 16位,所以数据报的最大长度为 216-1=65 535字节.在IP层下面的每一种数据链路层都有自己的帧格式,其中包括帧格式中的数据字段的最大长度,即最大传送单元 MTU (Maximum Transfer Unit).当一个数据报封装成链路层的帧时,此数据报的总长度 (即首部加上数据部分)一定不能超过下面的数据链路层的MTU值,否则要分片. 标识 (Identification):占 16位.IP软件在存储器中维持一个计数器,每产生一个数据报,计数器就加 1,并将此值赋给标识字段.但这个\"标识\"并不是序号,因为 IP是无连接的服务,数据报不存在按序接收的问题.当数据报由于长度超过网络的 MTU 而必须分片时,这个标识字段的值就被复制到所有分片后的数据报的标识字段中.相同的标识字段的值使分片后的各数据报片最后能正确地重装成为原来的数据报. 标志 (Flag):占3 位,但目前只有2位有意义. 标志字段中的最低位记为 MF(More Fragment).MF=1即表示后面\"还有分片\"的数据报.MF=0表示这已是若干数据报片中的最后一个.标志字段中间的一位记为DF(Don’t Fragment),意思是\"不能分片\",只有当 DF=0时才允许分片. 片偏移:占 13位.较长的分组在分片后,某片在原分组中的相对位置.也就是说,相对用户数据字段的起点,该片从何处开始.片偏移以 8个字节为偏移单位,这就是说,每个分片的长度一定是 8字节(64位)的整数倍. 生存时间:占 8位,生存时间字段常用的英文缩写是TTL(Time To Live),其表明数据报在网络中的寿命. 协议:占 8 位.协议字段指出此数据报携带的数据是使用何种协议,以便使目的主机的IP层知道应将数据部分上交给哪个处理过程。比如交给tcp还是udp。TCP的协议号为6，UDP的协议号为17。ICMP的协议号为1，IGMP的协议号为2. ","date":"2017-02-01","objectID":"/posts/network_%E5%88%86%E5%B1%82/:4:0","tags":["网络"],"title":"网络_分层","uri":"/posts/network_%E5%88%86%E5%B1%82/"},{"categories":["笔记"],"content":"网络—TCP","date":"2017-02-01","objectID":"/posts/network_tcp/","tags":["网络"],"title":"网络—TCP","uri":"/posts/network_tcp/"},{"categories":["笔记"],"content":"TCP是什么？ TCP（Transmission Control Protocol 传输控制协议）是一种面向连接的、可靠的、基于字节流的传输层通信协议。 TCP头部 主要字段的作用： Source Port和Destination Port:分别占用16位，表示源端口号和目的端口号；用于区别主机中的不同进程，而IP地址是用来区分不同的主机的，源端口号和目的端口号配合上IP首部中的源IP地址和目的IP地址就能唯一的确定一个TCP连接； Sequence Number:用来标识从TCP发端向TCP收端发送的数据字节流，它表示在这个报文段中的的第一个数据字节在数据流中的序号；主要用来解决网络报乱序的问题； Acknowledgment Number:32位确认序列号包含发送确认的一端所期望收到的下一个序号，比如收到了12345这5个字节的数据，序列号是1，那确认序列号就是6，因为下次需要从第6个字节发了。因此，确认序号应当是上次已成功收到数据字节序号加1。不过，只有当标志位中的ACK标志（下面介绍）为1时该确认序列号的字段才有效。主要用来解决不丢包的问题； Offset:首部长度，或者理解为数据部分距离整个tcp报文开始的偏移量，需要这个值是因为任选字段的长度是可变的。这个字段占4bit（最大能表示15，但这里1代表4个字节，即首部长度为4*15=60个字节），因此TCP最多有60字节的首部。然而，没有任选字段，正常的长度是20字节； TCP Flags:TCP首部中有6个标志比特，它们中的多个可同时被设置为1，主要是用于操控TCP的状态机的，依次为URG，ACK，PSH，RST，SYN，FIN。每个标志位的意思如下： URG：发送端的缓存窗口中的数据是顺序发送，如果想插队先发送这段数据，可设置该标志位1。配合紧急指针使用。 ACK：此标志表示应答域有效，就是说前面所说的TCP应答号将会包含在TCP数据包中；有两个取值：0和1，为1的时候表示应答域有效，反之为0；连接建立后ACK都要是1。 PUSH：这个标志位表示Push操作。所谓Push操作就是指在数据包到达接收端以后，立即传送给应用程序，而不是在缓冲区中排队,意思就是在接收端进行插队，可以和URG类比记忆。 RST：发生了异常，需要重新建立链接。 SYN：表示同步序号，用来建立连接。SYN标志位和ACK标志位搭配使用，当连接请求的时候，SYN=1，ACK=0；连接被响应的时候，SYN=1，ACK=1；这个标志的数据包经常被用来进行端口扫描。扫描者发送一个只有SYN的数据包，如果对方主机响应了一个数据包回来 ，就表明这台主机存在这个端口；但是由于这种扫描方式只是进行TCP三次握手的第一次握手，因此这种扫描的成功表示被扫描的机器不很安全，一台安全的主机将会强制要求一个连接严格的进行TCP的三次握手； FIN： 表示发送端已经达到数据末尾，也就是说双方的数据传送完成，没有数据可以传送了，发送FIN标志位的TCP数据包后，连接将被断开。这个标志的数据包也经常被用于进行端口扫描。 窗口：表示我本地的接收缓存窗口还能接收多少数据。 可选项：最大报文段长度MSS等。 流量控制的两种方案。 停止等待协议和滑动窗口协议。 停止等待就是A发给B，B收到后回复确认，A收到后再发。A收到确认之前是要停止等待的，所以效率比较低，早期链路层就是这样的。 第二种方案是滑动窗口，连续发多个，如果有丢失的怎么办？两种方案1.选择重传方案（SR)tcp默认也是这样的。。2.回退N帧方案（GBN)。 另外tcp还有个优化就是快重传，不用等到超时到期就能知道丢失了。 tcp怎么保证可靠传输？ 校验、序号、确认、重传。 Tcp的确认默认是累计重传。即接收方回复ack号是n,说明n号之前的数据都已经接收成功了。 超时重传 如图所示，接收方接收到数据后要给客户端响应，来告诉别人自己收到了数据。 如果压根就没发到服务端，肯定不会收到服务端的响应，客户端等，等到超时后重传上个丢失的数据。 超时时间是动态的。 确认丢失 客户端成功发M1到服务端，服务端响应确认，但是确认包丢失了，客户端一样还会等，重传M1，这时候服务端收到了重复的数据，就会舍弃第二次收到的包，并再发对M1的响应。 确认迟到 客户端在发送数据M1后，服务端发送响应，但是迟到了，客户端没有及时收到响应，超时后重发，服务端舍弃重复收到的数据并响应M1,此时客户端收到迟到的响应，并舍弃响应。 窗口 客户端每次发送后，服务端对其响应，没有收到响应时会等待并重发。这样效率是低下的，由此引入“窗口”来提高效率。 简单来说，在窗口范围内不用先等上次的响应，继续发下面的内容。服务端没有收到数据时会告诉客户端重发。客户端连续三次收到重发标示就会重发失败的数据。 发送窗口=min{接收窗口rwnd,拥塞窗口cwnd) 接收窗口：接收方根据接受缓存设置的值，并告知发送方，反映接收方容量。 拥塞窗口：发送方根据自己估算的网络拥塞程度而设置的窗口值，反映网络当前容量。 发送端有发送缓存窗口，接收端有接收缓存窗口。 流量控制 如果发送方把数据发送得过快，接收方可能会来不及接收，这就会造成数据的丢失。所谓流量控制就是让发送方的发送速率不要太快，要让接收方来得及接收。 在通信过程中，接收方根据自己的接收缓存大小、动态地调整发送窗口大小，即接收窗口rwnd(receive window接收方设置确认报文段的窗口字段来将rwnd通知给发送方),发送方的发送窗口取决于接收窗口rwnd和拥塞窗口cwnd的最小值）。 设A向B发送数据。在连接建立时，B告诉了A：“我的接收窗口是 rwnd = 400 ”(这里的 rwnd 表示 receiver window) 。因此，发送方的发送窗口不能超过接收方给出的接收窗口的数值。请注意，TCP的窗口单位是字节，不是报文段。 TCP为每一个连接设有一个持续计时器(persistence timer)。只要TCP连接的一方收到对方的零窗口通知，就启动持续计时器。若持续计时器设置的时间到期，就发送一个零窗口控测报文段（携1字节的数据），那么收到这个报文段的一方就重新设置持续计时器 拥塞控制 ","date":"2017-02-01","objectID":"/posts/network_tcp/:0:0","tags":["网络"],"title":"网络—TCP","uri":"/posts/network_tcp/"},{"categories":["笔记"],"content":"慢开始和拥塞避免 上面讲到，窗口可以提高传输效率，但是刚开始窗口就比较大，就很可能造成堵塞。所以提出一个慢启动的概念,即刚开始还是先发送1个，慢慢增加这就是慢开始，2个、4个、8个、这样指数型增长，指数增长速度极快，到达一个点后，开始慢速增长，就是为了拥塞避免，这时候换成线性增长，一次增加一个。我们把这个转换的转折点叫做门限ssthresh。发送方维持一个叫做拥塞窗口 cwnd (congestion window)的状态变量。拥塞窗口的大小取决于网络的拥塞程度，并且动态地在变化。 我们图中可以看到，门限变为拥塞窗口的一半，再从1开始慢慢增加。 问题来了，如果网络一直不拥塞就可以一直增大吗？不会，发送的窗口大小还要受接收窗口rwnd大小的限制，所以 客户端发送窗口的上限 = min(rwnd,cwnd) ","date":"2017-02-01","objectID":"/posts/network_tcp/:1:0","tags":["网络"],"title":"网络—TCP","uri":"/posts/network_tcp/"},{"categories":["笔记"],"content":"快重传 又一个问题来了，怎么及时知道网络拥塞了？ 快重传算法首先要求接收方每收到一个失序的报文段后就立即发出重复确认。这样做可以让发送方及早知道有报文段没有到达接收方。每当比期望序号大的失序报文到达时，就发送ack号为期望号。 发送方只要一连收到三个重复确认就应当立即重传对方尚未收到的报文段。 不难看出，快重传并非取消重传计时器，而是在某些情况下可更早地重传丢失的报文段。 ","date":"2017-02-01","objectID":"/posts/network_tcp/:2:0","tags":["网络"],"title":"网络—TCP","uri":"/posts/network_tcp/"},{"categories":["笔记"],"content":"快恢复 当发送端收到连续三个重复的确认时，就执行“乘法减小”算法，把慢开始门限 ssthresh 减半。但接下去不执行慢开始算法。 换句话说，拥塞后并不是从1开始慢慢增加，而是从拥塞窗口的一半也就是ssthresh处开始慢慢增加。 流量控制和拥塞控制有啥区别？ 流量控制是发送端控制的，根据接收端的情况选择发送速度，原理是通过滑动窗口的大小改变来实现。而拥塞控制是根据的传输线路的拥挤程度来选择发送速度。好比是两个车站间传送货物，一个是根据接收车站的接收效率决定，一个是根据路上的车辆拥挤决定。 三次握手 先举个形象的例子，两个人AB进行电话沟通，为了保证双向是通的，常进行以下对话： A：你能听到我说话吗？ B：可以，你能听到吗？ A：我也可以。 这样双方就知道无论是“去”还是“来”都是正常的，就可以聊天了。 第一次握手中seq=i的i值是随机的。 四次挥手 Tcp断开连接时需要4次挥手。 还是先举个例子，两个人进行电话沟通时突然有一个想挂断，一般会有如下对话： A：我不想说了（第一次挥手。A发送 FIN，表示不A不再说什么事情） B：好的 （第二次挥手。B发送ACK表示知道了，但是他可能有话没说话，会继续向A说） B：对了，还有一个事情我要说完…（B没说完话的话继续说） B：说完了，我也不想说了（第三次挥手。B发送FIN表示他也说完了） A：好的（第四次挥手。A发送ACK表示知道了，自此完全断开） 在释放连接时，由于TCP是全双工的，因此最后要由两端分别进行关闭，这个流程如下： 假设Client端发起中断连接请求，也就是发送FIN报文。Server端接到FIN报文后，意思是说\"我Client端没有数据要发给你了\"，但是如果你还有数据没有发送完成，则不必急着关闭Socket，可以继续发送数据。所以你先发送ACK，“告诉Client端，你的请求我收到了，但是我还没准备好，请继续你等我的消息”。这个时候Client端就进入FIN_WAIT状态，继续等待Server端的FIN报文。当Server端确定数据已发送完成，则向Client端发送FIN报文，“告诉Client端，好了，我这边数据发完了，准备好关闭连接了”。Client端收到FIN报文后，“就知道可以关闭连接了，但是他还是不相信网络，怕Server端不知道要关闭，所以发送ACK后进入TIME_WAIT状态，如果Server端没有收到ACK则可以重传。“，Server端收到ACK后，“就知道可以断开连接了”。Client端等待了2MSL后依然没有收到回复，则证明Server端已正常关闭，那好，我Client端也可以关闭连接了。Ok，TCP连接就这样关闭了！ 关闭连接有主动关闭和被动关闭一说，这里为了简化理解，我们以客户端作为主动关闭方，服务器为被动关闭方。 三次握手，四次挥手完整流程： 整个过程Client端所经历的状态如下： 而Server端所经历的过程如下： 【注意】 在TIME_WAIT状态中，如果TCP client端最后一次发送的ACK丢失了，它将重新发送。TIME_WAIT状态中所需要的时间是依赖于实现方法的。典型的值为30秒、1分钟和2分钟。等待之后连接正式关闭，并且所有的资源(包括端口号)都被释放。 我们不仅疑问，为啥断开的时候会比握手的时候多一次？ 第二次握手时，SYN+ACK其实可以放一起，但是第二次挥手时ACK 和 ACK却不能放一起，因为B也许有话没说完 粘包、拆包产生的原因 粘包、拆包问题的产生原因笔者归纳为以下3种： socket缓冲区与滑动窗口 MSS/MTU限制 Nagle算法 socket缓冲区与滑动窗口 每个TCP socket在内核中都有一个发送缓冲区(SO_SNDBUF )和一个接收缓冲区(SO_RCVBUF)，TCP的全双工的工作模式以及TCP的滑动窗口便是依赖于这两个独立的buffer的填充状态。 SO_SNDBUF： 进程发送的数据的时候假设调用了一个send方法，最简单情况(也是一般情况)，将数据拷贝进入socket的内核发送缓冲区之中，然后send便会在上层返回。换句话说，send返回之时，数据不一定会发送到对端去(和write写文件有点类似)，send仅仅是把应用层buffer的数据拷贝进socket的内核发送buffer中。 SO_RCVBUF： 把接受到的数据缓存入内核，应用进程一直没有调用read进行读取的话，此数据会一直缓存在相应socket的接收缓冲区内。再啰嗦一点，不管进程是否读取socket，对端发来的数据都会经由内核接收并且缓存到socket的内核接收缓冲区之中。read所做的工作，就是把内核缓冲区中的数据拷贝到应用层用户的buffer里面，仅此而已。 滑动窗口： TCP连接在三次握手的时候，会将自己的窗口大小(window size)发送给对方，其实就是SO_RCVBUF指定的值。之后在发送数据的时，发送方必须要先确认接收方的窗口没有被填充满，如果没有填满，则可以发送。 每次发送数据后，发送方将自己维护的对方的window size减小，表示对方的SO_RCVBUF可用空间变小。 当接收方处理开始处理SO_RCVBUF 中的数据时，会将数据从socket 在内核中的接受缓冲区读出，此时接收方的SO_RCVBUF可用空间变大，即window size变大，接受方会以ack消息的方式将自己最新的window size返回给发送方，此时发送方将自己的维护的接受的方的window size设置为ack消息返回的window size。 此外，发送方可以连续的给接受方发送消息，只要保证对方的SO_RCVBUF空间可以缓存数据即可，即window size\u003e0。当接收方的SO_RCVBUF被填充满时，此时window size=0，发送方不能再继续发送数据，要等待接收方ack消息，以获得最新可用的window size。 MSS/MTU分片 MTU (Maxitum Transmission Unit,最大传输单元)是链路层对一次可以发送的最大数据的限制。MSS(Maxitum Segment Size,最大分段大小)是TCP报文中data部分的最大长度，是传输层对一次可以发送的最大数据的限制。 SYN Flood洪泛攻击 原理：攻击者首先伪造地址对 服务器发起SYN请求，服务器回应(SYN+ACK)包，而真实的IP会认为，我没有发送请求，不作回应。服务 器没有收到回应，这样的话，服务器不知 道(SYN+ACK)是否发送成功，默认情况下会重试5次（tcp_syn_retries）。这样的话，对于服务器的内存，带宽都有很大的消耗。攻击者 如果处于公网，可以伪造IP的话，对于服务器就很难根据IP来判断攻击者，给防护带来很大的困难。 linux内核参数调优主要有下面三个： tcp_max_syn_backlog 从字面上就可以推断出是什么意思。在内核里有个队列用来存放还没有确认ACK的客户端请求，当等待的请求数大于tcp_max_syn_backlog时，后面的会被丢弃。 所以，适当增大这个值，可以在压力大的时候提高握手的成功率。手册里推荐大于1024。 tcp_synack_retries 这个是三次握手中，服务器回应ACK给客户端里，重试的次数。默认是5。显然攻击者是不会完成整个三次握手的，因此服务器在发出的ACK包在没有回应的情况下，会重试发送。当发送者是伪造IP时，服务器的ACK回应自然是无效的。 为了防止服务器做这种无用功，可以把tcp_synack_retries设置为0或者1。因为对于正常的客户端，如果它接收不到服务器回应的ACK包，它会再次发送SYN包，客户端还是能正常连接的，只是可能在某些情况下建立连接的速度变慢了一点。 tcp_syncookies Linux中SYN cookie是非常巧妙地利用了TCP规范来绕过了TCP连接建立过程的验证过程，从而让服务器的负载可以大大降低。 在三次握手中，当服务器回应（SYN + ACK）包后，客户端要回应一个n + 1的ACK到服务器。其中n是服务器自己指定的。当启用tcp_syncookies时，linux内核生成一个特定的n值，而不并把客户的连接放到半连接的队列里（即没有存储任何关于这个连接的信息）。当客户端提交第三次握手的ACK包时，linux内核取出n值，进行校验，如果通过，则认为这个是一个合法的连接。 面试问题 问题1. 为什么是四次挥手 发送FIN的一方就是主动关闭(客户端)，而另一方则为被动关闭(服务器)。当一方发送了FIN，则表示在这一方不再会有数据的发送。其中当被动关闭方受到对方的FIN时，此时往往可能还有数据需要发送过去，因此无法立即发送FIN(也就是无法将FIN与ACK合并发送)， 而是在等待自己的数据发送完毕后再单独发送FIN，因此整个过程需要四次交互。 问题2. 什么是半关闭 客户端在收到第一个FIN的ACK响应后，会进入FINWAIT2 状态时，此时服务器处于 CLOSEWAIT状态，这种状态就称之为半关闭。从半关闭到全关闭，需要等待第二次FIN的确认才算结束。此时，客户端要等到服务器的FIN才能进入TIMEWAIT， 如果对方迟迟不发送FIN呢，则会等待一段时间后超时，这个可以通过内核参数tcpfin_timeout控制，默认是60s。 问题3. RST 是什么，为什么会出现 RST 是一个特殊的标记，用来表示当前应该立即终止连接。以下这些情况都会产生RST： 向一个未被监听的端口发送数据 对方已经调用 close 关闭连接 存在一些数据未处理(接收缓冲区)，请求关","date":"2017-02-01","objectID":"/posts/network_tcp/:3:0","tags":["网络"],"title":"网络—TCP","uri":"/posts/network_tcp/"},{"categories":["笔记"],"content":"网络—UDP","date":"2017-02-01","objectID":"/posts/network_udp/","tags":["网络"],"title":"网络—UDP","uri":"/posts/network_udp/"},{"categories":["笔记"],"content":"udp首部格式 这个伪首部是模仿的IP数据报的首部，只有在计算校验和时才出现，不向下传递也不向上递交。 udp数据包的理论长度是多少，合适的udp数据包应该是多少呢？ 从udp数据包的包头可以看出，udp的最大包长度是2^16-1的个字节。由于udp包头占8个字节，而在ip层进行封装后的ip包头占去20字节，所以这个是udp数据包的最大理论长度是2^16-1-8-20=65507。 然而这个只是udp数据包的最大理论长度。UDP属于运输层，在传输过程中，udp包的整体是作为下层协议的数据字段进行传输的，它的长度大小受到下层ip层和数据链路层协议的制约。 udp是不会对应用层的数据报进行合并或拆分的。 ","date":"2017-02-01","objectID":"/posts/network_udp/:0:0","tags":["网络"],"title":"网络—UDP","uri":"/posts/network_udp/"},{"categories":["笔记"],"content":"网络—UDP和TCP区别","date":"2017-02-01","objectID":"/posts/network_udp_tcp%E5%8C%BA%E5%88%AB/","tags":["网络"],"title":"网络—UDP和TCP区别","uri":"/posts/network_udp_tcp%E5%8C%BA%E5%88%AB/"},{"categories":["笔记"],"content":"UDP特点： 面向无连接 首先 UDP 是不需要和 TCP一样在发送数据前进行三次握手建立连接的，想发数据就可以开始发送了。并且也只是数据报文的搬运工，不会对数据报文进行任何拆分和拼接操作。 UDP是面向报文的 发送方的UDP对应用程序交下来的报文，在添加首部后就向下交付IP层。UDP对应用层交下来的报文，既不合并，也不拆分，而是保留这些报文的边界。因此，应用程序必须选择合适大小的报文 不可靠性 首先不可靠性体现在无连接上，通信都不需要建立连接，想发就发，这样的情况肯定不可靠。 并且收到什么数据就传递什么数据，并且也不会备份数据，发送数据也不会关心对方是否已经正确接收到数据了。 再者网络环境时好时坏，但是 UDP 因为没有拥塞控制，一直会以恒定的速度发送数据。即使网络条件不好，也不会对发送速率进行调整。这样实现的弊端就是在网络条件不好的情况下可能会导致丢包，但是优点也很明显，在某些实时性要求高的场景（比如电话会议）就需要使用 UDP 而不是 TCP。 头部开销小，传输数据报文时是很高效的。 因此 UDP 的头部开销小，只有八字节，相比 TCP 的至少二十字节要少得多，在传输数据报文时是很高效的 TCP和UDP的比较 对比 ||UDP|TCP| |是否连接 |无连接 |面向连接| |是否可靠 |不可靠传输，不使用流量控制和拥塞控制 |可靠传输，使用流量控制和拥塞控制| |连接对象个数 |支持一对一，一对多，多对一和多对多交互通信 |只能是一对一通信| |传输方式 |面向报文 |面向字节流| |首部开销 |首部开销小，仅8字节 |首部最小20字节，最大60字节| |适用场景 |适用于实时应用（IP电话、视频会议、直播等） |适用于要求可靠传输的应用，例如文件传输| 总结 TCP向上层提供面向连接的可靠服务 ，UDP向上层提供无连接不可靠服务。 虽然 UDP 并没有 TCP 传输来的准确，但是也能在很多实时性要求高的地方有所作为 对数据准确性要求高，速度可以相对较慢的，可以选用TCP ","date":"2017-02-01","objectID":"/posts/network_udp_tcp%E5%8C%BA%E5%88%AB/:0:0","tags":["网络"],"title":"网络—UDP和TCP区别","uri":"/posts/network_udp_tcp%E5%8C%BA%E5%88%AB/"},{"categories":["笔记"],"content":"HTTP","date":"2017-01-02","objectID":"/posts/network_http/","tags":["网络"],"title":"HTTP","uri":"/posts/network_http/"},{"categories":["笔记"],"content":"HTTP/0.9 HTTP/0.9 虽然简单，但是它充分验证了 Web 服务的可行性」 首先它只有一个命令GET。 它没有HEADER等描述数据的信息。因为这个时候的请求非常简单，它需要达到的目的也非常简单，没有那么多数据格式。 服务器发送完内容之后，就关闭TCP连接。 HTTP/1.0 随着互联网的发展，之前的HTTP/0.9已经无法满足用户需求了，浏览器希望通过HTTP来传输脚本、样式、图片、音视频等不同类型的文件，所以在1996年HTTP进行了一次版本更新： 增加了HEAD、POST等新方法 增加了响应状态码，标记可能的错误原因 引入了协议版本号概念 引入了HTTP header的概念，让HTTP处理请求和响应更加灵活 传输的数据不再局限于文本 HTTP/1.0最主要的缺点还是跟HTTP/0.9一样，每一个TCP连接只能发送一个HTTP请求，服务器发送完响应，就关闭连接。 为了解决这个问题，在1.0版本使用了一个非标准的Connection头部字段。当客户端再请求头部信息里面带上Connection：keep-alive的时候，服务器在发送完响应数据之后，就不会断开TCP连接了，从而达到复用同一个TCP连接的目的。但是由于不是标准字段，不同的实现可能导致表现得不一致，因此不能从根本上解决这个问题。 HTTP/1.0最核心的改变是增加了头部设定，头部内容以键值对的形式设置。请求头部通过 Accept 字段来告诉服务端可以接收的文件类型，响应头部再通过 Content-Type 字段来告诉浏览器返回文件的类型。头部字段不仅用于解决不同类型文件传输的问题，也可以实现其他很多功能如缓存、认证信息等。 HTTP/1.0 并不是一个“标准”，只是一份参考文档，不具有实际的约束力。 HTTP/1.1 为了解决 HTTP/1.0 的问题，1999 年推出的 HTTP/1.1 有以下特点： 长连接：引入了 TCP 连接复用，即一个 TCP 默认不关闭，可以被多个请求复用 并发连接：对一个域名的请求允许分配多个长连接（缓解了长连接中的「队头阻塞」问题） 引入管道机制（pipelining），一个 TCP 连接，可以同时发送多个请求。（响应的顺序必须和请求的顺序一致，因此不常用） 增加了 PUT、DELETE、OPTIONS、PATCH 等新的方法 新增了一些缓存的字段（If-Modified-Since, If-None-Match） 请求头中引入了 range 字段，支持断点续传 允许响应数据分块（chunked），利于传输大文件 强制要求 Host 头，让互联网主机托管称为可能 ","date":"2017-01-02","objectID":"/posts/network_http/:0:0","tags":["网络"],"title":"HTTP","uri":"/posts/network_http/"},{"categories":["笔记"],"content":"http/1.1的keep-alive http 1.0中默认是关闭的，如果客户端浏览器支持Keep-Alive，那么就在HTTP请求头中添加一个字段 Connection: Keep-Alive，当服务器收到附带有Connection: Keep-Alive的请求时，它也会在响应头中添加一个同样的字段来使用Keep-Alive。 http 1.1中默认启用Keep-Alive， 默认情况下所在HTTP1.1中所有连接都被保持，除非在请求头或响应头中指明要关闭：Connection: Close ，这也就是为什么Connection: Keep-Alive字段再没有意义的原因。 ","date":"2017-01-02","objectID":"/posts/network_http/:1:0","tags":["网络"],"title":"HTTP","uri":"/posts/network_http/"},{"categories":["笔记"],"content":"HTTP管道机制（pipelining） 它指的是在一个TCP连接内，多个HTTP请求可以并行，客户端不用等待上一次请求结果返回，就可以发出下一次请求，但服务器端必须按照接收到客户端请求的先后顺序依次回送响应结果，以保证客户端能 够区分出每次请求的响应内容。 ","date":"2017-01-02","objectID":"/posts/network_http/:2:0","tags":["网络"],"title":"HTTP","uri":"/posts/network_http/"},{"categories":["笔记"],"content":"HTTP/1.1的局限 HTTP 1.1 还是暴露出一些局限性: 虽然加入 keep-alive 可以复用一部分连接，但域名分片等情况下仍然需要建立多个 connection，耗费资源，给服务器带来性能压力。 pipeling 只部分解决了队头阻塞（ HOLB）。 HTTP 1.1 尝试使用 pipeling 来解决队头阻塞问题，即浏览器可以一次性发出多个请求（同个域名、同一条 TCP 链接）。 但 pipeling 要求返回是按序的，那么前一个请求如果很耗时（比如处理大图片），那么后面的请求即使服务器已经处理完，仍会等待前面的请求处理完才开始按序返回。 协议开销大，没有相应的压缩传输优化方案。 HTTP/1.1 在使用时，header 里携带的内容过大，在一定程度上增加了传输的成本，并且每次请求 header 基本不怎么变化，尤其在移动端增加用户流量。 SPDY：HTTP1.X的优化（改进版HTTP/1.1） 2012年google提出了SPDY的方案，优化了HTTP1.X的请求延迟，解决了HTTP1.X的安全性，具体如下： 降低延迟： 针对HTTP高延迟的问题，SPDY优雅的采取了多路复用（multiplexing）。多路复用通过多个请求stream共享一个tcp连接的方式，解决了HOL blocking的问题，降低了延迟同时提高了带宽的利用率。 请求优先级（request prioritization）：多路复用带来一个新的问题是，在连接共享的基础之上有可能会导致关键请求被阻塞。SPDY允许给每个request设置优先级，这样重要的请求就会优先得到响应。比如浏览器加载首页，首页的html内容应该优先展示，之后才是各种静态资源文件，脚本文件等加载，这样可以保证用户能第一时间看到网页内容。 header压缩：前面提到HTTP1.x的header很多时候都是重复多余的。选择合适的压缩算法可以减小包的大小和数量。 基于HTTPS的加密协议传输：大大提高了传输数据的可靠性。 服务端推送(server push）：可以让服务端主动把资源文件推送给客户端。当然客户端也有权利选择是否接收。 HTTP/2.0 因为必须要保持功能上的兼容，所以 HTTP/2 把 HTTP 分解成了“语义”和“语法”两个部分，“语义”层不做改动，与 HTTP/1 完全一致（即 RFC7231）。比如请求方法、URI、状态码、头字段等概念都保留不变，这样就消除了再学习的成本，基于 HTTP 的上层应用也不需要做任何修改，可以无缝转换到 HTTP/2。 ","date":"2017-01-02","objectID":"/posts/network_http/:3:0","tags":["网络"],"title":"HTTP","uri":"/posts/network_http/"},{"categories":["笔记"],"content":"头部压缩 由于报文 Header 一般会携带“User Agent”“Cookie”“Accept”“Server”等许多固定的头字段，多达几百字节甚至上千字节，但 Body 却经常只有几十字节（比如 GET 请求、204/301/304 响应），成了不折不扣的“大头儿子”。更要命的是，成千上万的请求响应报文里有很多字段值都是重复的，非常浪费，“长尾效应”导致大量带宽消耗在了这些冗余度极高的数据上。 专门的“HPACK”算法，在客户端和服务器两端建立“字典”，用索引号表示重复的字符串，还釆用哈夫曼编码来压缩整数和字符串，可以达到 50%~90% 的高压缩率。 ","date":"2017-01-02","objectID":"/posts/network_http/:4:0","tags":["网络"],"title":"HTTP","uri":"/posts/network_http/"},{"categories":["笔记"],"content":"二进制格式 HTTP/1 里纯文本形式的报文了，它的优点是“一目了然”，用最简单的工具就可以开发调试，非常方便。 但 HTTP/2 在这方面没有“妥协”，决定改变延续了十多年的现状，不再使用肉眼可见的 ASCII 码，而是向下层的 TCP/IP 协议“靠拢”，全面采用二进制格式。 把原来的“Header+Body”的消息“打散”为数个小片的二进制“帧”（Frame），用“HEADERS”帧存放头数据、“DATA”帧存放实体数据。 ","date":"2017-01-02","objectID":"/posts/network_http/:5:0","tags":["网络"],"title":"HTTP","uri":"/posts/network_http/"},{"categories":["笔记"],"content":"虚拟的“流” 消息的“碎片”到达目的地后应该怎么组装起来呢？ HTTP/2 为此定义了一个“流”（Stream）的概念，它是二进制帧的双向传输序列，同一个消息往返的帧会分配一个唯一的流 ID。你可以想象把它成是一个虚拟的“数据流”，在里面流动的是一串有先后顺序的数据帧，这些数据帧按照次序组装起来就是 HTTP/1 里的请求报文和响应报文。 因为“流”是虚拟的，实际上并不存在，所以 HTTP/2 就可以在一个 TCP 连接上用“流”同时发送多个“碎片化”的消息，这就是常说的“多路复用”（ Multiplexing）——多个往返通信都复用一个连接来处理。 在“流”的层面上看，消息是一些有序的“帧”序列，而在“连接”的层面上看，消息却是乱序收发的“帧”。多个请求 / 响应之间没有了顺序关系，不需要排队等待，也就不会再出现“队头阻塞”问题，降低了延迟，大幅度提高了连接的利用率。 为了更好地利用连接，加大吞吐量，HTTP/2 还添加了一些控制帧来管理虚拟的“流”，实现了优先级和流量控制。 ","date":"2017-01-02","objectID":"/posts/network_http/:6:0","tags":["网络"],"title":"HTTP","uri":"/posts/network_http/"},{"categories":["笔记"],"content":"server push HTTP/2.0协议引入了Server Push机制，可以让服务端在不经过客户端请求的情况下向客户端推送资源，提高页面加载速度和性能。 具体实现步骤如下： 在HTTP/2.0的请求头中添加Link字段，指定需要推送的资源。例如： Link: \u003c/css/main.css\u003e; rel=preload; as=style 其中，\u003c/css/main.css\u003e表示需要推送的资源路径，rel=preload表示预加载，as=style表示资源类型为CSS。 服务端收到客户端请求后，解析请求头中的Link字段，判断是否需要推送资源。 如果需要推送资源，则使用HTTP/2.0的推送机制，将资源推送给客户端。推送的过程类似于HTTP/2.0的请求响应过程，但是不需要等待客户端请求，可以直接推送资源。 客户端收到推送的资源后，可以直接使用，无需再次请求。 需要注意的是，Server Push机制并不是适用于所有场景的，需要根据具体情况进行判断和使用。同时，由于推送的资源并不是客户端主动请求的，可能会出现浪费的情况，需要权衡利弊。 http2与websocket HTTP2虽然支持服务器推送资源到客户端，但那不是应用程序可以感知的，主要是让浏览器（用户代理）提前缓存静态资源，所以我们不能指望HTTP2可以像WebSocket建立双向实时通信。 ","date":"2017-01-02","objectID":"/posts/network_http/:7:0","tags":["网络"],"title":"HTTP","uri":"/posts/network_http/"},{"categories":["笔记"],"content":"响应无序，怎么与请求对应 HTTP/2.0引入了多路复用（Multiplexing）的机制，允许客户端同时发送多个请求，而这些请求可以在一个TCP连接上进行交错发送和接收。由于这种交错的特性，HTTP/2.0中的响应是无序的。 为了解决这个问题，HTTP/2.0引入了流（Stream）的概念。每个流代表一个独立的请求和响应。客户端和服务器可以通过流标识符（Stream Identifier）进行流的标识和区分。客户端在发送请求时分配一个唯一的流标识符，服务器在返回响应时使用相同的流标识符来标识响应属于哪个请求。 客户端可以通过流的标识符来将响应与请求对应。当客户端收到响应后，可以使用流标识符来确定响应属于哪个请求，从而完成请求和响应的对应。 ","date":"2017-01-02","objectID":"/posts/network_http/:8:0","tags":["网络"],"title":"HTTP","uri":"/posts/network_http/"},{"categories":["笔记"],"content":"MSS和MTU","date":"2017-01-02","objectID":"/posts/network_mss_mtu/","tags":["网络"],"title":"MSS和MTU","uri":"/posts/network_mss_mtu/"},{"categories":["笔记"],"content":"分段特指发生在使用TCP协议的传输层中的数据切分行为 分片特指发生在使用IPv4协议的网络IP层中的数据切分行为 最大传输单元(Maximum Transmission Unit) 最大传输单元(Maximum Transmission Unit)，即MTU，为数据链路层的最大载荷上限(即IP数据报最大长度)，每段链路的MTU可能都不相同，一条端到端路径的MTU由这条路径上MTU最小的那段链路的MTU决定。 MTU是链路层中的网络对数据帧的一个限制，以以太网为例，MTU通常为1500字节，采用巨帧(Jumbo Frame)时可以达到9000字节。所谓的MTU，是二层协议的一个限制，对不同的二层协议可能有不同的值，只有二层协议为以太网(Ethernet)时，MTU一般才取1500字节，注意它不是物理链路介质的限制，只有工作在二层的设备才需要指定MTU的值，如网卡、转发设备端口(统称为网络接口)等，通过同一段线缆直连的通信端口或网卡，其MTU值一定相同。 一个IP数据报在以太网中传输，如果它的长度大于当前链路MTU值，就要进行分片传输(这里指IP层分片)，使得每片数据报的长度都不超过MTU。分片传输的IP数据报不一定按序到达，但IP首部中的信息能让这些数据报片按序组装。IP数据报的分片与重组是在网络IP层完成的。 最大报文段长度(Maximum Segment Size) 最大报文段长度(Maximum Segment Size)，即MSS，为TCP传输层的最大载荷上限(即应用层数据最大长度)，TCP三次握手期间通过TCP首部选项中的MSS字段通知对端，通常一条TCP连接的MSS取通信双方较小的那一个MSS值,与MTU的换算关系为： MTU = MSS + TCP首部长度 + IP首部长度 故在以太网中(网络层以IPv4为例)： MSS = 以太网MTU - TCP首部长度 - IPv4首部长度 = 1500 - 20 - 20 = 1460字节 未指定MSS时默认值为536字节，这是因为在Internet中标准的MTU值为576字节，576字节MTU = TCP首部长度20字节 + IPv4首部长度20字节 + 536字节MSS。 一个应用程序如果要发送超过MSS大小的数据，就要进行分段传输(这里指TCP分段)，使得每个报文段长度都不超过MSS。分片传输的TCP报文段不一定按序到达，但实现可靠传输的TCP协议中有处理乱序的机制，即利用报文段序列号在接收缓冲区进行数据重排以实现重组。TCP分段的重组是在TCP传输层完成的。 为什么IP层会分片，TCP还要分段? 由于本身IP层就会做分片这件事情。就算TCP不分段，到了IP层，数据包也会被分片，数据也能正常传输。 既然网络层就会分片了，那么TCP为什么还要分段？是不是有些多此一举？ 假设有一份数据，较大，且在TCP层不分段，如果这份数据在发送的过程中出现丢包现象，TCP会发生重传，那么重传的就是这一大份数据（虽然IP层会把数据切分为MTU长度的N多个小包，但是TCP重传的单位却是那一大份数据）。 区别 有了前文的知识准备，不难得出结论： TCP分段的原因是因为TCP报文段大小受MSS限制 IP分片的原因则是因为IP数据报大小受MTU限制 一个值得注意的是，在分片的数据中，传输层的首部只会出现在第一个分片中，IP数据报分片后，只有第一片带有传输层首部(UDP或ICMP等)，后续分片只有IP首部和应用数据，到了目的地后根据IP首部中的信息在网络层进行重组，这一步骤对上层透明，即传输层根本不知道IP层发生了分片与重组。而TCP报文段的每个分段中都有TCP首部，到了目的地后根据TCP首部的信息在传输层进行重组。 ","date":"2017-01-02","objectID":"/posts/network_mss_mtu/:0:0","tags":["网络"],"title":"MSS和MTU","uri":"/posts/network_mss_mtu/"},{"categories":["笔记"],"content":"HTTPS","date":"2017-01-01","objectID":"/posts/network_https/","tags":["网络"],"title":"HTTPS","uri":"/posts/network_https/"},{"categories":["笔记"],"content":"为什么要有https http是明文，容易被监听拦截。 加密呗，用对称加密，秘钥传输的时候明文传输，会被监听拦截。 可以使用非对称加密，但是非对称加密过程慢，可以采用非对称+对称加密组合的方式，即用非对称加密方式来协商对称加密秘钥。 具体是这样子的： 服务器用明文的方式给客户端发送自己的公钥，客户端收到公钥之后，会生成一把密钥(对称加密用的)，然后用服务器的公钥对这把密钥进行加密，之后再把密钥传输给服务器，服务器收到之后进行解密，最后服务器就可以安全着得到这把密钥了，而客户端也有同样一把密钥，他们就可以进行对称加密了。 最后中间人再对这把密钥用刚才服务器的公钥进行加密，再发给服务器。如图： 毫无疑问，在这个过程中，中间人获取了对称加密中的密钥，在之后服务器和客户端的对称加密传输中，这些加密的数据对中间人来说，和明文没啥区别。 4. 非对称加密之所以不安全，是因为客户端不确定这个公钥是来自正确的服务器，解决了这个问题就好办了。数字证书登场。 5. 我们需要找到一个拥有公信力、大家都认可的认证中心(CA)。 服务器在给客户端传输公钥的过程中，会把公钥以及服务器的个人信息通过Hash算法生成信息摘要。如图 并且，最后还会把原来没Hash算法之前的个人信息以及公钥 和 数字签名合并在一起，形成数字证书。如图 当客户端拿到这份数字证书之后，就会用CA提供的公钥来对数字证书里面的数字签名进行解密来得到信息摘要，然后对数字证书里服务器的公钥以及个人信息进行Hash得到另外一份信息摘要。最后把两份信息摘要进行对比，如果一样，则证明这个人是服务器，否则就不是。如图： 什么是https https简单来说就是https = http + ssl(tls) 这样，http在到达tcp层之前就加密一次 简单一句话就是：https先利用非对称加密（RSA等）来协商对称秘钥，后续就用对称加密算法（AES等）来加密传输。 数字证书 服务端如果直接把公钥明文传输给客户端，容易被中间人接收。客户端必须知道接收的公钥是合法的。为了解决这个问题，把公钥告诉权威机构，来生成数字证书以便客户端验证。 验证证书的过程如下： 通过 HTTPS 建立了一个安全 Web 事务之后，现代的浏览器都会自动获取所连接服 务器的数字证书。如果服务器没有证书，安全连接就会失败。 浏览器收到证书时会对签名颁发机构进行检查。如果这个机构是个很有权威的公共签名机构，浏览器可能已经知道其公开密钥了(浏览器会预先安装很多签名颁发机构的证书)。 如果对签名颁发机构一无所知，浏览器就无法确定是否应该信任这个签名颁发机构， 它通常会向用户显示一个对话框，看看他是否相信这个签名发布者。签名发布者可 能是本地的 IT 部门或软件厂商。 数字证书校验一般是校验公钥是否正确，域名、有效期和是否被吊销等。 具体的证书生成及验证流程： CA 把有效日期，服务端公钥等基础信息hash取数据摘要，并对摘要进行CA私钥加密生成数字签名，签名和基础信息一同组成数字证书。 客户端一般有知名的CA公钥，先判断是否有对应的CA公钥，如果没有直接弹框不信任。如果有则对证书的签名进行CA公钥解密，然后按照和CA相同的hash方法对基础信息取得摘要，摘要和CA公钥解密的结果对比发现是一致的，则说明数据没有篡改过，顺利取得了服务端的公钥。这个方法很巧妙，即获得了公钥，而且还知道公钥是靠谱的。 数字签名 数字签名技术就是对“非对称密钥加解密”和“数字摘要“两项技术的应用，它将摘要信息用发送者的私钥加密，与原文一起传送给接收者。接收者只有用发送者的公钥才能解密被加密的摘要信息，然后用HASH函数对收到的原文产生一个摘要信息，与解密的摘要信息对比。如果相同，则说明收到的信息是完整的，在传输过程中没有被修改，否则说明信息被修改过，因此数字签名能够验证信息的完整性。 数字签名的过程如下： 明文 –\u003e hash运算 –\u003e 摘要 –\u003e 私钥加密 –\u003e 数字签名 数字签名有两种功效： 一、能确定消息确实是由发送方签名并发出来的，因为别人假冒不了发送方的签名。 二、数字签名能确定消息的完整性。 注意： 数字签名只能验证数据的完整性，数据本身是否加密不属于数字签名的控制范围 ","date":"2017-01-01","objectID":"/posts/network_https/:0:0","tags":["网络"],"title":"HTTPS","uri":"/posts/network_https/"},{"categories":["笔记"],"content":"ca数字证书 我们知道了服务端端的数字证书是来自ca的。验证服务端证书的时候需要用ca的公钥进行解密，然后校验。 怎么知道ca的公钥是安全的，不是伪造的？ 世界上的CA认证中心不止一家， CA认证中心之间是一个树状结构，根CA认证中心可以授权多个二级的CA认证中心，同理二级CA认证中心也可以授权多个3级的CA认证中心…如果你是数字证书申请人(比如说：交通银行)，你可以向根CA认证中心，或者二级，三级的CA认证中心申请数字证书，这是没有限制的，当你成功申请后，你就称为了数字证书所有人。值得注意的是，根CA认证中心是有多个的，也就是说会有多棵这样的结构树。 实际上每个CA认证中心/数字证书所有人，他们都有一个数字证书，和属于自己的RSA公钥和密钥，这些是他们的父CA认证中心给他们颁发的。 CA的数字证书和服务端的数字证书差不多，上一级Ca的秘钥对Hash(自己的ca信息)的结果进行加密。校验的时候需要上一级的公钥来解密，以此来验证该ca的证书是真的，那上一级的公钥怎么保证是安全的呢？就要再找上上一级的公钥，这样递归了，直到找到根证书，那根证书怎么保证呢，一般操作系统中都内置了根证书。 SSL握手流程 握手阶段分成五步。 第一步，爱丽丝给出协议版本号、一个客户端生成的随机数（Client random），以及客户端支持的加密方法。 第二步，鲍勃确认双方使用的加密方法，并给出数字证书、以及一个服务器生成的随机数（Server random）。 第三步，爱丽丝确认数字证书有效，然后生成一个新的随机数（Premaster secret），并使用数字证书中的公钥，加密这个随机数，发给鲍勃。 第四步，鲍勃使用自己的私钥，获取爱丽丝发来的随机数（即Premaster secret）。 第五步，爱丽丝和鲍勃根据约定的加密方法，使用前面的三个随机数，生成\"对话密钥\"（session key），用来加密接下来的整个对话过程。 注意： 整个握手阶段都不加密（也没法加密），都是明文的。因此，如果有人窃听通信，他可以知道双方选择的加密方法，以及三个随机数中的两个。整个通话的安全，只取决于第三个随机数（Premaster secret）能不能被破解。 ","date":"2017-01-01","objectID":"/posts/network_https/:1:0","tags":["网络"],"title":"HTTPS","uri":"/posts/network_https/"},{"categories":["笔记"],"content":"服务端对客户端验证 对于非常重要的保密数据，服务端还需要对客户端进行验证，以保证数据传送给了安全的合法的客户端。服务端可以向客户端发出 Cerficate Request 消息，要求客户端发送证书对客户端的合法性进行验证。比如，金融机构往往只允许认证客户连入自己的网络，就会向正式客户提供USB密钥，里面就包含了一张客户端证书。 抓包原理 HTTPS即使安全，也是能够被抓包的，常见的抓包工具有：Charles、fildder等。 常用的HTTPS抓包方式是作为中间人，对客户端伪装成服务端，对服务端伪装成客户端。简单来说： 截获客户端的HTTPS请求，伪装成中间人客户端去向服务端发送HTTPS请求 接受服务端返回，用自己的证书伪装成中间人服务端向客户端发送数据内容。 具体过程如下图所示： 反抓包策略 为了防止中间人攻击，可以使用SSL-Pinning的技术来反抓包。 可以发现中间人攻击的要点的伪造了一个假的服务端证书给了客户端，客户端误以为真。解决思路就是，客户端也预置一份服务端的证书，比较一下就知道真假了。 SSL-pinning有两种方式： 证书锁定（Certificate Pinning） 和公钥锁定（ Public Key Pinning）。 证书锁定 需要在客户端代码内置仅接受指定域名的证书，而不接受操作系统或浏览器内置的CA根证书对应的任何证书，通过这种授权方式，保障了APP与服务端通信的唯一性和安全性，因此客户端与服务端（例如API网关）之间的通信是可以保证绝对安全。但是CA签发证书都存在有效期问题，缺点是在 证书续期后需要将证书重新内置到APP中。 公钥锁定 提取证书中的公钥并内置到客户端中，通过与服务器对比公钥值来验证连接的正确性。制作证书密钥时，公钥在证书的续期前后都可以保持不变（即密钥对不变），所以可以避免证书有效期问题，一般推荐这种做法。 没有绝对的安全，这不能挡住逆向反编译。 问题 HTTPS和HTTP的区别 https协议需要到CA申请证书。 http是超文本传输协议，信息是明文传输；https 则是具有安全性的ssl加密传输协议。 http和https使用的是完全不同的连接方式，用的端口也不一样，前者是80，后者是443。 http的连接很简单，是无状态的；HTTPS协议是由SSL+HTTP协议构建的可进行加密传输、身份认证的网络协议，比http协议安全。 抓包 Q: 使用 HTTPS 会被抓包吗？ A: 会被抓包，HTTPS 只防止用户在不知情的情况下通信被监听，如果用户主动授信，是可以构建“中间人”网络，代理软件可以对传输内容进行解密。 我们使用抓包工具的第一步就是在你自己设备中信任 Charles 的 CA 证书，在自己的设备中添加了一个 CA，请求的时候，Charles 通过自己的 CA 签名了一个自己的公钥，发送给客户端，客户端就误以为是服务器了，这样之后的流程都会先走到 Charles 然后才会走到目标服务器。 Charles 扮演了一个中间人的角色，而且这个中间人是我们自己设置的。 因此要想防止抓包，应用应该自己做处理。 为什么一定要用三个随机数来生成”会话密钥”呢？ 机器产生的随机数也许是一个范围的伪随机数，容易被猜到破解，如果随机数不随机，那么premaster secret就有可能被猜出来，那么仅适用premaster secret作为密钥就不合适了，因此必须引入新的随机因素，那么客户端和服务器加上premaster secret三个随机数一同生成的密钥就不容易被猜出了，一个伪随机数可能完全不随机，可是三个伪随机数就十分接近随机了。 参考资料： http://www.wxtlife.com/2016/03/27/%E8%AF%A6%E8%A7%A3https%E6%98%AF%E5%A6%82%E4%BD%95%E7%A1%AE%E4%BF%9D%E5%AE%89%E5%85%A8%E7%9A%84%EF%BC%9F/ https://mp.weixin.qq.com/s?__biz=Mzg2NzA4MTkxNQ==\u0026mid=2247485216\u0026idx=1\u0026sn=fd119ae8e9d184a81cc1dd2984e8d4b8\u0026scene=21#wechat_redirect ","date":"2017-01-01","objectID":"/posts/network_https/:2:0","tags":["网络"],"title":"HTTPS","uri":"/posts/network_https/"}]