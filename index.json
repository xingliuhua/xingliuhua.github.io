[{"categories":["笔记"],"content":"linux","date":"2021-08-22","objectID":"/posts/linux_%E8%BD%AF%E7%A1%AC%E9%93%BE%E6%8E%A5/","tags":["linux"],"title":"linux软连接和硬连接","uri":"/posts/linux_%E8%BD%AF%E7%A1%AC%E9%93%BE%E6%8E%A5/"},{"categories":["笔记"],"content":"inode 文件还有元数据部分，例如名字、权限等，这就需要一个结构inode来存放。 什么是 inode 呢？inode 的“i”是 index 的意思，其实就是“索引”，类似图书馆的索引区域。既然如此，我们每个文件都会对应一个 inode；一个文件夹就是一个文件，也对应一个 inode。 struct ext4_inode { __le16 i_mode; /* File mode */ __le16 i_uid; /* Low 16 bits of Owner Uid */ __le32 i_size_lo; /* Size in bytes */ __le32 i_atime; /* Access time */ __le32 i_ctime; /* Inode Change time */ __le32 i_mtime; /* Modification time */ __le32 i_dtime; /* Deletion Time */ __le16 i_gid; /* Low 16 bits of Group Id */ __le16 i_links_count; /* Links count */ __le32 i_blocks_lo; /* Blocks count */ __le32 i_flags; /* File flags */ ...... __le32 i_block[EXT4_N_BLOCKS];/* Pointers to blocks */ __le32 i_generation; /* File version (for NFS) */ __le32 i_file_acl_lo; /* File ACL */ __le32 i_size_high; ...... }; 从这个数据结构中，我们可以看出，inode 里面有文件的读写权限 i_mode，属于哪个用户 i_uid，哪个组 i_gid，大小是多少 i_size_io，占用多少个块 i_blocks_io。咱们讲 ls 命令行的时候，列出来的权限、用户、大小这些信息，就是从这里面取出来的。 另外，这里面还有几个与文件相关的时间。i_atime 是 access time，是最近一次访问文件的时间；i_ctime 是 change time，是最近一次更改 inode 的时间；i_mtime 是 modify time，是最近一次更改文件的时间。 这里你需要注意区分几个地方。首先，访问了，不代表修改了，也可能只是打开看看，就会改变 access time。其次，修改 inode，有可能修改的是用户和权限，没有修改数据部分，就会改变 change time。只有数据也修改了，才改变 modify time。 我们平时使用的ls -l里面的数据其实就是取自inode。 硬连接和软连接 一种特殊的文件格式，硬链接（Hard Link）和软链接（Symbolic Link）。 ln [参数][源文件或目录][目标文件或目录] #为a生成一个软连接a1 ln -s a a1 #为a生成一个硬连接a2 ln a a2 硬连接时，a2和a共用一个inode,ls时，a2文件和a信息是一样的。但是 inode 是不跨文件系统的，每个文件系统都有自己的 inode 列表，因而硬链接是没有办法跨文件系统的。 但是软连接就不一样了，a1是单独的inode节点。 -rw-rw-r-- 2 xingliuhua xingliuhua 0 Aug 27 17:21 a lrwxrwxrwx 1 xingliuhua xingliuhua 1 Aug 27 17:22 a1 -\u003e a -rw-rw-r-- 2 xingliuhua xingliuhua 0 Aug 27 17:21 a2 还要注意，硬连接的开头并不是l,而软连接开头是l。 硬链接特点 具有相同inode节点号的多个文件互为硬链接文件； 删除硬链接文件或者删除源文件任意之一，文件实体并未被删除； 只有删除了源文件和所有对应的硬链接文件，文件实体才会被删除； 硬链接文件是文件的另一个入口； 可以通过给文件设置硬链接文件来防止重要文件被误删； 创建硬链接命令 ln 源文件 硬链接文件； 硬链接文件是普通文件，可以用rm删除； 对于静态文件（没有进程正在调用），当硬链接数为0时文件就被删除。注意：如果有进程正在调用，则无法删除或者即使文件名被删除但空间不会释放。 软链接特点 软链接类似windows系统的快捷方式； 软链接里面存放的是源文件的路径，指向源文件； 删除源文件，软链接依然存在，但无法访问源文件内容； 软链接失效时一般是白字红底闪烁； 创建软链接命令 ln -s 源文件 软链接文件； 软链接和源文件是不同的文件，文件类型也不同，inode号也不同； 软链接的文件类型是“l”，可以用rm删除。 硬链接和软链接的区别 原理上，硬链接和源文件的inode节点号相同，两者互为硬链接。 软连接和源文件的inode节点号不同，进而指向的block也不同，软连接block中存放了源文件的路径名。所以才能打开时看到是源文件的内容。 使用限制上，不能对目录创建硬链接，不能对不同文件系统创建硬链接，不能对不存在的文件创建硬链接；可以对目录创建软连接，可以跨文件系统创建软连接，可以对不存在的文件创建软连接。 ","date":"2021-08-22","objectID":"/posts/linux_%E8%BD%AF%E7%A1%AC%E9%93%BE%E6%8E%A5/:0:0","tags":["linux"],"title":"linux软连接和硬连接","uri":"/posts/linux_%E8%BD%AF%E7%A1%AC%E9%93%BE%E6%8E%A5/"},{"categories":["笔记"],"content":"channel","date":"2021-08-19","objectID":"/posts/go_channel/","tags":["golang"],"title":"channel","uri":"/posts/go_channel/"},{"categories":["笔记"],"content":"关闭不再需要使用的 channel 并不是必须的。跟其他资源比如打开的文件、socket 连接不一样，这类资源使用完后不关闭后会造成句柄泄露，channel 使用完后不关闭也没有关系，channel 没有被任何协程用到后最终会被 GC 回收。关闭 channel 一般是用来通知其他协程某个任务已经完成了。golang 也没有直接提供判断 channel 是否已经关闭的接口。 ","date":"2021-08-19","objectID":"/posts/go_channel/:0:0","tags":["golang"],"title":"channel","uri":"/posts/go_channel/"},{"categories":["笔记"],"content":"golang slice \u0026 map","date":"2021-08-19","objectID":"/posts/go_%E5%BC%95%E7%94%A8%E7%A7%81%E6%9C%89%E4%BB%93%E5%BA%93/","tags":["golang"],"title":"golang 引用私有仓库","uri":"/posts/go_%E5%BC%95%E7%94%A8%E7%A7%81%E6%9C%89%E4%BB%93%E5%BA%93/"},{"categories":["笔记"],"content":"` ","date":"2021-08-19","objectID":"/posts/go_%E5%BC%95%E7%94%A8%E7%A7%81%E6%9C%89%E4%BB%93%E5%BA%93/:0:0","tags":["golang"],"title":"golang 引用私有仓库","uri":"/posts/go_%E5%BC%95%E7%94%A8%E7%A7%81%E6%9C%89%E4%BB%93%E5%BA%93/"},{"categories":["笔记"],"content":"直接使用go get 直接使用go get …添加私有仓库依赖时，会出现以下错误： get \"gitlab.com/xxx\": found meta tag get.metaImport{Prefix:\"gitlab.com/xxx\", VCS:\"git\", RepoRoot:\"https://gitlab.com/xxx.git\"} at //gitlab.com/xxx?go-get=1 go get gitlab.com/xxx: git ls-remote -q https://gitlab.com/xxx.git in /Users/sulin/go/pkg/mod/cache/vcs/91fae55e78195f3139c4f56af15f9b47ba7aa6ca0fa761efbd5b6e2b16d5159d: exit status 128: fatal: could not read Username for 'https://gitlab.com': terminal prompts disabled Confirm the import path was entered correctly. If this is a private repository, see https://golang.org/doc/faq#git_https for additional information. 从错误信息可以明显地看出来，我们使用私有仓库时通常会配置ssh-pubkey进行鉴权，但是go get使用https而非ssh的方式来下载依赖，从而导致鉴权失败。 如果配置了GOPROXY代理，错误信息则是如下样式： go get gitlab.com/xxx: module gitlab.com/xxx: reading https://goproxy.io/gitlab.com/xxx/@v/list: 404 Not Found 从错误信息可以看出，go get通过代理服务拉取私有仓库，而代理服务当然不可能访问到私有仓库，从而出现了404错误。 1.12版本解决方案 在1.11和1.12版本中，比较主流的解决方案是配置git强制采用ssh。 这个解决方案在许多博客、问答中都可以看到： git config --global url.\"git@gitlab.com:xxx/zz.git\".insteadof \"https://gitlab.com/xxx/zz.git\" 但是它与GOPROXY存在冲突，也就是说，在使用代理时，这个解决方案也是不生效的。 1.13版本解决方案 在1.13版本之后，前面介绍的解决方案又会导致go get出现另一种错误： get \"gitlab.com/xxx/zz\": found meta tag get.metaImport{Prefix:\"gitlab.com/xxx/zz\", VCS:\"git\", RepoRoot:\"https://gitlab.com/xxx/zz.git\"} at //gitlab.com/xxx/zz?go-get=1 verifying gitlab.com/xxx/zz@v0.0.1: gitlab.com/xxx/zz@v0.0.1: reading https://sum.golang.org/lookup/gitlab.com/xxx/zz@v0.0.1: 410 Gone 这个错误是因为新版本go mod会对依赖包进行checksum校验，但是私有仓库对sum.golang.org是不可见的，它当然没有办法成功执行checksum。 也就是说强制git采用ssh的解决办法在1.13版本之后GG了。 当然Golang在堵上窗户之前，也开了大门，它提供了一个更方便的解决方案：GOPRIVATE环境变量。解决以上的错误，可以这样配置： export GOPRIVATE=gitlab.com/xxx 它可以声明指定域名为私有仓库，go get在处理该域名下的所有依赖时，会直接跳过GOPROXY和CHECKSUM等逻辑，从而规避掉前文遇到的所有问题。 另外域名gitlab.com/xxx非常灵活，它默认是前缀匹配的，所有的gitlab.com/xxx前缀的依赖模块都会被视为private-modules，它对于企业、私有Group等有着一劳永逸的益处。 提示：如果你通过ssh公钥访问私有仓库，记得配置git拉取私有仓库时使用ssh而非https。 可以通过命令git config …的方式来配置。也可以像我这样，直接修改~/.gitconfig添加如下配置： [url \"git@github.com:\"] insteadOf = https://github.com/ [url \"git@gitlab.com:\"] insteadOf = https://gitlab.com/ 即可强制go get针对github.com与gitlab.com使用ssh而非https。 ","date":"2021-08-19","objectID":"/posts/go_%E5%BC%95%E7%94%A8%E7%A7%81%E6%9C%89%E4%BB%93%E5%BA%93/:0:1","tags":["golang"],"title":"golang 引用私有仓库","uri":"/posts/go_%E5%BC%95%E7%94%A8%E7%A7%81%E6%9C%89%E4%BB%93%E5%BA%93/"},{"categories":["笔记"],"content":"mysql innodb记录结构","date":"2021-07-27","objectID":"/posts/mysql_innodb%E8%AE%B0%E5%BD%95%E7%BB%93%E6%9E%84/","tags":["mysql"],"title":"mysql innodb记录结构","uri":"/posts/mysql_innodb%E8%AE%B0%E5%BD%95%E7%BB%93%E6%9E%84/"},{"categories":["笔记"],"content":"innodb记录结构 ","date":"2021-07-27","objectID":"/posts/mysql_innodb%E8%AE%B0%E5%BD%95%E7%BB%93%E6%9E%84/:0:0","tags":["mysql"],"title":"mysql innodb记录结构","uri":"/posts/mysql_innodb%E8%AE%B0%E5%BD%95%E7%BB%93%E6%9E%84/"},{"categories":["笔记"],"content":"页 我们知道读写磁盘的速度非常慢，和内存读写差了几个数量级，所以当我们想从表中获取某些记录时，InnoDB存储引擎需要一条一条的把记录 从磁盘上读出来么? 不，那样会慢死，InnoDB采取的方式是:将数据划分为若干个页，以页作为磁盘和内存之间交互的基本单位，InnoDB中页的大小一般为 16 KB。也就是在一般情况下，一次最少从磁 盘中读取16KB的内容到内存中，一次最少把内存中的16KB内容刷新到磁盘中。 ","date":"2021-07-27","objectID":"/posts/mysql_innodb%E8%AE%B0%E5%BD%95%E7%BB%93%E6%9E%84/:1:0","tags":["mysql"],"title":"mysql innodb记录结构","uri":"/posts/mysql_innodb%E8%AE%B0%E5%BD%95%E7%BB%93%E6%9E%84/"},{"categories":["笔记"],"content":"InnoDB行格式 我们平时是以记录为单位来向表中插入数据的，这些记录在磁盘上的存放方式也被称为行格式或者记录格式。设计InnoDB存储引擎的大叔们到现在为止设计了4种不同类型的行格式，分别 是Compact、Redundant、Dynamic和Compressed行格式。 我们可以在创建或修改表的语句中指定行格式: CREATE TABLE 表名 (列的信息) ROW_FORMAT=行格式名称 ALTER TABLE 表名 ROW_FORMAT=行格式名称 ","date":"2021-07-27","objectID":"/posts/mysql_innodb%E8%AE%B0%E5%BD%95%E7%BB%93%E6%9E%84/:2:0","tags":["mysql"],"title":"mysql innodb记录结构","uri":"/posts/mysql_innodb%E8%AE%B0%E5%BD%95%E7%BB%93%E6%9E%84/"},{"categories":["笔记"],"content":"行溢出数据现象 MySQL对一条记录占用的最大存储空间是有限制的，除了BLOB或者TEXT类型的列之外，其他所有的列(不包括隐藏列和记录头信息)占用的字节长度加起来不能超过65535个字 节。所以MySQL服务器建议我们把存储类型改为TEXT或者BLOB的类型。 MySQL中磁盘和内存交互的基本单位是页，也就是说MySQL是以页为基本单位来管理存储空间的， 我们的记录都会被分配到某个页中存储。而一个页的大小一般是16KB，也就是16384字节，而一个VARCHAR(M)类型的列就最多可以存储65532个字节，这样就可能造成一个页存放不了一条记录的尴尬情况。 在Compact和Reduntant行格式中，对于占用存储空间非常大的列，在记录的真实数据处只会存储该列的一部分数据，把剩余的数据分散存储在几个其他的页中，然后记录的真实数据处用20个字节存储指向这些 页的地址。 ","date":"2021-07-27","objectID":"/posts/mysql_innodb%E8%AE%B0%E5%BD%95%E7%BB%93%E6%9E%84/:2:1","tags":["mysql"],"title":"mysql innodb记录结构","uri":"/posts/mysql_innodb%E8%AE%B0%E5%BD%95%E7%BB%93%E6%9E%84/"},{"categories":["笔记"],"content":"行溢出的临界点 那发生行溢出的临界点是什么呢?也就是说在列存储多少字节的数据时就会发生行溢出? MySQL中规定一个页中至少存放两行记录,只要知道如果我们想一个行中存储了很大的数据时，可能发 生行溢出的现象。 ","date":"2021-07-27","objectID":"/posts/mysql_innodb%E8%AE%B0%E5%BD%95%E7%BB%93%E6%9E%84/:2:2","tags":["mysql"],"title":"mysql innodb记录结构","uri":"/posts/mysql_innodb%E8%AE%B0%E5%BD%95%E7%BB%93%E6%9E%84/"},{"categories":["笔记"],"content":"页结构 一个InnoDB数据页的存储空间大致被划分成了7个部分，有的部分占用的字节数是确定的，有的部分占用的字节数是不确定的: 不论我们怎么对页中的记录做增删改操作，InnoDB始终会维护一条记录的单链表，链表中的各个节点是按照主键值由小到大的顺序连接起来的。 主键值为2的记录被我们删掉了，但是存储空间却没有回收，如果我们再次把这条记录插入到表中，InnoDB并没有因为新记录的插入而为它申请新的存储空间，而是直接复用了原来被删除记录的存储空间 》每个数据页的File Header部分都有上一个和下一个页的编号，所以所有的数据页会组成一个双链表。 ","date":"2021-07-27","objectID":"/posts/mysql_innodb%E8%AE%B0%E5%BD%95%E7%BB%93%E6%9E%84/:3:0","tags":["mysql"],"title":"mysql innodb记录结构","uri":"/posts/mysql_innodb%E8%AE%B0%E5%BD%95%E7%BB%93%E6%9E%84/"},{"categories":["笔记"],"content":"数据页中的目录 主键值由小到大顺序串联成一个单链表，我们想根据主键值查找页中的某条记录需要遍历，有了目录就效率提高了。 在一个数据页中查找指定主键值的记录的过程分为两步: 通过二分法确定该记录所在的槽，并找到该槽中主键值最小的那条记录。 通过记录的next_record属性遍历该槽所在的组中的各个记录 ","date":"2021-07-27","objectID":"/posts/mysql_innodb%E8%AE%B0%E5%BD%95%E7%BB%93%E6%9E%84/:4:0","tags":["mysql"],"title":"mysql innodb记录结构","uri":"/posts/mysql_innodb%E8%AE%B0%E5%BD%95%E7%BB%93%E6%9E%84/"},{"categories":["笔记"],"content":"mysql字符集","date":"2021-07-27","objectID":"/posts/mysql_%E5%AD%97%E7%AC%A6%E9%9B%86/","tags":["mysql"],"title":"mysql字符集","uri":"/posts/mysql_%E5%AD%97%E7%AC%A6%E9%9B%86/"},{"categories":["笔记"],"content":"字符集及编码 ","date":"2021-07-27","objectID":"/posts/mysql_%E5%AD%97%E7%AC%A6%E9%9B%86/:0:0","tags":["mysql"],"title":"mysql字符集","uri":"/posts/mysql_%E5%AD%97%E7%AC%A6%E9%9B%86/"},{"categories":["笔记"],"content":"MySQL中的 utf8和 utf8mb4 我们上边说utf8字符集表示一个字符需要使用1~4个字节，但是我们常用的一些字符使用1~3个字节就可以表示了。而在MySQL中字符集表示一 个字符所用最大字节长度在某些方面会影响系统的存储和性能，所以设计MySQL的大叔偷偷的定义了两个概念: utf8mb3:阉割过的utf8字符集，只使用1~3个字节表示字符。 utf8mb4:正宗的utf8字符集，使用1~4个字节表示字符。 有一点需要大家十分的注意，在MySQL中utf8是utf8mb3的别名，所以之后在MySQL中提到utf8就意味着使用1~3个字节来表示一个字符，如果大 家有使用4字节编码一个字符的情况，比如存储一些emoji表情啥的，那请使用utf8mb4。 ","date":"2021-07-27","objectID":"/posts/mysql_%E5%AD%97%E7%AC%A6%E9%9B%86/:1:0","tags":["mysql"],"title":"mysql字符集","uri":"/posts/mysql_%E5%AD%97%E7%AC%A6%E9%9B%86/"},{"categories":["笔记"],"content":"字符集相关配置 查看所支持的字符集。 SHOW (CHARACTER SET|CHARSET) [LIKE 匹配的模式]; 其中CHARACTER SET和CHARSET是同义词，用任意一个都可以。 查看MySQL中支持的比较规则的命令如下: SHOW COLLATION [LIKE 匹配的模式]; 我们前边说过一种字符集可能对应着若干种比较规则，MySQL支持的字符集就已经非常多了，所以支持的比较规则更多 各级别的字符集和比较规则 MySQL有4个级别的字符集和比较规则，分别是: 服务器级别 数据库级别 表级别 列级别 服务器级别 MySQL提供了两个系统变量来表示服务器级别的字符集和比较规则: character_set_server 服务器级别的字符集 collation_server 服务器级别的比较规则 SHOW VARIABLES LIKE ‘collation_server’; 数据库级别 我们在创建和修改数据库的时候可以指定该数据库的字符集和比较规则，具体语法如下: CREATE DATABASE 数据库名 [[DEFAULT] CHARACTER SET 字符集名称] [[DEFAULT] COLLATE 比较规则名称]; ALTER DATABASE 数据库名 [[DEFAULT] CHARACTER SET 字符集名称] [[DEFAULT] COLLATE 比较规则名称]; 表级别 我们也可以在创建和修改表的时候指定表的字符集和比较规则，语法如下: CREATE TABLE 表名 (列的信息) [[DEFAULT] CHARACTER SET 字符集名称] [COLLATE 比较规则名称]] ALTER TABLE 表名 [[DEFAULT] CHARACTER SET 字符集名称] [COLLATE 比较规则名称] 列级别 需要注意的是，对于存储字符串的列，同一个表中的不同的列也可以有不同的字符集和比较规则。我们在创建和修改列定义的时候可以指定该 列的字符集和比较规则，语法如下: CREATE TABLE 表名( 列名 字符串类型 [CHARACTER SET 字符集名称] [COLLATE 比较规则名称], 其他列…); ","date":"2021-07-27","objectID":"/posts/mysql_%E5%AD%97%E7%AC%A6%E9%9B%86/:2:0","tags":["mysql"],"title":"mysql字符集","uri":"/posts/mysql_%E5%AD%97%E7%AC%A6%E9%9B%86/"},{"categories":["笔记"],"content":"客户端和服务器通信中的字符集 说到底，字符串在计算机上的体现就是一个字节串，如果你使用不同字符集去解码这个字节串，最后得到的结果可能让你挠头。 从发送请求到返回结果这个过程中伴随着多次字符集 的转换，在这个过程中会用到3个系统变量，我们先把它们写出来看一下: |系统变量| 描述 c| |–|–| |character_set_client |服务器解码请求时使用的字符集 |character_set_connection| 服务器处理请求时会把请求字符串从character_set_client转为character_set_connection| |character_set_results| 服务器向客户端返回数据时使用的字符集 ","date":"2021-07-27","objectID":"/posts/mysql_%E5%AD%97%E7%AC%A6%E9%9B%86/:3:0","tags":["mysql"],"title":"mysql字符集","uri":"/posts/mysql_%E5%AD%97%E7%AC%A6%E9%9B%86/"},{"categories":["笔记"],"content":"mysql统计数据的收集","date":"2021-07-27","objectID":"/posts/mysql_%E7%BB%9F%E8%AE%A1%E6%95%B0%E6%8D%AE%E7%9A%84%E6%94%B6%E9%9B%86/","tags":["mysql"],"title":"mysql统计数据的收集","uri":"/posts/mysql_%E7%BB%9F%E8%AE%A1%E6%95%B0%E6%8D%AE%E7%9A%84%E6%94%B6%E9%9B%86/"},{"categories":["笔记"],"content":"InnoDB 统计数据是如何收集的 通过SHOW TABLE STATUS可以看到关于表的统计数据，通过SHOW INDEX可以看到关于索引的统计数据，那么 这些统计数据是怎么来的呢?它们是以什么方式收集的呢? InnoDB提供了两种存储统计数据的方式: 永久性的统计数据 这种统计数据存储在磁盘上，也就是服务器重启之后这些统计数据还在。 非永久性的统计数据 这种统计数据存储在内存中，当服务器关闭时这些这些统计数据就都被清除掉了，等到服务器重启之后，在某些适当的场景下才会重新收集这些统计数据。 系统变量innodb_stats_persistent来控制到底采用哪种方式去存储统计数据。在MySQL 5.6.6之前，innodb_stats_persistent的值默 认是OFF，也就是说InnoDB的统计数据默认是存储到内存的，之后的版本中innodb_stats_persistent的值默认是ON，也就是统计数据默认被存储到磁盘中。 InnoDB默认是以表为单位来收集和存储统计数据的，也就是说我们可以把某些表的统计数据(以及该表的索引统计数据)存储在磁盘上，把另一些表的统计数据存 储在内存中。 我们可以在创建和修改表的时候通过指定STATS_PERSISTENT属性来指明该表的统计数据存储方式: CREATE TABLE 表名 (...) Engine=InnoDB, STATS_PERSISTENT = (1|0); ALTER TABLE 表名 Engine=InnoDB, STATS_PERSISTENT = (1|0); ","date":"2021-07-27","objectID":"/posts/mysql_%E7%BB%9F%E8%AE%A1%E6%95%B0%E6%8D%AE%E7%9A%84%E6%94%B6%E9%9B%86/:0:0","tags":["mysql"],"title":"mysql统计数据的收集","uri":"/posts/mysql_%E7%BB%9F%E8%AE%A1%E6%95%B0%E6%8D%AE%E7%9A%84%E6%94%B6%E9%9B%86/"},{"categories":["笔记"],"content":"InnoDB统计表中行数 InnoDB统计表中有多少行记录的套路是这样的: 按照一定算法(并不是纯粹随机的)选取几个叶子节点页面，计算每个页面中主键值记录数量，然后计算平均一个页面中主键值的记录数量乘以全部叶子节点的 数量就算是该表的n_rows值。 ","date":"2021-07-27","objectID":"/posts/mysql_%E7%BB%9F%E8%AE%A1%E6%95%B0%E6%8D%AE%E7%9A%84%E6%94%B6%E9%9B%86/:1:0","tags":["mysql"],"title":"mysql统计数据的收集","uri":"/posts/mysql_%E7%BB%9F%E8%AE%A1%E6%95%B0%E6%8D%AE%E7%9A%84%E6%94%B6%E9%9B%86/"},{"categories":["笔记"],"content":"更新统计数据 开启innodb_stats_auto_recalc。 系统变量innodb_stats_auto_recalc决定着服务器是否自动重新计算统计数据，它的默认值是ON，也就是该功能默认是开启的。每个表都维护了一个变量，该变 量记录着对该表进行增删改的记录条数，如果发生变动的记录数量超过了表大小的10%，并且自动重新计算统计数据的功能是打开的，那么服务器会重新进行一次 统计数据的计算，并且更新innodb_table_stats和innodb_index_stats表。不过自动重新计算统计数据的过程是异步发生的，也就是即使表中变动的记录数超过 了10%，自动重新计算统计数据也不会立即发生，可能会延迟几秒才会进行计算。 手动调用ANALYZE TABLE语句来更新统计信息 如果innodb_stats_auto_recalc系统变量的值为OFF的话，我们也可以手动调用ANALYZE TABLE语句来重新计算统计数据.ANALYZE TABLE语句会立即重新计算统计数据，也就是这个过程是同步的。 ","date":"2021-07-27","objectID":"/posts/mysql_%E7%BB%9F%E8%AE%A1%E6%95%B0%E6%8D%AE%E7%9A%84%E6%94%B6%E9%9B%86/:2:0","tags":["mysql"],"title":"mysql统计数据的收集","uri":"/posts/mysql_%E7%BB%9F%E8%AE%A1%E6%95%B0%E6%8D%AE%E7%9A%84%E6%94%B6%E9%9B%86/"},{"categories":["笔记"],"content":"mysql配置","date":"2021-07-27","objectID":"/posts/mysql_%E9%85%8D%E7%BD%AE/","tags":["mysql"],"title":"mysql配置","uri":"/posts/mysql_%E9%85%8D%E7%BD%AE/"},{"categories":["笔记"],"content":"配置 除了在命令行启动的时候指定参数如：mysqld –default-storage-engine = MyISAM外，MySQL程序在启动时会寻找多个路径下的配置文件，这些路径有的是固定的，有的是可以在命令行指定的。根据操作系统的不 同，配置文件的路径也有所不同。 ","date":"2021-07-27","objectID":"/posts/mysql_%E9%85%8D%E7%BD%AE/:0:0","tags":["mysql"],"title":"mysql配置","uri":"/posts/mysql_%E9%85%8D%E7%BD%AE/"},{"categories":["笔记"],"content":"配置路径 在类UNIX操作系统中，MySQL会按照下列路径来寻找配置文件: |路径名|备注| |—|—| |/etc/my.cnf| |/etc/mysql/my.cnf| |SYSCONFDIR/my.cnf| |$MYSQL_HOME/my.cnf |特定于服务器的选项(仅限服务器)| |defaults-extra-file| 命令行指定的额外配置文件路径 | |~/.my.cnf |用户特定选项| |~/.mylogin.cnf |用户特定的登录路径选项(仅限客户端)| ","date":"2021-07-27","objectID":"/posts/mysql_%E9%85%8D%E7%BD%AE/:1:0","tags":["mysql"],"title":"mysql配置","uri":"/posts/mysql_%E9%85%8D%E7%BD%AE/"},{"categories":["笔记"],"content":"配置分组 配置文件里面会分组：server、mysqld、mysqld_safe、client、mysql、mysqladmin。 配置文件中不同的选项组是给不同的启动命令使用的，如果选项组名称与程序名称相同，则组中的选项将专门应用于该程序。 |启动命令|类别|能读取的组| |-|-| | mysqld|启动服务器|[mysqld]、[server]| | mysqld_safe |服务 |[mysqld]、[server]、[mysqld_safe]| | mysql.server | 启动服务器 | [mysqld]、[server]、[mysql.server] | | mysql |启动客户端 | [mysql]、[client] | | mysqladmin | 启动客户端 | [mysqladmin]、[client]| | mysqldump | 启动客户端 | [mysqldump]、[client] | 如果我们想指定mysql.server程序的启动参数，则必须将它们放在配置文件中，而不是放在命令行中。 mysql.server仅支持start和stop作为命令行参数。 ","date":"2021-07-27","objectID":"/posts/mysql_%E9%85%8D%E7%BD%AE/:2:0","tags":["mysql"],"title":"mysql配置","uri":"/posts/mysql_%E9%85%8D%E7%BD%AE/"},{"categories":["笔记"],"content":"特定MySQL版本的专用选项组 我们可以在选项组的名称后加上特定的MySQL版本号。只有版本号为5.7的mysqld程序才能使用这个选项组中的选项[mysqld-5.7] ","date":"2021-07-27","objectID":"/posts/mysql_%E9%85%8D%E7%BD%AE/:3:0","tags":["mysql"],"title":"mysql配置","uri":"/posts/mysql_%E9%85%8D%E7%BD%AE/"},{"categories":["笔记"],"content":"配置文件加载顺序 MySQL将按照我们在上表中给定的顺序依次读取各个配置文件，如果该文件不存在则忽略。值得注 意的是，如果我们在多个配置文件中设置了相同的启动选项，那以最后一个配置文件中的为准。 同一个配置文件中多个组的优先级 我们说同一个命令可以访问配置文件中的多个组，那么，将以最后一个出现的组中的启动选项为准 如果同一个启动选项既出现在命令行中，又出现在配置文件中，那么以命令行中的启动选项为准 ","date":"2021-07-27","objectID":"/posts/mysql_%E9%85%8D%E7%BD%AE/:4:0","tags":["mysql"],"title":"mysql配置","uri":"/posts/mysql_%E9%85%8D%E7%BD%AE/"},{"categories":["笔记"],"content":"go sync.map","date":"2021-06-19","objectID":"/posts/go_syncmap/","tags":["golang"],"title":"go sync.map","uri":"/posts/go_syncmap/"},{"categories":["笔记"],"content":"实现安全的map 自go 1.6之后， 并发地读写map会报错，这在一些知名的开源库中都存在这个问题，所以go 1.9之前的解决方案是额外绑定一个锁，封装成一个新的struct或者单独使用锁都可以。 ","date":"2021-06-19","objectID":"/posts/go_syncmap/:0:0","tags":["golang"],"title":"go sync.map","uri":"/posts/go_syncmap/"},{"categories":["笔记"],"content":"直接加锁 type syncMap struct { items map[string]interface{} sync.RWMutex } 读读不会阻塞，读写、写写会阻塞。 ","date":"2021-06-19","objectID":"/posts/go_syncmap/:1:0","tags":["golang"],"title":"go sync.map","uri":"/posts/go_syncmap/"},{"categories":["笔记"],"content":"分段加锁 具体实现就是，基于上面的 syncMap 再包装一次，用多个 syncMap 来模拟实现一个 map： type SyncMap struct { shards []*syncMap } 源码解析 type Map struct { mu Mutex read atomic.Value // readOnly read map dirty map[interface{}]*entry // dirty map misses int } 总体图： 查询： 插入或更新： 删除： read map 的值是什么时间更新的 ？ Load/LoadOrStore/LoadAndDelete 时，当 misses 数量大于等于 dirty map 的元素个数时，会整体复制 dirty map 到 read map Store/LoadOrStore 时，当 read map 中存在这个key，则更新 Delete/LoadAndDelete 时，如果 read map 中存在这个key，则设置这个值为 nil。 dirty map 的值是什么时间更新的 ？ 完全是一个新 key， 第一次插入 sync.Map，必先插入 dirty map Store/LoadOrStore 时，当 read map 中不存在这个key，在 dirty map 存在这个key，则更新 Delete/LoadAndDelete 时，如果 read map 中不存在这个key，在 dirty map 存在这个key，则从 dirty map 中删除这个key 当 misses 数量大于等于 dirty map 的元素个数时，会整体复制 dirty map 到 read map，同时设置 dirty map 为 nil read map 和 dirty map 是什么时间删除的？ 当 read map 中存在某个 key 的时候，这个时候只会删除 read map， 并不会删除 dirty map（因为 dirty map 不存在这个值） 当 read map 中不存在时，才会去删除 dirty map 里面的值 疑问：如果按照这个删除方式，那岂不是 dirty map 中会有残余的 key，导致没删除掉？ 答：其实并不会。当 misses 数量大于等于 dirty map 的元素个数时，会整体复制 dirty map 到 read map。这个过程中还附带了另外一个操作：将 dirty map 置为 nil。 read map 与 dirty map 的关系 ？ 在 read map 中存在的值，在 dirty map 中可能不存在。 在 dirty map 中存在的值，在 read map 中也可能存在。 当访问多次，发现 dirty map 中存在，read map 中不存在，导致 misses 数量大于等于 dirty map 的元素个数时，会整体复制 dirty map 到 read map。 当出现 dirty map 向 read map 复制后，dirty map 会被置成 nil。 当出现 dirty map 向 read map 复制后，readOnly.amended 等于了 false。当新插入了一个值时，会将 read map 中的值，重新给 dirty map 赋值一遍 sync.Map 是如何提高性能的？ 通过源码解析，我们知道 sync.Map 里面有两个普通 map，read map主要是负责读，dirty map 是负责读和写（加锁）。在读多写少的场景下，read map 的值基本不发生变化，可以让 read map 做到无锁操作，就减少了使用 Mutex + Map 必须的加锁/解锁环节，因此也就提高了性能。 不过也能够看出来，read map 也是会发生变化的，如果某些 key 写操作特别频繁的话，sync.Map 基本也就退化成了 Mutex + Map（有可能性能还不如 Mutex + Map）。 所以，不是说使用了 sync.Map 就一定能提高程序性能，我们日常使用中尽量注意拆分粒度来使用 sync.Map。 ","date":"2021-06-19","objectID":"/posts/go_syncmap/:2:0","tags":["golang"],"title":"go sync.map","uri":"/posts/go_syncmap/"},{"categories":["笔记"],"content":"基础","date":"2021-04-08","objectID":"/posts/mysql_join%E8%BF%87%E7%A8%8B/","tags":["mysql"],"title":"mysql join过程","uri":"/posts/mysql_join%E8%BF%87%E7%A8%8B/"},{"categories":["笔记"],"content":"被驱动表能用到索引 select * from t1 straight_join t2 on (t1.a=t2.a); 如果直接使用 join 语句，MySQL 优化器可能会选择表 t1 或 t2 作为驱动表，这样会影响我们分析 SQL 语句的执行过程。所以，为了便于分析执行过程中的性能问题，我改用 straight_join 让 MySQL 使用固定的连接方式执行查询，这样优化器只会按照我们指定的方式去 join。在这个语句里，t1 是驱动表，t2 是被驱动表。 在这条语句里，被驱动表 t2 的字段 a 上有索引，join 过程用上了这个索引，先遍历表 t1，然后根据从表 t1 中取出的每行数据中的 a 值，去表 t2 中查找满足条件的记录。在形式上，这个过程就跟我们写程序时的嵌套查询类似，并且可以用上被驱动表的索引，所以我们称之为“Index Nested-Loop Join”，简称 NLJ。 在这个 join 语句执行过程中，驱动表是走全表扫描，而被驱动表是走树搜索。 假设被驱动表的行数是 M。每次在被驱动表查一行数据，要先搜索索引 a，再搜索主键索引。每次搜索一棵树近似复杂度是以 2 为底的 M 的对数，记为 log2M，所以在被驱动表上查一行的时间复杂度是 2*log2M。 假设驱动表的行数是 N，执行过程就要扫描驱动表 N 行，然后对于每一行，到被驱动表上匹配一次。 因此整个执行过程，近似复杂度是 N + N2log2M。 显然，N 对扫描行数的影响更大，因此应该让小表来做驱动表。 被驱动表用不上索引 select * from t1 straight_join t2 on (t1.a=t2.b); 由于表 t2 的字段 b 上没有索引，因此再用图 2 的执行流程时，每次到 t2 去匹配的时候，就要做一次全表扫描。 你可以先设想一下这个问题，继续使用图 2 的算法，是不是可以得到正确的结果呢？如果只看结果的话，这个算法是正确的，而且这个算法也有一个名字，叫做“Simple Nested-Loop Join”。 如果 t1 和 t2 都是 10 万行的表（当然了，这也还是属于小表的范围），就要扫描 100 亿行，先拿出t1的一行，然后把t2的记录从磁盘一一读出来进行判断，那么t2表需要从内存中读取10次。 当然，MySQL 也没有使用这个 Simple Nested-Loop Join 算法，而是使用了另一个叫作“Block Nested-Loop Join”的算法，简称 BNL。 Block Nested-Loop Join 扫描一个表的过程其实是先把这个表从磁盘上加载到内存中，然后从内存中比较匹配条件是否满足。内存里可能并不能完全存放的下表中所有的记录，所以在扫描表前边记录的时候后边的记录可能还在磁盘上， 等扫描到后边记录的时候可能内存不足，所以需要把前边的记录从内存中释放掉。 我们前边又说过，采用嵌套循环连接算法的两表连接过程中，被驱动表可是要被访问好 多次的，如果这个被驱动表中的数据特别多而且不能使用索引进行访问，那就相当于要从磁盘上读好几次这个表，这个I/O代价就非常大了，所以我们得想办法:尽量减 少访问被驱动表的次数。 当被驱动表中的数据非常多时，每次访问被驱动表，被驱动表的记录会被加载到内存中，在内存中的每一条记录只会和驱动表结果集的一条记录做匹配，之后就会被从 内存中清除掉。然后再从驱动表结果集中拿出另一条记录，再一次把被驱动表的记录加载到内存中一遍，周而复始，驱动表结果集中有多少条记录，就得把被驱动表从 磁盘上加载到内存中多少次。所以我们可不可以在把被驱动表的记录加载到内存的时候，一次性和多条驱动表中的记录做匹配，这样就可以大大减少重复从磁盘上加载 被驱动表的代价了。所以设计MySQL的大叔提出了一个join buffer的概念，join buffer就是执行连接查询前申请的一块固定大小的内存，先把若干条驱动表结果集中的 记录装在这个join buffer中，然后开始扫描被驱动表，每一条被驱动表的记录一次性和join buffer中的多条驱动表记录做匹配，因为匹配的过程都是在内存中完成 的，所以这样可以显著减少被驱动表的I/O代价。 这时候，被驱动表上没有可用的索引，算法的流程是这样的： 把表 t1 的数据读入线程内存 join_buffer 中，由于我们这个语句中写的是 select *，因此是把整个表 t1 放入了内存； 扫描表 t2，把表 t2 中的每一行取出来，跟 join_buffer 中的数据做对比，满足 join 条件的，作为结果集的一部分返回。 在这个过程中，对表 t1 和 t2 都做了一次全表扫描，因此总的扫描行数是 1100。由于 join_buffer 是以无序数组的方式组织的，因此对表 t2 中的每一行，都要做 100 次判断，总共需要在内存中做的判断次数是：100*1000=10 万次。 如果使用 Simple Nested-Loop Join 算法进行查询，扫描行数也是 10 万行。因此，从时间复杂度上来说，这两个算法是一样的。但是，Block Nested-Loop Join 算法的这 10 万次判断是内存操作，速度上会快很多，性能也更好。 join_buffer 的大小是由参数 join_buffer_size 设定的，默认值是 256k。如果放不下表 t1 的所有数据话，策略很简单，就是分段放。 对于 优化被驱动表的查询来说，最好是为被驱动表加上效率高的索引，如果实在不能使用索引，并且自己的机器的内存也比较大可以尝试调大join_buffer_size的值来对连 接查询进行优化。 另外需要注意的是，驱动表的记录并不是所有列都会被放到join buffer中，只有查询列表中的列和过滤条件中的列才会被放到join buffer中，所以再次提醒我们，最 好不要把*作为查询列表，只需要把我们关心的列放到查询列表就好了，这样还可以在join buffer中放置更多的记录呢哈。 join成本选择 多表连接的成本分析 首先要考虑一下多表连接时可能产生出多少种连接顺序: 对于两表连接，比如表A和表B连接 只有 AB、BA这两种连接顺序。其实相当于2 × 1 = 2种连接顺序。 对于三表连接，比如表A、表B、表C进行连接 有ABC、ACB、BAC、BCA、CAB、CBA这么6种连接顺序。其实相当于3 × 2 × 1 = 6种连接顺序。 对于四表连接的话，则会有4 × 3 × 2 × 1 = 24种连接顺序。 对于n表连接的话，则有 n × (n-1) × (n-2) × ··· × 1种连接顺序，就是n的阶乘种连接顺序，也就是n!。 有n个表进行连接，MySQL查询优化器要每一种连接顺序的成本都计算一遍么?那可是n!种连接顺序呀。其实真的是要都算一遍，不过设计MySQL的大叔们想了很多办法减少计算非常多种连 接顺序的成本的方法: 提前结束某种顺序的成本评估 MySQL在计算各种链接顺序的成本之前，会维护一个全局的变量，这个变量表示当前最小的连接查询成本。如果在分析某个连接顺序的成本时，该成本已经超过当前最小的连接查询成 本，那就压根儿不对该连接顺序继续往下分析了。比方说A、B、C三个表进行连接，已经得到连接顺序ABC是当前的最小连接成本，比方说10.0，在计算连接顺序BCA时，发现B和C的 连接成本就已经大于10.0时，就不再继续往后分析BCA这个连接顺序的成本了。 系统变量optimizer_search_depth 为了防止无穷无尽的分析各种连接顺序的成本，设计MySQL的大叔们提出了optimizer_search_depth系统变量，如果连接表的个数小于该值，那么就继续穷举分析每一种连接顺序的成 本，否则只对与optimizer_search_depth值相同数量的表进行穷举分析。很显然，该值越大，成本分析的越精确，越容易得到好的执行计划，但是消耗的时间也就越长，否则得到不 是很好的执行计划，但可以省掉很多分析连接成本的时间。 根据某些规则压根儿就不考虑某些连接顺序 即使是有上边两条规则的限制，但是分析多个表不同连接顺序成本花费的时间还是会很长，所以设计MySQL的大叔干脆提出了一些所谓的启发式规则(就是根据以往经验指定的一些规 则)，凡是不满足这些规则的连接顺序压根儿就不分析，这样可以极大的减少需要分析的连接顺序的数量，但是也可能造成错失最优的执行计划。他们提供了一个系统变 量optimizer_prune_level来控制到底是不是用这些启发式规则。 总结 第一个问题：能不能使用 join 语句？ 如果可以使用 Index Nested-Loop Join 算法，也就是说可以用上被驱动表上的索引，其实是没问题的； 如果使用 Block Nested-Loop Join 算法，扫描行数就会过多。尤其是在大表上的 join 操作，这样可能要扫描被驱动表很多次，会占用大量的系统资源。所以这种 join 尽量不要用。 判断要不要使用 join 语句时，就是看 explain 结果里面，Extra 字段里面有没有出现“Block Nested Loop”字样。 第二个问题是：如果要使用 join，应该选择大表做驱动表还是选择小表做驱动表？ 如果是 Index Nested-Loop Join 算法，应该选择小表做驱动表； 如果是 Block Nested-Loop Join 算法： 在 join_buffer_size 足够大的时候，是一样的； 在 join_buffer_size 不够大的时候（这种情","date":"2021-04-08","objectID":"/posts/mysql_join%E8%BF%87%E7%A8%8B/:0:0","tags":["mysql"],"title":"mysql join过程","uri":"/posts/mysql_join%E8%BF%87%E7%A8%8B/"},{"categories":["笔记"],"content":"mysql是怎么运行的笔记","date":"2021-02-25","objectID":"/posts/mysql_%E6%98%AF%E6%80%8E%E6%A0%B7%E8%BF%90%E8%A1%8C%E7%9A%84/","tags":["mysql"],"title":"mysql是怎么运行的笔记","uri":"/posts/mysql_%E6%98%AF%E6%80%8E%E6%A0%B7%E8%BF%90%E8%A1%8C%E7%9A%84/"},{"categories":["笔记"],"content":"服务器层次划分 为了管理方便，人们把连接管理、查询缓存、语法解析、查询优化这些并不涉及真实数据存储的功能划分为MySQL server的功能，把真实存取数据的功能划 分为存储引擎的功能。 ","date":"2021-02-25","objectID":"/posts/mysql_%E6%98%AF%E6%80%8E%E6%A0%B7%E8%BF%90%E8%A1%8C%E7%9A%84/:0:0","tags":["mysql"],"title":"mysql是怎么运行的笔记","uri":"/posts/mysql_%E6%98%AF%E6%80%8E%E6%A0%B7%E8%BF%90%E8%A1%8C%E7%9A%84/"},{"categories":["笔记"],"content":"缓存 如果两个查询请求在任何字符上的不同(例如:空格、注释、大小写)，都会导致缓存不会命中。另外，如果查询 请求中包含某些系统函数、用户自定义变量和函数、一些系统表，如 mysql 、information_schema、 performance_schema 数据库中的表，那这个请求就不 会被缓存。 既然是缓存，那就有它缓存失效的时候。MySQL的缓存系统会监测涉及到的每张表，只要该表的结构或者数据被修改，如对该表使用了INSERT、 UPDATE、DELETE、TRUNCATE TABLE、ALTER TABLE、DROP TABLE或 DROP DATABASE语句，那使用该表的所有高速缓存查询都将变为无效并从高速缓存中 删除! 从MySQL 5.7.20开始，不推荐使用查询缓存，并在MySQL 8.0中删除。 ","date":"2021-02-25","objectID":"/posts/mysql_%E6%98%AF%E6%80%8E%E6%A0%B7%E8%BF%90%E8%A1%8C%E7%9A%84/:1:0","tags":["mysql"],"title":"mysql是怎么运行的笔记","uri":"/posts/mysql_%E6%98%AF%E6%80%8E%E6%A0%B7%E8%BF%90%E8%A1%8C%E7%9A%84/"},{"categories":["笔记"],"content":"查询优化 优化的结果就是生成一个执行计划，这个执行计划表明了应该使用哪些索引进行查询，表之间的连接顺序是啥样的。我们可以使 用EXPLAIN语句来查看某个语句的执行计划。 ","date":"2021-02-25","objectID":"/posts/mysql_%E6%98%AF%E6%80%8E%E6%A0%B7%E8%BF%90%E8%A1%8C%E7%9A%84/:2:0","tags":["mysql"],"title":"mysql是怎么运行的笔记","uri":"/posts/mysql_%E6%98%AF%E6%80%8E%E6%A0%B7%E8%BF%90%E8%A1%8C%E7%9A%84/"},{"categories":["笔记"],"content":"存储引擎 存储引擎原来叫表处理器 MySQL服务器把数据的存储和提取操作都封装到了一个叫存储引擎的模块 里。我们知道表是由一行一行的记录组成的，但这只是一个逻辑上的概念，物理上如何表示记录，怎么从表中读取数据，怎么把数据写入具体的物理 存储器上，这都是存储引擎负责的事情。 配置 除了在命令行启动的时候指定参数如：mysqld –default-storage-engine = MyISAM外，MySQL程序在启动时会寻找多个路径下的配置文件，这些路径有的是固定的，有的是可以在命令行指定的。根据操作系统的不 同，配置文件的路径也有所不同。 ","date":"2021-02-25","objectID":"/posts/mysql_%E6%98%AF%E6%80%8E%E6%A0%B7%E8%BF%90%E8%A1%8C%E7%9A%84/:3:0","tags":["mysql"],"title":"mysql是怎么运行的笔记","uri":"/posts/mysql_%E6%98%AF%E6%80%8E%E6%A0%B7%E8%BF%90%E8%A1%8C%E7%9A%84/"},{"categories":["笔记"],"content":"配置路径 在类UNIX操作系统中，MySQL会按照下列路径来寻找配置文件: |路径名|备注| |—|—| |/etc/my.cnf| |/etc/mysql/my.cnf| |SYSCONFDIR/my.cnf| |$MYSQL_HOME/my.cnf |特定于服务器的选项(仅限服务器)| |defaults-extra-file| 命令行指定的额外配置文件路径 | |~/.my.cnf |用户特定选项| |~/.mylogin.cnf |用户特定的登录路径选项(仅限客户端)| ","date":"2021-02-25","objectID":"/posts/mysql_%E6%98%AF%E6%80%8E%E6%A0%B7%E8%BF%90%E8%A1%8C%E7%9A%84/:4:0","tags":["mysql"],"title":"mysql是怎么运行的笔记","uri":"/posts/mysql_%E6%98%AF%E6%80%8E%E6%A0%B7%E8%BF%90%E8%A1%8C%E7%9A%84/"},{"categories":["笔记"],"content":"配置分组 配置文件里面会分组：server、mysqld、mysqld_safe、client、mysql、mysqladmin。 配置文件中不同的选项组是给不同的启动命令使用的，如果选项组名称与程序名称相同，则组中的选项将专门应用于该程序。 |启动命令|类别|能读取的组| |-|-| | mysqld|启动服务器|[mysqld]、[server]| | mysqld_safe |服务 |[mysqld]、[server]、[mysqld_safe]| | mysql.server | 启动服务器 | [mysqld]、[server]、[mysql.server] | | mysql |启动客户端 | [mysql]、[client] | | mysqladmin | 启动客户端 | [mysqladmin]、[client]| | mysqldump | 启动客户端 | [mysqldump]、[client] | 如果我们想指定mysql.server程序的启动参数，则必须将它们放在配置文件中，而不是放在命令行中。 mysql.server仅支持start和stop作为命令行参数。 ","date":"2021-02-25","objectID":"/posts/mysql_%E6%98%AF%E6%80%8E%E6%A0%B7%E8%BF%90%E8%A1%8C%E7%9A%84/:5:0","tags":["mysql"],"title":"mysql是怎么运行的笔记","uri":"/posts/mysql_%E6%98%AF%E6%80%8E%E6%A0%B7%E8%BF%90%E8%A1%8C%E7%9A%84/"},{"categories":["笔记"],"content":"特定MySQL版本的专用选项组 我们可以在选项组的名称后加上特定的MySQL版本号。只有版本号为5.7的mysqld程序才能使用这个选项组中的选项[mysqld-5.7] ","date":"2021-02-25","objectID":"/posts/mysql_%E6%98%AF%E6%80%8E%E6%A0%B7%E8%BF%90%E8%A1%8C%E7%9A%84/:6:0","tags":["mysql"],"title":"mysql是怎么运行的笔记","uri":"/posts/mysql_%E6%98%AF%E6%80%8E%E6%A0%B7%E8%BF%90%E8%A1%8C%E7%9A%84/"},{"categories":["笔记"],"content":"配置文件加载顺序 MySQL将按照我们在上表中给定的顺序依次读取各个配置文件，如果该文件不存在则忽略。值得注 意的是，如果我们在多个配置文件中设置了相同的启动选项，那以最后一个配置文件中的为准。 同一个配置文件中多个组的优先级 我们说同一个命令可以访问配置文件中的多个组，那么，将以最后一个出现的组中的启动选项为准 如果同一个启动选项既出现在命令行中，又出现在配置文件中，那么以命令行中的启动选项为准 ","date":"2021-02-25","objectID":"/posts/mysql_%E6%98%AF%E6%80%8E%E6%A0%B7%E8%BF%90%E8%A1%8C%E7%9A%84/:7:0","tags":["mysql"],"title":"mysql是怎么运行的笔记","uri":"/posts/mysql_%E6%98%AF%E6%80%8E%E6%A0%B7%E8%BF%90%E8%A1%8C%E7%9A%84/"},{"categories":["笔记"],"content":"mysql系统变量 MySQL服务器程序运行过程中会用到许多影响程序行为的变量，它们被称为MySQL系统变量，比如允许同时连入的客户端数量用 系统变量max_connections表示，表的默认存储引擎用系统变量default_storage_engine表示，查询缓存的大小用系统变 量query_cache_size表示，MySQL服务器程序的系统变量有好几百条，我们就不一一列举了。每个系统变量都有一个默认值， 我们可以使用命令行或者配置文件中的选项在启动服务器时改变一些系统变量的值。大多数的系统变量的值也可以在程序运行 过程中修改，而无需停止并重新启动它。 查看系统变量：SHOW VARIABLES [LIKE 匹配的模式]; 设置的时候可以通过命令行、配置文件、甚至运行期间。 ","date":"2021-02-25","objectID":"/posts/mysql_%E6%98%AF%E6%80%8E%E6%A0%B7%E8%BF%90%E8%A1%8C%E7%9A%84/:8:0","tags":["mysql"],"title":"mysql是怎么运行的笔记","uri":"/posts/mysql_%E6%98%AF%E6%80%8E%E6%A0%B7%E8%BF%90%E8%A1%8C%E7%9A%84/"},{"categories":["笔记"],"content":"系统变量的作用范围 设计MySQL的大叔提出了系统变量的作用范围的概念，具体来说作用范围分为这两种: GLOBAL:全局变量，影响服务器的整体操作。 SESSION:会话变量，影响某个客户端连接的操作。 SET [GLOBAL|SESSION] 系统变量名 = 值; 通过启动选项设置的系统变量的作用范围都是GLOBAL的，也就是对所有客户端都有效的 我们的SHOW VARIABLES语句查看的是什么作用范围的系统变量呢? 答:默认查看的是SESSION作用范围的系统变量。 当然我们也可以在查看系统变量的语句上加上要查看哪个作用范围的系统变量，就像这样: SHOW [GLOBAL|SESSION] VARIABLES [LIKE 匹配的模式]; 有一些系统变量只具有SESSION作用范围，比如insert_id，表示在对某个包含AUTO_INCREMENT列的表进行插入时， 该列初始的值。 启动选项和系统变量的区别? 启动选项是在程序启动时我们程序员传递的一些参数，而系统变量是影响服务器程序运行行为的变量，它们之间的关系如下: 大部分的系统变量都可以被当作启动选项传入。 有些系统变量是在程序运行过程中自动生成的，是不可以当作启动选项来设置，比 如auto_increment_offset、character_set_client啥的。 有些启动选项也不是系统变量，比如defaults-file。 ","date":"2021-02-25","objectID":"/posts/mysql_%E6%98%AF%E6%80%8E%E6%A0%B7%E8%BF%90%E8%A1%8C%E7%9A%84/:8:1","tags":["mysql"],"title":"mysql是怎么运行的笔记","uri":"/posts/mysql_%E6%98%AF%E6%80%8E%E6%A0%B7%E8%BF%90%E8%A1%8C%E7%9A%84/"},{"categories":["笔记"],"content":"mysql状态变量 为了让我们更好的了解服务器程序的运行情况，MySQL服务器程序中维护了好多关于程序运行状态的变量，它们被称为状态变 量。比方说Threads_connected表示当前有多少客户端与服务器建立了连接，Handler_update表示已经更新了多少行记录。 SHOW [GLOBAL|SESSION] STATUS [LIKE 匹配的模式]; 不写明作用范围，默认的作用范围是SESSION 字符集及编码 ","date":"2021-02-25","objectID":"/posts/mysql_%E6%98%AF%E6%80%8E%E6%A0%B7%E8%BF%90%E8%A1%8C%E7%9A%84/:9:0","tags":["mysql"],"title":"mysql是怎么运行的笔记","uri":"/posts/mysql_%E6%98%AF%E6%80%8E%E6%A0%B7%E8%BF%90%E8%A1%8C%E7%9A%84/"},{"categories":["笔记"],"content":"MySQL中的 utf8和 utf8mb4 我们上边说utf8字符集表示一个字符需要使用1~4个字节，但是我们常用的一些字符使用1~3个字节就可以表示了。而在MySQL中字符集表示一 个字符所用最大字节长度在某些方面会影响系统的存储和性能，所以设计MySQL的大叔偷偷的定义了两个概念: utf8mb3:阉割过的utf8字符集，只使用1~3个字节表示字符。 utf8mb4:正宗的utf8字符集，使用1~4个字节表示字符。 有一点需要大家十分的注意，在MySQL中utf8是utf8mb3的别名，所以之后在MySQL中提到utf8就意味着使用1~3个字节来表示一个字符，如果大 家有使用4字节编码一个字符的情况，比如存储一些emoji表情啥的，那请使用utf8mb4。 ","date":"2021-02-25","objectID":"/posts/mysql_%E6%98%AF%E6%80%8E%E6%A0%B7%E8%BF%90%E8%A1%8C%E7%9A%84/:10:0","tags":["mysql"],"title":"mysql是怎么运行的笔记","uri":"/posts/mysql_%E6%98%AF%E6%80%8E%E6%A0%B7%E8%BF%90%E8%A1%8C%E7%9A%84/"},{"categories":["笔记"],"content":"字符集相关配置 查看所支持的字符集。 SHOW (CHARACTER SET|CHARSET) [LIKE 匹配的模式]; 其中CHARACTER SET和CHARSET是同义词，用任意一个都可以。 查看MySQL中支持的比较规则的命令如下: SHOW COLLATION [LIKE 匹配的模式]; 我们前边说过一种字符集可能对应着若干种比较规则，MySQL支持的字符集就已经非常多了，所以支持的比较规则更多 各级别的字符集和比较规则 MySQL有4个级别的字符集和比较规则，分别是: 服务器级别 数据库级别 表级别 列级别 服务器级别 MySQL提供了两个系统变量来表示服务器级别的字符集和比较规则: character_set_server 服务器级别的字符集 collation_server 服务器级别的比较规则 SHOW VARIABLES LIKE ‘collation_server’; 数据库级别 我们在创建和修改数据库的时候可以指定该数据库的字符集和比较规则，具体语法如下: CREATE DATABASE 数据库名 [[DEFAULT] CHARACTER SET 字符集名称] [[DEFAULT] COLLATE 比较规则名称]; ALTER DATABASE 数据库名 [[DEFAULT] CHARACTER SET 字符集名称] [[DEFAULT] COLLATE 比较规则名称]; 表级别 我们也可以在创建和修改表的时候指定表的字符集和比较规则，语法如下: CREATE TABLE 表名 (列的信息) [[DEFAULT] CHARACTER SET 字符集名称] [COLLATE 比较规则名称]] ALTER TABLE 表名 [[DEFAULT] CHARACTER SET 字符集名称] [COLLATE 比较规则名称] 列级别 需要注意的是，对于存储字符串的列，同一个表中的不同的列也可以有不同的字符集和比较规则。我们在创建和修改列定义的时候可以指定该 列的字符集和比较规则，语法如下: CREATE TABLE 表名( 列名 字符串类型 [CHARACTER SET 字符集名称] [COLLATE 比较规则名称], 其他列…); ","date":"2021-02-25","objectID":"/posts/mysql_%E6%98%AF%E6%80%8E%E6%A0%B7%E8%BF%90%E8%A1%8C%E7%9A%84/:11:0","tags":["mysql"],"title":"mysql是怎么运行的笔记","uri":"/posts/mysql_%E6%98%AF%E6%80%8E%E6%A0%B7%E8%BF%90%E8%A1%8C%E7%9A%84/"},{"categories":["笔记"],"content":"客户端和服务器通信中的字符集 说到底，字符串在计算机上的体现就是一个字节串，如果你使用不同字符集去解码这个字节串，最后得到的结果可能让你挠头。 从发送请求到返回结果这个过程中伴随着多次字符集 的转换，在这个过程中会用到3个系统变量，我们先把它们写出来看一下: |系统变量| 描述 c| |–|–| |character_set_client |服务器解码请求时使用的字符集 |character_set_connection| 服务器处理请求时会把请求字符串从character_set_client转为character_set_connection| |character_set_results| 服务器向客户端返回数据时使用的字符集 innodb记录结构 ","date":"2021-02-25","objectID":"/posts/mysql_%E6%98%AF%E6%80%8E%E6%A0%B7%E8%BF%90%E8%A1%8C%E7%9A%84/:12:0","tags":["mysql"],"title":"mysql是怎么运行的笔记","uri":"/posts/mysql_%E6%98%AF%E6%80%8E%E6%A0%B7%E8%BF%90%E8%A1%8C%E7%9A%84/"},{"categories":["笔记"],"content":"页 我们知道读写磁盘的速度非常慢，和内存读写差了几个数量级，所以当我们想从表中获取某些记录时，InnoDB存储引擎需要一条一条的把记录 从磁盘上读出来么? 不，那样会慢死，InnoDB采取的方式是:将数据划分为若干个页，以页作为磁盘和内存之间交互的基本单位，InnoDB中页的大小一般为 16 KB。也就是在一般情况下，一次最少从磁 盘中读取16KB的内容到内存中，一次最少把内存中的16KB内容刷新到磁盘中。 ","date":"2021-02-25","objectID":"/posts/mysql_%E6%98%AF%E6%80%8E%E6%A0%B7%E8%BF%90%E8%A1%8C%E7%9A%84/:13:0","tags":["mysql"],"title":"mysql是怎么运行的笔记","uri":"/posts/mysql_%E6%98%AF%E6%80%8E%E6%A0%B7%E8%BF%90%E8%A1%8C%E7%9A%84/"},{"categories":["笔记"],"content":"InnoDB行格式 我们平时是以记录为单位来向表中插入数据的，这些记录在磁盘上的存放方式也被称为行格式或者记录格式。设计InnoDB存储引擎的大叔们到现在为止设计了4种不同类型的行格式，分别 是Compact、Redundant、Dynamic和Compressed行格式。 我们可以在创建或修改表的语句中指定行格式: CREATE TABLE 表名 (列的信息) ROW_FORMAT=行格式名称 ALTER TABLE 表名 ROW_FORMAT=行格式名称 ","date":"2021-02-25","objectID":"/posts/mysql_%E6%98%AF%E6%80%8E%E6%A0%B7%E8%BF%90%E8%A1%8C%E7%9A%84/:14:0","tags":["mysql"],"title":"mysql是怎么运行的笔记","uri":"/posts/mysql_%E6%98%AF%E6%80%8E%E6%A0%B7%E8%BF%90%E8%A1%8C%E7%9A%84/"},{"categories":["笔记"],"content":"行溢出数据现象 MySQL对一条记录占用的最大存储空间是有限制的，除了BLOB或者TEXT类型的列之外，其他所有的列(不包括隐藏列和记录头信息)占用的字节长度加起来不能超过65535个字 节。所以MySQL服务器建议我们把存储类型改为TEXT或者BLOB的类型。 MySQL中磁盘和内存交互的基本单位是页，也就是说MySQL是以页为基本单位来管理存储空间的， 我们的记录都会被分配到某个页中存储。而一个页的大小一般是16KB，也就是16384字节，而一个VARCHAR(M)类型的列就最多可以存储65532个字节，这样就可能造成一个页存放不了一条记录的尴尬情况。 在Compact和Reduntant行格式中，对于占用存储空间非常大的列，在记录的真实数据处只会存储该列的一部分数据，把剩余的数据分散存储在几个其他的页中，然后记录的真实数据处用20个字节存储指向这些 页的地址。 ","date":"2021-02-25","objectID":"/posts/mysql_%E6%98%AF%E6%80%8E%E6%A0%B7%E8%BF%90%E8%A1%8C%E7%9A%84/:14:1","tags":["mysql"],"title":"mysql是怎么运行的笔记","uri":"/posts/mysql_%E6%98%AF%E6%80%8E%E6%A0%B7%E8%BF%90%E8%A1%8C%E7%9A%84/"},{"categories":["笔记"],"content":"行溢出的临界点 那发生行溢出的临界点是什么呢?也就是说在列存储多少字节的数据时就会发生行溢出? MySQL中规定一个页中至少存放两行记录,只要知道如果我们想一个行中存储了很大的数据时，可能发 生行溢出的现象。 ","date":"2021-02-25","objectID":"/posts/mysql_%E6%98%AF%E6%80%8E%E6%A0%B7%E8%BF%90%E8%A1%8C%E7%9A%84/:14:2","tags":["mysql"],"title":"mysql是怎么运行的笔记","uri":"/posts/mysql_%E6%98%AF%E6%80%8E%E6%A0%B7%E8%BF%90%E8%A1%8C%E7%9A%84/"},{"categories":["笔记"],"content":"mysql实战45讲笔记","date":"2021-01-20","objectID":"/posts/mysql_45%E8%AE%B2%E7%AC%94%E8%AE%B0/","tags":["mysql"],"title":"mysql实战45讲笔记","uri":"/posts/mysql_45%E8%AE%B2%E7%AC%94%E8%AE%B0/"},{"categories":["笔记"],"content":" 树高其实取决于叶子树（数据行数）和“N叉树”的N。 而N是由页大小和索引大小决定的。 MySQL 5.6 引入的索引下推优化（index condition pushdown)， 可以在索引遍历过程中，对索引中包含的字段先做判断，直接过滤掉不满足条件的记录，减少回表次数。 在建立联合索引的时候，如何安排索引内的字段顺序。 这里我们的评估标准是，索引的复用能力。因为可以支持最左前缀，所以当已经有了 (a,b) 这个联合索引后，一般就不需要单独在 a 上建立索引了。因此，第一原则是，如果通过调整顺序，可以少维护一个索引，那么这个顺序往往就是需要优先考虑采用的。 InnoDB会把主键字段放到索引定义字段后面， 当然同时也会去重。 所以，当主键是(a,b)的时候， 定义为c的索引，实际上是（c,a,b); 定义为(c,a)的索引，实际上是(c,a,b) 定义为(c,b）的索引，实际上是（c,b,a) MDL元数据表级锁，作用是防止DDL和DML并发的冲突。DML所在的事务是读，DDL是写，不能同时进行，会阻塞。 MySQL 的行锁是在引擎层由各个引擎自己实现的。但并不是所有的引擎都支持行锁，比如 MyISAM 引擎就不支持行锁。不支持行锁意味着并发控制只能使用表锁，对于这种引擎的表，同一张表上任何时刻只能有一个更新在执行 在 InnoDB 事务中，行锁是在需要的时候才加上的，但并不是不需要了就立刻释放，而是要等到事务结束时才释放。这个就是两阶段锁协议。如果你的事务中需要锁多个行，要把最可能造成锁冲突、最可能影响并发度的锁尽量往后放。 事务 A 在等待事务 B 释放 id=2 的行锁，而事务 B 在等待事务 A 释放 id=1 的行锁。 事务 A 和事务 B 在互相等待对方的资源释放，就是进入了死锁状态。 当出现死锁以后，有两种策略： 一种策略是，直接进入等待，直到超时。这个超时时间可以通过参数 innodb_lock_wait_timeout 来设置。 另一种策略是，发起死锁检测，发现死锁后，主动回滚死锁链条中的某一个事务，让其他事务得以继续执行。将参数innodb_deadlock_detect 设置为 on，表示开启这个逻辑。默认值本身就是 on update或deleted在where条件上无索引的情况下，delete会锁整个表的记录，但是delete后加[limit row_count]，则是只锁[1-满足删除条数所在行]这些行。 InnoDB 的数据是按数据页为单位来读写的。也就是说，当需要读一条记录的时候，并不是将这个记录本身从磁盘读出来，而是以页为单位，将其整体读入内存。在 InnoDB 中，每个数据页的大小默认是 16KB。 当需要更新一个数据页时，如果数据页在内存中就直接更新，而如果这个数据页还没有在内存中的话，在不影响数据一致性的前提下，InooDB 会将这些更新操作缓存在 change buffer 中，这样就不需要从磁盘中读入这个数据页了。在下次查询需要访问这个数据页的时候，将数据页读入内存，然后执行 change buffer 中与这个页有关的操作。通过这种方式就能保证这个数据逻辑的正确性。 普通索引和唯一索引应该怎么选择。其实，这两类索引在查询能力上是没差别的，主要考虑的是对更新性能的影响。所以，我建议你尽量选择普通索引。 果要简单地对比这两个机制在提升更新性能上的收益的话，redo log 主要节省的是随机写磁盘的 IO 消耗（转成顺序写），而 change buffer 主要节省的则是随机读磁盘的 IO 消耗。 查询时采用 force index 强行选择一个索引。 InnoDB 会在后台刷脏页，而刷脏页的过程是要将内存页写入磁盘。所以，无论是你的查询语句在需要内存的时候可能要求淘汰一个脏页，还是由于刷脏页的逻辑会占用 IO 资源并可能影响到了你的更新语句，都可能是造成你从业务端感知到 MySQL“抖”了一下的原因。 WAL 技术，数据库将随机写转换成了顺序写，大大提升了数据库的性能。 delete 命令其实只是把记录的位置，或者数据页标记为了“可复用”，但磁盘文件的大小是不会变的。也就是说，通过 delete 命令是不能回收表空间的。 如果数据是按照索引递增顺序插入的，那么索引是紧凑的。但如果数据是随机插入的，就可能造成索引的数据页分裂。增删改都会造成数据页的空洞，重建表可以解决。 InnoDB 不会把整张表占满，每个页留了 1/16 给后续的更新用。也就是说，其实重建表之后不是“最”紧凑的。 count() 是一个聚合函数，对于返回的结果集，一行行地判断，如果 count 函数的参数不是 NULL，累计值就加 1，否则不加。 按照效率排序的话，count(字段)\u003ccount(主键 id)\u003ccount(1)≈count()，所以我建议你，尽量使用 count()。 联合索引一个正序一个倒序不能用到索引，mysql8.0以后可以指定联合索引正序或倒序，可以用到索引。 如果mysql不能判断目标值和现有值是否相同，就会更新一次，哪怕真的相同，比如update a where id =2;而update t a=3 where a = 3 这样是不会真的更新的。另外，如果有timestamp类型且要自动更新，他会判断目标和现有的值，是为了优化用。 对索引字段做函数操作;隐式类型转换;隐式字符编码转换;这三种情况查询的时候可能会破坏索引值的有序性，因此优化器就决定放弃走树搜索功能。 update from t set b = 3 where a =1; a没有索引，那么就会锁表 间隙锁和行锁合称 next-key lock，每个 next-key lock 是前开后闭区间。也就是说，我们的表 t 初始化以后，如果用 select * from t for update 要把整个表所有记录锁起来，就形成了 7 个 next-key lock，分别是 (-∞,0]、(0,5]、(5,10]、(10,15]、(15,20]、(20, 25]、(25, +supremum]。 互斥锁或共享锁本身不能解决幻读问题，间隙锁和 next-key lock 的引入，帮我们解决了幻读的问题。 select … lock in share mode和select … for update在可重复读级别下都有可能产生行锁+间隙锁，区别就是行锁是读锁还是写锁。 行锁有冲突关系的是“另外一个行锁”。 但是间隙锁不一样，跟间隙锁存在冲突关系的，是“往这个间隙中插入一个记录”这个操作。间隙锁之间都不存在冲突关系。 加锁规则里面，包含了两个“原则”、两个“优化”和一个“bug”。 原则 1：加锁的基本单位是 next-key lock。希望你还记得，next-key lock 是前开后闭区间。 原则 2：查找过程中访问到的对象才会加锁。 优化 1：索引上的等值查询，给唯一索引加锁的时候，next-key lock 退化为行锁。 优化 2：索引上的等值查询，向右遍历时且最后一个值不满足等值条件的时候，next-key lock 退化为间隙锁。 一个 bug：唯一索引上的范围查询会访问到不满足条件的第一个值为止。 注意，delete 语句加锁的逻辑，其实跟 select … for update 是类似的 锁是加在索引上的；同时，它给我们的指导是，如果你要用 lock in share mode 来给行加读锁避免数据被更新的话，就必须得绕过覆盖索引的优化，在查询字段中加入索引中不存在的字段。 MySQL 5.7 提供了 query_rewrite 功能，可以把输入的一种语句改写成另外一种模式。这样就不用改代码，直接数据库里替换sql语句 从库执行重放的时候也会生成binlog。主从可以相互备份，不会循环复制。 一个查询在发送过程中，占用的 MySQL 内部的内存最大就是 net_buffer_length 这么大,MySQL 是“边读边发的”,这就意味着，如果客户端接收得慢，会导致 MySQL 服务端由于结果发不出去，这个事务的执行时间变长。 判断要不要使用 join 语句时，就是看 explain 结果里面，Extra 字段里面有没有出现“Block Nested Loop”字样。如果使用 Block Nested-Loop Join 算法，扫描行数就会过多。尤其是在大表上的 join 操作，这样可能要扫描被驱动表很多次，会占用大量的系统资源。所以这种 join 尽量不要用。 一个临时表只能被创建它的 session 访问，对其他线程不可见。这个 session 结束的时候，会自动删除临时表 在 MySQL 5.7 及之前的版本，自增值保存在内存里，并没有持久化。每次重启后，第一次打开表的时候，都会去找自增值的最大值 max(id)，然后将 max(id)+1 作为这个表当前的自增值。﻿ 在 MySQL 8.0 版本，将自增值的变更记录在了 redo log 中，重启的时候依靠 redo log 恢复重启之前的值。 主键冲突插入失败、插入后事务回滚都会造成ID自增空洞。 类似 insert … select 这样的批量插入数据的语句，默认情况下自增锁还是要等语句结束后才被释放； insert into … on duplicate key update 这个语义的逻辑是，插入一行数据，如果碰到唯一键约束，就执行后面的更新语句。注意，如果有多个列违反了唯一性约束，就会按照索引的顺序，修改跟第一个索引冲突的行。affected rows 返回的是 2，很容易造成误解","date":"2021-01-20","objectID":"/posts/mysql_45%E8%AE%B2%E7%AC%94%E8%AE%B0/:0:0","tags":["mysql"],"title":"mysql实战45讲笔记","uri":"/posts/mysql_45%E8%AE%B2%E7%AC%94%E8%AE%B0/"},{"categories":["笔记"],"content":"基础","date":"2021-01-08","objectID":"/posts/%E5%9F%BA%E7%A1%80-%E5%AD%97%E7%AC%A6%E7%BC%96%E7%A0%81/","tags":["基础"],"title":"字符编码","uri":"/posts/%E5%9F%BA%E7%A1%80-%E5%AD%97%E7%AC%A6%E7%BC%96%E7%A0%81/"},{"categories":["笔记"],"content":"最早的计算机系统都是使用 EBCDIC(扩展的二进制的十进制转换码) 和 ASCII 编码，因为那时候只是用一些英文字母数字，加减号和其他一些字符，字符并不多，但是随着Internet的发展，网络遍布全球。全球有大概6000种语言（其中3000种在巴布亚新几内亚…） ，为了更好地服务更多的人，我们需要为不同语言的用户提供不同的语言支持。如果世界只需要 ASCII 编码，那样将会简单很多。但事实却是非常复杂的。 字符集 character code 字符编码就是将一个字符映射到一个整数，比如最常见的 ASCII编码，将 a 编码为 97（编码点,code point）， A编码为65.编码仍然是抽象的，它仍不是我们在文本或者TCP包中多见到的。 字符集常用的有ascii字符集,unicode字符集 ASCII 我们说ascii的时候，其实包含了ascii字符集和ascii编码。 ASCII 编码大家都很熟悉了，大一的时候专业基础课接触到的就是这些内容。作为最常用的编码，ASCII使用的编码点使用7-bit。所以他一共有128个编码。 这里问题来了，ASCII是一个字节，7位就能表示字符，那最左边的那一位有什么用？ 回答：最高位是表示扩展字符集的。换句话说当最高位为0时，表示这一个字节就表示一个字符；如果是1，那就代表要和下一个字节连起来看，这也就是ASCII码表示汉字需要两个字节的原因。 ISO 8859 现在一个字节的标准是8个比特位，作为ASCII的扩展多出来128个编码点。一些列不同的编码集使用了这128个编码点，它们被一些欧洲语言使用，合起来就是 ISO-8859系列。 ISO-8859-1也就是常说的Latin-1，它涵盖了大部分的欧洲语言。 ISO-8859是一个系列，系列中所有的低128个编码点就是ASCII编码，以实现兼容。 早期的HTML标准推荐使用ISO-8859-1字符集。不过在 HTML 4 之后就推荐使用 Unicode了。 Unicode 像ASCII和ISO8859这样的编码在象形文字（中日韩）语言面前显得就太小气了。中文常用的字多大几千个，至少需要两个字节才能涵括。最初没有统一的国际标准，因此出现了很多的2字节编码方案，比如台湾的Big5，国内的GB2312，BGK；再考虑日本的JIS X 0208等等，字符集简直是一个大乱斗。 Unicode是一个包含所有主要当前在用字符的新的标准，它包括了欧洲，亚洲，印度等等各种语言，Unicode的好处是它是可扩展的。到5.2版本，一共有超过 107000个字符。 Unicode编码点是兼容ISO8859的，也就是说它的前256个编码点就是ISO 8859-1.要在计算机系统表示一个Unicode字符，需要使用一种编码方式。UCS（通用编码方式）使用两个字节进行编码，然而随着Unicod囊括的字符越来越多，UCS不再使用，而是使用 UTF-*的编码。 unicode字符集其实就是一个字符和整型数的映射库。它本身并不会指定需要用几个字节去代表一个字符。 utf UTF即 Unicode Transformation format.常见的编码方式如下： UTF-32 4字节编码，不常用，尤其是 HTML5 规范明确反对使用它 UTF-16 UTF-16是一种可变长度字符编码方式，以16-bit 为单元，使用2个或4个字节为每个字符编码。 UTF-16的编码规则如下： |Unicode编码范围 （十六进制）| UTF-16编码占用字节| UTF-16 编码（二进制） |–|–|–| |U+0000 - U+FFFF| 2| xxxxxxxx xxxxxxxx| |U+10000 - U+10FFFF| 4| 110110yyyyyyyyyy 110111xxxxxxxxxx| UTF-8 一个字符使用1-4字节，不定长度，最常用的，对英文字符基本和ISO-8859对应，中文编码为3-4字符，相UTF-8 的编码规则很简单，只有二条： 1）对于单字节的符号，字节的第一位设为0，后面7位为这个符号的 Unicode 码。因此对于英语字母，UTF-8 编码和 ASCII 码是相同的。 2）对于n字节的符号（n \u003e 1），第一个字节的前n位都设为1，第n + 1位设为0，后面字节的前两位一律设为10。剩下的没有提及的二进制位，全部为这个符号的 Unicode 码。 如果一个字节的第一位是0，则这个字节单独就是一个字符；如果第一位是1，则连续有多少个1，就表示当前字符占用多少个字节。 |Unicode编码范围（十六进制）| UTF-8编码占用字节| UTF-8 编码（二进制）| |–|–|–| |U+0000 - U+007F| 1| 0xxxxxxx| |U+0080 - U+07FF| 2| 110xxxxx 10xxxxxx| |U+0800 - U+FFFF| 3| 1110xxxx 10xxxxxx 10xxxxxx| |U+10000 - U+10FFFF| 4| 11110xxx 10xxxxxx 10xxxxxx 10xxxxxx| UTF-7 有时使用，但不常用 要理解Unicode和UTF-8之间的区别和联系。知道UTF的名字是Unicode转换格式就可以了。Unicode是一个字符集，UTF-8是这个字符集的一种编码方式。 Go与字符集 UTF-8 Go的字符串的每一个字符称为一个 rune，它是 int32的别名，因为一个Unicode字符的长度可能是1，2，3，4个字节，如果要统计字符数，就需要计算的是rune的个数而不是字节数了。字符数和字节数只有在是字符串只由ASCII字符组成时才是一样的。 Rune选择int32而不是uint32呢？ This has been asked several times. rune occupies 4 bytes and not just one because it is supposed to store unicode codepoints and not just ASCII characters. Like array indices, the datatype is signed so that you can easily detect overflows or other errors while doing arithmetic with those types. 意思技术方便检测溢出或错误。 ","date":"2021-01-08","objectID":"/posts/%E5%9F%BA%E7%A1%80-%E5%AD%97%E7%AC%A6%E7%BC%96%E7%A0%81/:0:0","tags":["基础"],"title":"字符编码","uri":"/posts/%E5%9F%BA%E7%A1%80-%E5%AD%97%E7%AC%A6%E7%BC%96%E7%A0%81/"},{"categories":["笔记"],"content":"mysq crash-safe","date":"2020-12-28","objectID":"/posts/mysql_crash_safe/","tags":["mysql"],"title":"mysq crash-safe","uri":"/posts/mysql_crash_safe/"},{"categories":["笔记"],"content":"mysql的crash-safe MySQL 保证数据不会丢的能力主要体现在两方面： 能够恢复到任何时间点的状态； 能够保证MySQL在任何时间段突然奔溃，重启后之前提交的记录都不会丢失； 对于第一点将MySQL恢复到任何时间点的状态，相信很多人都知道，只要保留有足够的binlog，就能通过重跑binlog来实现。对于第二点的能力，也就是本文标题所讲的crash-safe。即在 InnoDB 存储引擎中，事务提交过程中任何阶段，MySQL突然奔溃，重启后都能保证事务的完整性，已提交的数据不会丢失，未提交完整的数据会自动进行回滚。这个能力依赖的就是redo log和unod log两个日志。 更新语句在MySQL中是怎么执行的，简单进行总结一下： 从内存中找出这条数据记录，对其进行更新； 将对数据页的更改记录到redo log中； 将逻辑操作记录到binlog中； 对于内存中的数据和日志，都是由后台线程，当触发到落盘规则后再异步进行刷盘； WAL机制 为什么不直接更改磁盘中的数据，而要在内存中更改，然后还需要写日志，最后再落盘这么复杂？ MySQL更改数据的时候，之所以不直接写磁盘文件中的数据，最主要就是性能问题。因为直接写磁盘文件是随机写，开销大性能低，没办法满足MySQL的性能要求。所以才会设计成先在内存中对数据进行更改，再异步落盘。但是内存总是不可靠，万一断电重启，还没来得及落盘的内存数据就会丢失，所以还需要加上写日志这个步骤，万一断电重启，还能通过日志中的记录进行恢复。 写日志虽然也是写磁盘，但是它是顺序写，相比随机写开销更小，能提升语句执行的性能。 日志先行的技术，指的是对数据文件进行修改前，必须将修改先记录日志。保证了数据一致性和持久性，并且提升语句执行性能。 核心日志模块 更新SQL语句执行流程中，总共需要写3个日志，这3个是不是都需要，能不能进行简化？更新SQL执行过程中，总共涉及MySQL日志模块其中的三个核心日志，分别是redo log（重做日志）、undo log（回滚日志）、binlog（归档日志）。这里提前预告，crash-safe的能力主要依赖的就是这三大日志。 ","date":"2020-12-28","objectID":"/posts/mysql_crash_safe/:0:0","tags":["mysql"],"title":"mysq crash-safe","uri":"/posts/mysql_crash_safe/"},{"categories":["笔记"],"content":"重做日志 redo log redo log也称为事务日志，由InnoDB存储引擎层产生。记录的是数据库中每个页的修改，而不是某一行或某几行修改成怎样，可以用来恢复提交后的物理数据页（恢复数据页，且只能恢复到最后一次提交的位置，因为修改会覆盖之前的）。 前面提到的WAL技术，redo log就是WAL的典型应用，MySQL在有事务提交对数据进行更改时，只会在内存中修改对应的数据页和记录redo log日志，完成后即表示事务提交成功，至于磁盘数据文件的更新则由后台线程异步处理。由于redo log的加入，保证了MySQL数据一致性和持久性（即使数据刷盘之前MySQL奔溃了，重启后仍然能通过redo log里的更改记录进行重放，重新刷盘），此外还能提升语句的执行性能（写redo log是顺序写，相比于更新数据文件的随机写，日志的写入开销更小，能显著提升语句的执行性能，提高并发量），由此可见redo log是必不可少的。 redo log是固定大小的，所以只能循环写，从头开始写，写到末尾就又回到开头，相当于一个环形。当日志写满了，就需要对旧的记录进行擦除，但在擦除之前，需要确保这些要被擦除记录对应在内存中的数据页都已经刷到磁盘中了。在redo log满了到擦除旧记录腾出新空间这段期间，是不能再接收新的更新请求，所以有可能会导致MySQL卡顿。（所以针对并发量大的系统，适当设置redo log的文件大小非常重要！！！） ","date":"2020-12-28","objectID":"/posts/mysql_crash_safe/:1:0","tags":["mysql"],"title":"mysq crash-safe","uri":"/posts/mysql_crash_safe/"},{"categories":["笔记"],"content":"回滚日志 undo log undo log顾名思义，主要就是提供了回滚的作用，但其还有另一个主要作用，就是多个行版本控制(MVCC)，保证事务的原子性。在数据修改的流程中，会记录一条与当前操作相反的逻辑日志到undo log中（可以认为当delete一条记录时，undo log中会记录一条对应的insert记录，反之亦然，当update一条记录时，它记录一条对应相反的update记录），如果因为某些原因导致事务异常失败了，可以借助该undo log进行回滚，保证事务的完整性，所以undo log也必不可少。 ","date":"2020-12-28","objectID":"/posts/mysql_crash_safe/:2:0","tags":["mysql"],"title":"mysq crash-safe","uri":"/posts/mysql_crash_safe/"},{"categories":["笔记"],"content":"归档日志 bin log binlog在MySQL的server层产生，不属于任何引擎，主要记录用户对数据库操作的SQL语句（除了查询语句）。之所以将binlog称为归档日志，是因为binlog不会像redo log一样擦掉之前的记录循环写，而是一直记录（超过有效期才会被清理），如果超过单日志的最大值（默认1G，可以通过变量 max_binlog_size 设置），则会新起一个文件继续记录。但由于日志可能是基于事务来记录的(如InnoDB表类型)，而事务是绝对不可能也不应该跨文件记录的，如果正好binlog日志文件达到了最大值但事务还没有提交则不会切换新的文件记录，而是继续增大日志，所以 max_binlog_size 指定的值和实际的binlog日志大小不一定相等。 两阶段提交 问题：为什么redo log要分两步写，中间再穿插写binlog呢？ 从上面可以看出，因为redo log影响主库的数据，binlog影响从库的数据，所以redo log和binlog必须保持一致才能保证主从数据一致，这是前提。 事务的提交过程有两个阶段，就是将redo log的写入拆成了两个步骤：prepare和commit，中间再穿插写入binlog。 如果只有binlog，那么不管先写binlog日志还是先写库，都有可能数据和日志不一致，可是主从同步就是利用的binlog，那么主从就不一致了。 数据恢复流程 我们先来看一下崩溃恢复时的判断规则： 如果 redo log 里面的事务是完整的，也就是已经有了 commit 标识，则直接提交； 如果 redo log 里面的事务只有完整的 prepare，则判断对应的事务 binlog 是否存在并完整： a. 如果是，则提交事务； b. 否则，回滚事务。 如果在图中时刻 A 的地方，也就是写入 redo log 处于 prepare 阶段之后、写 binlog 之前，发生了崩溃（crash），由于此时 binlog 还没写，redo log 也还没提交，所以崩溃恢复的时候，这个事务会回滚。这时候，binlog 还没写，所以也不会传到备库。到这里，大家都可以理解。 时刻 B，也就是 binlog 写完，redo log 还没 commit 前发生 crash。时刻 B 发生 crash 对应的就是 2(a) 的情况，崩溃恢复过程中事务会被提交。 追问 ","date":"2020-12-28","objectID":"/posts/mysql_crash_safe/:3:0","tags":["mysql"],"title":"mysq crash-safe","uri":"/posts/mysql_crash_safe/"},{"categories":["笔记"],"content":"追问 1：MySQL 怎么知道 binlog 是完整的? 回答：一个事务的 binlog 是有完整格式的： statement 格式的 binlog，最后会有 COMMIT； row 格式的 binlog，最后会有一个 XID event。 另外，在 MySQL 5.6.2 版本以后，还引入了 binlog-checksum 参数，用来验证 binlog 内容的正确性。对于 binlog 日志由于磁盘原因，可能会在日志中间出错的情况，MySQL 可以通过校验 checksum 的结果来发现。所以，MySQL 还是有办法验证事务 binlog 的完整性的。 ","date":"2020-12-28","objectID":"/posts/mysql_crash_safe/:4:0","tags":["mysql"],"title":"mysq crash-safe","uri":"/posts/mysql_crash_safe/"},{"categories":["笔记"],"content":"追问 2：redo log 和 binlog 是怎么关联起来的? 回答：它们有一个共同的数据字段，叫 XID。崩溃恢复的时候，会按顺序扫描 redo log： 如果碰到既有 prepare、又有 commit 的 redo log，就直接提交； 如果碰到只有 parepare、而没有 commit 的 redo log，就拿着 XID 去 binlog 找对应的事务。 ","date":"2020-12-28","objectID":"/posts/mysql_crash_safe/:5:0","tags":["mysql"],"title":"mysq crash-safe","uri":"/posts/mysql_crash_safe/"},{"categories":["笔记"],"content":"追问 3：处于 prepare 阶段的 redo log 加上完整 binlog，重启就能恢复，MySQL 为什么要这么设计? 回答：其实，这个问题还是跟我们在反证法中说到的数据与备份的一致性有关。在时刻 B，也就是 binlog 写完以后 MySQL 发生崩溃，这时候 binlog 已经写入了，之后就会被从库（或者用这个 binlog 恢复出来的库）使用。 所以，在主库上也要提交这个事务。采用这个策略，主库和备库的数据就保证了一致性。 ","date":"2020-12-28","objectID":"/posts/mysql_crash_safe/:6:0","tags":["mysql"],"title":"mysq crash-safe","uri":"/posts/mysql_crash_safe/"},{"categories":["笔记"],"content":"追问 4：如果这样的话，为什么还要两阶段提交呢？干脆先 redo log 写完，再写 binlog。崩溃恢复的时候，必须得两个日志都完整才可以。是不是一样的逻辑？ 回答：其实，两阶段提交是经典的分布式系统问题，并不是 MySQL 独有的。 如果必须要举一个场景，来说明这么做的必要性的话，那就是事务的持久性问题。 对于 InnoDB 引擎来说，如果 redo log 提交完成了，事务就不能回滚（如果这还允许回滚，就可能覆盖掉别的事务的更新）。而如果 redo log 直接提交，然后 binlog 写入的时候失败，InnoDB 又回滚不了，数据和 binlog 日志又不一致了。 两阶段提交就是为了给所有人一个机会，当每个人都说“我 ok”的时候，再一起提交。 ","date":"2020-12-28","objectID":"/posts/mysql_crash_safe/:7:0","tags":["mysql"],"title":"mysq crash-safe","uri":"/posts/mysql_crash_safe/"},{"categories":["笔记"],"content":"追问 5：不引入两个日志，也就没有两阶段提交的必要了。只用 binlog 来支持崩溃恢复，又能支持归档，不就可以了？ 回答：这位同学的意思是，只保留 binlog，然后可以把提交流程改成这样：… -\u003e “数据更新到内存” -\u003e “写 binlog” -\u003e “提交事务”，是不是也可以提供崩溃恢复的能力？ 答案是不可以。 ","date":"2020-12-28","objectID":"/posts/mysql_crash_safe/:8:0","tags":["mysql"],"title":"mysq crash-safe","uri":"/posts/mysql_crash_safe/"},{"categories":["笔记"],"content":"追问 6：那能不能反过来，只用 redo log，不要 binlog？ 回答：如果只从崩溃恢复的角度来讲是可以的。你可以把 binlog 关掉，这样就没有两阶段提交了，但系统依然是 crash-safe 的。 但是，如果你了解一下业界各个公司的使用场景的话，就会发现在正式的生产库上，binlog 都是开着的。因为 binlog 有着 redo log 无法替代的功能。 一个是归档。redo log 是循环写，写到末尾是要回到开头继续写的。这样历史日志没法保留，redo log 也就起不到归档的作用。 一个就是 MySQL 系统依赖于 binlog。binlog 作为 MySQL 一开始就有的功能，被用在了很多地方。其中，MySQL 系统高可用的基础，就是 binlog 复制。 ","date":"2020-12-28","objectID":"/posts/mysql_crash_safe/:9:0","tags":["mysql"],"title":"mysq crash-safe","uri":"/posts/mysql_crash_safe/"},{"categories":["笔记"],"content":"追问 9：redo log buffer 是什么？是先修改内存，还是先写 redo log 文件？ 回答：这两个问题可以一起回答。 在一个事务的更新过程中，日志是要写多次的。比如下面这个事务： begin; insert into t1 ... insert into t2 ... commit; 这个事务要往两个表中插入记录，插入数据的过程中，生成的日志都得先保存起来，但又不能在还没 commit 的时候就直接写到 redo log 文件里。 所以，redo log buffer 就是一块内存，用来先存 redo 日志的。也就是说，在执行第一个 insert 的时候，数据的内存被修改了，redo log buffer 也写入了日志。 但是，真正把日志写到 redo log 文件（文件名是 ib_logfile+ 数字），是在执行 commit 语句的时候做的。 扩展 在MySQL内部，在事务提交时利用两阶段提交(内部XA的两阶段提交)很好地解决了上面提到的binlog和redo log的一致性问题： 第一阶段： InnoDB Prepare阶段。此时SQL已经成功执行，并生成事务ID(xid)信息及redo和undo的内存日志。此阶段InnoDB会写事务的redo log，但要注意的是，此时redo log只是记录了事务的所有操作日志，并没有记录提交（commit）日志，因此事务此时的状态为Prepare。此阶段对binlog不会有任何操作。 第二阶段：commit 阶段，这个阶段又分成两个步骤。第一步写binlog（先调用write()将binlog内存日志数据写入文件系统缓存，再调用fsync()将binlog文件系统缓存日志数据永久写入磁盘）；第二步完成事务的提交（commit），此时在redo log中记录此事务的提交日志（增加commit 标签）。 可以看出，此过程中是先写redo log再写binlog的。但需要注意的是，在第一阶段并没有记录完整的redo log（不包含事务的commit标签），而是在第二阶段记录完binlog后再写入redo log的commit 标签。还要注意的是，在这个过程中是以第二阶段中binlog的写入与否作为事务是否成功提交的标志。 ","date":"2020-12-28","objectID":"/posts/mysql_crash_safe/:10:0","tags":["mysql"],"title":"mysq crash-safe","uri":"/posts/mysql_crash_safe/"},{"categories":["笔记"],"content":"mysq避免幻读的原理","date":"2020-12-28","objectID":"/posts/mysql_%E9%81%BF%E5%85%8D%E5%B9%BB%E8%AF%BB%E7%9A%84%E5%8E%9F%E7%90%86/","tags":["mysql"],"title":"mysq避免幻读的原理","uri":"/posts/mysql_%E9%81%BF%E5%85%8D%E5%B9%BB%E8%AF%BB%E7%9A%84%E5%8E%9F%E7%90%86/"},{"categories":["笔记"],"content":"mysql在可重复读隔离级别实现了可重复读靠两种技术：mvcc + next lock 多版本并发控制（MVCC）（快照读/一致性读） 多数数据库都实现了多版本并发控制，并且都是靠保存数据快照来实现的。以 InnoDB 为例，每一行中都冗余了两个字断。 一个是行的创建版本，一个是行的删除（过期）版本。具体的版本号（trx_id）存在 information_schema.INNODB_TRX 表中。版本号（trx_id）随着每次事务的开启自增。 事务每次取数据的时候都会取创建版本小于当前事务版本的数据，以及过期版本大于当前版本的数据。 普通的 select 就是快照读。 select * from T where number = 1; 原理：将历史数据存一份快照，所以其他事务增加与删除数据，对于当前事务来说是不可见的。 next-key 锁 （当前读） next-key 锁包含两部分： 记录锁（行锁） 间隙锁 记录锁是加在索引上的锁，间隙锁是加在索引之间的。 select * from T where number = 1 for update; select * from T where number = 1 lock in share mode; 原理：将当前数据行与上一条数据和下一条数据之间的间隙锁定，保证此范围内读取的数据是一致的。 ","date":"2020-12-28","objectID":"/posts/mysql_%E9%81%BF%E5%85%8D%E5%B9%BB%E8%AF%BB%E7%9A%84%E5%8E%9F%E7%90%86/:0:0","tags":["mysql"],"title":"mysq避免幻读的原理","uri":"/posts/mysql_%E9%81%BF%E5%85%8D%E5%B9%BB%E8%AF%BB%E7%9A%84%E5%8E%9F%E7%90%86/"},{"categories":["笔记"],"content":"mysq ACID实现原理","date":"2020-12-25","objectID":"/posts/mysql_acid%E5%AE%9E%E7%8E%B0/","tags":["mysql"],"title":"mysq ACID实现原理","uri":"/posts/mysql_acid%E5%AE%9E%E7%8E%B0/"},{"categories":["笔记"],"content":"事务 典型的MySQL事务是如下操作的： start transaction; # 一条或多条sql语句 commit; 其中start transaction标识事务开始，commit提交事务，将执行结果写入到数据库。如果sql语句执行出现问题，会调用rollback，回滚所有已经执行成功的sql语句。当然，也可以在事务中直接使用rollback语句进行回滚。 MySQL中默认采用的是自动提交（autocommit）模式。在自动提交模式下，如果没有start transaction显式地开始一个事务，那么每个sql语句都会被当做一个事务执行提交操作。 针对某次连接也可以临时指定不自动提交，set autocommit =0即可。 特殊操作： 在MySQL中，存在一些特殊的命令，如果在事务中执行了这些命令，会马上强制执行commit提交事务；如DDL语句(create table/drop table/alter/table)、lock tables语句等等。 不过，常用的select、insert、update和delete命令，都不会强制提交事务。 ACID特性 ACID是衡量事务的四个特性： 原子性（Atomicity，或称不可分割性） 一致性（Consistency） 隔离性（Isolation） 持久性（Durability） 按照严格的标准，只有同时满足ACID特性才是事务；但是在各大数据库厂商的实现中，真正满足ACID的事务少之又少。因此与其说ACID是事务必须满足的条件，不如说它们是衡量事务的四个维度。 原子性 实现原理：undo log。 MySQL的日志有很多种，如二进制日志、错误日志、查询日志、慢查询日志等，此外InnoDB存储引擎还提供了两种事务日志：redo log(重做日志)和undo log(回滚日志)。其中redo log用于保证事务持久性；undo log则是事务原子性和隔离性实现的基础。undo log属于逻辑日志，它记录的是sql执行相关的信息。 实现原子性的关键，是当事务回滚时能够撤销所有已经成功执行的sql语句。InnoDB实现回滚，靠的是undo log：当事务对数据库进行修改时，InnoDB会生成对应的undo log；如果事务执行失败或调用了rollback，导致事务需要回滚，便可以利用undo log中的信息将数据回滚到修改之前的样子。 持久性 持久性是指事务一旦提交，它对数据库的改变就应该是永久性的。接下来的其他操作或故障不应该对其有任何影响。 实现原理：redo log。 redo log和undo log都属于InnoDB特有的事务日志。redo log是物理日志。 ","date":"2020-12-25","objectID":"/posts/mysql_acid%E5%AE%9E%E7%8E%B0/:0:0","tags":["mysql"],"title":"mysq ACID实现原理","uri":"/posts/mysql_acid%E5%AE%9E%E7%8E%B0/"},{"categories":["笔记"],"content":"redo log的来历 InnoDB作为MySQL的存储引擎，数据是存放在磁盘中的，但如果每次读写数据都需要磁盘IO，效率会很低。为此，InnoDB提供了缓存(Buffer Pool)，Buffer Pool中包含了磁盘中部分数据页的映射，作为访问数据库的缓冲：当从数据库读取数据时，会首先从Buffer Pool中读取，如果Buffer Pool中没有，则从磁盘读取后放入Buffer Pool；当向数据库写入数据时，会首先写入Buffer Pool，Buffer Pool中修改的数据会定期刷新到磁盘中（这一过程称为刷脏）。 Buffer Pool的使用大大提高了读写数据的效率，但是也带了新的问题：如果MySQL宕机，而此时Buffer Pool中修改的数据还没有刷新到磁盘，就会导致数据的丢失，事务的持久性无法保证。于是，redo log被引入来解决这个问题：当数据修改时，除了修改Buffer Pool中的数据，还会在redo log记录这次操作；当事务提交时，会调用fsync接口对redo log进行刷盘。如果MySQL宕机，重启时可以读取redo log中的数据，对数据库进行恢复。redo log采用的是WAL（Write-ahead logging，预写式日志），所有修改先写入日志，再更新到Buffer Pool，保证了数据不会因MySQL宕机而丢失，从而满足了持久性要求。 ","date":"2020-12-25","objectID":"/posts/mysql_acid%E5%AE%9E%E7%8E%B0/:1:0","tags":["mysql"],"title":"mysq ACID实现原理","uri":"/posts/mysql_acid%E5%AE%9E%E7%8E%B0/"},{"categories":["笔记"],"content":"为什么binlog不能做到crash-safe 简单来说一句话：binlog可能和实际的库不一致。 假如只有binlog，有可能先提交事务再写binlog，有可能事务提交数据更新之后数据库崩了，还没来得及写binlog。我们都知道binlog一般用来做数据库的主从复制或恢复数据库，这样就导致主从数据库不一致或者无法恢复数据库了。同样即使先写binlog再提交事务更新数据库，还是有可能写binlog成功之后数据库崩掉而导致数据库更新失败，这样也会导致主从数据库不一致或者无法恢复数据库。所以只有binlog做不到crash-safe。为了支持crash-safe，需要redolog，而且为了保证逻辑一致，事务提交需要两个阶段：prepare阶段和commit阶段。写redolog并落入磁盘(prepare状态)–\u003e写binlog–\u003ecommit。commit的时候是不会落盘的。 如果binlog写成功之后，将redolog置成commit的时候数据库崩了，如果在commit的时候redolog才落盘，由于事务是否成功以binlog为依据，上面的情况下事务是成功的，但是redolog没有写到磁盘，丢了。恢复之后数据库与binlog就不一致了。如果在prepare阶段落盘，上面的情况下redolog已经写入到文件了（在prepare阶段已经写盘了），恢复的时候不会丢数据。 同样的，如果不分两个阶段。假如redolog和binlog独立，那么还是会出现“为什么binlog不能做到crash-safe”里面描述的问题：数据库与binlog不一致。 ","date":"2020-12-25","objectID":"/posts/mysql_acid%E5%AE%9E%E7%8E%B0/:2:0","tags":["mysql"],"title":"mysq ACID实现原理","uri":"/posts/mysql_acid%E5%AE%9E%E7%8E%B0/"},{"categories":["笔记"],"content":"为什么有了redo log可以做到crash-safe 先写redo log日志在改库,再加上二阶段。 ","date":"2020-12-25","objectID":"/posts/mysql_acid%E5%AE%9E%E7%8E%B0/:3:0","tags":["mysql"],"title":"mysq ACID实现原理","uri":"/posts/mysql_acid%E5%AE%9E%E7%8E%B0/"},{"categories":["笔记"],"content":"redo log与binlog 我们知道，在MySQL中还存在binlog(二进制日志)也可以记录写操作并用于数据的恢复，但二者是有着根本的不同的： （1）作用不同：redo log是用于crash recovery的，保证MySQL宕机也不会影响持久性；binlog是用于point-in-time recovery的，保证服务器可以基于时间点恢复数据，此外binlog还用于主从复制。 （2）层次不同：redo log是InnoDB存储引擎实现的，而binlog是MySQL的服务器层(可以参考文章前面对MySQL逻辑架构的介绍)实现的，同时支持InnoDB和其他存储引擎。 （3）内容不同：redo log是物理日志，内容基于磁盘的Page；binlog的内容是二进制的，根据binlog_format参数的不同，可能基于sql语句、基于数据本身或者二者的混合。 （4）写入时机不同：binlog在事务提交时写入；redo log的写入时机相对多元： 当事务提交时会调用fsync对redo log进行刷盘；这是默认情况下的策略，修改innodb_flush_log_at_trx_commit参数可以改变该策略，但事务的持久性将无法保证。 除了事务提交时，还有其他刷盘时机：如master thread每秒刷盘一次redo log等，这样的好处是不一定要等到commit时刷盘，commit速度大大加快。 innodb_flush_log_at_trx_commit=0：每秒一次将Log Buffer中数据写入到Log File中，并且Flush到磁盘。事务提交不会主动触发写磁盘操作。 innodb_flush_log_at_trx_commit=1：每次事务提交时将Log Buffer数据写入到Log File中，并且Flush到磁盘。 innodb_flush_log_at_trx_commit=2：每次事务提交时将Log Buffer数据写入到Log File中，但不立即Flush到磁盘，MySQL会每秒一次刷新到磁盘。 由于进程调度问题，每条一次操作不能保证每一秒都执行一次。 sync_binlog=0：每次事务提交后，将Binlog Cache中的数据写入到Binlog文件，但不立即刷新到磁盘。由文件系统(file system)决定何时刷新到磁盘中。 sync_binlog=N：每N次事务提交后，将Binlog Cache中的数据写入到Binlog文件,调用fdatasync()函数将数据刷新到磁盘中。 隔离性 隔离性研究的是不同事务之间的相互影响。隔离性是指，事务内部的操作与其他事务是隔离的，并发执行的各个事务之间不能互相干扰。严格的隔离性，对应了事务隔离级别中的Serializable (可串行化)，但实际应用中出于性能方面的考虑很少会使用可串行化。 隔离性追求的是并发情形下事务之间互不干扰。简单起见，我们主要考虑最简单的读操作和写操作(加锁读等特殊读操作会特殊说明)，那么隔离性的探讨，主要可以分为两个方面： (一个事务)写操作对(另一个事务)写操作的影响：锁机制保证隔离性 (一个事务)写操作对(另一个事务)读操作的影响：MVCC保证隔离性 不过需要说明的是，RR虽然避免了幻读问题，但是毕竟不是Serializable，不能保证完全的隔离: 例子，如果在事务中第一次读取采用非加锁读，第二次读取采用加锁读，则如果在两次读取之间数据发生了变化，两次读取到的结果不一样，因为加锁读时不会采用MVCC。 一致性 一致性是指事务执行结束后，数据库的完整性约束没有被破坏，事务执行的前后都是合法的数据状态。 可以说，一致性是事务追求的最终目标：前面提到的原子性、持久性和隔离性，都是为了保证数据库状态的一致性。 总结 原子性A实现原理：undo log。 持久性D实现原理：redo log。 隔离性I实现原理：mvcc + 锁。mvcc是解决读的隔离性，可以不用锁，更快。但是写写之间还是要靠排他锁。 ","date":"2020-12-25","objectID":"/posts/mysql_acid%E5%AE%9E%E7%8E%B0/:4:0","tags":["mysql"],"title":"mysq ACID实现原理","uri":"/posts/mysql_acid%E5%AE%9E%E7%8E%B0/"},{"categories":["笔记"],"content":"go 类型直接比较","date":"2020-12-08","objectID":"/posts/go_%E7%B1%BB%E5%9E%8B%E7%9B%B4%E6%8E%A5%E6%AF%94%E8%BE%83/","tags":["golang"],"title":"go 类型直接比较","uri":"/posts/go_%E7%B1%BB%E5%9E%8B%E7%9B%B4%E6%8E%A5%E6%AF%94%E8%BE%83/"},{"categories":["笔记"],"content":"结构体能否比较？数组能否比较？slice、map、channel能否比较？接口能否比较？函数能否比较？ 结构体比较 能用\"==\"直接比较的我们才称其为能比较。 类型不同的都不能直接比较，类型别名可以。不同类型可以先强转再比较。 相同类型时，字段都是可比较类型才能直接比较。 type T2 struct { Name string Age int } type T22 T2 type T222 = T2 type T3 struct { Name string Age int } func main() { t2 := T2{} t22 := T22{} t222 := T222{} t3 := T3{} fmt.Println(t2==t222) // =起别名能比较 fmt.Println(t2==t22) // 类型定义不能比较 fmt.Println(t2==t3) // 不同类型，哪怕字段完全相同不能比较 t33:=T3(t2) fmt.Println(t3==t33) // 不同类型，可以先强转再比较 } 结构体字段顺序、类型、字段名完全相同才可以强转。 数组比较 数组可以直接比较。前提是数组长度和类型相同，而且数组里面的元素是可以直接比较的。 func main() { arr1:=[2]int{} arr2:=[2]int{} fmt.Println(arr1==arr2) // 可以比较 arr11:=[2][]int{} arr22:=[2][]int{} fmt.Println(arr11==arr22) // invalid operation: arr11 == arr22 ([2][]int cannot be compared) } channel比较 channel可以直接比较，比较的是channel的地址 func main() { c1:=make(chan int) c2:=make(chan int) c11:=make(chan []int) c22:=make(chan []int) fmt.Println(c1==c2) // false fmt.Println(c11==c22) // false } slice、map比较 slice不能直接比较。slice之间之所以不能进行比较，是因为slice的元素是间接引用的。slice引用的底层数组的元素随时可能会被修改，即slice在不同的时间可能包含不同的值，所以无法进行比较。 map不能直接比较。 接口比较 接口类型的变量，包含该接口变量存储的值和值的类型两部分组成，分别称为接口的动态类型和动态值。只有动态类型和动态值都相同时，两个接口变量才相同 type Person interface { getName() string } type Student struct { Name string } type Teacher struct { Name string } func (s Student) getName() string { return s.Name } func (t Teacher) getName() string { return t.Name } func compare(s, t Person) bool { return s == t } func main() { s1 := Student{\"minping\"} s2 := Student{\"minping\"} t := Teacher{\"minping\"} fmt.Println(compare(s1, s2)) //true fmt.Println(compare(s1, t)) //false,类型不同 } 接口的动态类型必须要是可比较的，如果不能比较(比如slice，map)，则运行时会报panic。因为编译器在编译时无法获取接口的动态类型，所以编译能通过，但是运行时直接panic 函数比较 golang的func作为一等公民，也是一种类型，而且不可比较 type GetName func () type GetName2 func () func main() { var f1 GetName var f2 GetName fmt.Println(f1==f2) // invalid operation: f1 == f2 (func can only be compared to nil) } 总结 1.结构体等复合类型，只有每个元素(成员)可比较且相同类型时能比较。类型和值都相等时，两个复合元素才相等 2.slice，map不可比较，但是可以用reflect或者cmp包来比较 3.func作为golnag的一等公民，也是一个类型，也不能比较。 4.channel引用类型的比较是看指向的是不是同一个变量 5.类型再定义(type A string)不可比较，是两种不同的类型 6.类型别名(type A = string)可比较，是同一种类型。 7.reflect.DeepEqual函数可以用来比较两个任意类型的变量 ","date":"2020-12-08","objectID":"/posts/go_%E7%B1%BB%E5%9E%8B%E7%9B%B4%E6%8E%A5%E6%AF%94%E8%BE%83/:0:0","tags":["golang"],"title":"go 类型直接比较","uri":"/posts/go_%E7%B1%BB%E5%9E%8B%E7%9B%B4%E6%8E%A5%E6%AF%94%E8%BE%83/"},{"categories":["笔记"],"content":"curl","date":"2020-11-11","objectID":"/posts/curl/","tags":["http"],"title":"curl笔记","uri":"/posts/curl/"},{"categories":["笔记"],"content":"快速入门 curl 是常用的命令行工具，用来请求 Web 服务器。它的名字就是客户端（client）的 URL 工具的意思. -i参数可以显示http response的头信息，连同网页代码一起。 curl -i www.sina.com -v参数可以显示一次http通信的整个过程，包括端口连接和http request头信息。 HTTP动词 curl默认的HTTP动词是GET，使用-X参数可以支持其他动词。 curl -X POST www.example.com cookie 使用--cookie参数，可以让curl发送cookie。 curl --cookie \"name=xxx\" www.example.com -c cookie-file可以保存服务器返回的cookie到文件，-b cookie-file可以使用这个文件作为cookie信息，进行后续的请求 curl -c cookies http://example.com curl -b cookies http://example.com 增加头信息 有时需要在http request之中，自行增加一个头信息。--header参数就可以起到这个作用。也可以缩写为-H curl --header \"Content-Type:application/json\" http://example.com User-Agent -A参数指定客户端的用户代理标头，即User-Agent。curl 的默认用户代理字符串是curl/[version]。 curl -A 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/76.0.3809.100 Safari/537.36' https://google.com 也可以直接指定头字段 curl -H 'User-Agent: php/1.0' https://google.com -d -d参数用于发送 POST 请求的数据体。也就是–data的缩写 $ curl -d'login=emma＆password=123'-X POST https://google.com/login # 或者 $ curl -d 'login=emma' -d 'password=123' -X POST https://google.com/login 使用-d参数以后，HTTP 请求会自动加上标头Content-Type : application/x-www-form-urlencoded。并且会自动将请求转为 POST 方法，因此可以省略-X POST。 -d参数可以读取本地文本文件的数据，向服务器发送。 curl -d '@data.txt' https://google.com/login –data-urlencode参数等同于-d，发送 POST 请求的数据体，区别在于会自动将发送的数据进行 URL 编码。 -F 上传二进制文件。 -F参数用来向服务器上传二进制文件。 curl -F 'file=@photo.png' https://google.com/profile 上面命令会给 HTTP 请求加上标头Content-Type: multipart/form-data，然后将文件photo.png作为file字段上传。 -F参数可以指定 MIME 类型。 curl -F 'file=@photo.png;type=image/png' https://google.com/profile 上面命令指定 MIME 类型为image/png，否则 curl 会把 MIME 类型设为application/octet-stream。 -F参数也可以指定文件名。 -k参数指定跳过 SSL 检测 不会检查服务器的 SSL 证书是否正确。 curl -k https://www.example.com –limit-rate限制速度 –limit-rate用来限制 HTTP 请求和回应的带宽，模拟慢网速的环境。 curl --limit-rate 200k https://google.com 限制在每秒 200K 字节。 -o保存响应 -o参数将服务器的回应保存成文件，等同于wget命令。 curl -o example.html https://www.example.com ","date":"2020-11-11","objectID":"/posts/curl/:0:0","tags":["http"],"title":"curl笔记","uri":"/posts/curl/"},{"categories":["笔记"],"content":"go test","date":"2020-06-20","objectID":"/posts/go_test/","tags":["golang"],"title":"go test","uri":"/posts/go_test/"},{"categories":["笔记"],"content":"概述 我们可以为 Go 程序编写三类测试，即：功能测试（test）、基准测试（benchmark，也称性能测试），以及示例测试（example） 测试源码文件的主名称应该以被测源码文件的主名称为前导，并且必须以“_test”为后缀。例如，如果被测源码文件的名称为 demo52.go，那么针对它的测试源码文件的名称就应该是 demo52_test.go。 Go 语言对测试函数的名称和签名都有哪些规定？ 对于功能测试函数来说，其名称必须以Test为前缀，并且参数列表中只应有一个*testing.T类型的参数声明。 对于性能测试函数来说，其名称必须以Benchmark为前缀，并且唯一参数的类型必须是*testing.B类型的。 对于示例测试函数来说，其名称必须以Example为前缀，但对函数的参数列表没有强制规定。 ","date":"2020-06-20","objectID":"/posts/go_test/:0:1","tags":["golang"],"title":"go test","uri":"/posts/go_test/"},{"categories":["笔记"],"content":"测试流程 go test命令就会针对每个被测代码包，依次地进行构建、执行包中符合要求的测试函数，清理临时文件，打印测试结果。这就是通常情况下的主要测试流程。 输入了go test puzzlers/article20/q2，这表示我想对导入路径为puzzlers/article20/q2的代码包进行测试。 ","date":"2020-06-20","objectID":"/posts/go_test/:0:2","tags":["golang"],"title":"go test","uri":"/posts/go_test/"},{"categories":["笔记"],"content":"功能测试 t.Log方法以及t.Logf方法的作用，就是打印常规的测试日志，只不过当测试成功的时候，go test命令就不会打印这类日志了。如果你想在测试结果中看到所有的常规测试日志，那么可以在运行go test命令的时候加入标记-v。 若我们想让某个测试函数在执行的过程中立即失败，则可以在该函数中调用t.FailNow方法。我在下面把TestFail函数中的t.Fail()改为t.FailNow()。与t.Fail()不同，在t.FailNow()执行之后，当前函数会立即终止执行。 如果你想在测试失败的同时打印失败测试日志，那么可以直接调用t.Error方法或者t.Errorf方法。前者相当于t.Log方法和t.Fail方法的连续调用 ","date":"2020-06-20","objectID":"/posts/go_test/:0:3","tags":["golang"],"title":"go test","uri":"/posts/go_test/"},{"categories":["笔记"],"content":"性能测试 在运行go test命令的时候加了两个标记。 第一个标记及其值为-bench=.，只有有了这个标记，命令才会进行性能测试。该标记的值.表明需要执行任意名称的性能测试函数，当然了，函数名称还是要符合 Go 程序测试的基本规则的。 第二个标记及其值是-run=^$，这个标记用于表明需要执行哪些功能测试函数，这同样也是以函数名称为依据的。该标记的值^$意味着：只执行名称为空的功能测试函数，换句话说，不执行任何功能测试函数。 如果运行go test命令的时候不加-run标记，那么就会使它执行被测代码包中的所有功能测试函数。(可是我们现在不想测试功能测试，只想性能测试) $go test -bench=. -run=^$ puzzlers/article20/q3 goos: darwin goarch: amd64 pkg: puzzlers/article20/q3 BenchmarkGetPrimes-8 500000 2314 ns/op PASS ok puzzlers/article20/q3 1.192s 核心数 BenchmarkGetPrimes-8被称为单个性能测试的名称，它表示命令执行了性能测试函数BenchmarkGetPrimes，并且当时所用的最大 P 数量为8。 最大 P 数量相当于可以同时运行 goroutine 的逻辑 CPU 的最大个数。这里的逻辑 CPU，也可以被称为 CPU 核心，但它并不等同于计算机中真正的 CPU 核心，只是 Go 语言运行时系统内部的一个概念，代表着它同时运行 goroutine 的能力。 顺便说一句，一台计算机的 CPU 核心的个数，意味着它能在同一时刻执行多少条程序指令，代表着它并行处理程序指令的能力。 我们可以通过调用 runtime.GOMAXPROCS函数改变最大 P 数量，也可以在运行go test命令时，加入标记-cpu来设置一个最大 P 数量的列表，以供命令在多次测试时使用。 -cpu的值应该是一个正整数的列表,比如1,2,4。针对于此值中的每一个正整数，go test命令都会先设置最大 P 数量为该数，然后再执行测试函数。 go test命令会先以1,2,4为最大 P 数量分别去执行第一个测试函数，之后再用同样的方式执行第二个测试函数，以此类推。 执行次数 它指的是被测函数的执行次数，而不是性能测试函数的执行次数。 go test命令在执行性能测试函数的时候会给它一个正整数，若该测试函数的唯一参数的名称为b，则该正整数就由b.N代表。我们应该在测试函数中配合着编写代码，比如： for i := 0; i \u003c b.N; i++ { GetPrimes() } go test命令会先尝试把b.N设置为1，然后执行测试函数。如果测试函数的执行时间没有超过上限，此上限默认为 1 秒，那么命令就会改大b.N的值，然后再次执行测试函数，如此往复，直到这个时间大于或等于上限为止。 当某次执行的时间大于或等于上限时，我们就说这是命令此次对该测试函数的最后一次执行。这时的b.N的值就会被包含在测试结果中，也就是上述测试结果中的500000。 ","date":"2020-06-20","objectID":"/posts/go_test/:0:4","tags":["golang"],"title":"go test","uri":"/posts/go_test/"},{"categories":["笔记"],"content":"参数 go test 后面可以跟参数： -cpu p的数量，可以是个列表，如果不设置默认是机器核心数 -count标记是专门用于重复执行测试函数的。它的值必须大于或等于0，并且默认值为1。 如果我们在运行go test命令的时候追加了-count 5，那么对于每一个测试函数，命令都会在预设的不同条件下（比如不同的最大 P 数量下）分别重复执行五次。 注意：这是测试函数的执行数量，不是被测试函数 -benchmem 输出基准测试的内存分配统计信息。 -benchtime 用于指定基准测试的探索式测试执行时间上限 -coverprofile=xxxx.out 输出覆盖率的out文件，使用go tool cover -html=xxxx.out 命令转换成Html的覆盖率测试报告。 覆盖率测试将被测试的代码拷贝一份，在每个语句块中加入bool标识变量，测试结束后统计覆盖率并输出成out文件，因此性能上会有一定的影响。 ","date":"2020-06-20","objectID":"/posts/go_test/:0:5","tags":["golang"],"title":"go test","uri":"/posts/go_test/"},{"categories":["笔记"],"content":"Redis学习-lru和redis实现","date":"2020-06-16","objectID":"/posts/redis_lru%E5%92%8Credis%E5%AE%9E%E7%8E%B0/","tags":["redis"],"title":"Redis学习-lru和redis实现","uri":"/posts/redis_lru%E5%92%8Credis%E5%AE%9E%E7%8E%B0/"},{"categories":["笔记"],"content":"lru实现 数组实现 用一个数组来存储数据，给每一个数据项标记一个访问时间戳，每次插入新数据项的时候，先把数组中存在的数据项的时间戳自增，并将新数据项的时间戳置为0并插入到数组中。每次访问数组中的数据项的时候，将被访问的数据项的时间戳置为0。当数组空间已满时，将时间戳最大的数据项淘汰。 链表实现 利用一个链表来实现，每次新插入数据的时候将新数据插到链表的头部；每次缓存命中（即数据被访问），则将数据移到链表头部；那么当链表满的时候，就将链表尾部的数据丢弃。 hashmap + 链表 整体的设计思路是，可以使用 HashMap 存储 key，这样可以做到 save 和 get key的时间都是 O(1)，而 HashMap 的 Value 指向双向链表实现的 LRU 的 Node 节点， 其中 head 代表双向链表的表头，tail 代表尾部。首先预先设置 LRU 的容量，如果存储满了，可以通过 O(1) 的时间淘汰掉双向链表的尾部，每次新增和访问数据，都可以通过 O(1)的效率把新的节点增加到对头，或者把已经存在的节点移动到队头。 ","date":"2020-06-16","objectID":"/posts/redis_lru%E5%92%8Credis%E5%AE%9E%E7%8E%B0/:0:1","tags":["redis"],"title":"Redis学习-lru和redis实现","uri":"/posts/redis_lru%E5%92%8Credis%E5%AE%9E%E7%8E%B0/"},{"categories":["笔记"],"content":"Redis的LRU实现 redis服务器实际使用的是惰性删除和定期删除两种策略：通过配合使用这两种删除策略，服务器可很好的使用CPU的时间和避免浪费内存空间之间取得平衡。 如果按照HashMap和双向链表实现，需要额外的存储存放 next 和 prev 指针，牺牲比较大的存储空间，显然是不划算的。所以Redis采用了一个近似的做法，就是随机取出若干个key，然后按照访问时间排序后，淘汰掉最不经常使用的. redis中有很多数据类型（以后会出一个redis系列），为了实现key-value新老判断，不能像上面算法题中简单的链表就能实现. Redis采用了一个全局时钟在redisServer这个struct中的lruclock，这个时钟供每个object更新自己object的时间。其中存储了服务器自启动之后的lru时钟，该时钟是全局的lru时钟。 默认的LRU时钟的分辨率是1秒，可以通过改变REDIS_LRU_CLOCK_RESOLUTION宏的值来改变，Redis会在serverCron()中调用updateLRUClock定期的更新LRU时钟，更新的频率和hz参数有关，默认为100ms一次。 Redis最为一款优秀的内存数据库，用途非常广泛，其缓存代码设计和实现很值得学习，实现步骤主要有： 用一个全局时钟作为参照 对每个object初始化和操作的时候都更新它各自的lru时钟 随机挑选几个key，根据lru时钟计算idle的时间排序放入EvictionPool中，最终挑选idle时间最长的free，以释放空间。至于为什么随机和只选择5个，是为了性能考虑，如果做到全局一个一个排序就非常消耗CPU，而实际应用中没必要这么精确。 ","date":"2020-06-16","objectID":"/posts/redis_lru%E5%92%8Credis%E5%AE%9E%E7%8E%B0/:0:2","tags":["redis"],"title":"Redis学习-lru和redis实现","uri":"/posts/redis_lru%E5%92%8Credis%E5%AE%9E%E7%8E%B0/"},{"categories":["笔记"],"content":"redis lru配置 Redis配置中和LRU有关的有三个： maxmemory: 配置Redis存储数据时指定限制的内存大小，比如100m。当缓存消耗的内存超过这个数值时, 将触发数据淘汰。该数据配置为0时，表示缓存的数据量没有限制, 即LRU功能不生效。64位的系统默认值为0，32位的系统默认内存限制为3GB maxmemory_policy: 触发数据淘汰后的淘汰策略 maxmemory_samples: 随机采样的精度，也就是随即取出key的数目。该数值配置越大, 越接近于真实的LRU算法，但是数值越大，相应消耗也变高，对性能有一定影响，样本值默认为5。 ","date":"2020-06-16","objectID":"/posts/redis_lru%E5%92%8Credis%E5%AE%9E%E7%8E%B0/:0:3","tags":["redis"],"title":"Redis学习-lru和redis实现","uri":"/posts/redis_lru%E5%92%8Credis%E5%AE%9E%E7%8E%B0/"},{"categories":["笔记"],"content":"Redis学习-与数据库一致性问题","date":"2020-06-16","objectID":"/posts/redis_%E4%B8%8E%E6%95%B0%E6%8D%AE%E5%BA%93%E4%B8%80%E8%87%B4%E6%80%A7/","tags":["redis"],"title":"Redis学习-与数据库一致性问题","uri":"/posts/redis_%E4%B8%8E%E6%95%B0%E6%8D%AE%E5%BA%93%E4%B8%80%E8%87%B4%E6%80%A7/"},{"categories":["笔记"],"content":"背景 我们使用redis作为缓存，查询的时候先去redis查，如果有数据直接返回，降低数据库的压力。如果没有的话，查数据库，如果数据库中查到了数据则把数据写入到redis,并返回给调用方。 如果只是这样的读就没有啥问题了，如果是更新呢？ 先操作数据库？还是先操作缓存？ 操作缓存是直接删缓存还是更新缓存？ ","date":"2020-06-16","objectID":"/posts/redis_%E4%B8%8E%E6%95%B0%E6%8D%AE%E5%BA%93%E4%B8%80%E8%87%B4%E6%80%A7/:0:1","tags":["redis"],"title":"Redis学习-与数据库一致性问题","uri":"/posts/redis_%E4%B8%8E%E6%95%B0%E6%8D%AE%E5%BA%93%E4%B8%80%E8%87%B4%E6%80%A7/"},{"categories":["笔记"],"content":"是更新还是删除缓存 一般我们都是采取删除缓存缓存策略的，原因如下： 高并发环境下，无论是先操作数据库还是后操作数据库而言，如果加上更新缓存，那就更加容易导致数据库与缓存数据不一致问题。(删除缓存直接和简单很多) 如果每次更新了数据库，都要更新缓存【这里指的是频繁更新的场景，这会耗费一定的性能】，倒不如直接删除掉。等再次读取时，缓存里没有，那我到数据库找，在数据库找到再写到缓存里边 (体现懒加载) ","date":"2020-06-16","objectID":"/posts/redis_%E4%B8%8E%E6%95%B0%E6%8D%AE%E5%BA%93%E4%B8%80%E8%87%B4%E6%80%A7/:0:2","tags":["redis"],"title":"Redis学习-与数据库一致性问题","uri":"/posts/redis_%E4%B8%8E%E6%95%B0%E6%8D%AE%E5%BA%93%E4%B8%80%E8%87%B4%E6%80%A7/"},{"categories":["笔记"],"content":"先数据库还是先缓存 不管是先写MySQL数据库，再删除Redis缓存；还是先删除缓存，再写库，都有可能出现数据不一致的情况。举一个例子： 如果删除了缓存Redis，还没有来得及写库MySQL，另一个线程就来读取，发现缓存为空，则去数据库中读取数据写入缓存，此时缓存中为脏数据。 如果先写了库，在删除缓存前，写库的线程宕机了，没有删除掉缓存，则也会出现数据不一致情况。 ","date":"2020-06-16","objectID":"/posts/redis_%E4%B8%8E%E6%95%B0%E6%8D%AE%E5%BA%93%E4%B8%80%E8%87%B4%E6%80%A7/:0:3","tags":["redis"],"title":"Redis学习-与数据库一致性问题","uri":"/posts/redis_%E4%B8%8E%E6%95%B0%E6%8D%AE%E5%BA%93%E4%B8%80%E8%87%B4%E6%80%A7/"},{"categories":["笔记"],"content":"缓存和数据库一致性解决方案 1.第一种方案：采用延时双删策略 在写库前后都进行redis.del(key)操作，并且设定合理的超时时间。 public void write( String key, Object data ) { redis.delKey( key ); db.updateData( data ); Thread.sleep( 500 ); redis.delKey( key ); } 2、第二种方案：异步更新缓存(基于订阅binlog的同步机制) 技术整体思路：MySQL binlog增量订阅消费+消息队列+增量数据更新到redis 读取binlog后分析 ，利用消息队列,推送更新各台的redis缓存数据。这样一旦MySQL中产生了新的写入、更新、删除等操作，就可以把binlog相关的消息推送至Redis，Redis再根据binlog中的记录，对Redis进行更新。其实这种机制，很类似MySQL的主从备份机制，因为MySQL的主备也是通过binlog来实现的数据一致性。 这里可以结合使用canal(阿里的一款开源框架)，通过该框架可以对MySQL的binlog进行订阅，而canal正是模仿了mysql的slave数据库的备份请求，使得Redis的数据更新达到了相同的效果。 ","date":"2020-06-16","objectID":"/posts/redis_%E4%B8%8E%E6%95%B0%E6%8D%AE%E5%BA%93%E4%B8%80%E8%87%B4%E6%80%A7/:0:4","tags":["redis"],"title":"Redis学习-与数据库一致性问题","uri":"/posts/redis_%E4%B8%8E%E6%95%B0%E6%8D%AE%E5%BA%93%E4%B8%80%E8%87%B4%E6%80%A7/"},{"categories":["笔记"],"content":"mysql分布式事务","date":"2020-06-15","objectID":"/posts/mysql_%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1/","tags":["mysql"],"title":"mysql分布式事务","uri":"/posts/mysql_%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1/"},{"categories":["笔记"],"content":"mysql的事务 undolog 实现原子性A。 UndoLog的原理很简单，为了满足事务的原子性，在操作任何数据之前，首先将数据备份到一个地方（这个存储数据备份的地方称为UndoLog）。然后进行数据的修改。如果出现了错误或者用户执行了ROLLBACK语句，系统可以利用Undo Log中的备份将数据恢复到事务开始之前的状态。 redolog 实现持久性D。 和Undo Log相反，RedoLog记录的是新数据的备份。在事务提交前，只要将RedoLog持久化即可，不需要将数据持久化。当系统崩溃时，虽然数据没有持久化，但是RedoLog已经持久化。系统可以根据RedoLog的内容，将所有数据恢复到最新的状态。 mvcc+锁 mysql的隔离性I。 mvcc+锁来实现事务的隔离性，各个事务看到的是自己的快照。 一致性 mysql的一致性是通过上面的原子性、隔离性、持久性来实现的。 ","date":"2020-06-15","objectID":"/posts/mysql_%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1/:0:1","tags":["mysql"],"title":"mysql分布式事务","uri":"/posts/mysql_%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1/"},{"categories":["笔记"],"content":"分布式事务 分布式事务就是指事务的参与者、支持事务的服务器、资源服务器以及事务管理器分别位于不同的分布式系统的不同节点之上。简单的说，就是一次大的操作由不同的小操作组成，这些小的操作分布在不同的服务器上，且属于不同的应用，分布式事务需要保证这些小操作要么全部成功，要么全部失败。本质上来说，分布式事务就是为了保证不同数据库的数据一致性。 CAP CAP定理，又被叫作布鲁尔定理。对于设计分布式系统来说(不仅仅是分布式事务)的架构师来说，CAP就是你的入门理论。 C (一致性):对某个指定的客户端来说，读操作能返回最新的写操作。对于数据分布在不同节点上的数据上来说，如果在某个节点更新了数据，那么在其他节点如果都能读取到这个最新的数据，那么就称为强一致，如果有某个节点没有读取到，那就是分布式不一致。 A (可用性)：也就是满足高并发。非故障的节点在合理的时间内返回合理的响应(不是错误和超时的响应)。可用性的两个关键一个是合理的时间，一个是合理的响应。合理的时间指的是请求不能无限被阻塞，应该在合理的时间给出返回。合理的响应指的是系统应该明确返回结果并且结果是正确的，这里的正确指的是比如应该返回50，而不是返回40。 P (分区容错性):当出现网络分区后，系统能够继续工作。打个比方，这里个集群有多台机器，有台机器网络出现了问题，但是这个集群仍然可以正常工作。 cap同时最多满足两个。试想，又要多个服务器，又要高并发，那只能牺牲一致性了。 BASE base就是牺牲了一致性，满足ap，但是会最终一致性。 BASE 是 Basically Available(基本可用)、Soft state(软状态)和 Eventually consistent (最终一致性)三个短语的缩写。是对CAP中AP的一个扩展。 基本可用:分布式系统在出现故障时，允许损失部分可用功能，保证核心功能可用。 软状态:允许系统中存在中间状态，这个状态不影响系统可用性，这里指的是CAP中的不一致。 最终一致:最终一致是指经过一段时间后，所有节点数据都将会达到一致。 BASE解决了CAP中理论没有网络延迟，在BASE中用软状态和最终一致，保证了延迟后的一致性。BASE和 ACID 是相反的，它完全不同于ACID的强一致性模型，而是通过牺牲强一致性来获得可用性，并允许数据在一段时间内是不一致的，但最终达到一致状态。 具体解决方案 分布式事务又很多具体的解决方案， TCC 有TCC（Try-Confirm-Cancel）的概念； Try阶段：尝试执行,完成所有业务检查（一致性）,预留必须业务资源（准隔离性） Confirm阶段：确认执行真正执行业务，不作任何业务检查，只使用Try阶段预留的业务资源，Confirm操作满足幂等性。要求具备幂等设计，Confirm失败后需要进行重试。 Cancel阶段：取消执行，释放Try阶段预留的业务资源 Cancel操作满足幂等性Cancel阶段的异常和Confirm阶段异常处理方案基本上一致。 2pc 2PC就不得不聊数据库分布式事务中的 XA Transactions。 在XA协议中分为两阶段: 第一阶段：事务管理器要求每个涉及到事务的数据库预提交(precommit)此操作，并反映是否可以提交. 第二阶段：事务协调器要求每个数据库提交数据，或者回滚数据。 优点： 尽量保证了数据的强一致，实现成本较低，在各大主流数据库都有自己实现，对于MySQL是从5.5开始支持。 缺点: 单点问题:事务管理器在整个流程中扮演的角色很关键，如果其宕机，比如在第一阶段已经完成，在第二阶段正准备提交的时候事务管理器宕机，资源管理器就会一直阻塞，导致数据库无法使用。 同步阻塞:在准备就绪之后，资源管理器中的资源一直处于阻塞，直到提交完成，释放资源。 数据不一致:两阶段提交协议虽然为分布式数据强一致性所设计，但仍然存在数据不一致性的可能，比如在第二阶段中，假设协调者发出了事务commit的通知，但是因为网络问题该通知仅被一部分参与者所收到并执行了commit操作，其余的参与者则因为没有收到通知一直处于阻塞状态，这时候就产生了数据的不一致性。 总的来说，XA协议比较简单，成本较低，但是其单点问题，以及不能支持高并发(由于同步阻塞)依然是其最大的弱点。 本地消息表 本地消息表这个方案最初是ebay提出的 ebay的完整方案https://queue.acm.org/detail.cfm?id=1394128。 此方案的核心是将需要分布式处理的任务通过消息日志的方式来异步执行。消息日志可以存储到本地文本、数据库或消息队列，再通过业务规则自动或人工发起重试。人工重试更多的是应用于支付场景，通过对账系统对事后问题的处理。 1.当你扣钱的时候，你需要在你扣钱的服务器上新增加一个本地消息表，你需要把你扣钱和写入减去水的库存到本地消息表放入同一个事务(依靠数据库本地事务保证一致性。 2.这个时候有个定时任务去轮询这个本地事务表，把没有发送的消息，扔给商品库存服务器，叫他减去水的库存，到达商品服务器之后这个时候得先写入这个服务器的事务表，然后进行扣减，扣减成功后，更新事务表中的状态。 3.商品服务器通过定时任务扫描消息表或者直接通知扣钱服务器，扣钱服务器本地消息表进行状态更新。 4.针对一些异常情况，定时扫描未成功处理的消息，进行重新发送，在商品服务器接到消息之后，首先判断是否是重复的，如果已经接收，在判断是否执行，如果执行在马上又进行通知事务，如果未执行，需要重新执行需要由业务保证幂等，也就是不会多扣一瓶水。 怎么保证幂等性？ 我们可以把减库存+扣钱消息表的主键ID通过消息传到商品服务器，商品服务器去减库存，同时商品服务器也有一个处理历史表，把主键ID存进去，以后再来消息先判断历史记录表有没有处理过。 本地消息队列是BASE理论，是最终一致模型，适用于对一致性要求不高的。实现这个模型时需要注意重试的幂等。 参考资料： https://juejin.im/post/5b5a0bf9f265da0f6523913b ","date":"2020-06-15","objectID":"/posts/mysql_%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1/:0:2","tags":["mysql"],"title":"mysql分布式事务","uri":"/posts/mysql_%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1/"},{"categories":["笔记"],"content":"sql执行顺序","date":"2020-06-14","objectID":"/posts/mysql_sql%E6%89%A7%E8%A1%8C%E9%A1%BA%E5%BA%8F/","tags":["mysql"],"title":"sql执行顺序","uri":"/posts/mysql_sql%E6%89%A7%E8%A1%8C%E9%A1%BA%E5%BA%8F/"},{"categories":["笔记"],"content":"sql执行顺序 (1)from (3) join (2) on (4) where (5)group by(开始使用select中的别名，后面的语句中都可以使用) (6) avg,sum…. (7)having (8) select (9) distinct (10) order by (11) limit 从这个顺序中我们不难发现，所有的 查询语句都是从from开始执行的，在执行过程中，每个步骤都会为下一个步骤生成一个虚拟表，这个虚拟表将作为下一个执行步骤的输入。 第一步：首先对from子句中的前两个表执行一个笛卡尔乘积，此时生成虚拟表 vt1（选择相对小的表做基础表） 第二步：接下来便是应用on筛选器，on 中的逻辑表达式将应用到 vt1 中的各个行，筛选出满足on逻辑表达式的行，生成虚拟表 vt2 第三步：如果是outer join 那么这一步就将添加外部行，left outer jion 就把左表在第二步中过滤的添加进来，如果是right outer join 那么就将右表在第二步中过滤掉的行添加进来，这样生成虚拟表 vt3 第四步：如果 from 子句中的表数目多余两个表，那么就将vt3和第三个表连接从而计算笛卡尔乘积，生成虚拟表，该过程就是一个重复1-3的步骤，最终得到一个新的虚拟表 vt3。 第五步：应用where筛选器，对上一步生产的虚拟表引用where筛选器，生成虚拟表vt4，在这有个比较重要的细节不得不说一下，对于包含outer join子句的查询，就有一个让人感到困惑的问题，到底在on筛选器还是用where筛选器指定逻辑表达式呢？on和where的最大区别在于，如果在on应用逻辑表达式那么在第三步outer join中还可以把移除的行再次添加回来，而where的移除的最终的。举个简单的例子，有一个学生表（班级,姓名）和一个成绩表(姓名,成绩)，我现在需要返回一个x班级的全体同学的成绩，但是这个班级有几个学生缺考，也就是说在成绩表中没有记录。为了得到我们预期的结果我们就需要在on子句指定学生和成绩表的关系（学生.姓名=成绩.姓名）那么我们是否发现在执行第二步的时候，对于没有参加考试的学生记录就不会出现在vt2中，因为他们被on的逻辑表达式过滤掉了,但是我们用left outer join就可以把左表（学生）中没有参加考试的学生找回来，因为我们想返回的是x班级的所有学生，如果在on中应用学生.班级='x’的话，left outer join会把x班级的所有学生记录找回（感谢网友康钦谋__康钦苗的指正），所以只能在where筛选器中应用学生.班级='x’ 因为它的过滤是最终的。 第六步：group by 子句将中的唯一的值组合成为一组，得到虚拟表vt5。如果应用了group by，那么后面的所有步骤都只能得到的vt5的列或者是聚合函数（count、sum、avg等）。原因在于最终的结果集中只为每个组包含一行。这一点请牢记。 第七步：应用cube或者rollup选项，为vt5生成超组，生成vt6. 第八步：应用having筛选器，生成vt7。having筛选器是第一个也是为唯一一个应用到已分组数据的筛选器。 第九步：处理select子句。将vt7中的在select中出现的列筛选出来。生成vt8. 第十步：应用distinct子句，vt8中移除相同的行，生成vt9。事实上如果应用了group by子句那么distinct是多余的，原因同样在于，分组的时候是将列中唯一的值分成一组，同时只为每一组返回一行记录，那么所以的记录都将是不相同的。 第十一步：应用order by子句。按照order_by_condition排序vt9，此时返回的一个游标，而不是虚拟表。sql是基于集合的理论的，集合不会预先对他的行排序，它只是成员的逻辑集合，成员的顺序是无关紧要的。对表进行排序的查询可以返回一个对象，这个对象包含特定的物理顺序的逻辑组织。这个对象就叫游标。正因为返回值是游标，那么使用order by 子句查询不能应用于表表达式。排序是很需要成本的，除非你必须要排序，否则最好不要指定order by，最后，在这一步中是第一个也是唯一一个可以使用select列表中别名的步骤。 第十二步：应用top选项（limit）。此时才返回结果给请求者即用户。 ","date":"2020-06-14","objectID":"/posts/mysql_sql%E6%89%A7%E8%A1%8C%E9%A1%BA%E5%BA%8F/:0:0","tags":["mysql"],"title":"sql执行顺序","uri":"/posts/mysql_sql%E6%89%A7%E8%A1%8C%E9%A1%BA%E5%BA%8F/"},{"categories":["笔记"],"content":"protobuf v2","date":"2020-06-01","objectID":"/posts/protobuf_v2/","tags":["protobuf"],"title":"protobuf v2","uri":"/posts/protobuf_v2/"},{"categories":["笔记"],"content":"protobuf 是什么 Protocol Buffer (简称Protobuf) 是Google出品的性能优异、跨语言、跨平台的序列化库。 2001年初，Protobuf首先在Google内部创建，很多项目也采用Protobuf进行消息的通讯，还有基于Protobuf的微服务框架GRPC。 可以看做xml、json等序列化的又一种形式，只不过序列化后它是二进制的。 Protobuf支持很多语言，比如C++、C#、Dart、Go、Java、Python、Rust等，同时也是跨平台的。 序列化(serialization、marshalling)的过程是指将数据结构或者对象的状态转换成可以存储(比如文件、内存)或者传输的格式(比如网络)。反向操作就是反序列化(deserialization、unmarshalling)的过程。 ","date":"2020-06-01","objectID":"/posts/protobuf_v2/:1:0","tags":["protobuf"],"title":"protobuf v2","uri":"/posts/protobuf_v2/"},{"categories":["笔记"],"content":"protobuf 为什么要有 二十世纪九十年代后期，XML开始流行，它是一种人类易读的基于文本的编码方式，易于阅读和理解。 JSON是一种更轻量级的基于文本的编码方式，经常用在client/server端的通讯中。 除此之外还有很多序列化格式。 protobuf序列化和反序列化速度更快； 文件更小存储需要更少的空间，传输时间短。 ","date":"2020-06-01","objectID":"/posts/protobuf_v2/:2:0","tags":["protobuf"],"title":"protobuf v2","uri":"/posts/protobuf_v2/"},{"categories":["笔记"],"content":"protobuf 基础 这个教程主要介绍proto2的开发。 使用protobuf需要一个.proto文件，在这里定义要序列化的格式。可以理解为我们工作中和服务端定义的接口文档或者java bean。 举例： user.proto syntax = \"proto2\";//生成java类所在的包名 package com.example.protobuftest.bean;message SearchRequest { required string query = 1; optional int32 page_number = 2; optional int32 result_per_page = 3;}第一行指定protobuf的版本，可以指定为proto2或3。如果没有指定，默认以proto2格式定义。 在这里我们定义了一个User类型，包括name和age字段。 ","date":"2020-06-01","objectID":"/posts/protobuf_v2/:3:0","tags":["protobuf"],"title":"protobuf v2","uri":"/posts/protobuf_v2/"},{"categories":["笔记"],"content":"package package是可选的。对于生成的java语言对于Java，包声明符会变为java的一个包，除非在.proto文件中提供了一个明确有java_package； 如果不写package,默认是文件名作为包名。 写了包名后，就可以用包名加以区分。比如test.model.UserInfo。 显示设置包名后生成的对应语言文件就按照这个来生成包名了。 option java_package = \"test.protobuf.sample\";option go_package = \"test.protobuf.sample\";","date":"2020-06-01","objectID":"/posts/protobuf_v2/:3:1","tags":["protobuf"],"title":"protobuf v2","uri":"/posts/protobuf_v2/"},{"categories":["笔记"],"content":"字段规则 所指定的消息字段修饰符必须是如下之一： required：一个格式良好的消息一定要含有1个这种字段。表示该值是必须要设置的； optional：消息格式中该字段可以有0个或1个值（不超过1个）。 repeated：在一个格式良好的消息中，这种字段可以重复任意多次（包括0次）。重复的值的顺序会被保留。表示该值可以重复，相当于java中的List。 required是永久性的：在将一个字段标识为required的时候，应该特别小心。如果在某些情况下不想写入或者发送一个required的字段，将原始该字段修饰符更改为optional可能会遇到问题——旧版本的使用者会认为不含该字段的消息是不完整的，从而可能会无目的的拒绝解析。 换句话说，字段设置了required，如果不设置就会序列化失败，同理，如果也会反序列化失败。 ","date":"2020-06-01","objectID":"/posts/protobuf_v2/:3:2","tags":["protobuf"],"title":"protobuf v2","uri":"/posts/protobuf_v2/"},{"categories":["笔记"],"content":"字段类型 .proto Type Notes Java Type Go Type double double float64 float float float32 int32 使用可变长度编码。编码负数的效率低 - 如果您的字段可能有负值，请改用sint32。 int int32 int64 使用可变长度编码。编码负数的效率低 - 如果您的字段可能有负值，请改用sint64。 long int64 uint32 使用可变长度编码 int uint32 uint64 使用可变长度编码. long uint64 sint32 使用可变长度编码。签名的int值。这些比常规int32更有效地编码负数。 int int32 sint64 使用可变长度编码。签名的int值。这些比常规int64更有效地编码负数。 long int64 fixed32 总是四个字节。如果值通常大于228，则比uint32更有效。 int uint32 fixed64 总是八个字节。如果值通常大于256，则比uint64更有效 long uint64 sfixed32 总是四个字节 int int32 sfixed64 总是八个字节 long int64 bool boolean bool string String string bytes 可以包含不超过232的任意字节序列。 String []byte ","date":"2020-06-01","objectID":"/posts/protobuf_v2/:3:3","tags":["protobuf"],"title":"protobuf v2","uri":"/posts/protobuf_v2/"},{"categories":["笔记"],"content":"标识号 正如上述文件格式，在消息定义中，每个字段都有唯一的一个数字标识符。这些标识符是用来在消息的二进制格式中识别各个字段的，一旦开始使用就不能够再改变。注：[1,15]之内的标识号在编码的时候会占用一个字节。[16,2047]之内的标识号则占用2个字节。所以应该为那些频繁出现的消息元素保留 [1,15]之内的标识号。切记：要为将来有可能添加的、频繁出现的标识号预留一些标识号。 最小的标识号可以从1开始，最大到2^29 - 1, or 536,870,911。不可以使用其中的[19000－19999]的标识号， Protobuf协议实现中对这些进行了预留。如果非要在.proto文件中使用这些预留标识号，编译时就会报警。 ","date":"2020-06-01","objectID":"/posts/protobuf_v2/:3:4","tags":["protobuf"],"title":"protobuf v2","uri":"/posts/protobuf_v2/"},{"categories":["笔记"],"content":"类型嵌套及导入 在一个.proto文件中可以定义多个消息类型,可以引用，也可以嵌套 message SearchResponse { repeated Result result = 1;}message Result { required string url = 1; optional string title = 2; repeated string snippets = 3;}要导入其他.proto文件的定义，你需要在你的文件中添加一个导入声明，如： import \"myproject/other_protos.proto\";嵌套使用： message SearchResponse { message Result { required string url = 1; optional string title = 2; repeated string snippets = 3; } repeated Result result = 1;}如果你想在它的父消息类型的外部重用这个消息类型，你需要以Parent.Type的形式使用它 Optional的字段和默认值 如上所述，消息描述中的一个元素可以被标记为“可选的”（optional）。一个格式良好的消息可以包含0个或一个optional的元素。当解 析消息时，如果它不包含optional的元素值，那么解析出来的对象中的对应字段就被置为默认值。默认值可以在消息描述文件中指定。 optional int32 result_per_page = 3 [default = 10];如果没有为optional的元素指定默认值，就会使用与特定类型相关的默认值：对string来说，默认值是空字符串。对bool来说，默认值是false。对数值类型来说，默认值是0。对枚举来说，默认值是枚举类型定义中的第一个值。 ","date":"2020-06-01","objectID":"/posts/protobuf_v2/:3:5","tags":["protobuf"],"title":"protobuf v2","uri":"/posts/protobuf_v2/"},{"categories":["笔记"],"content":"枚举 message SearchRequest { required string query = 1; optional int32 page_number = 2; optional int32 result_per_page = 3 [default = 10]; enum Corpus { UNIVERSAL = 0; WEB = 1; IMAGES = 2; LOCAL = 3; NEWS = 4; PRODUCTS = 5; VIDEO = 6; } optional Corpus corpus = 4 [default = UNIVERSAL];}枚举常量必须在32位整型值的范围内。因为enum值是使用可变编码方式的，对负数不够高效，因此不推荐在enum中使用负数。 ","date":"2020-06-01","objectID":"/posts/protobuf_v2/:3:6","tags":["protobuf"],"title":"protobuf v2","uri":"/posts/protobuf_v2/"},{"categories":["笔记"],"content":"扩展 通过扩展，可以将一个范围内的字段标识号声明为可被第三方扩展所用。然后，其他人就可以在他们自己的.proto文件中为该消息类型声明新的字段，而不必去编辑原始文件了。 message Foo { // ... extensions 100 to 199;}在消息Foo中，范围[100,199]之内的字段标识号被保留为扩展用。现在，其他人就可以在他们自己的.proto文件中添加新字段到Foo里了。 extend Foo { optional int32 bar = 126;}消息Foo现在有一个名为bar的optional int32字段。然而，要在程序代码中访问扩展字段的方法与访问普通的字段稍有不同。 如果你的消息中有很多可选字段， 并且同时至多一个字段会被设置， 你可以加强这个行为，使用oneof特性节省内存. ","date":"2020-06-01","objectID":"/posts/protobuf_v2/:3:7","tags":["protobuf"],"title":"protobuf v2","uri":"/posts/protobuf_v2/"},{"categories":["笔记"],"content":"Oneof Oneof字段就像可选字段， 除了它们会共享内存， 至多一个字段会被设置。 设置其中一个字段会清除其它oneof字段。 message SampleMessage { oneof test_oneof { string name = 4; SubMessage sub_message = 9; }}oneof中字段不能使用 required, optional, repeated 关键字. ","date":"2020-06-01","objectID":"/posts/protobuf_v2/:3:8","tags":["protobuf"],"title":"protobuf v2","uri":"/posts/protobuf_v2/"},{"categories":["笔记"],"content":"Map 如果你希望创建一个关联映射，protocol buffer提供了一种快捷的语法： map\u003ckey_type, value_type\u003e map_field = N;其中key_type可以是任意Integer或者string类型（所以，除了floating和bytes的任意标量类型都是可以的）value_type可以是任意类型。 Map的字段不可以是repeated，optional,required。 序列化后的顺序和map迭代器的顺序是不确定的，所以你不要期望以固定顺序处理Map ","date":"2020-06-01","objectID":"/posts/protobuf_v2/:3:9","tags":["protobuf"],"title":"protobuf v2","uri":"/posts/protobuf_v2/"},{"categories":["笔记"],"content":"定义服务(Service) 如果想要将消息类型用在RPC(远程方法调用)系统中，可以在.proto文件中定义一个RPC服务接口。 service SearchService { rpc Search (SearchRequest) returns (SearchResponse);}","date":"2020-06-01","objectID":"/posts/protobuf_v2/:3:10","tags":["protobuf"],"title":"protobuf v2","uri":"/posts/protobuf_v2/"},{"categories":["笔记"],"content":"protobuf v3","date":"2020-06-01","objectID":"/posts/protobuf_v3/","tags":["protobuf"],"title":"protobuf v3","uri":"/posts/protobuf_v3/"},{"categories":["笔记"],"content":"protobuf 是什么 Protocol Buffer (简称Protobuf) 是Google出品的性能优异、跨语言、跨平台的序列化库。 2001年初，Protobuf首先在Google内部创建，很多项目也采用Protobuf进行消息的通讯，还有基于Protobuf的微服务框架GRPC。 可以看做xml、json等序列化的又一种形式，只不过序列化后它是二进制的。 Protobuf支持很多语言，比如C++、C#、Dart、Go、Java、Python、Rust等，同时也是跨平台的。 序列化(serialization、marshalling)的过程是指将数据结构或者对象的状态转换成可以存储(比如文件、内存)或者传输的格式(比如网络)。反向操作就是反序列化(deserialization、unmarshalling)的过程。 ","date":"2020-06-01","objectID":"/posts/protobuf_v3/:0:1","tags":["protobuf"],"title":"protobuf v3","uri":"/posts/protobuf_v3/"},{"categories":["笔记"],"content":"protobuf 为什么要有 二十世纪九十年代后期，XML开始流行，它是一种人类易读的基于文本的编码方式，易于阅读和理解。 JSON是一种更轻量级的基于文本的编码方式，经常用在client/server端的通讯中。 除此之外还有很多序列化格式。 protobuf序列化和反序列化速度更快； 文件更小存储需要更少的空间，传输时间短。 ","date":"2020-06-01","objectID":"/posts/protobuf_v3/:0:2","tags":["protobuf"],"title":"protobuf v3","uri":"/posts/protobuf_v3/"},{"categories":["笔记"],"content":"protobuf 基础 官方推荐新代码采用proto3,这个教程主要介绍proto3的开发。 使用protobuf需要一个.proto文件，在这里定义要序列化的格式。可以理解为我们工作中和服务端定义的接口文档或者java bean。 举例： user.proto syntax = \"proto3\";//生成java类所在的包名 package com.example.protobuftest.bean;message SearchRequest { required string query = 1; optional int32 page_number = 2; optional int32 result_per_page = 3;}第一行指定protobuf的版本，可以指定为proto2或3。如果没有指定，默认以proto2格式定义。 在这里我们定义了一个User类型，包括name和age字段。 package package是可选的。对于生成的java语言对于Java，包声明符会变为java的一个包，除非在.proto文件中提供了一个明确有java_package； 如果不写package,默认是文件名作为包名。 写了包名后，就可以用包名加以区分。比如test.model.UserInfo。 显示设置包名后生成的对应语言文件就按照这个来生成包名了。 option java_package = \"test.protobuf.sample\";option go_package = \"test.protobuf.sample\";字段规则 所指定的消息字段修饰符必须是如下之一： singular：一个格式良好的消息应该有0个或者1个这种字段（但是不能超过1个）。这是proto3语法的默认字段规则。 repeated：在一个格式良好的消息中，这种字段可以重复任意多次（包括0次）。重复的值的顺序会被保留。 protobuf2中的required、optional规则已经不能使用了。 字段类型 .proto Type Notes Java Type Go Type double double float64 float float float32 int32 使用可变长度编码。编码负数的效率低 - 如果您的字段可能有负值，请改用sint32。 int int32 int64 使用可变长度编码。编码负数的效率低 - 如果您的字段可能有负值，请改用sint64。 long int64 uint32 使用可变长度编码 int uint32 uint64 使用可变长度编码. long uint64 sint32 使用可变长度编码。签名的int值。这些比常规int32更有效地编码负数。 int int32 sint64 使用可变长度编码。签名的int值。这些比常规int64更有效地编码负数。 long int64 fixed32 总是四个字节。如果值通常大于228，则比uint32更有效。 int uint32 fixed64 总是八个字节。如果值通常大于256，则比uint64更有效 long uint64 sfixed32 总是四个字节 int int32 sfixed64 总是八个字节 long int64 bool boolean bool string String string bytes 可以包含不超过232的任意字节序列。 String []byte 标识号 正如上述文件格式，在消息定义中，每个字段都有唯一的一个数字标识符。这些标识符是用来在消息的二进制格式中识别各个字段的，一旦开始使用就不能够再改变。注：[1,15]之内的标识号在编码的时候会占用一个字节。[16,2047]之内的标识号则占用2个字节。所以应该为那些频繁出现的消息元素保留 [1,15]之内的标识号。切记：要为将来有可能添加的、频繁出现的标识号预留一些标识号。 最小的标识号可以从1开始，最大到2^29 - 1, or 536,870,911。不可以使用其中的[19000－19999]的标识号， Protobuf协议实现中对这些进行了预留。如果非要在.proto文件中使用这些预留标识号，编译时就会报警。 ","date":"2020-06-01","objectID":"/posts/protobuf_v3/:0:3","tags":["protobuf"],"title":"protobuf v3","uri":"/posts/protobuf_v3/"},{"categories":["笔记"],"content":"保留标识符（Reserved） 如果你通过删除或者注释所有域，以后的用户在更新这个类型的时候可能重用这些标识号。如果你使用旧版本加载相同的.proto文件会导致严重的问题，包括数据损坏、隐私错误等等。现在有一种确保不会发生这种情况的方法就是为字段tag（reserved name可能会JSON序列化的问题）指定reserved标识符，protocol buffer的编译器会警告未来尝试使用这些域标识符的用户。 message Foo { reserved 2, 15, 9 to 11; reserved \"foo\", \"bar\";}注：不要在同一行reserved声明中同时声明域名字和tag number。 类型嵌套及导入 在一个.proto文件中可以定义多个消息类型,可以引用，也可以嵌套 message SearchResponse { repeated Result result = 1;}message Result { required string url = 1; optional string title = 2; repeated string snippets = 3;}要导入其他.proto文件的定义，你需要在你的文件中添加一个导入声明，如： import \"myproject/other_protos.proto\";嵌套使用： message SearchResponse { message Result { required string url = 1; optional string title = 2; repeated string snippets = 3; } repeated Result result = 1;}如果你想在它的父消息类型的外部重用这个消息类型，你需要以Parent.Type的形式使用它 枚举 message SearchRequest { required string query = 1; optional int32 page_number = 2; optional int32 result_per_page = 3 [default = 10]; enum Corpus { UNIVERSAL = 0; WEB = 1; IMAGES = 2; LOCAL = 3; NEWS = 4; PRODUCTS = 5; VIDEO = 6; } optional Corpus corpus = 4 [default = UNIVERSAL];}protbuf3中第一个字段必须是0，枚举类的第一个值总是默认值。 Oneof Oneof字段就像可选字段， 除了它们会共享内存， 至多一个字段会被设置。 设置其中一个字段会清除其它oneof字段。 message SampleMessage { oneof test_oneof { string name = 4; SubMessage sub_message = 9; }}oneof中字段不能使用 required, optional, repeated 关键字. Map 如果你希望创建一个关联映射，protocol buffer提供了一种快捷的语法： map\u003ckey_type, value_type\u003e map_field = N;其中key_type可以是任意Integer或者string类型（所以，除了floating和bytes的任意标量类型都是可以的）value_type可以是任意类型。 Map的字段不可以是repeated。 序列化后的顺序和map迭代器的顺序是不确定的，所以你不要期望以固定顺序处理Map 定义服务(Service) 如果想要将消息类型用在RPC(远程方法调用)系统中，可以在.proto文件中定义一个RPC服务接口。 service SearchService { rpc Search (SearchRequest) returns (SearchResponse);}","date":"2020-06-01","objectID":"/posts/protobuf_v3/:0:4","tags":["protobuf"],"title":"protobuf v3","uri":"/posts/protobuf_v3/"},{"categories":["笔记"],"content":"android protobuf grpc","date":"2020-05-15","objectID":"/posts/android_protobuf_grpc/","tags":["android"],"title":"android protobuf grpc","uri":"/posts/android_protobuf_grpc/"},{"categories":["笔记"],"content":"android 中如何使用 1.配置依赖 根目录build.gradle: dependencies { classpath 'com.google.protobuf:protobuf-gradle-plugin:0.8.8' } app build.gradle: apply plugin: 'com.google.protobuf' //... android{ //... sourceSets { main { java { srcDir 'src/main/java' } //定义protobuf文件夹 proto { srcDir 'src/main/proto' } } } //... } sourceSets { main { java { srcDir 'src/main/java' } //定义protobuf文件夹 proto { srcDir 'src/main/proto' } } } dependencies { implementation 'com.google.protobuf:protobuf-java:3.5.1' implementation 'com.google.protobuf:protoc:3.5.1' //这个是网络请求用的 implementation 'com.squareup.okhttp3:okhttp:3.11.0' } 2. 创建.proto文件 根据配置的目录，我们在proto目录下创建.proto文件。 3.生成java文件 build即可自动生成java文件，生成的在app/build/source/proto/debug/ 文件夹下。 生成的.java文件可读性也不强。 4.序列化和反序列化 // 序列化 User.UserBean user = User.UserBean.newBuilder().setName(\"lili\").setAge(5).build(); byte[] bytes = user.toByteArray(); // 反序列化 User.UserBean userBean = User.UserBean.parseFrom(bytes); 网络中使用protobuf 我们这里用okhttp + protobuf 来实现。 private void netProtobufTest() { final Login.LoginRequest loginRequest = Login.LoginRequest.newBuilder().setUsername(\"zhangsan\").setPassword(\"123\").build(); RequestBody requestBody = FormBody.create(MediaType.get(\"application/octet-stream\"), loginRequest.toByteArray()); Request request = new Request.Builder().url(\"http://10.240.47.89:8080/login\").post(requestBody).build(); Call call = new OkHttpClient().newCall(request); call.enqueue(new Callback() { @Override public void onFailure(Call call, IOException e) { Log.e(\"xx\", \"onFailure\"); e.printStackTrace(); } @Override public void onResponse(Call call, Response response) throws IOException { Login.LoginResponse loginResponse = Login.LoginResponse.parseFrom(response.body().bytes()); Log.e(\"xx\", loginResponse.toString()); } }); } android 使用grpc RPC就是要像调用本地的函数一样去调远程函数。Remote Procedure Call的简称。 RPC 是一种技术思想而非一种规范或协议，常见 RPC 技术和框架有： 阿里的 Dubbo/Dubbox、Google gRPC、Spring Boot/Spring Cloud。 Grpc是由Google主导开发的RPC框架，使用HTTP/2协议并用ProtoBuf作为序列化工具。其客户端提供Objective-C、Java接口，服务器侧则有Java、Golang、C++等接口，从而为移动端（iOS/Androi）到服务器端通讯提供了一种解决方案。Google对其的声音是： 基于Http2.0标准设计，带来诸如双向流、流控、头部压缩、单 TCP 连接上的多复用请求等特。这些特性使得其在移动设备上表现更好，更省电和节省空间占用 1.根目录配置 根目录build.gradle配置 dependencies { // ... classpath 'com.google.protobuf:protobuf-gradle-plugin:0.8.8' // ... } 2.app配置 app内build.gradle配置 apply plugin: 'com.google.protobuf' // ... android{ sourceSets { main { java { srcDir 'src/main/java' } //定义protobuf文件夹 proto { srcDir 'src/main/proto' } } } } protobuf { //配置protoc编译器 protoc { artifact = 'com.google.protobuf:protoc:3.5.1' } plugins { javalite { artifact = \"com.google.protobuf:protoc-gen-javalite:3.0.0\" } // grpc grpc { artifact = 'io.grpc:protoc-gen-grpc-java:1.19.0' // CURRENT_GRPC_VERSION } } //这里配置生成目录，编译后会在build的目录下生成对应的java文件 generateProtoTasks { all().each { task -\u003e // grpc task.plugins { javalite {} grpc { // Options added to --grpc_out option 'lite' } } } } } dependencies { // ... // 增加插件依赖 implementation 'javax.annotation:javax.annotation-api:1.2' implementation 'io.grpc:grpc-protobuf-lite:1.21.0' implementation 'io.grpc:grpc-okhttp:1.21.0' implementation 'io.grpc:grpc-stub:1.21.0' } java代码： private void initView() { findViewById(R.id.btn_test1).setOnClickListener(new View.OnClickListener() { @Override public void onClick(View v) { indexServiceGrpc.indexServiceStub mStub = indexServiceGrpc.newStub(newChannel(\"10.240.47.89\", 8080)); Indexdata.IndexRequest request = Indexdata.IndexRequest.newBuilder().setId(\"1\").build(); //进行请求 mStub.getIndexData(request, new StreamObserver\u003cIndexdata.IndexResponse\u003e() { @Override public void onNext(Indexdata.IndexResponse value) { Log.e(\"demo\", value.getMsg()); } @Override public void onError(Throwable t) { Log.e(\"demo\", t.getMessage()); } @Override public void onCompleted() { } }); } }); } public ManagedChannel newChannel(String host, int port) { return ManagedChannelBuilder.forAddress(host, port) .usePlaintext() .build(); } ","date":"2020-05-15","objectID":"/posts/android_protobuf_grpc/:0:1","tags":["android"],"title":"android protobuf grpc","uri":"/posts/android_protobuf_grpc/"},{"categories":["笔记"],"content":"android glide","date":"2020-04-08","objectID":"/posts/android_glide/","tags":["android"],"title":"android glide","uri":"/posts/android_glide/"},{"categories":["笔记"],"content":"glide概述 Glide是⾕歌为我们推荐的⼀个图⽚加载库。为什么要选择使⽤Glide呢？ 1、代码有⼈维护，不⾄于出现问题，项⽬组都搞不定的时候问题⽆法解决。（ImageLoader已没⼈维护了） 2、代码简洁，可读性很好。（Fresco是⼀个⾮常优秀的库，但是配置稍显⿇烦，同时代码风格读起来有些⽣疏） 3、功能强⼤（包含很多功能，例如：像加载Gif图⽚就是Picasso做不到的） ","date":"2020-04-08","objectID":"/posts/android_glide/:0:1","tags":["android"],"title":"android glide","uri":"/posts/android_glide/"},{"categories":["笔记"],"content":"glide使⽤ 加载⽹络图⽚ Glide.with(context).load(internetUrl).into(targetImageView); 从⽂件加载图⽚ File file = new File(Environment.getExternalStoragePublicDirectory(Environment.DIRECTORY_PICTURES),”Test.jpg”); Glide.with(context).load(file).into(imageViewFile); 从资源id加载图⽚ int resourceId = R.mipmap.ic_launcher; Glide.with(context).load(resourceId).into(imageViewResource); 从uri加载图⽚ Glide.with(context).load(uri).into(imageViewUri); 播放本地mp4,只能是本地(获取MP4视频的缩略图） String filePath = “/storage/emulated/0/Pictures/example_video.mp4”; Glide.with( context ).load( Uri.fromFile( new File( filePath ) ) ).into( imageViewGifAsBitmap ); 加载Gif图⽚ String gifUrl = “xxxxx”; Glide.with( context ).load( gifUrl ).into( imageViewGif ); ⽤bitMap播放Gif.asBitmap() Glide.with( context ).load( gifUrl ).asBitmap().into( imageViewGifAsBitmap ); 强制转化为Gif.asGif() Glide.with( context ).load( gifUrl ).asGif().error( R.drawable.full_cake ).into( imageViewGif ); 设置默认占位图.placeholder() 设置加载失败的图⽚.error() Glide.with( context ).load( gifUrl ).placeholder( R.drawable.cupcake ).error( R.drawable.full_cake ).into( imageViewGif ); .fallback() 设置加载动画 其实这个是默认的，但是你还是可以写出来,渐显动画 1、.crossFade() ：Glide提供淡如淡出 Glide.with(context).load().placeholder(R.mipmap.ic_launcher) .error(R.mipmap.future_studio_launcher).crossFade().into(imageViewFade); 这⾥还有⼀个.fadeFade(int duration)，设置动画时间。如果你不想要动画可以加上.dontAnimate() 2、.animate(android.R.anim.slide_in_left)：Android系统提供，从左到右滑出加载动画 调整图⽚⼤⼩.resize(int ,int ) 单位是像素，裁剪你的图⽚⼤⼩。其实Glide已经会⾃动根据你ImageView裁剪照⽚来放在缓存中了。但是不想适应ImageView⼤⼩的时候，可以调⽤这个⽅法.override()为ImageView指定⼤⼩。 Glide.with(context).load(image).override(600, 200) .into(imageViewResize); 裁剪图⽚.fitCenter()和.CenterCrop() Glide清楚在合适的ImageView中加载合适的Image.当需要裁剪⼤⼩时，有个.centerCrop⽅法，这个⽅法的裁剪会让你的ImageView周围不会留 ⽩，还有⼀个.fitCenter()⽅法，表⽰让你的Image完全显⽰，尺⼨不对时，周围会留⽩。 设置缩略图.thumbnail() .thumbnail()⽅法的⽬的就是让⽤户先看到⼀个低解析度的图，点开后，再加载⼀个⾼解析度的图。 //表⽰为原图的⼗分之⼀ Glide.with( context ).load(image).thumbnail( 0.1f ).into( imageView2 ); 设置图⽚显⽰效果（圆⾓、圆形、⾼斯模糊、蒙板、裁剪等等）.bitmapTransform（） Glide.with(this).load(R.mipmap.ic_image_sample) //模糊 .bitmapTransform(new BlurTransformation(this)) //圆⾓ .bitmapTransform(new RoundedCornersTransformation(this, 24, 0, RoundedCornersTransformation.CornerType.ALL)) //遮盖 .bitmapTransform(new MaskTransformation(this, R.mipmap.ic_launcher)) //灰度 .bitmapTransform(new GrayscaleTransformation(this)) //圆形 .bitmapTransform(new CropCircleTransformation(this)) .into(mResultIv); 除此之外还有实现诸如马赛克、明暗度等更多滤镜处理： ToonFilterTransformation SepiaFilterTransformation ContrastFilterTransformation InvertFilterTransformation PixelationFilterTransformation SketchFilterTransformation SwirlFilterTransformation BrightnessFilterTransformation KuwaharaFilterTransformation VignetteFilterTransformation ","date":"2020-04-08","objectID":"/posts/android_glide/:0:2","tags":["android"],"title":"android glide","uri":"/posts/android_glide/"},{"categories":["笔记"],"content":"Glide的缓存 ⽤过⼿机的都知道，当划上划下⼀个ListView的时候，第⼆次都⽐第⼀次快，就是因为为GlideView对资源进⾏了缓存，⽽且封装的很好，甚⾄ 不需要⾃⼰去设定缓存⼤⼩，Glide会智能地⾃⼰给我们根据设备设置缓存⼤⼩。 缓存是为了减少或者杜绝多的⽹络请求。为了避免缓存，Glide⽤了内存缓存和‘外存缓存机制’,并且 提供了相应的⽅法，完全封装，不需要处理 细节。Glide会⾃动缓存到内存，除⾮调⽤.skipMemoryCache( true )。尽管调⽤了这个，Glide还是会缓存到外存，还有⼀种情形，就是有⼀张 图⽚，但是这张图变化⾮常快，这个时候可能并不想缓存到外存中，就使⽤.diskCacheStrategy( DiskCacheStrategy.NONE )。如果你两种都不 需要，可以两个⽅法组合着⼀起使⽤。 ⾃定义外存缓存机制 Glide默认会缓存Image的很多个版本，⽐如原图，如果你的imageView⼤⼩的缓存。.diskCacheStrategy()有以下⼏种缓存策略： DiskCacheStrategy.NONE 什么都不缓存 DiskCacheStrategy.SOURCE 只缓存最⾼解析图的image DiskCacheStrategy.RESULT 缓存最后⼀次那个image,⽐如有可能你对image做了转化 DiskCacheStrategy.ALL image的所有版本都会缓存 Glide.with( context ).load( image ).diskCacheStrategy( DiskCacheStrategy.SOURCE ).into( imageViewFile ); 修改缓存⼤⼩、位置、加载图⽚质量 和指定HttpClent为OkHttp⼀样，只不过我们需要配置⼀些信息在applyOptions()函数⾥⾯ public class GlideConfigModule implements GlideModule { @Override public void applyOptions(Context context, GlideBuilder builder) { // 指定位置在packageName/cache/glide_cache,⼤⼩为MAX_CACHE_DISK_SIZE的磁盘缓存 builder.setDiskCache(new InternalCacheDiskCacheFactory(context, \"glide_cache\", ConfigConstants.MAX_CA CHE_DISK_SIZE)); //指定内存缓存⼤⼩ builder.setMemoryCache(new LruResourceCache(ConfigConstants.MAX_CACHE_MEMORY_SIZE)); //全部的内存缓存⽤来作为图⽚缓存 builder.setBitmapPool(new LruBitmapPool(ConfigConstants.MAX_CACHE_MEMORY_SIZE)); builder.setDecodeFormat(DecodeFormat.PREFER_ARGB_8888);//和Picasso配置⼀样 } @Override public void registerComponents(Context context, Glide glide) { } } 请求优先级.priority() 加载图⽚肯定也是有先后顺序，Glide提供了.priority()这个⽅法，它接收以下⼏个参数： Priority.LOW Priority.NORMAL Priority.HIGH Priority.IMMEDIATE 但是Glide并不⼀定会按照你的顺序来，只是尽量按照你的顺序来。 利⽤callback在⾮标准情况下加载图⽚ 上⾯所有的情况都是加载图⽚到ImageView中，但是并不是所有的情况都是这样。譬如加载的控件类型不是ImageView，是个⾃定义的布局。或 者加载为Background的形式。 可以使⽤SimpleTarget类型，这⾥指定他的⼤⼩为500*100，加载为背景图⽚ .into(new SimpleTarget\u003cDrawable\u003e(500, 100) { @Override public void onResourceReady (Drawable resource, GlideAnimation \u003c ? super Drawable \u003e glideAnimation){ mBtnClear.setBackground(resource); } } 同理下载图⽚原理是⼀样。 ","date":"2020-04-08","objectID":"/posts/android_glide/:0:3","tags":["android"],"title":"android glide","uri":"/posts/android_glide/"},{"categories":["笔记"],"content":"常见问题 1、前⾯我们已经学习到asGif（）可以加载gif图，asBitmap()可以加载静态gif图即gif图的第⼀帧，如果⾮gif图⽤asGif（）⽅法加载呢？这时候 会报错。。Glide默认可以⾃动识别图⽚格式，加载gif图，所以在不确定图⽚格式的情况下，不要直接写asGif哦。 2、You cannot start a load for a destroyed activity这样的异常如何处理？ 记住不要再⾮主线程⾥⾯使⽤Glide加载图⽚，如果真的使⽤了，请把context参数换成getApplicationContext。希望可以帮你避免这个问题。 3、为什么有的图⽚第⼀次加载的时候只显⽰占位图，第⼆次才显⽰正常的图⽚呢？ .如果你刚好使⽤了这个圆形Imageview库或者其他的⼀些⾃定义的圆形Imageview，⽽你⼜刚好设置了占位的话，那么，你就会遇到第⼀个问 题。如何解决呢？ ⽅案⼀: 不设置占位； ⽅案⼆：使⽤Glide的Transformation API⾃定义圆形Bitmap的转换。 ⽅案三：使⽤下⾯的代码加载图⽚： Glide.with(mContext) .load(url) .placeholder(R.drawable.loading_spinner) .into(new SimpleTarget\u003cBitmap\u003e(width, height) { @Override public void onResourceReady(Bitmap bitmap, GlideAnimation anim) { // setImageBitmap(bitmap) on CircleImageView } }; 4、图⽚⼤⼩拉伸问题 有时候你会发现⽹络加载完了之后会有拉伸现象，⽽你的控件⼤⼩明明是⾃适应的呀，这是为什么呢，请你检查下你是否设置了占位图，有的 话请去掉就ok了。 ","date":"2020-04-08","objectID":"/posts/android_glide/:0:4","tags":["android"],"title":"android glide","uri":"/posts/android_glide/"},{"categories":["笔记"],"content":"android图片加载框架","date":"2020-04-08","objectID":"/posts/android_%E5%9B%BE%E7%89%87%E5%8A%A0%E8%BD%BD/","tags":["android"],"title":"android图片加载框架","uri":"/posts/android_%E5%9B%BE%E7%89%87%E5%8A%A0%E8%BD%BD/"},{"categories":["笔记"],"content":"主流图⽚加载框架对⽐ ","date":"2020-04-08","objectID":"/posts/android_%E5%9B%BE%E7%89%87%E5%8A%A0%E8%BD%BD/:1:0","tags":["android"],"title":"android图片加载框架","uri":"/posts/android_%E5%9B%BE%E7%89%87%E5%8A%A0%E8%BD%BD/"},{"categories":["笔记"],"content":"imageloader 整个库分为 ImageLoaderEngine，Cache 及 ImageDownloader，ImageDecoder，BitmapDisplayer，BitmapProcessor 五⼤模块，其中 Cache 分为 MemoryCache 和 DiskCache 两部分。 简单的讲就是 ImageLoader 收到加载及显⽰图⽚的任务，并将它交给 ImageLoaderEngine，ImageLoaderEngine 分发任务到具体线程池去执 ⾏，任务通过 Cache 及 ImageDownloader 获取图⽚，中间可能经过 BitmapProcessor 和 ImageDecoder 处理，最终转换为Bitmap 交给 BitmapDisplayer 在 ImageAware 中显⽰。 ImageLoader 优点: ⽀持下载进度监听 可以在 View 滚动中暂停图⽚加载。 通过 PauseOnScrollListener 接⼝可以在 View 滚动中暂停图⽚加载。 默认实现多种内存缓存算法 这⼏个图⽚缓存都可以配置缓存算法，不过 ImageLoader 默认实现了较多缓存算法，如 Size 最⼤先删除、使 ⽤最少先删除、最近最少使⽤、先进先删除、时间最长先删除等。 ⽀持本地缓存⽂件名规则定义。 ","date":"2020-04-08","objectID":"/posts/android_%E5%9B%BE%E7%89%87%E5%8A%A0%E8%BD%BD/:2:0","tags":["android"],"title":"android图片加载框架","uri":"/posts/android_%E5%9B%BE%E7%89%87%E5%8A%A0%E8%BD%BD/"},{"categories":["笔记"],"content":"Picasso 整个库分为 Dispatcher，RequestHandler 及 Downloader，PicassoDrawable 等模块。 Dispatcher 负责分发和处理 Action，包括提交、暂停、继续、取消、⽹络状态变化、重试等等。 简单的讲就是 Picasso 收到加载及显⽰图⽚的任务，创建 Request 并将它交给 Dispatcher，Dispatcher 分发任务到具体 RequestHandler，任 务通过 MemoryCache 及 Handler(数据获取接⼝) 获取图⽚，图⽚获取成功后通过 PicassoDrawable 显⽰到 Target 中。 需要注意的是上⾯ Data 的 File system 部分，Picasso 没有⾃定义本地缓存的接⼝，默认使⽤ http 的本地缓存，API 9 以上使⽤ okhttp，以下 使⽤ Urlconnection，所以如果需要⾃定义本地缓存就需要重定义 Downloader。 优点： ⾃带统计监控功能 ⽀持图⽚缓存使⽤的监控，包括缓存命中率、已使⽤内存⼤⼩、节省的流量等。 ⽀持优先级处理 每次任务调度前会选择优先级⾼的任务，⽐如 App 页⾯中 Banner 的优先级⾼于 Icon 时就很适⽤。 ⽀持延迟到图⽚尺⼨计算完成加载 ⽀持飞⾏模式、并发线程数根据⽹络类型⽽变 ⼿机切换到飞⾏模式或⽹络类型变换时会⾃动调整线程池最⼤并发数，⽐如 wifi 最⼤并发为 4， 4g 为 3，3g 为 2。 这⾥ Picasso 根据⽹络类型来决定最⼤并发数，⽽不是 CPU 核数。 “⽆”本地缓存 ⽆”本地缓存，不是说没有本地缓存，⽽是 Picasso ⾃⼰没有实现，交给了 Square 的另外⼀个⽹络库 okhttp 去实现，这样的好处是可以通 过请求 Response Header 中的 Cache-Control 及 Expired 控制图⽚的过期时间。 ","date":"2020-04-08","objectID":"/posts/android_%E5%9B%BE%E7%89%87%E5%8A%A0%E8%BD%BD/:2:1","tags":["android"],"title":"android图片加载框架","uri":"/posts/android_%E5%9B%BE%E7%89%87%E5%8A%A0%E8%BD%BD/"},{"categories":["笔记"],"content":"Glide 整个库分为 RequestManager(请求管理器)，Engine(数据获取引擎)、 Fetcher(数据获取器)、MemoryCache(内存缓存)、DiskLRUCache、 Transformation(图⽚处理)、Encoder(本地缓存存储)、Registry(图⽚类型及解析器配置)、Target(⽬标) 等模块。 简单的讲就是 Glide 收到加载及显⽰资源的任务，创建 Request 并将它交给RequestManager，Request 启动 Engine 去数据源获取资源(通过 Fetcher )，获取到后 Transformation 处理后交给 Target。 Glide 依赖于 DiskLRUCache、GifDecoder 等开源库去完成本地缓存和 Gif 图⽚解码⼯作。 优点： 图⽚缓存-\u003e媒体缓存 Glide 不仅是⼀个图⽚缓存，它⽀持 Gif、WebP、缩略图。甚⾄是 Video，所以更该当做⼀个媒体缓存。 ⽀持优先级处理 与 Activity/Fragment ⽣命周期⼀致，⽀持 trimMemory Glide 对每个 context 都保持⼀个 RequestManager，通过 FragmentTransaction 保持与 Activity/Fragment ⽣命周期⼀致，并且有对应的 trimMemory 接⼝实现可供调⽤。 ⽀持 okhttp、Volley Glide 默认通过 UrlConnection 获取数据，可以配合 okhttp 或是 Volley 使⽤。实际 ImageLoader、Picasso 也都⽀持 okhttp、Volley。 Glide 的内存缓存有个 active 的设计 从内存缓存中取数据时，不像⼀般的实现⽤ get，⽽是⽤ remove，再将这个缓存数据放到⼀个 value 为软引⽤的 activeResources map 中，并计数引⽤数，在图⽚加载完成后进⾏判断，如果引⽤计数为空则回收掉。 内存缓存更⼩图⽚ Glide 以 url、view_width、view_height、屏幕的分辨率等做为联合 key，将处理后的图⽚缓存在内存缓存中，⽽不是原始图⽚以节省⼤⼩ 与 Activity/Fragment ⽣命周期⼀致，⽀持 trimMemory 图⽚默认使⽤默认 RGB_565 ⽽不是 ARGB_888 虽然清晰度差些，但图⽚更⼩，也可配置到 ARGB_888。 其他：Glide 可以通过 signature 或不使⽤本地缓存⽀持 url 过期 ","date":"2020-04-08","objectID":"/posts/android_%E5%9B%BE%E7%89%87%E5%8A%A0%E8%BD%BD/:2:2","tags":["android"],"title":"android图片加载框架","uri":"/posts/android_%E5%9B%BE%E7%89%87%E5%8A%A0%E8%BD%BD/"},{"categories":["笔记"],"content":"基础","date":"2020-04-08","objectID":"/posts/%E5%9F%BA%E7%A1%80-%E5%8F%8D%E7%A0%81%E8%A1%A5%E7%A0%81/","tags":["基础"],"title":"反码补码","uri":"/posts/%E5%9F%BA%E7%A1%80-%E5%8F%8D%E7%A0%81%E8%A1%A5%E7%A0%81/"},{"categories":["笔记"],"content":"各种码来了 原码：第一位表示符号位，0是正，1是负。其余表示值。 反码：正数的反码和原码相同。负数时第一个符号位不变，其他取反。 补码：正数的补码与与原码相同。负数时第一个符号位不变，其余各位取反，然后再加1。即负数时补码为反码加1。 原码 +1 0000 0001 -1 1000 0001 反码 +1 0000 0001 -1 1111 1110 补码 +1 0000 0001 -1 1111 1111 ","date":"2020-04-08","objectID":"/posts/%E5%9F%BA%E7%A1%80-%E5%8F%8D%E7%A0%81%E8%A1%A5%E7%A0%81/:1:0","tags":["基础"],"title":"反码补码","uri":"/posts/%E5%9F%BA%E7%A1%80-%E5%8F%8D%E7%A0%81%E8%A1%A5%E7%A0%81/"},{"categories":["笔记"],"content":"为何要有反码、补码 我们知道, 根据运算法则减去一个正数等于加上一个负数, 即: 1-1 = 1 + (-1) = 0 , 所以机器可以只有加法而没有减法, 这样计算机运算的设计就更简单了。 如果用原码来计算1-1=0 1 - 1 = 1 + (-1) = [00000001]原 + [10000001]原 = [10000010]原 = -2 如果用原码表示, 让符号位也参与计算, 显然对于减法来说, 结果是不正确的.这也就是为何计算机内部不使用原码表示一个数. 为了解决原码做减法的问题, 出现了反码: 1 - 1 = 1 + (-1) = [0000 0001]原 + [1000 0001]原= [0000 0001]反 + [1111 1110]反 = [1111 1111]反 = [1000 0000]原 = -0 发现用反码计算减法, 结果的真值部分是正确的. 而唯一的问题其实就出现在\"0\"这个特殊的数值上. 虽然人们理解上+0和-0是一样的, 但是0带符号是没有任何意义的. 而且会有[0000 0000]原和[1000 0000]原两个编码表示0. 于是补码的出现, 解决了0的符号以及两个编码的问题: 1-1 = 1 + (-1) = [0000 0001]原 + [1000 0001]原 = [0000 0001]补 + [1111 1111]补 = [0000 0000]补=[0000 0000]原 这样0用[0000 0000]表示, 而以前出现问题的-0则不存在了.而且可以用[1000 0000]表示-128: (-1) + (-127) = [1000 0001]原 + [1111 1111]原 = [1111 1111]补 + [1000 0001]补 = [1000 0000]补 -1-127的结果应该是-128, 在用补码运算的结果中, [1000 0000]补 就是-128. 但是注意因为实际上是使用以前的-0的补码来表示-128, 所以-128并没有原码和反码表示.(对-128的补码表示[1000 0000]补算出来的原码是[0000 0000]原, 这是不正确的) 简单总结：原码运算不能让符号位参与运算、反码运算会出现 使用补码, 不仅仅修复了0的符号以及存在两个编码的问题, 而且还能够多表示一个最低数. 这就是为什么8位二进制, 使用原码或反码表示的范围为[-127, +127], 而使用补码表示的范围为[-128, 127]。 一个+0表示为：00000000，一个-0表示为：1000000，因为符号位不算在里面，所以就会有两个0，所以从一开始发明二进制的时候，就把-0规定为-128，之所以这样是因为[1000 0000]补 就是-128. （这是国内教材中的解释） 补码可以表示负数，且与其他数运算的时候符号位也一起参与运算。这样减法可以用加法实现。除法可以用减法实现，10 /3 可化为 10 -3 -3 -3最后余1，见了3次，就是商3余1。乘法直接用加法实现即可。 ","date":"2020-04-08","objectID":"/posts/%E5%9F%BA%E7%A1%80-%E5%8F%8D%E7%A0%81%E8%A1%A5%E7%A0%81/:1:1","tags":["基础"],"title":"反码补码","uri":"/posts/%E5%9F%BA%E7%A1%80-%E5%8F%8D%E7%A0%81%E8%A1%A5%E7%A0%81/"},{"categories":["笔记"],"content":"linux ssh","date":"2020-02-22","objectID":"/posts/linux_ssh/","tags":["linux"],"title":"linux ssh","uri":"/posts/linux_ssh/"},{"categories":["笔记"],"content":"ssh远程登录 ","date":"2020-02-22","objectID":"/posts/linux_ssh/:0:0","tags":["linux"],"title":"linux ssh","uri":"/posts/linux_ssh/"},{"categories":["笔记"],"content":"密码登录 整个过程所示： （1）远程主机收到用户的登录请求，把自己的公钥发给用户。 （2）用户使用这个公钥，将登录密码加密后，发送回来。 （3）远程主机用自己的私钥，解密登录密码，如果密码正确，就同意用户登录。 问题 这个过程本身是安全的，但是实施的时候存在一个风险： Client端如何保证接受到的公钥就是目标Server端的？ 如果一个攻击者中途拦截Client的登录请求，向其发送自己的公钥，Client端用攻击者的公钥进行数据加密。攻击者接收到加密信息后再用自己的私钥进行解密，不就窃取了Client的登录信息了吗？因为不像https协议，SSH协议的公钥是没有证书中心（CA）公证的，也就是说，都是自己签发的。 通常在第一次登录的时候，系统会出现下面提示信息：无法确认主机host(12.18.429.21)的真实性，不过知道它的公钥指纹，询问你是否继续连接？ 当远程主机的公钥被接受以后，它就会被保存在文件$HOME/.ssh/known_hosts之中。下次再连接这台主机，系统就会认出它的公钥已经保存在本地了，从而跳过警告部分，直接提示输入密码。 ","date":"2020-02-22","objectID":"/posts/linux_ssh/:1:0","tags":["linux"],"title":"linux ssh","uri":"/posts/linux_ssh/"},{"categories":["笔记"],"content":"公钥登录（基于公钥的认证） 使用密码登录，每次都必须输入密码，非常麻烦。好在SSH提供了另外一种可以免去输入密码过程的登录方式：公钥登录。 所谓\"公钥登录”，原理很简单，就是用户将自己的公钥储存在远程主机上。 公钥认证流程： Client端用户TopGun将自己的公钥存放在Server上，追加在文件authorized_keys中。 Server收到登录请求后，随机生成一个字符串str1，并发送给Client。 Client用自己的私钥对字符串str1进行加密。 将加密后字符串发送给Server。 Server用之前存储的公钥进行解密，比较解密后的str2和str1。 根据比较结果，返回客户端登陆结果。 ","date":"2020-02-22","objectID":"/posts/linux_ssh/:2:0","tags":["linux"],"title":"linux ssh","uri":"/posts/linux_ssh/"},{"categories":["笔记"],"content":"生成公钥 1、在本机生成密钥对 使用ssh-keygen命令生成密钥对： ssh-keygen -t rsa #-t表示类型选项，这里采用rsa加密算法 然后根据提示一步步的按enter键即可（其中有一个提示是要求设置私钥口令passphrase，不设置则为空，这里看心情吧，如果不放心私钥的安全可以设置一下），执行结束以后会在 /home/当前用户 目录下生成一个 .ssh 文件夹,其中包含私钥文件 id_rsa 和公钥文件 id_rsa.pub。 2、将公钥复制到远程主机中 使用ssh-copy-id命令将公钥复制到远程主机。ssh-copy-id会将公钥写到远程主机的 ~/ .ssh/authorized_key 文件中 ssh-copy-id ldz@192.168.0.1 经过以上两个步骤，以后再登录这个远程主机就不用再输入密码了。 SSH端口转发 SSH 不仅仅能够自动加密和解密 SSH 客户端与服务端之间的网络数据，同时，SSH 还能够提供了一个非常有用的功能，那就是端口转发，即将TCP 端口的网络数据，转发到指定的主机某个端口上，在转发的同时会对数据进行相应的加密及解密。如果工作环境中的防火墙限制了一些网络端口的使用，但是允许 SSH 的连接，那么也是能够通过使用SSH转发后的端口进行通信。转发，主要分为本地转发与远程转发两种类型。 转发的参数： -C：压缩数据 -f ：后台认证用户/密码，通常和-N连用，不用登录到远程主机。 -N ：不执行脚本或命令，通常与-f连用。 -g ：在-L/-R/-D参数中，允许远程主机连接到建立的转发的端口，如果不加这个参数，只允许本地主机建立连接。 -L : 本地端口:目标IP:目标端口 -D : 动态端口转发 -R : 远程端口转发 -T ：不分配 TTY 只做代理用 -q ：安静模式，不输出 错误/警告 信息 ","date":"2020-02-22","objectID":"/posts/linux_ssh/:3:0","tags":["linux"],"title":"linux ssh","uri":"/posts/linux_ssh/"},{"categories":["笔记"],"content":"本地转发 有本地网络服务器的某个端口，转发到远程服务器某个端口。说白了就是，将发送到本地端口的请求，转发到目标端口。格式如下： ssh -L 本地网卡地址:本地端口:目标地址:目标端口 用户@目标地址。 案例：B服务器上的mysql只允许127.0.0.1连接，A服务器想要连接mysql只能 ssh -L 127.0.0.1:3306:127.0.0.1:3306 root@192.168.13.142 因为本地网卡地址是可以省略的，上面的转发，可以简写为： ssh -L 3306:127.0.0.1:3306 root@192.168.13.142 ","date":"2020-02-22","objectID":"/posts/linux_ssh/:4:0","tags":["linux"],"title":"linux ssh","uri":"/posts/linux_ssh/"},{"categories":["笔记"],"content":"远程转发 由远程服务器的某个端口，转发到本地网络的服务器某个端口。说白了，就是将发送到远程端口的请求，转发到目标端口。格式如下： ssh -R 远程网卡地址:远程端口:目标地址:目标端口 ","date":"2020-02-22","objectID":"/posts/linux_ssh/:5:0","tags":["linux"],"title":"linux ssh","uri":"/posts/linux_ssh/"},{"categories":["笔记"],"content":"pb,rpc,grpc","date":"2019-11-05","objectID":"/posts/go_grpc/","tags":["golang"],"title":"pb,rpc,grpc","uri":"/posts/go_grpc/"},{"categories":["笔记"],"content":"protobuf 需要下载protoc来进行编译.proto文件https://github.com/protocolbuffers/protobuf/releases 它是可执行文件，可加入到$path中， 另外需要 go get -u github.com/golang/protobuf/proto //库文件 go get -u github.com/golang/protobuf/protoc-gen-go //go代码生成器 ","date":"2019-11-05","objectID":"/posts/go_grpc/:0:1","tags":["golang"],"title":"pb,rpc,grpc","uri":"/posts/go_grpc/"},{"categories":["笔记"],"content":"rpc RPC是Remote Procedure CallProtocol的缩写，即—远程过程调用协议。 RPC是一个计算机通信协议。该协议允许运行于一台计算机的程序调用另一台计算机的子程序，而程序员无需额外地为这个交互作用编程。如果涉及的软件采用面向对象编程，那么远程过程调用亦可称作远程调用或远程方法调用，信息数据。通过它可以使函数调用模式网络化。在OSI网络通信模型中，RPC跨越了传输层和应用层。RPC使得开发包括网络分布式多程序在内的应用程序更加容易。 rpc是一个协议，这个协议是基于连接的，在连接上绑定的方法，在哪些连接上绑定，是需要确定的，这就是载体。 如mysql协议-基于连接的协议-她的连接载体是 tcp的基于所有ip的3306端口。 go语言中自带的rpc包，是基于tcp的载体，数据传输格式是gob，go自带的jsonrpc是基于http的，数据传输是json格式。 而gRPC的载体是http/2,数据格式是protobuf. rest的载体是http1,数据传输格式多种：如json. net/rpc Go语言标准库能够自带一个rpc框架还是非常给力的，这可以很大程度的降低写后端网络通信服务的门槛，特别是在大规模的分布式系统中，rpc基本是跨机器通信的标配。rpc能够最大程度屏蔽网络细节，让开发者专注在服务功能的开发上面 ","date":"2019-11-05","objectID":"/posts/go_grpc/:0:2","tags":["golang"],"title":"pb,rpc,grpc","uri":"/posts/go_grpc/"},{"categories":["笔记"],"content":"grpc gRPC 是一个高性能、开源、通用的RPC框架，由Google推出，基于HTTP/2协议标准设计开发，默认采用Protocol Buffers数据序列化协议，支持多种开发语言。gRPC提供了一种简单的方法来精确的定义服务，并且为客户端和服务端自动生成可靠的功能库。 在gRPC客户端可以直接调用不同服务器上的远程程序，使用姿势看起来就像调用本地程序一样，很容易去构建分布式应用和服务。和很多RPC系统一样，服务端负责实现定义好的接口并处理客户端的请求，客户端根据接口描述直接调用需要的服务。客户端和服务端可以分别使用gRPC支持的不同语言实现。 gRPC的go语言版 grpc-go github地址：https://github.com/grpc/grpc-go 获取： go get -u google.golang.org/grpc ","date":"2019-11-05","objectID":"/posts/go_grpc/:0:3","tags":["golang"],"title":"pb,rpc,grpc","uri":"/posts/go_grpc/"},{"categories":["笔记"],"content":"简单实例 创建编译.proto文件 syntax = \"proto3\"; //指定版本，必须要写（proto3、proto2） package proto.test;message HelloWorldRequest { string name = 1;}message HelloWorldResponse { string reply = 1;}service HelloService { rpc SayHello(HelloWorldRequest) returns (HelloWorldResponse) {}}编译proto文件生成go代码 protoc –go_out=plugins=grpc:. ./proto/test/*.proto 编译后会.pb.go文件 查看pb.go文件我们可以看到里面有两个重要信息，HelloWorldRequest和HelloWorldResponse两个message都变成了两个结构体，还出现了两个接口HelloServiceClient和HelloServiceServer，要实现的方法为SayHello。 服务端 服务端实现接口 type server struct { } func (s *server) SayHello(ctx context.Context, in *proto_test.HelloWorldRequest) (*proto_test.HelloWorldResponse, error) { return \u0026proto_test.HelloWorldResponse{Reply: in.Name + \"ni hao\"}, nil } 注册服务 lis, err := net.Listen(\"tcp\", \":8080\") if err != nil { log.Fatalf(\"failed to listen: %v\", err) } // Creates a new gRPC server s := grpc.NewServer() proto_test.RegisterHelloServiceServer(s, \u0026server{}) s.Serve(lis) 客户端 conn, err := grpc.Dial(\"localhost:8080\", grpc.WithInsecure()) if err != nil { log.Fatalf(\"did not connect: %v\", err) } defer conn.Close() // Creates a new CustomerClient client := proto_test.NewHelloServiceClient(conn) request := proto_test.HelloWorldRequest{ Name: \"zhangsan\", } response, e := client.SayHello(context.Background(), \u0026request) fmt.Println(response, e) ","date":"2019-11-05","objectID":"/posts/go_grpc/:0:4","tags":["golang"],"title":"pb,rpc,grpc","uri":"/posts/go_grpc/"},{"categories":["笔记"],"content":"rabbitmq","date":"2019-09-27","objectID":"/posts/go_rabbitmq/","tags":["golang"],"title":"rabbitmq","uri":"/posts/go_rabbitmq/"},{"categories":["笔记"],"content":"安装和端口 这里只给出一种基于docker安装的简单形式。 docker run -d –hostname my-rabbit –name rmq -p 15672:15672 -p 5672:5672 -p 25672:25672 -e RABBITMQ_DEFAULT_USER=用户名 -e RABBITMQ_DEFAULT_PASS=密码 rabbitmq:3-management 通过命令可以看出，一共映射了三个端口，简单说下这三个端口是干什么的。 5672：连接生产者、消费者的端口。 15672：WEB管理页面的端口。 25672：分布式集群的端口。 ","date":"2019-09-27","objectID":"/posts/go_rabbitmq/:0:1","tags":["golang"],"title":"rabbitmq","uri":"/posts/go_rabbitmq/"},{"categories":["笔记"],"content":"基础概念 下面是图中出现的单词的详细解释： Message 消息，消息是不具名的，它由消息头和消息体组成。消息体是不透明的，而消息头则由一系列的可选属性组成，这些属性包括routing-key（路由键）、priority（相对于其他消息的优先权）、delivery-mode（指出该消息可能需要持久性存储）等。 Publisher 消息的生产者，也是一个向交换器发布消息的客户端应用程序。 Exchange 交换器，用来接收生产者发送的消息并将这些消息路由给服务器中的队列。 Binding 绑定，用于消息队列和交换器之间的关联。一个绑定就是基于路由键将交换器和消息队列连接起来的路由规则，所以可以将交换器理解成一个由绑定构成的路由表。 Queue 消息队列，用来保存消息直到发送给消费者。它是消息的容器，也是消息的终点。一个消息可投入一个或多个队列。消息一直在队列里面，等待消费者连接到这个队列将其取走。 Connection 网络连接，比如一个TCP连接。 Channel 信道，多路复用连接中的一条独立的双向数据流通道。信道是建立在真实的TCP连接内地虚拟连接，AMQP 命令都是通过信道发出去的，不管是发布消息、订阅队列还是接收消息，这些动作都是通过信道完成。因为对于操作系统来说建立和销毁 TCP 都是非常昂贵的开销，所以引入了信道的概念，以复用一条 TCP 连接。 Consumer 消息的消费者，表示一个从消息队列中取得消息的客户端应用程序。 Virtual Host 虚拟主机，表示一批交换器、消息队列和相关对象。虚拟主机是共享相同的身份认证和加密环境的独立服务器域。每个 vhost 本质上就是一个 mini 版的 RabbitMQ 服务器，拥有自己的队列、交换器、绑定和权限机制。vhost 是 AMQP 概念的基础，必须在连接时指定，RabbitMQ 默认的 vhost 是 / 。 Broker：表示消息队列服务器实体。 BandingKey：绑定键，一个队列可以有一个到多个绑定键，通过绑定操作可以绑定交换器和队列，交换器会根据绑定键的名称找到对应的队列。 RotingKey：路由键，发送消息时，需要附带一条路由键，交换器会对路由键和绑定键进行匹配，如果匹配成功，则消息会转发到绑定键对应的队列中。 Consumer：消费者，负责处理消息。 交换机类型 注：这里只关注fanout、direct、topic三种类型，header类型没用过 fanout – 扇出型 用于支持发布、订阅模式（pub/sub） 交换器把消息转发到所有与之绑定的队列中。 扇出类型交换器会屏蔽掉路由键、绑定键的作用。 direct – 直接匹配 直接匹配交换器会对比路由键和绑定键，如果路由键和绑定键完全相同，则把消息转发到绑定键所对应的队列中。 topic – 模式匹配 与直接匹配相对应，可以用一些模式来代替字符串的完全匹配。 规则： 以 ‘.’ 来分割单词。 ‘#’ 表示一个或多个单词。 ‘*’ 表示一个单词。 如： RoutingKey为： aaa.bbb.ccc BindingKey可以为： *.bbb.ccc aaa.# 默认交换机 当交换器名称为空时，表示使用默认交换器。空的意思是空字符串。 默认交换器是一个特殊的交换器，他无需进行绑定操作，可以以直接匹配的形式直接把消息发送到任何队列中。 ","date":"2019-09-27","objectID":"/posts/go_rabbitmq/:0:2","tags":["golang"],"title":"rabbitmq","uri":"/posts/go_rabbitmq/"},{"categories":["笔记"],"content":"golang使用 使用github.com/streadway/amqp第三方库 创建交换器 func (ch *Channel) ExchangeDeclare(name, kind string, durable, autoDelete, internal, noWait bool, args Table) error name:交换器的名称，对应图中exchangeName。 kind:也叫作type，表示交换器的类型。有四种常用类型：direct、fanout、topic、headers。 durable:是否持久化，true表示是。持久化表示会把交换器的配置存盘，当RMQ Server重启后，会自动加载交换器。 autoDelete:是否自动删除，true表示是。至少有一条绑定才可以触发自动删除，当所有绑定都与交换器解绑后，会自动删除此交换器。 internal:是否为内部，true表示是。客户端无法直接发送msg到内部交换器，只有交换器可以发送msg到内部交换器。 noWait:是否非阻塞，true表示是。阻塞：表示创建交换器的请求发送后，阻塞等待RMQ Server返回信息。非阻塞：不会阻塞等待RMQ Server的返回信息，而RMQ Server也不会返回信息。（不推荐使用） args:直接写nil 创建队列 func (ch *Channel) QueueDeclare(name string, durable, autoDelete, exclusive, noWait bool, args Table) (Queue, error) name：队列名称 durable：是否持久化，true为是。持久化会把队列存盘，服务器重启后，不会丢失队列以及队列内的信息。（注：1、不丢失是相对的，如果宕机时有消息没来得及存盘，还是会丢失的。2、存盘影响性能。） autoDelete：是否自动删除，true为是。至少有一个消费者连接到队列时才可以触发。当所有消费者都断开时，队列会自动删除。 exclusive：是否设置排他，true为是。如果设置为排他，则队列仅对首次声明他的连接可见，并在连接断开时自动删除。（注意，这里说的是连接不是信道，相同连接不同信道是可见的）。 nowait：是否非阻塞，true表示是。阻塞：表示创建交换器的请求发送后，阻塞等待RMQ Server返回信息。非阻塞：不会阻塞等待RMQ Server的返回信息，而RMQ Server也不会返回信息。（不推荐使用） args：参数，比如设置消息过期x-dead-letter-exchange等。 队列绑定 func (ch *Channel) QueueBind(name, key, exchange string, noWait bool, args Table) error name：队列名称 key：对应图中BandingKey，表示要绑定的键。 exchange：交换器名称 nowait：是否非阻塞，true表示是。阻塞：表示创建交换器的请求发送后，阻塞等待RMQ Server返回信息。非阻塞：不会阻塞等待RMQ Server的返回信息，而RMQ Server也不会返回信息。（不推荐使用） 交换器绑定 func (ch *Channel) ExchangeBind(destination, key, source string, noWait bool, args Table) error 源交换器根据路由键\u0026绑定键把msg转发到目的交换器。 destination：目的交换器，通常是内部交换器。 key：对应图中BandingKey，表示要绑定的键。 source：源交换器。 nowait：是否非阻塞，true表示是。阻塞：表示创建交换器的请求发送后，阻塞等待RMQ Server返回信息。非阻塞：不会阻塞等待RMQ Server的返回信息，而RMQ Server也不会返回信息。（不推荐使用） args：直接写nil，没研究过，不解释。 发送消息 func (ch *Channel) Publish(exchange, key string, mandatory, immediate bool, msg Publishing) error exchange：要发送到的交换机名称，对应图中exchangeName。 key：路由键，对应图中RoutingKey。 mandatory：直接false，不建议使用， immediate ：直接false，不建议使用 msg：要发送的消息，msg对应一个Publishing结构，Publishing结构里面有很多参数，这里只强调几个参数，其他参数暂时列出，但不解释。 ####Qos func (ch *Channel) Qos(prefetchCount, prefetchSize int, global bool) error 1 注意：这个在推送模式下非常重要，通过设置Qos用来防止消息堆积。 prefetchCount：消费者未确认消息的个数。 prefetchSize ：消费者未确认消息的大小。 global ：是否全局生效，true表示是。全局生效指的是针对当前connect里的所有channel都生效。 接受消息 – 推模式 RMQ Server主动把消息推给消费者 func (ch *Channel) Consume(queue, consumer string, autoAck, exclusive, noLocal, noWait bool, args Table) (\u003c-chan Delivery, error) queue:队列名称。 consumer:消费者标签，用于区分不同的消费者。 autoAck:是否自动回复ACK，true为是，回复ACK表示高速服务器我收到消息了。建议为false，手动回复，这样可控性强。 exclusive:设置是否排他，排他表示当前队列只能给一个消费者使用。 noLocal:如果为true，表示生产者和消费者不能是同一个connect。 如果不ack，那么RabbitMQ Server会重新分发，并且RabbitMQ Server不会再发送数据给它 两个消费者从一个队列取数据时，会产生竞争条件。此时消息只能给其中的一个消费者。如果两个消费者均没有在收到消息后做应答操作，则消息会平均发送给两个消费者。如果收到消息后做了应答操作，则会采取能者多劳的模式。 接受消息 – 拉模式 消费者主动从RMQ Server拉消息 func (ch *Channel) Get(queue string, autoAck bool) (msg Delivery, ok bool, err error) queue：队列名称 autoAck：是否开启自动回复 手动回复消息 简单看一眼，函数2调用了函数1，本质上两个函数没区别。 这里推荐使用第二个，因为方便。 另外说一下multiple参数。true表示回复当前信道所有未回复的ack，用于批量确认。false表示回复当前条目。 函数三： 拒绝本条消息。如果requeue为true，则RMQ会把这条消息重新加入队列，如果requeue为false，则RMQ会丢弃本条消息。 注：推荐手动回复，尽量不要使用autoACK，因autoACK不可控。 ","date":"2019-09-27","objectID":"/posts/go_rabbitmq/:0:3","tags":["golang"],"title":"rabbitmq","uri":"/posts/go_rabbitmq/"},{"categories":["笔记"],"content":"延时队列 rabbitmq延时队列注意：先入队列，也是先被消费者接收，哪怕它是最后一个过期，提前过期的一定会等待第一个入队的。需要将延迟时间一致的业务放入同一队列即可。 延时队列原理： rabbitMQ的queue可配置x-dead-letter-exchange和x-dead-letter-routing-key两个参数,意为如果队列中出现了dead letter,则按照这两个参数重新路由转发到指定队列。 x-dead-letter-exchange : 出现dead letter之后,将dead letter重新发送到exchange x-dead-letter-routing-key : 出现dead letter之后,将dead letter重新按照指定的routing-key发送 队列出现dead letter情况有: 消息或队列的TTL过期 队列达到最大长度 消息被消费端拒绝,并且requeue = false ","date":"2019-09-27","objectID":"/posts/go_rabbitmq/:0:4","tags":["golang"],"title":"rabbitmq","uri":"/posts/go_rabbitmq/"},{"categories":["笔记"],"content":"关于消息顺序 我们遇到的大多数场景都不需要消息的有序的，如果对于消息顺序敏感，那么我们这里给出的方法是 消息体通过hash分派到队列里，每个队列对应一个消费者，多分拆队列。 为什么要这么设计？ 同一组的任务会被分配到同一个队列里，每个队列只能有一个worker来消费，这样避免了同一个队列多个消费者消费时，乱序的可能！ t1, t2 两个任务， t1 虽然被c1先pop了，但是有可能c2先把 t2 任务给完成了。 一句话，主动去分配队列，单个消费者。 ","date":"2019-09-27","objectID":"/posts/go_rabbitmq/:0:5","tags":["golang"],"title":"rabbitmq","uri":"/posts/go_rabbitmq/"},{"categories":["笔记"],"content":"go select","date":"2019-08-13","objectID":"/posts/go_select/","tags":["golang"],"title":"go select","uri":"/posts/go_select/"},{"categories":["笔记"],"content":"select使用 select只会执行一次 case语句必须是对channel的操作 case语句不管是接收还是发送，语句表达式都会执行（执行顺序是从左到右，从上到下，这里只是语句表达式，而不是发送和接收操作的顺序，判断发送和接收是随机的顺序） 会对所有case语句进行判断，如果多个都符合，随机选一个 如果case都不符合，default（如果存在，不存在就会阻塞等着）执行 func test1(ctx context.Context) { for { select { case \u003c-ctx.Done(): //context到期或主动取消时，channel就被关闭，零值被取出，语句得到执行 fmt.Println(ctx.Err() == context.DeadlineExceeded) fmt.Println(\"test1 stop\") return case \u003c-time.Tick(time.Second): fmt.Println(\"tick\") default: fmt.Println(\"test1\") time.Sleep(time.Second) } } } case表达式都会执行，验证： func main() { select { case getChan(\"this is 1 get chan\") \u003c- getInt(\"this is 1 get int\"): //getChan(),getInt都会执行，从左到右，从上到下 fmt.Println(\"1 被选中\") case getChan(\"this is 2 get chan\") \u003c- getInt(\"this is 2 get int\"): fmt.Println(\"2 被选中\") default: fmt.Println(\"default 被选中\") } } func getChan(s string) chan int { fmt.Println(s) c1 := make(chan int, 1) return c1 } func getInt(s string) int { fmt.Println(s) return 1 } ","date":"2019-08-13","objectID":"/posts/go_select/:0:1","tags":["golang"],"title":"go select","uri":"/posts/go_select/"},{"categories":["笔记"],"content":"select原理 定义了一个数据结构表示每个case语句(含defaut，default实际上是一种特殊的case)，select执行过程可以类比成一个函数，函数输入case数组，输出选中的case，然后程序流程转到选中的case块。 case数据结构 type scase struct { c *hchan // chan kind uint16 elem unsafe.Pointer // data element } scase.c为当前case语句所操作的channel指针，这也说明了一个case语句只能操作一个channel。 scase.kind表示该case的类型，分为读channel、写channel和default，三种类型分别由常量定义： caseRecv：case语句中尝试读取scase.c中的数据； caseSend：case语句中尝试向scase.c中写入数据； caseDefault： default语句 scase.elem表示缓冲区地址，跟据scase.kind不同，有不同的用途： scase.kind == caseRecv ： scase.elem表示读出channel的数据存放地址； scase.kind == caseSend ： scase.elem表示将要写入channel的数据存放地址； 真正选择case的函数是selectgo函数， ","date":"2019-08-13","objectID":"/posts/go_select/:0:2","tags":["golang"],"title":"go select","uri":"/posts/go_select/"},{"categories":["笔记"],"content":"总结 select 结构的执行过程与实现原理，首先在编译期间，Go 语言会对 select 语句进行优化，以下是根据 select 中语句的不同选择了不同的优化路径： 空的 select 语句会被直接转换成 block 函数的调用，直接挂起当前 Goroutine； 如果 select 语句中只包含一个 case，就会被转换成 if ch == nil { block }; n; 表达式； 首先判断操作的 Channel 是不是空的； 然后执行 case 结构中的内容； 如果 select 语句中只包含两个 case 并且其中一个是 default，那么 Channel 和接收和发送操作都会使用 selectnbrecv 和 selectnbsend 非阻塞地执行接收和发送操作； 在默认情况下会通过 selectgo 函数选择需要执行的 case 并通过多个 if 语句执行 case 中的表达式； 在编译器已经对 select 语句进行优化之后，Go 语言会在运行时执行编译期间展开的 selectgo 函数，这个函数会按照以下的过程执行： 随机生成一个遍历的轮询顺序 pollOrder 并根据 Channel 地址生成一个用于遍历的锁定顺序 lockOrder； 根据 pollOrder 遍历所有的 case 查看是否有可以立刻处理的 Channel 消息； 如果有消息就直接获取 case 对应的索引并返回； 如果没有消息就会创建 sudog 结构体，将当前 Goroutine 加入到所有相关 Channel 的 sendq 和 recvq 队列中并调用 gopark 触发调度器的调度； 当调度器唤醒当前 Goroutine 时就会再次按照 lockOrder 遍历所有的 case，从中查找需要被处理的 sudog 结构并返回对应的索引； 然而并不是所有的 select 控制结构都会走到 selectgo 上，很多情况都会被直接优化掉，没有机会调用 selectgo 函数。 ","date":"2019-08-13","objectID":"/posts/go_select/:0:3","tags":["golang"],"title":"go select","uri":"/posts/go_select/"},{"categories":["笔记"],"content":"唯一id","date":"2019-07-25","objectID":"/posts/go_unique_id/","tags":["golang"],"title":"唯一id","uri":"/posts/go_unique_id/"},{"categories":["笔记"],"content":"UUID UUID是由一组32位数的16进制数字所构成，所以UUID理论上的总数为16^32=2^128。 以连字号分为五段，形式为8-4-4-4-12的32个字符。示例： 550e8400-e29b-41d4-a716-446655440000 为保证UUID的唯一性，规范定义了包括网卡MAC地址、时间戳、名字空间（Namespace）、随机或伪随机数、时序等元素。 UUID具有多个版本，每个版本的算法不同，应用范围也不同。 基于时间的UUID 基于时间的UUID通过计算当前时间戳、随机数和机器MAC地址得到。由于在算法中使用了MAC地址，这个版本的UUID可以保证在全球范围的唯一性。 DCE安全的UUID DCE（Distributed Computing Environment）安全的UUID和基于时间的UUID算法相同，但会把时间戳的前4位置换为POSIX的UID或GID。这个版本的UUID在实际中较少用到。 基于名字的UUID（MD5） 基于名字的UUID通过计算名字和名字空间的MD5散列值得到。这个版本的UUID保证了：相同名字空间中不同名字生成的UUID的唯一性；不同名字空间中的UUID的唯一性；相同名字空间中相同名字的UUID重复生成是相同的。 随机UUID 根据随机数，或者伪随机数生成UUID。这种UUID产生重复的概率是可以计算出来的。 ","date":"2019-07-25","objectID":"/posts/go_unique_id/:0:1","tags":["golang"],"title":"唯一id","uri":"/posts/go_unique_id/"},{"categories":["笔记"],"content":"snowflake(雪花id) 时间戳。时间戳段位共41位，单位毫秒，可以使用约70年。为了增加剩余可用期限，一般都会把起始日期尽量后移而不是直接使用1970-01-01。（ps：如果是使用1970，你的程序只能支持到2039年了） 机器id。用于区分集群内不同机器，因为Snowflake生成ID是在每台机器上进行的。5个bit是数据中心，5个bit的机器ID 序列号。由于高并发的特性，即使时间戳精确到了毫秒，也有可能出现重复。序列号用于同一时间戳下生成多个id。12位的长度，同一机器一毫秒可以有2^12=4096个id。 ","date":"2019-07-25","objectID":"/posts/go_unique_id/:0:2","tags":["golang"],"title":"唯一id","uri":"/posts/go_unique_id/"},{"categories":["笔记"],"content":"总结 uuid参数的数据长度32位，比较长；无序，没有规律； snowflake长度为64bit位二进制，可以转为int64；总体呈现递增趋势；如果时间回拨，会有重复的风险。 ","date":"2019-07-25","objectID":"/posts/go_unique_id/:0:3","tags":["golang"],"title":"唯一id","uri":"/posts/go_unique_id/"},{"categories":["笔记"],"content":"range语句","date":"2019-07-04","objectID":"/posts/go_range/","tags":["golang"],"title":"range语句","uri":"/posts/go_range/"},{"categories":["笔记"],"content":"range slice func main() { s := []int{1, 2, 3} for i := 0; i \u003c len(s); i++ { if i == 1 { s = append(s, 4) } fmt.Println(i, s[i]) } } 上面的代码，遍历中新增元素是没有问题的。 但是下面的range遍历，结果是不一样的： func main() { s := []int{1, 2, 3} fmt.Println(s) //[1 2 3] for i, v := range s { if i == 0 { s = append(s, 4) } fmt.Println(s) //[1 2 3 4] fmt.Println(i, v) } fmt.Println(s) //[1 2 3 4] } 这里在range中增加元素，并没有改变range的次数。 其实这里只是一个语法糖。 对于切片的for…range底层源码是这样的： // for_temp := range // len_temp := len(for_temp) // for index_temp = 0; index_temp \u003c len_temp; index_temp++ { // value_temp = for_temp[index_temp] // index = index_temp // value = value_temp // original body // } 可以看到，在遍历之前，就取到了切片的长度。后面增删元素，并不影响遍历的次数。 仅仅不会改变遍历的次数，但是切片里的数据是会改动的，所以不要在遍历的时候删除数据。 ","date":"2019-07-04","objectID":"/posts/go_range/:0:1","tags":["golang"],"title":"range语句","uri":"/posts/go_range/"},{"categories":["笔记"],"content":"range map // Lower a for range over a map. // The loop we generate: // var hiter map_iteration_struct // for mapiterinit(type, range, \u0026hiter); hiter.key != nil; mapiternext(\u0026hiter) { // index_temp = *hiter.key // value_temp = *hiter.val // index = index_temp // value = value_temp // original body // } func mtest() { m := make(map[int]int) m[0] = 1 m[1] = 2 var count int for k, v := range m { fmt.Println(k, v) count++ if count == 1 { m[3] = 3 fmt.Println(\"add\") } } } 输出： 0 1 add 1 2 3 3 可以看出，range过程中增加元素是会改变range次数的。实验证明，删除元素也有同样的效果。 ","date":"2019-07-04","objectID":"/posts/go_range/:0:2","tags":["golang"],"title":"range语句","uri":"/posts/go_range/"},{"categories":["笔记"],"content":"range channel // Lower a for range over a channel. // The loop we generate: // for { // index_temp, ok_temp = \u003c-range // if !ok_temp { // break // } // index = index_temp // original body // } ","date":"2019-07-04","objectID":"/posts/go_range/:0:3","tags":["golang"],"title":"range语句","uri":"/posts/go_range/"},{"categories":["笔记"],"content":"range array // Lower a for range over an array. // The loop we generate: // len_temp := len(range) // range_temp := range // for index_temp = 0; index_temp \u003c len_temp; index_temp++ { // value_temp = range_temp[index_temp] // index = index_temp // value = value_temp // original body // } 可以看到输入值和slice类似，都是提前取到长度。 ","date":"2019-07-04","objectID":"/posts/go_range/:0:4","tags":["golang"],"title":"range语句","uri":"/posts/go_range/"},{"categories":["笔记"],"content":"range string // Lower a for range over a string. // The loop we generate: // len_temp := len(range) // var next_index_temp int // for index_temp = 0; index_temp \u003c len_temp; index_temp = next_index_temp { // value_temp = rune(range[index_temp]) // if value_temp \u003c utf8.RuneSelf { // next_index_temp = index_temp + 1 // } else { // value_temp, next_index_temp = decoderune(range, index_temp) // } // index = index_temp // value = value_temp // // original body // } func stringtest() { s := \"abcd\" var count int for _, v := range s { fmt.Printf(\"%c\\n\", v) count++ if count == 1 { s = s + \"ss\" fmt.Println(\"add\") } } fmt.Println(s) } 验证的结果为range过程中对字符串的追加及截取，都不影响range。 ","date":"2019-07-04","objectID":"/posts/go_range/:0:5","tags":["golang"],"title":"range语句","uri":"/posts/go_range/"},{"categories":["笔记"],"content":"go内存分配1","date":"2019-06-24","objectID":"/posts/go_memory1/","tags":["golang"],"title":"go内存分配1","uri":"/posts/go_memory1/"},{"categories":["笔记"],"content":"概述 为了方便自主管理内存，做法便是先向系统申请一块内存，然后将内存切割成小块，通过一定的内存分配算法管理内存。 预申请的内存划分为spans、bitmap、arena三部分。其中arena即为所谓的堆区，应用中需要的内存从这里分配。 其中spans和bitmap是为了管理arena区而存在的。 arena的大小为512G，为了方便管理把arena区域划分成一个个的page，每个page为8KB,一共有512GB/8KB个页； spans区域存放span的指针，每个指针对应一个page，所以span区域的大小为(512GB/8KB)*指针大小8byte = 512M bitmap区域大小也是通过arena计算出来，不过主要用于GC。 ","date":"2019-06-24","objectID":"/posts/go_memory1/:0:1","tags":["golang"],"title":"go内存分配1","uri":"/posts/go_memory1/"},{"categories":["笔记"],"content":"spans span是内存管理的基本单位,每个span用于管理特定的class对象, 每个class都代表一个固定大小的对象，跟据对象大小，span将一个或多个页拆分成多个块进行管理。 换句话说为了更好的应对不同大小的对象的分配，有了class的概念，一共67中class，每种class所拥有的page数量是不同的。 src/runtime/mheap.go:mspan定义了其数据结构： type mspan struct { next *mspan //链表前向指针，用于将span链接起来 prev *mspan //链表前向指针，用于将span链接起来 startAddr uintptr // 起始地址，也即所管理页的地址 npages uintptr // 管理的页数 nelems uintptr // 块个数，也即有多少个块可供分配 allocBits *gcBits //分配位图，每一位代表一个块是否已分配 allocCount uint16 // 已分配块的个数 spanclass spanClass // class表中的class ID elemsize uintptr // class表中的对象大小，也即块大小 } 以class 10为例，span和管理的内存如下图所示： ","date":"2019-06-24","objectID":"/posts/go_memory1/:0:2","tags":["golang"],"title":"go内存分配1","uri":"/posts/go_memory1/"},{"categories":["笔记"],"content":"chache 有了管理内存的基本单位span，还要有个数据结构来管理span，这个数据结构叫mcentral，各线程需要内存时从mcentral管理的span中申请内存，为了避免多线程申请内存时不断的加锁，Golang为每个线程分配了span的缓存，这个缓存即是cache。 ","date":"2019-06-24","objectID":"/posts/go_memory1/:0:3","tags":["golang"],"title":"go内存分配1","uri":"/posts/go_memory1/"},{"categories":["笔记"],"content":"central cache作为线程的私有资源为单个线程服务，而central则是全局资源，为多个线程服务，当某个线程内存不足时会向central申请，当某个线程释放内存时又会回收进central。 ","date":"2019-06-24","objectID":"/posts/go_memory1/:0:4","tags":["golang"],"title":"go内存分配1","uri":"/posts/go_memory1/"},{"categories":["笔记"],"content":"heap central内存不够时，从heap申请。 ","date":"2019-06-24","objectID":"/posts/go_memory1/:0:5","tags":["golang"],"title":"go内存分配1","uri":"/posts/go_memory1/"},{"categories":["笔记"],"content":"内存分配过程 以申请size为n的内存为例，分配步骤如下： 获取当前线程的私有缓存mcache 跟据size计算出适合的class的ID 从mcache的alloc[class]链表中查询可用的span 如果mcache没有可用的span则从mcentral申请一个新的span加入mcache中 如果mcentral中也没有可用的span则从mheap中申请一个新的span加入mcentral 从该span中获取到空闲对象地址并返回 ","date":"2019-06-24","objectID":"/posts/go_memory1/:0:6","tags":["golang"],"title":"go内存分配1","uri":"/posts/go_memory1/"},{"categories":["笔记"],"content":"总结 Golang程序启动时申请一大块内存，并划分成spans、bitmap、arena区域 arena区域按页划分成一个个小块 span管理一个或多个页 mcentral管理多个span供线程申请使用 mcache作为线程私有资源，资源来源于mcentral ","date":"2019-06-24","objectID":"/posts/go_memory1/:0:7","tags":["golang"],"title":"go内存分配1","uri":"/posts/go_memory1/"},{"categories":["笔记"],"content":"go内存分配2","date":"2019-06-24","objectID":"/posts/go_memory2/","tags":["golang"],"title":"go内存分配2","uri":"/posts/go_memory2/"},{"categories":["笔记"],"content":"内存基础 内存： cpu与硬盘速度不匹配，引入内存作为中间缓冲。 cache: cpu与内存速度也不匹配，引入chache作为中间缓冲，并逐渐发展为3级cache。L1、L2、L3。其中l1速度最快 虚拟内存： 虚拟内存是当代操作系统必备的一项重要功能了，它向进程屏蔽了底层了RAM和磁盘，并向进程提供了远超物理内存大小的内存空间。 进程访问数据，当Cache没有命中的时候，访问虚拟内存获取数据，当前要访问的虚拟内存地址，是否已经加载到了物理内存，如果已经在物理内存，则取物理内存数据，如果没有对应的物理内存，则从磁盘加载数据到物理内存，并把物理内存地址和虚拟内存地址更新到页表。 在没有虚拟内存的时代，物理内存对所有进程是共享的，多进程同时访问同一个物理内存存在并发访问问题。引入虚拟内存后，每个进程都要各自的虚拟内存，内存的并发访问问题的粒度从多进程级别，可以降低到多线程级别。 堆和栈： 虚拟内存中的栈和堆，也就是进程对内存的管理。 栈和堆相比有这么几个好处： 栈的内存管理简单，分配比堆上快。 栈的内存不需要回收，而堆需要，无论是主动free，还是被动的垃圾回收，这都需要花费额外的CPU。 ","date":"2019-06-24","objectID":"/posts/go_memory2/:0:1","tags":["golang"],"title":"go内存分配2","uri":"/posts/go_memory2/"},{"categories":["笔记"],"content":"TCMalloc TCMalloc是go内存分配的前辈，Go的内存管理是借鉴了TCMalloc，随着Go的迭代，Go的内存管理与TCMalloc不一致地方在不断扩大，但其主要思想、原理和概念都是和TCMalloc一致的。 引入虚拟内存后，让内存的并发访问问题的粒度从多进程级别，降低到多线程级别。但是同一个进程中多个线程申请内存时需要加锁，如果不加锁就存在同一块内存被2个线程同时访问的问题。 TCMalloc的做法是什么呢？为每个线程预分配一块缓存，线程申请小内存时，可以从缓存分配内存，这样有2个好处： 以后线程在再分配内存就是在用户态，不需要系统调用即可。 线程利用自己的内存缓存，不用加锁了。 TCMalloc的几个重要概念 **Page：**操作系统对内存管理以页为单位，TCMalloc也是这样，只不过TCMalloc里的Page大小与操作系统里的大小并不一定相等，x64下Page大小是8KB。 **Span：**一组连续的Page被称为Span，比如可以有2个页大小的Span，也可以有16页大小的Span，Span比Page高一个层级，是为了方便管理一定大小的内存区域，Span是TCMalloc中内存管理的基本单位。 **ThreadCache：**每个线程各自的Cache。由于每个线程有自己的ThreadCache，所以ThreadCache访问是无锁的。 **CentralCache：**是所有线程共享的缓存，也是保存的空闲内存块链表，链表的数量与ThreadCache中链表数量相同，当ThreadCache内存块不足时，可以从CentralCache取，当ThreadCache内存块多时，可以放回CentralCache。由于CentralCache是共享的，所以它的访问是要加锁的。 **PageHeap：**PageHeap是堆内存的抽象，当CentralCache没有内存的时，会从PageHeap取，把1个Span拆成若干内存块，添加到对应大小的链表中，当CentralCache内存多的时候，会放回PageHeap。 TCMalloc中有小、中、大对象概念，Go内存管理中也有类似的概念，我们瞄一眼TCMalloc的定义： 小对象大小：0~256KB 中对象大小：257~1MB 大对象大小：\u003e1MB 小对象的分配流程：ThreadCache -\u003e CentralCache -\u003e HeapPage，大部分时候，ThreadCache缓存都是足够的，不需要去访问CentralCache和HeapPage，无锁分配加无系统调用，分配效率是非常高的。 中对象分配流程：直接在PageHeap中选择适当的大小即可，128 Page的Span所保存的最大内存就是1MB。 大对象分配流程：从large span set选择合适数量的页面组成span，用来存储数据。 ","date":"2019-06-24","objectID":"/posts/go_memory2/:0:2","tags":["golang"],"title":"go内存分配2","uri":"/posts/go_memory2/"},{"categories":["笔记"],"content":"Go内存管理 通过TCMalloc的了解，可以看出他的精髓是内存的分层。go借鉴了他。 go内存的概念 **Page：**与TCMalloc中的Page相同，x64下1个Page的大小是8KB。 **pan：**与TCMalloc中的Span相同，Span是内存管理的基本单位，代码中为mspan，一组连续的Page组成1个Span。 **mcache：**mcache与TCMalloc中的ThreadCache类似，mcache保存的是各种大小的Span，并按Span class分类，小对象直接从mcache分配内存，它起到了缓存的作用，并且可以无锁访问。 但mcache与ThreadCache也有不同点，TCMalloc中是每个线程1个ThreadCache，Go中是每个P拥有1个mcache，因为在Go程序中，当前最多有GOMAXPROCS个线程在用户态运行，所以最多需要GOMAXPROCS个mcache就可以保证各线程对mcache的无锁访问，线程的运行又是与P绑定的，把mcache交给P刚刚好。 **mcentral：**mcentral与TCMalloc中的CentralCache类似，是所有线程共享的缓存，需要加锁访问，它按Span class对Span分类，串联成链表，当mcache的某个级别Span的内存被分配光时，它会向mcentral申请1个当前级别的Span。 **mheap：**mheap与TCMalloc中的PageHeap类似，它是堆内存的抽象，把从OS申请出的内存页组织成Span，并保存起来。当mcentral的Span不够用时会向mheap申请，mheap的Span不够用时会向OS申请。 对象的分级 为了更好的应对不同大小的对象的分配，有了class的概念，一共67中class，每种class所拥有的page数量是不同的。size class 0实际并未使用到。 **object size：**代码里简称size，指申请内存的对象大小。 **size class：**代码里简称class，它是size的级别，相当于把size归类到一定大小的区间段，比如size[1,8]属于size class 1，size(8,16]属于size class 2。 **span class：**指span的级别，但span class的大小与span的大小并没有正比关系。span class主要用来和size class做对应，1个size class对应2个span class，2个span class的span大小相同，只是功能不同，1个用来存放包含指针的对象，一个用来存放不包含指针的对象，不包含指针对象的Span就无需GC扫描了。 **num of page：**代码里简称npage，代表Page的数量，其实就是Span包含的页数，用来分配内存。 ","date":"2019-06-24","objectID":"/posts/go_memory2/:0:3","tags":["golang"],"title":"go内存分配2","uri":"/posts/go_memory2/"},{"categories":["笔记"],"content":"go内存分配 Go中的内存分类并不像TCMalloc那样分成小、中、大对象，但是它的小对象里又细分了一个Tiny对象，Tiny对象指大小在1Byte到16Byte之间并且不包含指针的对象。小对象和大对象只用大小划定，无其他区分。 小对象是在mcache中分配的，而大对象是直接从mheap分配的。 小对象分配 寻找span的流程如下： 计算对象所需内存大小size 根据size到size class映射，计算出所需的size class 根据size class和对象是否包含指针计算出span class 获取该span class指向的span。 对象size-\u003e size class -\u003e span class -\u003e span class -\u003e span -\u003e 从span分配地址。 大对象分配 大对象的分配比小对象省事多了，99%的流程与mcentral向mheap申请内存的相同，所以不重复介绍了，不同的一点在于mheap会记录一点大对象的统计信息。 ","date":"2019-06-24","objectID":"/posts/go_memory2/:0:4","tags":["golang"],"title":"go内存分配2","uri":"/posts/go_memory2/"},{"categories":["笔记"],"content":"go map","date":"2019-06-19","objectID":"/posts/go_map/","tags":["golang"],"title":"go map","uri":"/posts/go_map/"},{"categories":["笔记"],"content":"hash冲突 哈希查找表一般会存在“碰撞”的问题，就是说不同的 key 被哈希到了同一个 bucket。一般有两种应对方法：链表法和开放地址法。链表法将一个 bucket 实现成一个链表，落在同一个 bucket 中的 key 都会插入这个链表。开放地址法则是碰撞发生后，通过一定的规律，在数组的后面挑选“空位”，用来放置新的 key。 go使用的是链表法来解决hash冲突。 ","date":"2019-06-19","objectID":"/posts/go_map/:0:1","tags":["golang"],"title":"go map","uri":"/posts/go_map/"},{"categories":["笔记"],"content":"map内存模型 // A header for a Go map. type hmap struct { // 元素个数，调用 len(map) 时，直接返回此值 count int flags uint8 // buckets 的对数 log_2 B uint8 // overflow 的 bucket 近似数 noverflow uint16 // 计算 key 的哈希的时候会传入哈希函数 hash0 uint32 // 指向 buckets 数组，大小为 2^B // 如果元素个数为0，就为 nil buckets unsafe.Pointer // 扩容的时候，buckets 长度会是 oldbuckets 的两倍 oldbuckets unsafe.Pointer // 指示扩容进度，小于此地址的 buckets 迁移完成 nevacuate uintptr extra *mapextra // optional fields } B就是buckets数组的长度的对数，即有2^B个桶。 bucket桶的运行时数据结构为： type bmap struct { topbits [8]uint8 keys [8]keytype values [8]valuetype pad uintptr overflow uintptr } bmap 就是我们常说的“桶”，桶里面会最多装 8 个 key，这些 key 之所以会落入同一个桶，是因为它们经过哈希计算后，哈希结果是“一类”的。在桶内，又会根据 key 计算出来的 hash 值的高 8 位来决定 key 到底落入桶内的哪个位置（一个桶内最多有8个位置）。 我们可以看到桶里面有个数组是存放各key的hash值的高8位，key是放一起，value又是放一起。 key和value为什么要分开？ 源码里说明这样的好处是在某些情况下可以省略掉 padding 字段，节省内存空间。 map[int64]int8 如果按照 key/value/key/value/… 这样的模式存储，那在每一个 key/value 对之后都要额外 padding 7 个字节；而将所有的 key，value 分别绑定到一起，这种形式 key/key/…/value/value/…，则只需要在最后添加 padding。 ","date":"2019-06-19","objectID":"/posts/go_map/:0:2","tags":["golang"],"title":"go map","uri":"/posts/go_map/"},{"categories":["笔记"],"content":"使用map 使用map很简单，利用make内建函数，通过汇编语言可以看到，实际上底层调用的是 makemap 函数，主要做的工作就是初始化 hmap 结构体的各种字段，例如计算 B 的大小，设置哈希种子 hash0 等等。 注意，这个函数返回的结果：*hmap，它是一个指针，所以map看起来像是引用。 ","date":"2019-06-19","objectID":"/posts/go_map/:0:3","tags":["golang"],"title":"go map","uri":"/posts/go_map/"},{"categories":["笔记"],"content":"key定位过程 key 经过哈希计算后得到哈希值，共 64 个 bit 位（64位机，32位机就不讨论了，现在主流都是64位机），计算它到底要落在哪个桶时，只会用到最后 B 个 bit 位。还记得前面提到过的 B 吗？如果 B = 5，那么桶的数量，也就是 buckets 数组的长度是 2^5 = 32。 例如，现在有一个 key 经过哈希函数计算后，得到的哈希结果是： 10010111 | 000011110110110010001111001010100010010110010101010 │ 01010 用最后的 5 个 bit 位，也就是 01010，值为 10，也就是 10 号桶。这个操作实际上就是取余操作，但是取余开销太大，所以代码实现上用的位操作代替。 再用哈希值的高 8 位，找到此 key 在 bucket 中的位置，这是在寻找已有的 key。最开始桶内还没有 key，新加入的 key 会找到第一个空位，放入。 当两个不同的 key 落在同一个桶中，也就是发生了哈希冲突。冲突的解决手段是用链表法：在 bucket 中，从前往后找到第一个空位。这样，在查找某个 key 时，先找到对应的桶，再去遍历 bucket 中的 key。 如果在 bucket 中没找到，并且 overflow 不为空，还要继续去 overflow bucket 中寻找，直到找到或是所有的 key 槽位都找遍了，包括所有的 overflow bucket。 总结：对key进行hash运算取到值，低位也是就hash的右边（具体几位看B）来确定是在哪个桶中，高位也就是hash的左边8位来确定key在桶中的位置。 ","date":"2019-06-19","objectID":"/posts/go_map/:0:4","tags":["golang"],"title":"go map","uri":"/posts/go_map/"},{"categories":["笔记"],"content":"map的初始化 根据传入的 bucket 类型，获取其类型能够申请的最大容量大小。并对其长度 make(map[k]v, hint) 进行边界值检验 初始化 hmap 初始化哈希因子 根据传入的 hint，计算一个可以放下 hint 个元素的桶 B 的最小值 分配并初始化 hash table。如果 B 为 0 将在后续懒惰分配桶，大于 0 则会马上进行分配 返回初始化完毕的 hmap 当hint\u003c8时，最少一个bucket就可以了，否则，至少需要两个bucket，就需要立刻分配hash table。 ","date":"2019-06-19","objectID":"/posts/go_map/:0:5","tags":["golang"],"title":"go map","uri":"/posts/go_map/"},{"categories":["笔记"],"content":"扩容 在向 map 插入新 key 的时候，会进行条件检测，符合条件就会触发扩容。 扩容的方式 溢出桶太多，相同容量扩容 达到加载因子，2倍容量扩容 除了满足两个条件之一外，还要满足“不在扩容中”。 if (不是正在扩容 \u0026\u0026 (元素个数/bucket数超过某个值 || 太多overflow bucket)) { 进行扩容 } 啥意思呢？第一种出现的情况是：因为map不断的put和delete，出现了很多空格，这些空格会导致bmap很长，但是中间有很多空的地方，扫描时间变长。所以第一种扩容实际是一种整理，将数据整理到前面一起。第二种呢：就是真的不够用了，扩容两倍。 2倍扩容 理想中每个bucket里面只放一个元素，这样最快，但是空间太大。go采用链表解决冲突，但是，如果所有的key都在一个bucket里面，那就退化成了链表，因此需要衡量。 装载因子就是衡量标准， loadFactor := count / (2^B) 当loadFactor为6.5的时候就要扩容。 相同容量扩容 所谓的相同容量扩容，说白了就是不增加bucket的数量，只是整理现在的数据分布。 相同容量扩容的原因是overflow 的 bucket 数量过多： 当 B 小于 15，如果 overflow 的 bucket 数量超过 2^B ；当 B \u003e= 15，如果 overflow 的 bucket 数量超过 2^15 。 其实就是map元素本身不多（达不到加载因子的条件），但是有的bucket的溢出桶overflow太多，这样就造成效率低下。 这样的场景好理解：先添加一些元素，对前面的bucket元素删除，这样就造成大量的bucket不满，整理后可以提高效率。 扩容流程 等量扩容流程 其实元素没那么多，但是 overflow bucket 数特别多，说明很多 bucket 都没装满。解决办法就是开辟一个新 bucket 空间，将老 bucket 中的元素移动到新 bucket，使得同一个 bucket 中的 key 排列地更紧密。 2倍的扩容流程 元素太多，而 bucket 数量太少，很简单：将 B 加 1，bucket 最大数量（2^B）直接变成原来 bucket 数量的 2 倍。于是，就有新老 bucket 了。注意，这时候元素都在老 bucket 里，还没迁移到新的 bucket 来。而且，新 bucket 只是最大数量变为原来最大数量（2^B）的 2 倍（2^B * 2）。 2倍扩容流程，原来的一个bucket会裂变成两个bucket，理由很简单，多看了一位，该位0或1。 不管哪种方式，确定扩容的数量后，扩容本身是慢慢的过程。真正搬迁 buckets 的动作在 growWork() 函数中，而调用 growWork() 函数的动作是在 mapassign 和 mapdelete 函数中。**也就是插入或修改、删除 key 的时候，都会尝试进行搬迁 buckets 的工作。**先检查 oldbuckets 是否搬迁完毕，具体来说就是检查 oldbuckets 是否为 nil。 ","date":"2019-06-19","objectID":"/posts/go_map/:0:6","tags":["golang"],"title":"go map","uri":"/posts/go_map/"},{"categories":["笔记"],"content":"put hash表如果正在扩容，并且这次要操作的bucket还没搬到新hash表中，那么先进行搬迁（扩容细节下面细说）。 在buck中寻找key，同时记录下第一个空位置，如果找不到，那么就在空位置中插入数据；如果找到了，那么就更新对应的value； 找不到key就看下需不需要扩容，需要扩容并且没有正在扩容，那么就进行扩容，然后回到第一步。 找不到key，不需要扩容，但是没有空slot，那么就分配一个overflow bucket挂在链表结尾，用新bucket的第一个slot放存放数据。 向map插入数据，第一步还是先找bucket，对key取hash，低位找桶，再遍历桶中的数据，如果已经满了就新增一个溢出桶挂上去，如果不满就插入即可。 有一个细节注意，只有进行完了这个搬迁操作后，我们才能放心地在新 bucket 里定位 key 要安置的地址，再进行之后的操作。 找到桶后，应该先看下该桶对应的原始桶是否已经迁移完毕，如果还没迁移完毕，就应该先迁移，然后插入之前再看是否需要扩容（不用担心迁移过程中又要扩容，前面有条件），不需要的话再插。 ","date":"2019-06-19","objectID":"/posts/go_map/:0:7","tags":["golang"],"title":"go map","uri":"/posts/go_map/"},{"categories":["笔记"],"content":"get 先定位出bucket，如果正在扩容，并且这个bucket还没搬到新的hash表中，那么就从老的hash表中查找。 在bucket中进行顺序查找，使用高八位进行快速过滤，高八位相等，再比较key是否相等，找到就返回value。如果当前bucket找不到，就往下找overflow bucket，都没有就返回零值。 这里我们可以看到，访问的时候，并不进行扩容的数据搬迁。 ","date":"2019-06-19","objectID":"/posts/go_map/:0:8","tags":["golang"],"title":"go map","uri":"/posts/go_map/"},{"categories":["笔记"],"content":"delete 如果正在扩容，并且操作的bucket还没搬迁完，那么搬迁bucket。 找出对应的key，如果key、value是包含指针的那么会清理指针指向的内存，否则不会回收内存。 ","date":"2019-06-19","objectID":"/posts/go_map/:0:9","tags":["golang"],"title":"go map","uri":"/posts/go_map/"},{"categories":["笔记"],"content":"其他 bucket中key为何不直接和value放一起？ 之所以把所有k1k2放一起而不是k1v1是因为key和value的数据类型内存大小可能差距很大，比如map[int64]int8，考虑到字节对齐，kv存在一起会浪费很多空间。 map遍历为何无序？ map里的数据如果不进行操作，每次遍历应该是一样的，但是扩容以后就会变化。为了统一记忆，所有遍历都是无序。 实现方案就是每次遍历随机指定bucket和bucket中的key offset，这样遍历的位置就是随机的了。 map是否线程安全？ 不是的，对同一map进行写，会抛异常。 map的key限制 map的key不能是切片、map、函数。可以为interface{},但是运行时还是不能放这三种；key可以为数组，同样数组元素也不能为这三种。总之，key一定可以是“可比较”类型即可以使用==判断，nil==nil是不合法的，所以map不支持key为nil。另外value也不能为nil map的key占用空间越小，hash的速度越快，操作起来也是更快，尽量别用自定义类型 历史版本 在Go 1.6之前， 内置的map类型是部分goroutine安全的，并发的读没有问题，并发的写可能有问题。自go 1.6之后，并发地读写map会报错。牵涉到并发，应该用sync.map map桶的数量是2^N，为何一定要是2的指数次幂？ 在定位桶即tab的index时，一般是取余，hashCode % length,但是取余是复杂的操作，当length为2^N时，hashCode % length == hashCode \u0026 (length - 1)，这样就转为了更快的与运算。 ","date":"2019-06-19","objectID":"/posts/go_map/:0:10","tags":["golang"],"title":"go map","uri":"/posts/go_map/"},{"categories":["笔记"],"content":"总结 map的赋值（增和改）会造成扩容。 map扩容和迁移是分开的，迁移是渐进的，map的增，删，改都会进行迁移操作，查找并不能进行数据的搬迁。 ","date":"2019-06-19","objectID":"/posts/go_map/:0:11","tags":["golang"],"title":"go map","uri":"/posts/go_map/"},{"categories":["笔记"],"content":"go绝知—unsafe包","date":"2019-06-09","objectID":"/posts/go_unsafe_package/","tags":["golang"],"title":"go绝知—unsafe包","uri":"/posts/go_unsafe_package/"},{"categories":["笔记"],"content":"go中指针 先看下go中的指针，go中的指针与c语言中的指针有很大不同： go指针不支持运算 不同类型无法转换 不同类型指针不可比较 我们不难看出，go中的指针更加安全，但是却失去了灵活性，而且有些通过指针高效的处理数据的能力也失去了。 ","date":"2019-06-09","objectID":"/posts/go_unsafe_package/:0:1","tags":["golang"],"title":"go绝知—unsafe包","uri":"/posts/go_unsafe_package/"},{"categories":["笔记"],"content":"unsafe包 unsafe包的出现，可以让我们更高效的处理数据，但是和他的名字一样，不安全！ 我们可以利用unsafe包拿到结构体struct中的未导出字段。 ","date":"2019-06-09","objectID":"/posts/go_unsafe_package/:0:2","tags":["golang"],"title":"go绝知—unsafe包","uri":"/posts/go_unsafe_package/"},{"categories":["笔记"],"content":"使用unsafe 阅读unsafe包文档中列出的规则： 任何类型的指针值都可以转换为unsafe.Pointer。 unsafe.Pointer可以转换为任何类型的指针值。 uintptr可以转换为unsafe.Pointer。 unsafe.Pointer可以转换为uintptr。 简单而言就是：unsafe.Pointer可以与任何类型指针值转换，unintptr可以与unsafe.Pointer互转。 unsafe 包还有其他三个函数： func Sizeof(x ArbitraryType) uintptr func Offsetof(x ArbitraryType) uintptr func Alignof(x ArbitraryType) uintptr ArbitraryType是任意的意思。 获取slice长度 我们换种方式来取slice的长度 type slice struct { array unsafe.Pointer // 元素指针 len int // 长度 cap int // 容量 } func main() { s := make([]int, 9, 20) var Len = *(*int)(unsafe.Pointer(uintptr(unsafe.Pointer(\u0026s)) + uintptr(8))) fmt.Println(Len, len(s)) // 9 9 var Cap = *(*int)(unsafe.Pointer(uintptr(unsafe.Pointer(\u0026s)) + uintptr(16))) fmt.Println(Cap, cap(s)) // 20 20 } Offsetof 获取成员偏移量 对于一个结构体，通过 offset 函数可以获取结构体成员的偏移量，进而获取成员的地址，读写该地址的内存，就可以达到改变成员值的目的。 这里有一个内存分配相关的事实：结构体会被分配一块连续的内存，结构体的地址也代表了第一个成员的地址。 type Programmer struct { name string language string } func main() { p := Programmer{\"stefno\", \"go\"} fmt.Println(p) name := (*string)(unsafe.Pointer(\u0026p)) *name = \"qcrao\" lang := (*string)(unsafe.Pointer(uintptr(unsafe.Pointer(\u0026p)) + unsafe.Offsetof(p.language))) *lang = \"Golang\" fmt.Println(p) } 这里name成员的地址就是结构体的地址。 string 和 slice 的相互转换 string和slice的转换直接强转即可。 func main() { str := \"abc\" var s []byte = []byte(str) fmt.Println(cap(s)) str2 := string(s) fmt.Println(str2) } 这里转换的时候是有内存重新分配转换的。他们底层虽然都是数组，但是string的底层数组是只读的，不能修改的，slice底层数组是可以修改的。 这是一个非常精典的例子。实现字符串和 bytes 切片之间的转换，要求是 zero-copy。想一下，一般的做法，都需要遍历字符串或 bytes 切片，再挨个赋值。 完成这个任务，我们需要了解 slice 和 string 的底层数据结构： type StringHeader struct { Data uintptr Len int } type SliceHeader struct { Data uintptr Len int Cap int } 我们可以看到，string本质还是个结构体，只需要共享底层 []byte 数组就可以实现 zero-copy。 func string2bytes(s string) []byte { stringHeader := (*reflect.StringHeader)(unsafe.Pointer(\u0026s)) bh := reflect.SliceHeader{ Data: stringHeader.Data, Len: stringHeader.Len, Cap: stringHeader.Len, } return *(*[]byte)(unsafe.Pointer(\u0026bh)) } func bytes2string(b []byte) string{ sliceHeader := (*reflect.SliceHeader)(unsafe.Pointer(\u0026b)) sh := reflect.StringHeader{ Data: sliceHeader.Data, Len: sliceHeader.Len, } return *(*string)(unsafe.Pointer(\u0026sh)) } ","date":"2019-06-09","objectID":"/posts/go_unsafe_package/:0:3","tags":["golang"],"title":"go绝知—unsafe包","uri":"/posts/go_unsafe_package/"},{"categories":["笔记"],"content":"总结 unsafe 包绕过了 Go 的类型系统，达到直接操作内存的目的，使用它有一定的风险性。但是在某些场景下，使用 unsafe 包提供的函数会提升代码的效率，Go 源码中也是大量使用 unsafe 包。 ","date":"2019-06-09","objectID":"/posts/go_unsafe_package/:0:4","tags":["golang"],"title":"go绝知—unsafe包","uri":"/posts/go_unsafe_package/"},{"categories":["笔记"],"content":"docker-compose","date":"2019-05-27","objectID":"/posts/docker-compose/","tags":["docker"],"title":"docker-compose","uri":"/posts/docker-compose/"},{"categories":["笔记"],"content":"一份标准配置文件应该包含 version、services、networks 三大部分，其中最关键的就是 services 和 networks 两个部分。 version:'2'services:web:image:dockercloud/hello-worldports:- 8080networks:- front-tier- back-tierredis:image:redislinks:- webnetworks:- back-tierlb:image:dockercloud/haproxyports:- 80:80links:- webnetworks:- front-tier- back-tiervolumes:- /var/run/docker.sock:/var/run/docker.socknetworks:front-tier:driver:bridgeback-tier:driver:bridge","date":"2019-05-27","objectID":"/posts/docker-compose/:0:0","tags":["docker"],"title":"docker-compose","uri":"/posts/docker-compose/"},{"categories":["笔记"],"content":"services services下的一级标签是用户自己定义的，是服务名。 image 则是指定服务的镜像名称或镜像 ID。 build 镜像还可以根据自己的dockerfile生成。build指定Dockerfile文件所在的文件夹路径。可以是相对路径。 command 使用 command 可以覆盖容器启动后默认执行的命令 container_name Compose 的容器名称格式是：\u003c项目名称\u003e\u003c服务名称\u003e\u003c序号\u003e,这个可以指定名字 depends_on 容器依赖，被依赖的容器先执行 environment 这个标签的作用是设置镜像变量，它可以保存变量到镜像里面，也就是说启动的容器也会包含这些变量设置 expose 暴露接口 volumes 挂载目录 networks 加入指定网络 ","date":"2019-05-27","objectID":"/posts/docker-compose/:0:1","tags":["docker"],"title":"docker-compose","uri":"/posts/docker-compose/"},{"categories":["笔记"],"content":"networks networks:front:# Use a custom driverdriver:custom-driver-1back:# Use a custom driver which takes special optionsdriver:custom-driver-2driver_opts:foo:\"1\"bar:\"2\"这样就定义了两个网络。 一些场景下，我们并不需要创建新的网络，而只需加入已存在的网络，此时可使用external选项。 ","date":"2019-05-27","objectID":"/posts/docker-compose/:0:2","tags":["docker"],"title":"docker-compose","uri":"/posts/docker-compose/"},{"categories":["笔记"],"content":"docker","date":"2019-05-20","objectID":"/posts/docker/","tags":["docker"],"title":"docker","uri":"/posts/docker/"},{"categories":["笔记"],"content":"镜像 对于 Linux 而言，内核启动后，会挂载 root 文件系统为其提供用户空间支持。而 Docker 镜像（Image），就相当于是一个 root 文件系统。比如官方镜像 ubuntu:18.04 就包含了完整的一套 Ubuntu 18.04 最小系统的 root 文件系统。 Docker 镜像是一个特殊的文件系统，除了提供容器运行时所需的程序、库、资源、配置等文件外，还包含了一些为运行时准备的一些配置参数（如匿名卷、环境变量、用户等）。 ","date":"2019-05-20","objectID":"/posts/docker/:0:0","tags":["docker"],"title":"docker","uri":"/posts/docker/"},{"categories":["笔记"],"content":"镜像管理 ","date":"2019-05-20","objectID":"/posts/docker/:1:0","tags":["docker"],"title":"docker","uri":"/posts/docker/"},{"categories":["笔记"],"content":"获取镜像 docker pull [选项] [Docker Registry 地址[:端口号]/]仓库名[:标签] 命令举例：docker pull ubuntu:18.04 镜像的运行 docker run -it --rm \\ ubuntu:18.04 \\ bash docker run其实是运行容器的命令。 -it：这是两个参数，一个是 -i：交互式操作，一个是 -t 终端。我们这里打算进入 bash 执行一些命令并查看返回结果，因此我们需要交互式终端。 –rm：这个参数是说容器退出后随之将其删除。默认情况下，为了排障需求，退出的容器并不会立即删除，除非手动 docker rm。我们这里只是随便执行个命令，看看结果，不需要排障和保留结果，因此使用 –rm 可以避免浪费空间。 ubuntu:18.04：这是指用 ubuntu:18.04 镜像为基础来启动容器。 bash：放在镜像名后的是命令，这里我们希望有个交互式 Shell，因此用的是 bash。 ","date":"2019-05-20","objectID":"/posts/docker/:1:1","tags":["docker"],"title":"docker","uri":"/posts/docker/"},{"categories":["笔记"],"content":"列出镜像 docker image ls 列表包含了 仓库名、标签、镜像 ID、创建时间 以及 所占用的空间。 可以通过以下命令来便捷的查看镜像、容器、数据卷所占用的空间。docker system df 上面的镜像列表中，还可以看到一个特殊的镜像，这个镜像既没有仓库名，也没有标签，均为 none。由于镜像出现了同名或者镜像更新就会出现，可以删除。 镜像是分层的，相同的层只会存一遍。 ","date":"2019-05-20","objectID":"/posts/docker/:1:2","tags":["docker"],"title":"docker","uri":"/posts/docker/"},{"categories":["笔记"],"content":"删除镜像 docker image rm [选项] \u003c镜像1\u003e [\u003c镜像2\u003e …] 其中，\u003c镜像\u003e 可以是 镜像短 ID、镜像长 ID、镜像名 如果有用这个镜像启动的容器存在（即使容器没有运行），那么同样不可以删除这个镜像。应该先删除容器，再删除镜像。 ","date":"2019-05-20","objectID":"/posts/docker/:1:3","tags":["docker"],"title":"docker","uri":"/posts/docker/"},{"categories":["笔记"],"content":"commit镜像 对运行中的容器继续一些修改，比如: docker exec -it webserver bash通过bash进行一些系统的修改，我们可以使用docker commit命令对容器进行保存为镜像，以后启动该镜像就可以是修改后了。 但是，不要使用 docker commit 定制镜像，定制镜像应该使用 Dockerfile 来完成。慎用docker commit。 ","date":"2019-05-20","objectID":"/posts/docker/:1:4","tags":["docker"],"title":"docker","uri":"/posts/docker/"},{"categories":["笔记"],"content":"dockerfile定制镜像 如果我们可以把每一层修改、安装、构建、操作的命令都写入一个脚本，用这个脚本来构建、定制镜像，那么之前提及的无法重复的问题、镜像构建透明性的问题、体积的问题就都会解决。这个脚本就是 Dockerfile。 ","date":"2019-05-20","objectID":"/posts/docker/:2:0","tags":["docker"],"title":"docker","uri":"/posts/docker/"},{"categories":["笔记"],"content":"FROM 命令 FROM 指定基础镜像。 所谓定制镜像，那一定是以一个镜像为基础，在其上进行定制。 Docker 还存在一个特殊的镜像，名为 scratch。这个镜像是虚拟的概念，并不实际存在，它表示一个空白的镜像。比如go程序就可以直接在系统上运行。 ","date":"2019-05-20","objectID":"/posts/docker/:2:1","tags":["docker"],"title":"docker","uri":"/posts/docker/"},{"categories":["笔记"],"content":"RUN 执行命令 dockerfile中每一个指令都会建立一层，run也是，所以我们合起来，比如： FROM debian:stretch RUN buildDeps='gcc libc6-dev make wget' \\ \u0026\u0026 apt-get update \\ \u0026\u0026 apt-get install -y $buildDeps \\\u0026\u0026 wget -O redis.tar.gz \"http://download.redis.io/releases/redis-5.0.3.tar.gz\" \\\u0026\u0026 mkdir -p /usr/src/redis \\\u0026\u0026 tar -xzf redis.tar.gz -C /usr/src/redis --strip-components=1\\\u0026\u0026 make -C /usr/src/redis \\\u0026\u0026 make -C /usr/src/redis install \\\u0026\u0026 rm -rf /var/lib/apt/lists/*\\\u0026\u0026 rm redis.tar.gz \\\u0026\u0026 rm -r /usr/src/redis \\dockerfile支持shell类的行尾添加\\命令来换行，这样可读性更高。 ","date":"2019-05-20","objectID":"/posts/docker/:2:2","tags":["docker"],"title":"docker","uri":"/posts/docker/"},{"categories":["笔记"],"content":"ADD，COPY add和copy功能类似，都是把上下文中的文件拷贝到目的路径，add是更高级一点。 因此在 COPY 和 ADD 指令中选择的时候，可以遵循这样的原则，所有的文件复制均使用 COPY 指令，仅在需要自动解压缩的场合使用 ADD。 ","date":"2019-05-20","objectID":"/posts/docker/:2:3","tags":["docker"],"title":"docker","uri":"/posts/docker/"},{"categories":["笔记"],"content":"CMD容器启动命令 用来指定容器启动时运行的命令 CMD 指令的格式和 RUN 相似，也是两种格式： shell 格式：CMD \u003c命令\u003e exec 格式：CMD [“可执行文件”, “参数1”, “参数2”…] 参数列表格式：CMD [“参数1”, “参数2”…] 在指令格式上，一般推荐使用 exec 格式，这类格式在解析时会被解析为 JSON 数组，因此一定要使用双引号 “，而不要使用单引号。 shell格式其实执行的时候也是变为 CMD [ “sh”,\"-c”,“echo …\"] 注意：cmd命令如果有多个，只有最后的那个有效。另外，当docker run的时候后面指定了要运行的命令，那么cmd就被替代了。 docker容器没有后台服务的概念，因为他不是虚拟机或物理机。 ","date":"2019-05-20","objectID":"/posts/docker/:2:4","tags":["docker"],"title":"docker","uri":"/posts/docker/"},{"categories":["笔记"],"content":"ENTRYPOINT 入口点 ENTRYPOINT 的目的和 CMD 一样，都是在指定容器启动程序及参数。ENTRYPOINT 在运行时也可以替代，不过比 CMD 要略显繁琐，需要通过 docker run 的参数 –entrypoint 来指定。 当指定了 ENTRYPOINT 后，CMD 的含义就发生了改变，不再是直接的运行其命令，而是将 CMD 的内容作为参数传给 ENTRYPOINT 指令。 ","date":"2019-05-20","objectID":"/posts/docker/:2:5","tags":["docker"],"title":"docker","uri":"/posts/docker/"},{"categories":["笔记"],"content":"ENV 设置环境变量 格式有两种： ENV \u003ckey\u003e \u003cvalue\u003e ENV \u003ckey1\u003e=\u003cvalue1\u003e \u003ckey2\u003e=\u003cvalue2\u003e... 定义了环境变量，那么在后续的指令中，就可以使用这个环境变量 ","date":"2019-05-20","objectID":"/posts/docker/:3:0","tags":["docker"],"title":"docker","uri":"/posts/docker/"},{"categories":["笔记"],"content":"EXPOSE 声明端口 格式为 EXPOSE \u003c端口1\u003e [\u003c端口2\u003e…]。 EXPOSE 指令是声明运行时容器提供服务端口，这只是一个声明，在运行时并不会因为这个声明应用就会开启这个端口的服务。 要将 EXPOSE 和在运行时使用 -p \u003c宿主端口\u003e:\u003c容器端口\u003e 区分开来。-p，是映射宿主端口和容器端口，换句话说，就是将容器的对应端口服务公开给外界访问，而 EXPOSE 仅仅是声明容器打算使用什么端口而已，并不会自动在宿主进行端口映射。 ","date":"2019-05-20","objectID":"/posts/docker/:3:1","tags":["docker"],"title":"docker","uri":"/posts/docker/"},{"categories":["笔记"],"content":"WORKDIR 指定工作目录 使用 WORKDIR 指令可以来指定工作目录（或者称为当前目录），以后各层的当前目录就被改为指定的目录，如该目录不存在，WORKDIR 会帮你建立目录。 如果不用workdir，而是cd来切换，那么下一个行的命令又是新的一层镜像，就失效了。 ","date":"2019-05-20","objectID":"/posts/docker/:3:2","tags":["docker"],"title":"docker","uri":"/posts/docker/"},{"categories":["笔记"],"content":"USER 指定当前用户 USER 指令和 WORKDIR 相似，都是改变环境状态并影响以后的层。 ","date":"2019-05-20","objectID":"/posts/docker/:3:3","tags":["docker"],"title":"docker","uri":"/posts/docker/"},{"categories":["笔记"],"content":"VOLUME 定义匿名卷 容器的存储默认会随着容器消失而消失，不过我们可以定义数据卷，写到这里的数据是持久的，多个容器可以使用一个数据卷。 格式为： VOLUME [“\u003c路径1\u003e”, “\u003c路径2\u003e”…] VOLUME \u003c路径\u003e 命令举例： VOLUME /data 这样写入到/data的数据就能保留了。 注意：docker run -v /mydata:/data也可以指定数据卷，会覆盖这里的数据卷设置 ","date":"2019-05-20","objectID":"/posts/docker/:3:4","tags":["docker"],"title":"docker","uri":"/posts/docker/"},{"categories":["笔记"],"content":"构建镜像 docker build -t nginx:v3 . -t 镜像名字及版本 最后一个点代表当前目录是上下文目录，在dockerfile中的copy等命令中的相对目录。 容器 镜像（Image）和容器（Container）的关系，就像是面向对象程序设计中的 类 和 实例 一样，镜像是静态的定义，容器是镜像运行时的实体。容器可以被创建、启动、停止、删除、暂停等。 容器的实质是进程，但与直接在宿主执行的进程不同，容器进程运行于属于自己的独立的 命名空间。因此容器可以拥有自己的 root 文件系统、自己的网络配置、自己的进程空间，甚至自己的用户 ID 空间。容器内的进程是运行在一个隔离的环境里，使用起来，就好像是在一个独立于宿主的系统下操作一样。 容器有容器存储层，但是容器消亡时，容器存储层也随之消亡。 容器不应该向其存储层内写入任何数据，容器存储层要保持无状态化。所有的文件写入操作，都应该使用 数据卷（Volume）、或者绑定宿主目录，在这些位置的读写会跳过容器存储层，直接对宿主（或网络存储）发生读写，其性能和稳定性更高。 ","date":"2019-05-20","objectID":"/posts/docker/:3:5","tags":["docker"],"title":"docker","uri":"/posts/docker/"},{"categories":["笔记"],"content":"启动容器 启动容器有两种方式，一种是基于镜像新建一个容器并启动，另外一个是将在终止状态（stopped）的容器重新启动。 ","date":"2019-05-20","objectID":"/posts/docker/:4:0","tags":["docker"],"title":"docker","uri":"/posts/docker/"},{"categories":["笔记"],"content":"新建并启动 docker run -t -i ubuntu:18.04 /bin/bash -t 选项让Docker分配一个伪终端（pseudo-tty）并绑定到容器的标准输入上。 -i 则让容器的标准输入保持打开。 –name 给容器起名，后面可以根据名字对容器操作，比较方便。 -p 映射容器与宿主 ","date":"2019-05-20","objectID":"/posts/docker/:4:1","tags":["docker"],"title":"docker","uri":"/posts/docker/"},{"categories":["笔记"],"content":"启动已终止容器 可以利用 docker container start 命令，直接将一个已经终止的容器启动运行。 更多的时候，需要让 Docker 在后台运行而不是直接把执行命令的结果输出在当前宿主机下。此时，可以通过添加 -d 参数来实现。 注： 容器是否会长久运行，是和 docker run 指定的命令有关，和 -d 参数无关。 ","date":"2019-05-20","objectID":"/posts/docker/:4:2","tags":["docker"],"title":"docker","uri":"/posts/docker/"},{"categories":["笔记"],"content":"查看容器列表 docker container ls -a ","date":"2019-05-20","objectID":"/posts/docker/:5:0","tags":["docker"],"title":"docker","uri":"/posts/docker/"},{"categories":["笔记"],"content":"终止容器 可以使用 docker container stop 来终止一个运行中的容器。 ","date":"2019-05-20","objectID":"/posts/docker/:6:0","tags":["docker"],"title":"docker","uri":"/posts/docker/"},{"categories":["笔记"],"content":"进入容器 在使用 -d 参数时，容器启动后会进入后台。某些时候需要进入容器进行操作，可以使用docker exec命令。 docker exec -i 69d1 bash 当 -i -t 参数一起使用时，则可以看到我们熟悉的 Linux 命令提示符。 exit退出与容器的交互，容器不停止。 ","date":"2019-05-20","objectID":"/posts/docker/:7:0","tags":["docker"],"title":"docker","uri":"/posts/docker/"},{"categories":["笔记"],"content":"删除容器 可以使用 docker container rm 来删除一个处于终止状态的容器。 docker container prune删除所有终止状态的容器。 registry 镜像构建完成后，可以很容易的在当前宿主机上运行，但是，如果需要在其它服务器上使用这个镜像，我们就需要一个集中的存储、分发镜像的服务，Docker Registry 就是这样的服务。 一个 Docker Registry 中可以包含多个仓库（Repository）；每个仓库可以包含多个标签（Tag）；每个标签对应一个镜像。最常使用的 Registry 公开服务是官方的 Docker Hub。 ubuntu:18.04 来具体指定所需哪个版本的镜像。如果忽略了标签，比如 ubuntu，那将视为 ubuntu:latest。 数据卷 数据卷的作用就是对数据持久化， 不然数据随着容器的消失而消失。 ","date":"2019-05-20","objectID":"/posts/docker/:8:0","tags":["docker"],"title":"docker","uri":"/posts/docker/"},{"categories":["笔记"],"content":"新增数据卷 docker volume create my-vol ","date":"2019-05-20","objectID":"/posts/docker/:9:0","tags":["docker"],"title":"docker","uri":"/posts/docker/"},{"categories":["笔记"],"content":"查看数据卷列表 docker volume ls ","date":"2019-05-20","objectID":"/posts/docker/:10:0","tags":["docker"],"title":"docker","uri":"/posts/docker/"},{"categories":["笔记"],"content":"使用数据卷 docker run -d -P \\ --name web \\ # -v my-vol:/wepapp \\ --mount source=my-vol,target=/webapp \\ training/webapp \\ python app.py ","date":"2019-05-20","objectID":"/posts/docker/:11:0","tags":["docker"],"title":"docker","uri":"/posts/docker/"},{"categories":["笔记"],"content":"删除数据卷 docker volume rm my-vol 容器互联 ","date":"2019-05-20","objectID":"/posts/docker/:12:0","tags":["docker"],"title":"docker","uri":"/posts/docker/"},{"categories":["笔记"],"content":"link 可以使用 - -link 参数来使容器互联。 随着 Docker 网络的完善，强烈建议大家将容器加入自定义的 Docker 网络来连接多个容器，而不是使用 –link 参数。 运行容器的时候指定相同的网络，接口让他们连接。 已经不推荐使用。 ","date":"2019-05-20","objectID":"/posts/docker/:13:0","tags":["docker"],"title":"docker","uri":"/posts/docker/"},{"categories":["笔记"],"content":"network Docker 网络从覆盖范围可分为单个 host 上的容器网络和跨多个 host 的网络 Docker 安装时会自动在 host 上创建三个网络none、host、bridge，我们可用 docker network ls 命令查看。 ","date":"2019-05-20","objectID":"/posts/docker/:14:0","tags":["docker"],"title":"docker","uri":"/posts/docker/"},{"categories":["笔记"],"content":"none 网络 故名思议，none 网络就是什么都没有的网络。挂在这个网络下的容器除了 lo，没有其他任何网卡。容器创建时，可以通过 –network=none 指定使用 none 网络。 比如某个容器的唯一用途是生成随机密码，就可以放到 none 网络中避免密码被窃取。 ","date":"2019-05-20","objectID":"/posts/docker/:14:1","tags":["docker"],"title":"docker","uri":"/posts/docker/"},{"categories":["笔记"],"content":"host 网络 连接到 host 网络的容器共享 Docker host 的网络栈，容器的网络配置与 host 完全一样。 直接使用 Docker host 的网络最大的好处就是性能，如果容器对网络传输效率有较高要求，则可以选择 host 网络。 Docker host 上已经使用的端口就不能再用了。 ","date":"2019-05-20","objectID":"/posts/docker/:14:2","tags":["docker"],"title":"docker","uri":"/posts/docker/"},{"categories":["笔记"],"content":"bridge 网络 Docker 安装时会创建一个 命名为 docker0 的 linux bridge。如果不指定–network，创建的容器默认都会挂到 docker0 上。 bridge 网络配置的 subnet 就是 172.17.0.0/16，并且网关是 172.17.0.1。这个网关在哪儿呢？大概你已经猜出来了，就是 docker0。 ","date":"2019-05-20","objectID":"/posts/docker/:14:3","tags":["docker"],"title":"docker","uri":"/posts/docker/"},{"categories":["笔记"],"content":"新建网络 除了 none, host, bridge 这三个自动创建的网络，用户也可以根据业务需要创建 user-defined 网络。 Docker 提供三种 user-defined 网络驱动：bridge, overlay 和 macvlan。overlay 和 macvlan 用于创建跨主机的网络， docker network create -d bridge my-net 我们还可以指定ip网段。 只需在创建网段时指定 –subnet 和 –gateway docker network create -d bridge my-net –subnet 172.22.16.0/24 –gateway 172.22.16.1 my-net2 到目前为止，容器的 IP 都是 docker 自动从 subnet 中分配，我们能否指定一个静态 IP 呢？ 通过–ip指定。 docker network create -d bridge my-net –ip 172.22.16.8 注意：只有使用 --subnet 创建的网络才能指定静态 IP ","date":"2019-05-20","objectID":"/posts/docker/:15:0","tags":["docker"],"title":"docker","uri":"/posts/docker/"},{"categories":["笔记"],"content":"查看网络 docker network ls docker network inspect xx //查看xx网络信息 ","date":"2019-05-20","objectID":"/posts/docker/:16:0","tags":["docker"],"title":"docker","uri":"/posts/docker/"},{"categories":["笔记"],"content":"连接容器 同一网络中的容器、网关之间都是可以通信的。不同网络下容器是不能直接通信的。 ","date":"2019-05-20","objectID":"/posts/docker/:17:0","tags":["docker"],"title":"docker","uri":"/posts/docker/"},{"categories":["笔记"],"content":"ip通信 容器就可以通过 IP 交互了。具体做法是在容器创建时通过 –network 指定相应的网络，或者通过 docker network connect 将现有容器加入到指定网络。 docker run -it –rm –name busybox1 –network my-net busybox sh docker run -it –rm –name busybox2 –network my-net busybox sh ","date":"2019-05-20","objectID":"/posts/docker/:17:1","tags":["docker"],"title":"docker","uri":"/posts/docker/"},{"categories":["笔记"],"content":"Docker DNS Server通信 通过 IP 访问容器虽然满足了通信的需求，但还是不够灵活。因为我们在部署应用之前可能无法确定 IP，部署之后再指定要访问的 IP 会比较麻烦。对于这个问题，可以通过 docker 自带的 DNS 服务解决。 从 Docker 1.10 版本开始，docker daemon 实现了一个内嵌的 DNS server，使容器可以直接通过“容器名”通信。 给容器起名方法很简单，只要在启动时用 –name 为容器命名就可以了。 使用 docker DNS 有个限制：只能在 user-defined 网络中使用。也就是说，默认的 bridge 网络是无法使用 DNS 的。 ","date":"2019-05-20","objectID":"/posts/docker/:17:2","tags":["docker"],"title":"docker","uri":"/posts/docker/"},{"categories":["笔记"],"content":"joined 容器通信 joined 容器是另一种实现容器间通信的方式。 joined 容器非常特别，它可以使两个或多个容器共享一个网络栈，共享网卡和配置信息，joined 容器之间可以通过 127.0.0.1 直接通信。 方法也简单： 先创建一个 httpd 容器，名字为 web1。 docker run -d -it –name=web1 httpd 然后创建 busybox 容器并通过 –network=container:web1 指定 jointed 容器为 web1。这样busybox可以使用127.0.0.1访问web1 ","date":"2019-05-20","objectID":"/posts/docker/:17:3","tags":["docker"],"title":"docker","uri":"/posts/docker/"},{"categories":["笔记"],"content":"git","date":"2019-05-06","objectID":"/posts/git/","tags":["git"],"title":"git","uri":"/posts/git/"},{"categories":["笔记"],"content":"快速入门 ","date":"2019-05-06","objectID":"/posts/git/:0:0","tags":["git"],"title":"git","uri":"/posts/git/"},{"categories":["笔记"],"content":"克隆仓库 git clone http://git.oschina.net/yiibai/git-start.git //如果想在克隆远程仓库的时候，自定义本地仓库的名字，可以使用如下命令： git clone http://git.oschina.net/yiibai/git-start.git mygit-start ","date":"2019-05-06","objectID":"/posts/git/:1:0","tags":["git"],"title":"git","uri":"/posts/git/"},{"categories":["笔记"],"content":"在现有目录中初始化仓库 如果不克隆现有的仓库，而是打算使用 Git 来对现有的项目进行管理。假设有一个项目的目录是：D:\\worksp\\git_sample，只需要进入该项目的目录并输入： git init ","date":"2019-05-06","objectID":"/posts/git/:2:0","tags":["git"],"title":"git","uri":"/posts/git/"},{"categories":["笔记"],"content":"更新提交到仓库 工作目录下的每一个文件都不外乎这两种状态：已跟踪或未跟踪。 已跟踪的文件是指那些被纳入了版本控制的文件，在上一次快照中有它们的记录，在工作一段时间后，它们的状态可能处于未修改，已修改或已放入暂存区。 工作目录中除已跟踪文件以外的所有其它文件都属于未跟踪文件，它们既不存在于上次快照的记录中，也没有放入暂存区。 初次克隆某个仓库的时候，工作目录中的所有文件都属于已跟踪文件，并处于未修改状态 ","date":"2019-05-06","objectID":"/posts/git/:3:0","tags":["git"],"title":"git","uri":"/posts/git/"},{"categories":["笔记"],"content":"检查当前文件状态 git status git status -s 命令或 git status –short 命令，将得到一种更为紧凑的格式输出。 ","date":"2019-05-06","objectID":"/posts/git/:4:0","tags":["git"],"title":"git","uri":"/posts/git/"},{"categories":["笔记"],"content":"跟踪新文件 git add mytext.txt git add . ","date":"2019-05-06","objectID":"/posts/git/:5:0","tags":["git"],"title":"git","uri":"/posts/git/"},{"categories":["笔记"],"content":"忽略文件 文件 .gitignore 的格式规范如下： 所有空行或者以 ＃ 开头的行都会被 Git 忽略。 可以使用标准的 glob 模式匹配。 匹配模式可以以(/)开头防止递归。 匹配模式可以以(/)结尾指定目录。 要忽略指定模式以外的文件或目录，可以在模式前加上惊叹号(!)取反 所谓的 glob 模式是指 shell 所使用的简化了的正则表达式。 星号()匹配零个或多个任意字符；[abc]匹配任何一个列在方括号中的字符(这个例子要么匹配一个字符 a，要么匹配一个字符 b，要么匹配一个字符 c)；问号(?)只匹配一个任意字符；如果在方括号中使用短划线分隔两个字符，表示所有在这两个字符范围内的都可以匹配(比如 [0-9] 表示匹配所有 0 到 9 的数字)。 使用两个星号() 表示匹配任意中间目录，比如a/**/z 可以匹配 a/z, a/b/z 或 a/b/c/z等。 ","date":"2019-05-06","objectID":"/posts/git/:6:0","tags":["git"],"title":"git","uri":"/posts/git/"},{"categories":["笔记"],"content":"查看修改内容 git diff ","date":"2019-05-06","objectID":"/posts/git/:7:0","tags":["git"],"title":"git","uri":"/posts/git/"},{"categories":["笔记"],"content":"提交 把暂存区的内容提交前先确定文件是否都添加到暂存区内了 Git commit -m \"提交信息\" Git commit -a 参数可以把变化的文件先添加到暂存区并提交，换句话说Git 就会自动把所有已经跟踪过的文件暂存起来一并提交，从而跳过 git add，新增的为追踪的不行。 ","date":"2019-05-06","objectID":"/posts/git/:8:0","tags":["git"],"title":"git","uri":"/posts/git/"},{"categories":["笔记"],"content":"移除文件 git rm会删除本地文件，推送后也会把仓库里的删除。 如果文件已经添加到缓存区了，那么git rm命令是删除不了的 可以使用下面两个命令： git rm -f 文件名 // 缓存区和工作目录的都会删除 git rm --cached 文件名 // 只删除缓存区，工作目录的保留 ","date":"2019-05-06","objectID":"/posts/git/:9:0","tags":["git"],"title":"git","uri":"/posts/git/"},{"categories":["笔记"],"content":"查看提交历史 git log 会按提交时间列出所有的更新，最近的更新排在最上面。 Git log -p // 用来显示每次提交的内容差异 另外一个常用的选项是 –pretty。 这个选项可以指定使用不同于默认格式的方式展示提交历史。 这个选项有一些内建的子选项供你使用。 比如用 oneline 将每个提交放在一行显示，查看的提交数很大时非常有用。 另外还有 short，full 和 fuller 可以用，展示的信息或多或少有些不同 git log --pretty=oneline ","date":"2019-05-06","objectID":"/posts/git/:10:0","tags":["git"],"title":"git","uri":"/posts/git/"},{"categories":["笔记"],"content":"远程仓库 可以有好几个远程仓库，通常有些仓库对你只读，有些则可以读写。 与他人协作涉及管理远程仓库以及根据需要推送或拉取数据。 ","date":"2019-05-06","objectID":"/posts/git/:11:0","tags":["git"],"title":"git","uri":"/posts/git/"},{"categories":["笔记"],"content":"查看远程仓库 git remote 命令。 它会列出你指定的每一个远程服务器的简写。 如果已经克隆了自己的仓库，那么至少应该能看到 origin - 这是 Git 给你克隆的仓库服务器的默认名字。 也可以指定选项 -v，会显示需要读写远程仓库使用的 Git 保存的简写与其对应的 URL。 ","date":"2019-05-06","objectID":"/posts/git/:12:0","tags":["git"],"title":"git","uri":"/posts/git/"},{"categories":["笔记"],"content":"添加远程仓库 运行 git remote add 添加一个新的远程 Git 仓库，同时指定一个可以轻松引用的简写 ","date":"2019-05-06","objectID":"/posts/git/:13:0","tags":["git"],"title":"git","uri":"/posts/git/"},{"categories":["笔记"],"content":"从远程仓库中抓取与拉取 就如刚才所见，从远程仓库中获得数据，可以执行: git fetch [remote-name] 这个命令会访问远程仓库，从中拉取所有还没有的数据。执行完成后，将会拥有那个远程仓库中所有分支的引用，可以随时合并或查看。 必须注意 git fetch 命令会将数据拉取到本地仓库 - 它并不会自动合并或修改当前的工作 如果你有一个分支设置为跟踪一个远程分支，可以使用 git pull 命令来自动的抓取然后合并远程分支到当前分支。 这对你来说可能是一个更简单或更舒服的工作流程；默认情况下，git clone 命令会自动设置本地 master 分支跟踪克隆的远程仓库的 master。 ","date":"2019-05-06","objectID":"/posts/git/:14:0","tags":["git"],"title":"git","uri":"/posts/git/"},{"categories":["笔记"],"content":"推送到远程仓库 git push [remote-name] [branch-name]。 当你想要将 master 分支推送到 origin 服务器时(再次说明，克隆时通常会自动帮你设置好那两个名字) git push origin master 只有当你有所克隆服务器的写入权限，并且之前没有人推送过时，这条命令才能生效。 当你和其他人在同一时间克隆，他们先推送到上游然后你再推送到上游，你的推送就会毫无疑问地被拒绝。 你必须先将他们的工作拉取下来并将其合并进你的工作后才能推送。 ","date":"2019-05-06","objectID":"/posts/git/:15:0","tags":["git"],"title":"git","uri":"/posts/git/"},{"categories":["笔记"],"content":"查看远程仓库 如果想要查看某一个远程仓库的更多信息，可以使用 git remote show [remote-name] 命令 git remote show origin * remote origin Fetch URL: http://git.oschina.net/yiibai/git-start.git Push URL: http://git.oschina.net/yiibai/git-start.git HEAD branch: master Remote branch: master tracked Local branch configured for 'git pull': master merges with remote master Local ref configured for 'git push': master pushes to master (fast-forwardable) 它同样会列出远程仓库的 URL 与跟踪分支的信息。 这些信息非常有用，它告诉你正处于 master 分支，并且如果运行 git pull，就会抓取所有的远程引用，然后将远程 master 分支合并到本地 master 分支。 它也会列出拉取到的所有远程引用。 它也同样地列出了哪些远程分支不在你的本地，哪些远程分支已经从服务器上移除了，还有当你执行 git pull 时哪些分支会自动合并。 ","date":"2019-05-06","objectID":"/posts/git/:16:0","tags":["git"],"title":"git","uri":"/posts/git/"},{"categories":["笔记"],"content":"远程仓库的移除与重命名 如果想要重命名引用的名字可以运行 git remote rename 去修改一个远程仓库的简写名。 例如，想要将 gs 重命名为 newgs，可以用 git remote rename 这样做： git remote rename gs newgs 值得注意的是这同样也会修改你的远程分支名字。 那些过去引用 gs/master 的现在会引用 newgs/master。 删除远程库: git remote rm newgs 命令详解 ","date":"2019-05-06","objectID":"/posts/git/:17:0","tags":["git"],"title":"git","uri":"/posts/git/"},{"categories":["笔记"],"content":"git add git add 命令。 这是个多功能命令：可以用它开始跟踪新文件，或者把已跟踪的文件放到暂存区，还能用于合并时把有冲突的文件标记为已解决状态等。 将这个命令理解为“添加内容到下一次提交中”而不是“将一个文件添加到项目中”要更加合适。 ","date":"2019-05-06","objectID":"/posts/git/:18:0","tags":["git"],"title":"git","uri":"/posts/git/"},{"categories":["笔记"],"content":"查看更改记录 git log //查看更改记录 git show 版本号 // 查看某次提交详细信息 ","date":"2019-05-06","objectID":"/posts/git/:19:0","tags":["git"],"title":"git","uri":"/posts/git/"},{"categories":["笔记"],"content":"git rm 删除了文件，也属于文件变化，可以使用： git add 文件 //或者git rm 文件 git commit 来提交变化。 如果想撤销删除，可以参考上面的撤销工作区、暂存区内容。 git rm –cached newfile_name 就可以将这个文件从暂存区移除掉，但是在工作区里没有消失，如果不加 –cached 参数，就会从工作区和版本库暂存区同时删除，相当于执行了 rm newfile_name 和 git add new_file 两条命令。 ","date":"2019-05-06","objectID":"/posts/git/:20:0","tags":["git"],"title":"git","uri":"/posts/git/"},{"categories":["笔记"],"content":"git pull 执行git pull命令以将其本地存储库与远程存储库同步。 拉取远程代码时别人已经提的可能会与你的冲突 打开冲突文件，eg: a = 10 b = 20 \u003c\u003c\u003c\u003c\u003c\u003c\u003c HEAD c = a + b print(\"The value of c is \", c) def mul(a, b): return (a * b) ======= def sum(a, b): return (a+b) c = sum(a, b) print(\"The value of c is \", c) \u003e\u003e\u003e\u003e\u003e\u003e\u003e 01c54624879782e4657dd6c166ce8818f19e8251 并发现其它开发人员的提交的详细信息，提交ID为：01c54624879782e4657dd6c166ce8818f19e8251 解决完冲突，把冲突文件重新add 一次，再重新push一次， ","date":"2019-05-06","objectID":"/posts/git/:21:0","tags":["git"],"title":"git","uri":"/posts/git/"},{"categories":["笔记"],"content":"暂存stash 代码写到一半需要切换分支，但是写到一半提交上去又不合适，可以先把改变暂存起来，等后面再还原。暂存起来的分支可以在其他分支还原。 git stash // 把当前分支的改变暂存 git stash list // 命令来查看已存在更改的列表。 git stash pop // 命令即可从堆栈中删除更改并将其放置在当前工作目录中 git stash show [\u003cstash\u003e] // 查看某次暂存内容 git stash drop [-q|--quiet] [\u003cstash\u003e] // 删除暂存 git stash apply [-q|--quiet] [\u003cstash\u003e] // 还原暂存 git stash clear git stash save [\u003cmessage\u003e] // 带信息的暂存 ","date":"2019-05-06","objectID":"/posts/git/:22:0","tags":["git"],"title":"git","uri":"/posts/git/"},{"categories":["笔记"],"content":"checkout Git checkout用于切换分支或恢复文件。 Git checkout -b dev origin/dev 生成一个新的dev分支，远程分支对应origin/dev Git branch -D 分支名 删除本地分支（不管有没有合并） Git checkout 文件 // 恢复工作区修改 Git checkout head 文件 // 恢复已经提交到暂存区的修改,工作区也不会保留修改 ","date":"2019-05-06","objectID":"/posts/git/:23:0","tags":["git"],"title":"git","uri":"/posts/git/"},{"categories":["笔记"],"content":"git commit –amend 修改当前最新的提交，可以修改提交信息、文件。注意，只能改当前最新的那次，如果想改以前的，那就麻烦一点，需要先回到那个版本才行。 eg: Git commit –amend -m “这才是正确的，刚才那次是错误的” Git commit -a -m \"xx\" // 添加到暂存区并提交文件 标准的git commit分为3个部分，header、body、footer,后两个一般不用。 heder结构为 type(scope):subject Type一般有几种类型： feat:新功能 fix:修复bug style:格式 refactor:代码重构 chore:项目构建 比如fix(pay):修复了xxbug ","date":"2019-05-06","objectID":"/posts/git/:24:0","tags":["git"],"title":"git","uri":"/posts/git/"},{"categories":["笔记"],"content":"git merge Git merge dev // 从dev合并到当前分支 合并后可能会有冲突，如果想放弃合并回到刚才，使用git merger –abort命令。 ","date":"2019-05-06","objectID":"/posts/git/:25:0","tags":["git"],"title":"git","uri":"/posts/git/"},{"categories":["笔记"],"content":"git tag 使用git tag命令来标记当前HEAD指针。在创建标签时需要提供-a选项的标签名称，并提供带-m选项的标签消息。 Git tag -a 'v1.0.0' -m \"v1.0.0 tag\" 如果要标记特定提交，则使用相应的COMMIT ID而不是HEAD指针。使用以下命令将标签推送到远程存储库。 Git tag -a 'v1.0.0' -m \"v1.0.0 tag\" 1852b48f8d1d86bce1694f99039833e3eaa55bc9 Git push origin tag v1.0.0 // 推送tag至远程 Git show v1.0.0 // 查看tag详细信息 Git tag -d v1.0.0 // 删除标签 ","date":"2019-05-06","objectID":"/posts/git/:26:0","tags":["git"],"title":"git","uri":"/posts/git/"},{"categories":["笔记"],"content":"git config git的配置分为全局和各仓库的单独配置。 查看配置 git config –list 如果当前目录是git仓库就能看到该仓库的单独配置。 git config –global user.name配置全局用户名 git config –global user.email配置全局邮箱 git config –local user.name在仓库地址路径配置该仓库的特定用户名，如果不指定–local默认也是该效果，配置邮箱等信息同理。 如果不对各仓库配置自己的邮箱，可能会造成邮箱不必要的暴露。 git各区 ","date":"2019-05-06","objectID":"/posts/git/:27:0","tags":["git"],"title":"git","uri":"/posts/git/"},{"categories":["笔记"],"content":"git本地仓库 各区之间数据状态转换流程 ","date":"2019-05-06","objectID":"/posts/git/:28:0","tags":["git"],"title":"git","uri":"/posts/git/"},{"categories":["笔记"],"content":"工作区和暂存区 我们先来理解下Git 工作区、暂存区和版本库概念 工作区：就是你在电脑里能看到的目录,工作区就是项目所在的目录 暂存区：英文叫stage, 或index。一般存放在\"git目录\"下的index文件（.git/index）中，所以我们把暂存区有时也叫作索引（index）。文件修改后，用add命令放到暂存区，然后在通过commit命令放到分支上。每次的修改只有先add到暂存区才能被提交。 版本库：工作区有一个隐藏目录.git，这个不算工作区，而是Git的版本库。 总结起来一个文件的状态通常可以分为： 不受版本控制的 untracked 状态 受版本控制并且已修改的 modified 状态 受版本控制已修改并提交到暂存区的 staged 状态 从暂存区已经提交到本地仓库的 committed 状态 提交到本地仓库未修改或者从远程仓库克隆下来的 unmodified 状态 Git回退命令 ","date":"2019-05-06","objectID":"/posts/git/:29:0","tags":["git"],"title":"git","uri":"/posts/git/"},{"categories":["笔记"],"content":"git checkout 这个命令又出现了，上次是总结 git branch 分支操作的时候，git checkout 可以用来新建或者切换分支，这次总结回退版本的命令，git checkout 也可以用来回退文件版本，很神奇吧。 其实这个命令的作用就是它单词的本义——检出，他的常用操作也取自这个意思，比如 git checkout branch_name 切换分支操作，实际上就是把指定分支在仓库中对应的所有文件检出来覆盖当前工作区，最终表现就是切换了分支。 而针对于文件的检出可以使用 git checkout – file_name，当不指定 commit id 就是将暂存区的内容恢复到工作区，也就可以达到回退本地修改的作用。 不过，这个身兼数职的 git checkout 命令现在可以轻松一些了，从 Git 2.23 版本开始引入了两个新的命令： git switch 用来切换分支，git restore用来还原工作区的文件 ","date":"2019-05-06","objectID":"/posts/git/:30:0","tags":["git"],"title":"git","uri":"/posts/git/"},{"categories":["笔记"],"content":"git restore 这个命令是 Git 2.23 版本之后新加的，用来分担之前 git checkout 命令的功能, git restore –staged 从暂存区撤销，回到工作区，就是git add 的逆操作。 git restore 从工作区撤销，就是放弃更改 ","date":"2019-05-06","objectID":"/posts/git/:31:0","tags":["git"],"title":"git","uri":"/posts/git/"},{"categories":["笔记"],"content":"git reset reset 重新设置的意思，其实就是用来设置分支的头部指向，当进行了一系列的提交之后，忽然发现最近的几次提交有问题，想从提交记录中删除，这是就会用到 git reset 命令，这个命令后面跟 commit id，表示当前分支回退到这个 commit id 对应的状态，之后的日志记录被删除，工作区中的文件状态根据参数的不同会恢复到不同的状态。 –soft: 被回退的那些版本的修改会被放在暂存区，可以再次提交。 –mixed: 默认选项，被回退的那些版本的修改会放在工作目录，可以先加到暂存区，然后再提交。 –hard: 被回退的那些版本的修改会直接舍弃，好像它们没有来过一样。 这样来看，git reset 命令好像是用来回退版本的，但是如果使用 git rest HEAD file_name 命令就可以将一个文件回退到 HEAD 指向版本所对应的状态，其实就是当前版本库中的状态，也就相当于还原了本地的修改。 git每次commit都是一个快照，方便以后查找或恢复。 在Git中，用HEAD表示当前版本，上一个版本就是HEAD^，上上一个版本就是HEAD^^，当然往上100个版本写100个^比较容易数不过来，所以写成HEAD~100。 git reset –hard HEAD^ 这样就回到了上个快照，在查看git log发现刚提交的那条记录不在了。当然如果你还能记住刚才那个最新的快照id还是可以回去的。 如果记不住了就再也回不去了吗？ 当然不是，Git提供了一个命令git reflog用来记录你的每一次命令。 然后 git reset –hard 快照id 就可以再回来啦。 git revert 也可以回退一个已经提交到仓库的版本 git revert 版本号 即可。会生成一个提交记录，提交记录明确记录着revert 的是哪个版本，原来的版本记录都是在的。如果对revert再revert一次，那就等于负负得正，回到原来的版本中。 注意，revert针对最后一次的commit比较方便，如果想revert上上一次或更前面的，就会冲突。 如果修改已经push到远程了，需要本地回退后，在push一次 merge和rebase git rebase和git merge区别： git rebase后变成了一个线，且不会生成新的合并节点。 git merge后两个分支记录依然各自存在，会生成新的合并节点。 git merge冲突后需要手动add并commit，git rebase自动生成一次提交。 当完成一个特性的开发并将其合并到 master 分支时，我们有两种方式：git merge 和 git rebase。 看merge图解： rebase图解： 我们可以看到，merge后会产生一个多余的commit,但是每个提交记录先后顺序不变。 rebase没有生产新的commit，把两个分支合成一个线性的记录，可读性更强，但是提交记录的先后顺序变了。 总结： 当需要保留详细的合并信息的时候建议使用git merge，特别是需要将分支合并进入master分支时；当发现自己修改某个功能时，频繁进行了git commit提交时，发现其实过多的提交信息没有必要时，可以尝试git rebase。 ","date":"2019-05-06","objectID":"/posts/git/:32:0","tags":["git"],"title":"git","uri":"/posts/git/"},{"categories":["笔记"],"content":"树结构","date":"2019-04-12","objectID":"/posts/tree/","tags":["数据结构"],"title":"树结构","uri":"/posts/tree/"},{"categories":["笔记"],"content":"平衡二叉树 平衡二叉树是基于二分法的策略提高数据的查找速度的二叉树的数据结构。 特点 非叶子节点只能允许最多两个子节点存在。 每一个非叶子节点数据分布规则为左边的子节点小当前节点的值，右边的子节点大于当前节点的值 树的左右两边的层级数相差不会大于1; 没有值相等重复的节点; 优势 层级越深，查找速度越慢。 为了保证树的结构左右两端数据大致平衡降低二叉树的查询难度一般会采用一种算法机制实现节点数据结构的平衡，实现了这种算法的有比如AVL、Treap、红黑树，使用平衡二叉树能保证数据的左右两边的节点层级相差不会大于1，通过这样避免树形结构由于删除增加变成线性链表影响查询效率，保证数据平衡的情况下查找数据的速度近于二分法查找。 ","date":"2019-04-12","objectID":"/posts/tree/:0:1","tags":["数据结构"],"title":"树结构","uri":"/posts/tree/"},{"categories":["笔记"],"content":"红黑树 红黑树是一颗二叉树。它有自己的特定的规则来保证树的深度比普通的二叉排序树小，但它不是完全的平衡二叉树。 特点 每个结点或者为黑色或者为红色。 根结点为黑色。 每个叶结点(实际上就是NULL指针)都是黑色的。 如果一个结点是红色的，那么它的两个子节点都是黑色的（也就是说，不能有两个相邻的红色结点）。 对于每个结点，从该结点到其所有子孙叶结点的路径中所包含的黑色结点数量必须相同。 优势 红黑树通过上面这种限制来保证它大致是平衡的——因为红黑树的高度不会无限增高。 红黑树是一个更高效的检索二叉树，因此常常用来实现关联数组。典型地，JDK 提供的集合类 TreeMap 本身就是一个红黑树的实现。 红黑树左旋 红黑树插入或删除后，以不满足条件，要进行旋转并着色。 左旋： 红黑树右旋 ","date":"2019-04-12","objectID":"/posts/tree/:0:2","tags":["数据结构"],"title":"树结构","uri":"/posts/tree/"},{"categories":["笔记"],"content":"b树 B树和平衡二叉树稍有不同的是B树属于多叉树又名平衡多路查找树（查找路径不只两个）。 特点 所有节点关键字是按递增次序排列，并遵循左小右大原则； 非叶节点的子节点数\u003e1，且\u003c=M ，且M\u003e=2。M就是叉路树。 枝节点的关键字数量大于等于ceil(m/2)-1个且小于等于M-1个（注：ceil()是个朝正无穷方向取整的函数 如ceil(1.1)结果为2); 所有叶子节点均在同一层、叶子节点除了包含了关键字和关键字记录的指针外也有指向其子节点的指针只不过其指针地址都为null对应下图最后一层节点的空格子; 查找流程 如上图我要从上图中找到E字母，查找流程如下 获取根节点的关键字进行比较，当前根节点关键字为M，E\u003cM（26个字母顺序），所以往找到指向左边的子节点（二分法规则，左小右大，左边放小于当前节点值的子节点、右边放大于当前节点值的子节点）； 拿到关键字D和G，D\u003cE\u003cG 所以直接找到D和G中间的节点； 拿到E和F，因为E=E 所以直接返回关键字和指针信息（如果树结构里面没有包含所要查找的节点则返回null）； 优势 B树相对于平衡二叉树的不同是，每个节点包含的关键字增多了，把树的节点关键字增多后树的层级比原来的二叉树少了，减少数据查找的次数和复杂度。 ","date":"2019-04-12","objectID":"/posts/tree/:0:3","tags":["数据结构"],"title":"树结构","uri":"/posts/tree/"},{"categories":["笔记"],"content":"b+树 B+树是B树的一个升级版，相对于B树来说B+树更充分的利用了节点的空间，让查询速度更加稳定，其速度完全接近于二分法查找。 特点 B+跟B树不同B+树的非叶子节点不保存关键字记录的指针，只进行数据索引，这样使得B+树每个非叶子节点所能保存的关键字大大增加； B+树叶子节点保存了父节点的所有关键字记录的指针，所有数据地址必须要到叶子节点才能获取到。所以每次数据查询的次数都一样； B+树叶子节点的关键字从小到大有序排列，左边结尾数据都会保存右边节点开始数据的指针。 非叶子节点的子节点数=关键字数 优势 B+树的层级更少：相较于B树B+每个非叶子节点存储的关键字数更多，树的层级更少所以查询数据更快； B+树查询速度更稳定：B+所有关键字数据地址都存在叶子节点上，所以每次查找的次数都相同所以查询速度要比B树更稳定; B+树天然具备排序功能：B+树所有的叶子节点数据构成了一个有序链表，在查询大小区间的数据时候更方便，数据紧密性很高，缓存的命中率也会比B树高。 B+树全节点遍历更快：B+树遍历整棵树只需要遍历所有的叶子节点即可，，而不需要像B树一样需要对每一层进行遍历，这有利于数据库做全表扫描 ","date":"2019-04-12","objectID":"/posts/tree/:0:4","tags":["数据结构"],"title":"树结构","uri":"/posts/tree/"},{"categories":["笔记"],"content":"elasticsearch","date":"2019-03-27","objectID":"/posts/elasticsearch/","tags":["golang"],"title":"elasticsearch","uri":"/posts/elasticsearch/"},{"categories":["笔记"],"content":"安装启动 elasticsearch需要依赖jdk elastic默认是9200端口 ctrl+c elastic就会停止 默认情况下，Elastic 只允许本机访问，如果需要远程访问，可以修改 Elastic 安装目录的config/elasticsearch.yml文件，去掉network.host的注释，将它的值改成0.0.0.0，然后重新启动 Elastic。 ","date":"2019-03-27","objectID":"/posts/elasticsearch/:0:1","tags":["golang"],"title":"elasticsearch","uri":"/posts/elasticsearch/"},{"categories":["笔记"],"content":"基本概念 elastic可以看作是分布式数据库，每台elastic实例就是一个node，一组node构成一个集群cluster。 index会索引所有字段，elastic顶层单位就是index。类似于mysql的数据库。 document 就是index里面的单条记录。存储格式为json。类似于mysql的记录。 type是单条记录的分组，一个index里面可以有多个type，但是推荐一index里面一个type。Elastic 6.x 版只允许每个 Index 包含一个 Type，7.x 版将会彻底移除 Type。类似于msyql中的表。 ","date":"2019-03-27","objectID":"/posts/elasticsearch/:0:2","tags":["golang"],"title":"elasticsearch","uri":"/posts/elasticsearch/"},{"categories":["笔记"],"content":"库表操作 查看所有index curl -X GET ‘http://localhost:9200/_cat/indices?v’ 删除index curl -X DELETE ‘localhost:9200/weather’ 查看映射器（表结构） curl ‘localhost:9200/_mapping?pretty=true’ 新增index并指定表数据结构 $curl -X PUT 'localhost:9200/accounts' -d ' { \"mappings\": { \"person\": { \"properties\": { \"user\": { \"type\": \"text\", \"analyzer\": \"ik_max_word\", \"search_analyzer\": \"ik_max_word\" }, \"title\": { \"type\": \"text\", \"analyzer\": \"ik_max_word\", \"search_analyzer\": \"ik_max_word\" }, \"desc\": { \"type\": \"text\", \"analyzer\": \"ik_max_word\", \"search_analyzer\": \"ik_max_word\" } } } } }' 这里有三个字段，而且类型都是文本（text），所以需要指定中文分词器，不能使用默认的英文分词器。分词器要安装插件。 ","date":"2019-03-27","objectID":"/posts/elasticsearch/:0:3","tags":["golang"],"title":"elasticsearch","uri":"/posts/elasticsearch/"},{"categories":["笔记"],"content":"数据curd 新增 $curl -X PUT 'localhost:9200/accounts/person/1' -d ' { \"user\": \"张三\", \"title\": \"工程师\", \"desc\": \"数据库管理\" }' 向accounts索引person类别（表）中插入数据，指定记录的id为1。如果不指定，那么会自动随机生成一个字符串作为id，新增记录的时候，也可以不指定 Id，这时要改成 POST 请求。 查看 curl ‘localhost:9200/accounts/person/1?pretty=true’ { \"_index\" : \"accounts\", \"_type\" : \"person\", \"_id\" : \"1\", \"_version\" : 1, \"found\" : true, \"_source\" : { \"user\" : \"张三\", \"title\" : \"工程师\", \"desc\" : \"数据库管理\" } } 我们还可以对字段完全相等查询 GET /accounts/person/_search?q=user:Smith 删除 curl -X DELETE ‘localhost:9200/accounts/person/1’ 更新 更新记录就是使用 PUT 请求，重新发送一次数据。 $curl -X PUT 'localhost:9200/accounts/person/1' -d ' { \"user\" : \"张三\", \"title\" : \"工程师\", \"desc\" : \"数据库管理，软件开发\" }' { \"_index\":\"accounts\", \"_type\":\"person\", \"_id\":\"1\", \"_version\":2, \"result\":\"updated\", \"_shards\":{\"total\":2,\"successful\":1,\"failed\":0}, \"created\":false } ","date":"2019-03-27","objectID":"/posts/elasticsearch/:0:4","tags":["golang"],"title":"elasticsearch","uri":"/posts/elasticsearch/"},{"categories":["笔记"],"content":"复杂查询 查询所有记录。 使用 GET 方法，直接请求/Index/Type/_search，就会返回所有记录。 $curl 'localhost:9200/accounts/person/_search' { \"took\":2, \"timed_out\":false, \"_shards\":{\"total\":5,\"successful\":5,\"failed\":0}, \"hits\":{ \"total\":2, \"max_score\":1.0, \"hits\":[{ \"_index\":\"accounts\", \"_type\":\"person\", \"_id\":\"AV3qGfrC6jMbsbXb6k1p\", \"_score\":1.0, \"_source\": { \"user\": \"李四\", \"title\": \"工程师\", \"desc\": \"系统管理\" } }, { \"_index\":\"accounts\", \"_type\":\"person\", \"_id\":\"1\", \"_score\":1.0, \"_source\": { \"user\" : \"张三\", \"title\" : \"工程师\", \"desc\" : \"数据库管理，软件开发\" } } ]} } 上面代码中，返回结果的 took字段表示该操作的耗时（单位为毫秒），timed_out字段表示是否超时，hits字段表示命中的记录，里面子字段的含义如下。 total：返回记录数，本例是2条。 max_score：最高的匹配程度，本例是1.0。 hits：返回的记录组成的数组。 返回的记录中，每条记录都有一个_score字段，表示匹配的程序，默认是按照这个字段降序排列。 精准查询 精准查询用的是term { \"term\" : { \"price\" : 20 } } 精准查询的时候就不用再对结果项进行匹配度评分了，用constant_score 模糊查询 match是模糊查询，只要包含就行。 Elastic 的查询非常特别，使用自己的查询语法，要求 GET 请求带有数据体。 $curl 'localhost:9200/accounts/person/_search' -d '{ \"query\" : { \"match\" : { \"desc\" : \"软件\" }} }' 上面代码使用 Match 查询，指定的匹配条件是desc字段里面包含\"软件\"这个词。返回结果如下。 { \"took\":3, \"timed_out\":false, \"_shards\":{\"total\":5,\"successful\":5,\"failed\":0}, \"hits\":{ \"total\":1, \"max_score\":0.28582606, \"hits\":[ { \"_index\":\"accounts\", \"_type\":\"person\", \"_id\":\"1\", \"_score\":0.28582606, \"_source\": { \"user\" : \"张三\", \"title\" : \"工程师\", \"desc\" : \"数据库管理，软件开发\" } } ] } } 默认是10条记录,我们还可以分页 $curl 'localhost:9200/accounts/person/_search' -d ' { \"query\" : { \"match\" : { \"desc\" : \"管理\" }}, \"from\": 1, //默认是从位置0开始 \"size\": 1}' 短语查询 上面的查询表达式中match是只要包含就行，那怕他们是不在一起的，但有时我们需要查询必须是在一起的，比如短语。 GET /megacorp/employee/_search { \"query\" : { \"match_phrase\" : { \"about\" : \"rock climbing\" } } } 这里match换成了match_phrase 高亮搜索 就是把搜索的结果高亮显示 组合查询 有时候需要把多个查询组合起来，用bool { \"bool\" : { \"must\" : [],//and \"should\" : [],//or \"must_not\" : [],//非 } } must,should,must_not不必全出现。 ","date":"2019-03-27","objectID":"/posts/elasticsearch/:0:5","tags":["golang"],"title":"elasticsearch","uri":"/posts/elasticsearch/"},{"categories":["笔记"],"content":"集群 一个运行中的 Elasticsearch 实例称为一个 节点，而集群是由一个或者多个拥有相同 cluster.name 配置的节点组成， 它们共同承担数据和负载的压力。当有节点加入集群中或者从集群中移除节点时，集群将会重新平均分布所有的数据。 当一个节点被选举成为 主 节点时， 它将负责管理集群范围内的所有变更，例如增加、删除索引，或者增加、删除节点等。 而主节点并不需要涉及到文档级别的变更和搜索等操作，所以当集群只拥有一个主节点的情况下，即使流量的增加它也不会成为瓶颈。 任何节点都可以成为主节点。我们的示例集群就只有一个节点，所以它同时也成为了主节点。 作为用户，我们可以将请求发送到 集群中的任何节点 ，包括主节点。 每个节点都知道任意文档所处的位置，并且能够将我们的请求直接转发到存储我们所需文档的节点。 无论我们将请求发送到哪个节点，它都能负责从各个包含我们所需文档的节点收集回数据，并将最终结果返回給客户端。 Elasticsearch 对这一切的管理都是透明的。 GET /_cluster/health可以查看集群的状态。 ","date":"2019-03-27","objectID":"/posts/elasticsearch/:0:6","tags":["golang"],"title":"elasticsearch","uri":"/posts/elasticsearch/"},{"categories":["笔记"],"content":"golang使用elastic 使用第三方库github.com/olivere/elastic 插入 func main() { client, err := elastic.NewClient(elastic.SetURL(\"http://localhost:9200\")) if err != nil { fmt.Println(\"connect es error\") } //insertElastic(client) //delElastic(client) //queryElasticById(client) BulkAdd(client) } 查询 func queryDocument(client *elastic.Client) { defer client.Stop() boolQuery := elastic.NewBoolQuery() boolQuery =boolQuery.Should(elastic.NewMatchQuery(\"Name\",\"中\")) boolQuery =boolQuery.Should(elastic.NewMatchQuery(\"Age\",12)) //query = query.Must(elastic.NewTermQuery(\"Name\", \"a bc\")) // //query = query.Must(elastic.NewMatchQuery(\"Name\", \"ab\")) //query = query.Must(elastic.NewRangeQuery(\"update_timestamp\").Gte(criteria.UpdateTime)) src, err := boolQuery.Source() if err != nil { panic(err) } data, err := json.MarshalIndent(src, \"\", \" \") if err != nil { panic(err) } fmt.Println(string(data)) esResponse, err := client.Search().Index(\"user\").Type(\"user\"). Query(boolQuery). Sort(\"Age\", true). From(0).Size(10). Do(context.Background()) fmt.Println(esResponse, err) for _, value := range esResponse.Hits.Hits { var doc *model.User json.Unmarshal(*value.Source,\u0026doc) fmt.Println(doc) } } ","date":"2019-03-27","objectID":"/posts/elasticsearch/:0:7","tags":["golang"],"title":"elasticsearch","uri":"/posts/elasticsearch/"},{"categories":["笔记"],"content":"elasticsearch和mysql同步","date":"2019-03-27","objectID":"/posts/es_mysql%E5%90%8C%E6%AD%A5/","tags":["golang"],"title":"elasticsearch和mysql同步","uri":"/posts/es_mysql%E5%90%8C%E6%AD%A5/"},{"categories":["笔记"],"content":"es mysql同步 go-mysql-elasticsearch是用于同步mysql数据到ES集群的一个开源工具。 go-mysql-elasticsearch的基本原理是：如果是第一次启动该程序，首先使用mysqldump工具对源mysql数据库进行一次全量同步，通过elasticsearch client执行操作写入数据到ES；然后实现了一个mysql client,作为slave连接到源mysql,源mysql作为master会将所有数据的更新操作通过binlog event同步给slave， 通过解析binlog event就可以获取到数据的更新内容，之后写入到ES. 我们在启动时的配置的文件中指定数据库地址端口，用户密码，es的地址端口密码，还可以指定同步的库，表对应es的index，type，字段。 启动命令 ./bin/go-mysql-elasticsearch -config=./etc/river.toml 问题 mysql binlog必须是ROW模式。 要同步的mysql数据表必须包含主键，否则直接忽略，这是因为如果数据表没有主键，UPDATE和DELETE操作就会因为在ES中找不到对应的document而无法进行同步。 ","date":"2019-03-27","objectID":"/posts/es_mysql%E5%90%8C%E6%AD%A5/:0:1","tags":["golang"],"title":"elasticsearch和mysql同步","uri":"/posts/es_mysql%E5%90%8C%E6%AD%A5/"},{"categories":["笔记"],"content":"golang性能分析","date":"2019-03-27","objectID":"/posts/go_pprof%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90/","tags":["golang"],"title":"golang性能分析","uri":"/posts/go_pprof%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90/"},{"categories":["笔记"],"content":"概述 runtime/pprof：采集程序（非 Server）的运行数据进行分析。手动调用runtime.StartCPUProfile或者runtime.StopCPUProfile等 API来生成和写入采样文件，灵活性高，通过 runtime/pprof 实现。 net/http/pprof：采集 HTTP Server 的运行时数据进行分析 go test: 通过 go test -bench . -cpuprofile prof.cpu生成采样文件 适用对函数进行针对性测试。通常使用上面两个进行整体分析，找出热点以后再go test进行分析 pprof 是用于可视化和分析性能分析数据的工具 pprof 以 profile.proto 读取分析样本的集合，并生成报告以可视化并帮助分析数据（支持文本和图形报告） 可以做什么 主要分析cpu，内存，阻塞概要文件 web查看 这种是最简单 import _ “net/http/pprof” 开启服务 浏览器中http://127.0.0.1:8080/debug/pprof/即可看各种信息，默认是收集30秒，每秒100次。 通过交互式 go tool pprof http://localhost:6060/debug/pprof/profile?seconds=60 go tool pprof http://localhost:6060/debug/pprof/heap go tool pprof http://localhost:6060/debug/pprof/block go tool pprof http://localhost:6060/debug/pprof/mutex 在命令行中通过命令交互查看 其实我们可以通过http://127.0.0.1:8080/debug/pprof/priifle来下载profile文件，然后go tool pprof 项目可执行文件 profile文件来查看cpu，其他堆什么的同理。 命令行长交互是可以top命令查看占用资源最高的多少条。 web可以在浏览器中看可视化的数据。 非web程序 上面的方法适用web程序，不要担心，非web程序可以用runtime包下的pprof // 生成 CPU 报告 func cpuProfile() { f, err := os.OpenFile(\"cpu.prof\", os.O_RDWR|os.O_CREATE, 0666) if err != nil { log.Fatal(err) } defer f.Close() log.Println(\"CPU Profile started\") pprof.StartCPUProfile(f) defer pprof.StopCPUProfile() for i := 0; i \u003c 4000; i++ { u := model.User{Name: \"sdfsf\"} bytes, _ := json.Marshal(u) fmt.Println(bytes) } fmt.Println(\"CPU Profile stopped\") } 会生成 cpu.prof 文件。仍然可以使用 go tool pprof 工具进行分析 go tool pprof hello cpu.prof 基准测试查看性能 func BenchmarkPPROF(b *testing.B) { for i := 0; i \u003c b.N; i++ { u := model.User{Name: \"sdfsf\"} bytes, _ := json.Marshal(u) fmt.Println(bytes) } } go test -bench=. -cpuprofile=cpu.prof 这样就生成了.prof文件 ","date":"2019-03-27","objectID":"/posts/go_pprof%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90/:0:1","tags":["golang"],"title":"golang性能分析","uri":"/posts/go_pprof%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90/"},{"categories":["笔记"],"content":"golang zaplog使用","date":"2019-03-22","objectID":"/posts/go_zaplog/","tags":["golang"],"title":"golang zaplog使用","uri":"/posts/go_zaplog/"},{"categories":["笔记"],"content":"zap ","date":"2019-03-22","objectID":"/posts/go_zaplog/:0:0","tags":["golang"],"title":"golang zaplog使用","uri":"/posts/go_zaplog/"},{"categories":["笔记"],"content":"zap使用 uber开源，zap可以在控制台、文件甚至发送数据到其他系统中，以此来记录日志。我们可以指定日志的级别，支持json结构化，方便查询。 和logrus类似，简单来讲，日志有两个概念：字段和消息。字段用来结构化输出错误相关的上下文环境，而消息简明扼要的阐述错误本身。 开发模式下是普通文本的结构： package main import ( \"go.uber.org/zap\" \"time\" ) func main() { // zap.NewDevelopment 开发模式，格式化输出 logger, _ := zap.NewDevelopment() // zap.NewProduction 生产模式json序列化输出 // logger, _ := zap.NewProduction() defer logger.Sync() logger.Info(\"无法获取网址\", zap.String(\"url\", \"http://www.baidu.com\"), zap.Int(\"attempt\", 3), zap.Duration(\"backoff\", time.Second), ) } 自定义配置： func main() { encoderConfig := zapcore.EncoderConfig{ TimeKey: \"time\", LevelKey: \"level\", NameKey: \"logger\", CallerKey: \"caller\", MessageKey: \"msg\", StacktraceKey: \"stacktrace\", LineEnding: zapcore.DefaultLineEnding, EncodeLevel: zapcore.LowercaseLevelEncoder, // 小写编码器 EncodeTime: zapcore.ISO8601TimeEncoder, // ISO8601 UTC 时间格式 EncodeDuration: zapcore.SecondsDurationEncoder, EncodeCaller: zapcore.FullCallerEncoder, // 全路径编码器 } // 设置日志级别 atom := zap.NewAtomicLevelAt(zap.DebugLevel) config := zap.Config{ Level: atom, // 日志级别 Development: true, // 开发模式，堆栈跟踪 Encoding: \"json\", // 输出格式 console 或 json EncoderConfig: encoderConfig, // 编码器配置 InitialFields: map[string]interface{}{\"serviceName\": \"spikeProxy\"}, // 初始化字段，如：添加一个服务器名称 OutputPaths: []string{\"stdout\", \"./logs/spikeProxy.log\"}, // 输出到指定文件 stdout（标准输出，正常颜色） stderr（错误输出，红色） ErrorOutputPaths: []string{\"stderr\"}, } // 构建日志 logger, err := config.Build() if err != nil { panic(fmt.Sprintf(\"log 初始化失败: %v\", err)) } logger.Info(\"log 初始化成功\") logger.Info(\"无法获取网址\", zap.String(\"url\", \"http://www.baidu.com\"), zap.Int(\"attempt\", 3), zap.Duration(\"backoff\", time.Second), ) } ","date":"2019-03-22","objectID":"/posts/go_zaplog/:0:1","tags":["golang"],"title":"golang zaplog使用","uri":"/posts/go_zaplog/"},{"categories":["笔记"],"content":"lumberjack归档 日志文件越来越大，我们根据大小、日期进行归档。 zap可以写入文件，但是并没有归档的功能。借助于lumberjack第三方库，利用hook进行归档。 import ( \"go.uber.org/zap\" \"go.uber.org/zap/zapcore\" \"time\" \"gopkg.in/natefinch/lumberjack.v2\" \"os\" ) func main() { hook := lumberjack.Logger{ Filename: \"./logs/spikeProxy1.log\", // 日志文件路径 MaxSize: 128, // 每个日志文件保存的最大尺寸 单位：M MaxBackups: 30, // 日志文件最多保存多少个备份 MaxAge: 7, // 文件最多保存多少天 Compress: true, // 是否压缩 } encoderConfig := zapcore.EncoderConfig{ TimeKey: \"time\", LevelKey: \"level\", NameKey: \"logger\", CallerKey: \"linenum\", MessageKey: \"msg\", StacktraceKey: \"stacktrace\", LineEnding: zapcore.DefaultLineEnding, EncodeLevel: zapcore.LowercaseLevelEncoder, // 小写编码器 EncodeTime: zapcore.ISO8601TimeEncoder, // ISO8601 UTC 时间格式 EncodeDuration: zapcore.SecondsDurationEncoder, // EncodeCaller: zapcore.FullCallerEncoder, // 全路径编码器 EncodeName: zapcore.FullNameEncoder, } // 设置日志级别 atomicLevel := zap.NewAtomicLevel() atomicLevel.SetLevel(zap.InfoLevel) core := zapcore.NewCore( zapcore.NewJSONEncoder(encoderConfig), // 编码器配置 zapcore.NewMultiWriteSyncer(zapcore.AddSync(os.Stdout), zapcore.AddSync(\u0026hook)), // 打印到控制台和文件 atomicLevel, // 日志级别 ) // 开启开发模式，堆栈跟踪 caller := zap.AddCaller() // 开启文件及行号 development := zap.Development() // 设置初始化字段 filed := zap.Fields(zap.String(\"serviceName\", \"serviceName\")) // 构造日志 logger := zap.New(core, caller, development, filed) logger.Info(\"log 初始化成功\") logger.Info(\"无法获取网址\", zap.String(\"url\", \"http://www.baidu.com\"), zap.Int(\"attempt\", 3), zap.Duration(\"backoff\", time.Second)) } ","date":"2019-03-22","objectID":"/posts/go_zaplog/:0:2","tags":["golang"],"title":"golang zaplog使用","uri":"/posts/go_zaplog/"},{"categories":["笔记"],"content":"zap优势及原理 标准log没有日志分级。seelog可分级，支持归档，比较灵活，但是利用反射，效率低。 避免 GC: 对象复用 zap 通过 sync.Pool 提供的对象池，复用了大量可以复用的对象，避开了 gc 这个大麻烦。 内建的 Encoder: 避免反射 标准库中的 json.Marshaler 提供的是基于类型反射的拼接方式，代价是高昂的。 zap 选择了自己实现 json Encoder。 通过明确的类型调用，直接拼接字符串，最小化性能开销。 level handler level handler 是 zap 提供的一种 level 的处理方式，通过 http 请求动态改变日志组件级别。 ","date":"2019-03-22","objectID":"/posts/go_zaplog/:0:3","tags":["golang"],"title":"golang zaplog使用","uri":"/posts/go_zaplog/"},{"categories":["笔记"],"content":"go值传递","date":"2019-03-22","objectID":"/posts/go_%E5%80%BC%E4%BC%A0%E9%80%92/","tags":["golang"],"title":"go值传递","uri":"/posts/go_%E5%80%BC%E4%BC%A0%E9%80%92/"},{"categories":["笔记"],"content":"参数值传递 函数传递的总是原来这个东西的一个副本，一副拷贝。比如我们传递一个int类型的参数，传递的其实是这个参数的一个副本；传递一个指针类型的参数，其实传递的是这个该指针的一份拷贝，而不是这个指针指向的值。 对于int这类基础类型我们可以很好的理解，它们就是一个拷贝，但是指针呢？我们觉得可以通过它修改原来的值，怎么会是一个拷贝呢？下面我们看个例子。 func main() { i:=10 ip:=\u0026i fmt.Printf(\"原始指针的内存地址是：%p\\n\",\u0026ip) modify(ip) fmt.Println(\"int值被修改了，新值为:\",i) } func modify(ip *int){ fmt.Printf(\"函数里接收到的指针的内存地址是：%p\\n\",\u0026ip) *ip=1 } 运行结果： 原始指针的内存地址是：0xc42000c028 函数里接收到的指针的内存地址是：0xc42000c038 int值被修改了，新值为: 1 首先我们要知道，任何存放在内存里的东西都有自己的地址，指针也不例外，它虽然指向别的数据，但是也有存放该指针的内存。 所以通过输出我们可以看到，这是一个指针的拷贝，因为存放这两个指针的内存地址是不同的，虽然指针的值相同，但是是两个不同的指针。 ","date":"2019-03-22","objectID":"/posts/go_%E5%80%BC%E4%BC%A0%E9%80%92/:0:1","tags":["golang"],"title":"go值传递","uri":"/posts/go_%E5%80%BC%E4%BC%A0%E9%80%92/"},{"categories":["笔记"],"content":"参数引用传递 Go语言(Golang)是没有引用传递的。 ","date":"2019-03-22","objectID":"/posts/go_%E5%80%BC%E4%BC%A0%E9%80%92/:0:2","tags":["golang"],"title":"go值传递","uri":"/posts/go_%E5%80%BC%E4%BC%A0%E9%80%92/"},{"categories":["笔记"],"content":"引用类型 func main() { persons:=make(map[string]int) persons[\"张三\"]=19 mp:=\u0026persons fmt.Printf(\"原始map的内存地址是：%p\\n\",mp) modify(persons) fmt.Println(\"map值被修改了，新值为:\",persons) } func modify(p map[string]int){ fmt.Printf(\"函数里接收到map的内存地址是：%p\\n\",\u0026p) p[\"张三\"]=20 } 输出： 原始map的内存地址是：0xc42000c028 函数里接收到map的内存地址是：0xc42000c038 map值被修改了，新值为: map[张三:20] map作为函数的参数时，也是值传递，因为地址是不一样的。但是通过函数我们却能改变map里的值，为何？ 通过查看src/runtime/hashmap.go源代码发现，make函数返回的是一个hmap类型的指针*hmap。也就是说map == *hmap。 现在看func modify(p map)这样的函数，其实就等于func modify(p *hmap)。 所以在这里，Go语言通过make函数，字面量的包装，为我们省去了指针的操作，让我们可以更容易的使用map。这里的map可以理解为引用类型，但是记住引用类型不是传引用。 channel类型与map类型同理。 slice和map、chan都不太一样的，一样的是，它也是引用类型，它也可以在函数中修改对应的内容。 但是它并不是对指针的换个面具。而是它字段里有指针类型。 我们来看slice大概的结构： type slice struct { array unsafe.Pointer len int cap int } 所以修改类型的内容的办法有很多种，类型本身作为指针可以，类型里有指针类型的字段也可以。 单纯的从slice这个结构体看，我们可以通过modify修改存储元素的内容，但是永远修改不了len和cap，因为他们只是一个拷贝，如果要修改，那就要传递*slice作为参数才可以。 ","date":"2019-03-22","objectID":"/posts/go_%E5%80%BC%E4%BC%A0%E9%80%92/:0:3","tags":["golang"],"title":"go值传递","uri":"/posts/go_%E5%80%BC%E4%BC%A0%E9%80%92/"},{"categories":["笔记"],"content":"总结 最终我们可以确认的是Go语言中所有的传参都是值传递（传值），都是一个副本，一个拷贝。因为拷贝的内容有时候是非引用类型（int、string、struct等这些），这样就在函数中就无法修改原内容数据；有的是引用类型（指针、map、slice、chan、func等这些），这样就可以修改原内容数据。 是否可以修改原内容数据，和传值、传引用没有必然的关系。 在Go语言里，虽然只有传值，但是我们也可以修改原内容数据，因为参数是引用类型。 这里也要记住，引用类型和传引用是两个概念，go函数只有值传递。 ","date":"2019-03-22","objectID":"/posts/go_%E5%80%BC%E4%BC%A0%E9%80%92/:0:4","tags":["golang"],"title":"go值传递","uri":"/posts/go_%E5%80%BC%E4%BC%A0%E9%80%92/"},{"categories":["笔记"],"content":"hash一致性","date":"2019-03-22","objectID":"/posts/hash%E4%B8%80%E8%87%B4%E6%80%A7/","tags":["golang"],"title":"hash一致性","uri":"/posts/hash%E4%B8%80%E8%87%B4%E6%80%A7/"},{"categories":["笔记"],"content":"普通hash 假如现在需要向4个redis存取图片，以图片的名字为key，value为图片实际存放的路径，我们可以取到key的hash值，然后就hash值进行模运算，然后就定位到该条数据应该存储到哪个redis节点上了。 通过上面的方法提高了效率，我们在取数据的时候就不用遍历4个redis节点，直接到正确的节点去取数据即可，但是有个缺点：当增加或删除节点时，大量的数据就失效，因为再进行取余运算结果已不同。 ","date":"2019-03-22","objectID":"/posts/hash%E4%B8%80%E8%87%B4%E6%80%A7/:0:1","tags":["golang"],"title":"hash一致性","uri":"/posts/hash%E4%B8%80%E8%87%B4%E6%80%A7/"},{"categories":["笔记"],"content":"一致性hash 其实一致性hash还是进行取模运算，只不过取模不在除以节点数，而是2^23。 有个虚拟的圆环，上面均匀分布0-2^23 -1，就像表盘一样，对key取余后就定位到该数据在圆盘的位置了。 我们同样对redis实例节点进行hash取余，也得到了节点的位置。 对数据顺时针查找最近的redis节点，该节点就是应该存取的节点。 根据一致性Hash算法，数据A会被定为到Node A上，B被定为到Node B上，C被定为到Node C上，D被定为到Node D上。 ","date":"2019-03-22","objectID":"/posts/hash%E4%B8%80%E8%87%B4%E6%80%A7/:0:2","tags":["golang"],"title":"hash一致性","uri":"/posts/hash%E4%B8%80%E8%87%B4%E6%80%A7/"},{"categories":["笔记"],"content":"添加或删除节点 现假设Node C不幸宕机，可以看到此时对象A、B、D不会受到影响，只有C对象被重定位到Node D。一般的，在一致性Hash算法中，如果一台服务器不可用，则受影响的数据仅仅是此服务器到其环空间中前一台服务器（即沿着逆时针方向行走遇到的第一台服务器）之间数据，其它不会受到影响 新增节点X时，此时对象Object A、B、D不受影响，只有对象C需要重定位到新的Node X ！一般的，在一致性Hash算法中，如果增加一台服务器，则受影响的数据仅仅是新服务器到其环空间中前一台服务器（即沿着逆时针方向行走遇到的第一台服务器）之间数据，其它数据也不会受到影响。 综上所述，一致性Hash算法对于节点的增减都只需重定位环空间中的一小部分数据，具有较好的容错性和可扩展性。 ","date":"2019-03-22","objectID":"/posts/hash%E4%B8%80%E8%87%B4%E6%80%A7/:0:3","tags":["golang"],"title":"hash一致性","uri":"/posts/hash%E4%B8%80%E8%87%B4%E6%80%A7/"},{"categories":["笔记"],"content":"Hash环的数据倾斜问题 一致性Hash算法在服务节点太少时，容易因为节点分部不均匀而造成数据倾斜（被缓存的对象大部分集中缓存在某一台服务器上）问题，例如系统中只有两台服务器，其环分布如下： 此时必然造成大量数据集中到Node A上，而只有极少量会定位到Node B上。 为了解决这种数据倾斜问题，一致性Hash算法引入了虚拟节点机制，即对每一个服务节点计算多个哈希，每个计算结果位置都放置一个此服务节点，称为虚拟节点。具体做法可以在服务器IP或主机名的后面增加编号来实现。 例如上面的情况，可以为每台服务器计算三个虚拟节点，于是可以分别计算 “Node A#1”、“Node A#2”、“Node A#3”、“Node B#1”、“Node B#2”、“Node B#3”的哈希值，于是形成六个虚拟节点： 例如定位到“Node A#1”、“Node A#2”、“Node A#3”三个虚拟节点的数据均定位到Node A上。这样就解决了服务节点少时数据倾斜的问题。在实际应用中，通常将虚拟节点数设置为32甚至更大，因此即使很少的服务节点也能做到相对均匀的数据分布。 ","date":"2019-03-22","objectID":"/posts/hash%E4%B8%80%E8%87%B4%E6%80%A7/:0:4","tags":["golang"],"title":"hash一致性","uri":"/posts/hash%E4%B8%80%E8%87%B4%E6%80%A7/"},{"categories":["笔记"],"content":"总结 hash一致性当节点增加或减少的时候并不能完全做到无感，但是只有少量的数据进行迁移，这样比原来的hash好多了。 ","date":"2019-03-22","objectID":"/posts/hash%E4%B8%80%E8%87%B4%E6%80%A7/:0:5","tags":["golang"],"title":"hash一致性","uri":"/posts/hash%E4%B8%80%E8%87%B4%E6%80%A7/"},{"categories":["笔记"],"content":"linux","date":"2019-03-22","objectID":"/posts/linux/","tags":["linux"],"title":"linux","uri":"/posts/linux/"},{"categories":["笔记"],"content":"linux主要目录 /bin 放二进制文件，都是可以运行的 /lib 存放系统最基本的动态连接共享库 /dev 放置外接设备，光盘u判断等，外接设备是不能直接使用的，需要挂载（类似于windows的分配盘符） /etc 放置配置文件 /home 除了root用户以外的用户家目录 /proc linux运行时候的进程 /root root账户自己独有的家目录 /sbin 全称是super binary，就是super权限才能运行的二进制文件 /tmp 临时文件 /opt 可选的意思，放些大型软件服务，产生的所有文件也和改软件同目录，卸载的时候直接删除目录即可 /usr 用户自己安装的软件，类似windows的programfiles目录 /usr/local 主要放手动安装的软件，既不是软件管理软件安装的 /var 程序或系统的日志目录 /media 系统会自动识别一些设备，比如u盘 /mnt 用户临时外接设备挂载的时候，挂载到此目录 ","date":"2019-03-22","objectID":"/posts/linux/:0:1","tags":["linux"],"title":"linux","uri":"/posts/linux/"},{"categories":["笔记"],"content":"命令 指令格式： 指令 选项 操作对象 命令的查找顺序为：先在当前目录查找，如果没有再去PATH中查找。 常用 ls -a 所有文件 -l 多列信息 -t 按时间 -F 简单名称并显示文件类型 -h 文件大小带单位 ls -l共8列信息： 文件权限、链接数、用户名、组名、大小、最后修改日期、最后修改时间、文件名 mkdir -p 多级目录 cp cp src dst cp复制文件夹时，需要添加-r选项 mv 移动文件，或者重命名 mv src dst 输出重定向 “\u003e”会覆盖原内容 “»\"追加内容 进阶指令 df 查看磁盘空间 df -h 一般看结果的第一行 free 查看内存 free -m以M为单位 tail 查看一个文件的末尾n行,默认是10行 tail -f还可以实时查看文件的内容 wc 文件内容信息统计 -l 行数 -c 字节数 -w单词数 -m字符数 date 读取、设置时间 时间格式： CET欧洲时间 GMT格林威治时间 UTC世界标准时间 CST中国标准时间 CET=UTC/GMT + 1小时 CST=UTC/GMT +8 小时 CST=CET+9 读取时间： date +%F 2016-01-02 date \"+%Y-%m-%d %H:%m:%S\" 2016-01-02 15:04:05 date -d \"-/+1 day/month/year\" \"+%Y-%m-%d %H:%m:%S\" 2016-01-02 15:04:05 id 查看用户的id，组id等 ps process status的缩写 -a 同一终端下的所有进程 -A 所有进程，等于-e top 实时显示进程的状态。 lsof list of file的缩写 可以查看端口占用lsof “:8080” netstat 查看网络连接状态 常用的是netstat -tnpl -t只列出tcp连接 -n显示IP地址和端口 -p显示程序名或pid -l显示作为监听的进程 du 查看文件夹的大小 -s 汇总大小 -h 带单位 du -sh 目录 find 在路径下搜索文件的文件 find 路径 选项 选项值 -name 按文件名来搜索 -type 按文件类型来搜索 f文件 d文件夹 例子 find /etc -name xx.conf service 启动关闭服务 service 服务名 start/stop/restart uname 查看系统信息 -a all详细的信息 uname -all 链接文件 为文件创建快捷方式（链接文件） ln -s 源文件 目标文件 ","date":"2019-03-22","objectID":"/posts/linux/:0:2","tags":["linux"],"title":"linux","uri":"/posts/linux/"},{"categories":["笔记"],"content":"vim 命令模式：通过快捷键进行删除行，粘贴、光标移动等操作 编辑模式：进行编辑 末尾模式：进行搜索、替换、保存、退出等 模式切换 命令\u003e末尾 ： 末尾\u003e命令 连续两次esc 命令\u003e编辑 i 编辑\u003e末尾 esc 命令模式 撤销上一次操作 u 光标移动到行首 ^按键，也就是shift + 6 光标移动到行尾 $按键，也就是shirft + 4 光标移动到首行 gg (good game)重新开始意思 光标移动到末行 G 向上翻屏 ctrl + b backwards 向下翻屏 ctrl + f forward 复制一行 yy 只是复制，并没有粘贴 粘贴 p 编辑模式 进行编辑即可 末尾模式 显示行号 set number 不显示行号 set nonumber 光标移动到特定行 行号+回车 w保存 q退出 wq保存退出 x智能退出，如果修改了就会保存后退出，没修改直接退出 搜索 /关键字 默认是大小写敏感的，在关键字后面加\\c即可不敏感 set hlsearch 搜索后的关键字就会高亮 set nohlsearch 替换所有的关键字 %s/搜索的关键字/替换字/g ","date":"2019-03-22","objectID":"/posts/linux/:0:3","tags":["linux"],"title":"linux","uri":"/posts/linux/"},{"categories":["笔记"],"content":"init进程 初始化进程init运行级别 0 关机 3 不带桌面的 5 带桌面的 6 重启 例子：init 0关机，通过init进程关机。 init 命令需要root权限 ","date":"2019-03-22","objectID":"/posts/linux/:0:4","tags":["linux"],"title":"linux","uri":"/posts/linux/"},{"categories":["笔记"],"content":"用户管理 主要涉及三个文件 /etc/passwd 用户信息 /etc/group 用户组的信息 /etc/shadow 用户的密码信息 useradd useradd 可选项 用户名 -g 主要用户组(一个用户可以有多个组) -G 附加组，非主要组 -u 用户ID 例子：useradd zhangsan usermod usermod 可选项 用户名 -g 主要用户组(一个用户可以有多个组) -G 附加组，非主要组 -u 用户ID -l 修改用户名 修改用户密码 passwd 用户名 注意：用户修改自己的密码，不用跟用户名 切换用户 su 用户名 普通用户名 su 切换到用户 删除用户 userdel 用户名 -r 删除用户的时候，家目录也删除 用户组管理 groupadd 添加用户组 groupmod 修改用户组 groupdel 删除用户组 ","date":"2019-03-22","objectID":"/posts/linux/:0:5","tags":["linux"],"title":"linux","uri":"/posts/linux/"},{"categories":["笔记"],"content":"用户权限 默认用户是没有suod命令，用户或用户组可以执行的sudo权限可以在/etc/sudoers配置 ","date":"2019-03-22","objectID":"/posts/linux/:0:6","tags":["linux"],"title":"linux","uri":"/posts/linux/"},{"categories":["笔记"],"content":"ssh ssh远程连接传输协议，默认端口22 注意：端口0-65535 ","date":"2019-03-22","objectID":"/posts/linux/:0:7","tags":["linux"],"title":"linux","uri":"/posts/linux/"},{"categories":["笔记"],"content":"文件权限 文件权限 d rwx rwx rwx 首位d代表文件夹，-代表文件，l代表链接文件。 第一组rwx是文件的拥有者权限，第二组是组内其他成员操作权限，第三组是组外其他人员。 文件可读：可以读取里面的数据。 文件可写：可以写数据，可以删除该文件。 文件可执行：是可执行文件。 特殊权限SUID、SGID、Sticky SUID(Set User ID) 该属性只对有执行权限的文件有效，对目录无效。执行具有SUID权限的程序时，引发的进程的所有者是程序文件的所有者，而不是启动程序的用户。 例子：-rwsr-xr-x 1 root root 47032 Feb 16 2014 /usr/bin/passwd SGID(Set Group ID） 对于可执行文件，SGID与SUID类似，引发的进程的所有组是程序文件所属的组。 例子：drwxrwsr-x 2 root staff 4096 Apr 10 2014 local Sticky 仅对目录有效。 带sticky属性的目录下的文件或目录可以被其拥有者删除或改名。常利用sticky属性创建这样的目录：组用户可以在此目录中创建新文件、修改文件内容，但只有文件所有者才能对自己的文件进行删除或改名。 例子：drwxrwxrwt 8 root root 4096 Apr 4 23:57 tmp 修改特殊位 那么原来的执行标志x到哪里去了呢? 系统是这样规定的, 如果本来在该位上有x, 则这些特殊标志显示为小写字母 (s, s, t). 否则, 显示为大写字母 (S, S, T)。 chmod u+s temp 加setuid权限 chmod g+s temp 加setgid权限 chmod o+t temp 加sticky权限 修改文件属性 修改文件所有者 chown -R 属主名 文件名 -R是递归，所有子文件均修改 修改文件组 chgrp -R 组名 文件名 更改文件权限 修改文件权限一种是通过数字，一种是符号。 数字法：chmod [-R] 777 文件 符号法：chmod[-R] u/g/o/a =/+/- r/wx ","date":"2019-03-22","objectID":"/posts/linux/:0:8","tags":["linux"],"title":"linux","uri":"/posts/linux/"},{"categories":["笔记"],"content":"目录权限 目录和常规文件一样使用相同的权限位进行标识，但是它们的翻译不同。 文件夹的读：允许用户使用该权限列出目录内容。 文件夹的写：写权限意味着用户使用该权限能够在目录中创建或者删除文件。 文件夹的执行：是否可以cd到改目录下 ","date":"2019-03-22","objectID":"/posts/linux/:0:9","tags":["linux"],"title":"linux","uri":"/posts/linux/"},{"categories":["笔记"],"content":"shell编程 变量 注意，变量名和等号之间不能有空格 声明、赋值变量是不需要$符的，引用的时候需要 使用 readonly 命令可以将变量定义为只读变量，只读变量的值不能被改变 使用 unset 命令可以删除变量 可以在定义时加上local命令，此时该变量就成了局部变量。 在 Shell 中定义的变量，默认就是全局变量。 全局变量只在当前 Shell 进程中有效，对其它 Shell 进程和子进程都无效。如果使用export命令将全局变量导出，那么它就在所有的子进程中也有效了，这称为“环境变量”。 读取输入 read -p '请输入' 变量 字符串 字符串单引号、双引号包括都可以，但是双引号里面可以有转义和变量 获取字符串长度，echo ${#str} 获取子串，echo ${string:1:4} 数组 bash支持一维数组（不支持多维数组），并且没有限定数组的大小。 数组名=(值1 值2 ... 值n) array_name[0]=value0 获取数组所有元素，echo ${array_name[@]} 获取数组长度，length=${#array_name[@]} 注释 多行注释： :«EOF … EOF 单行注释：一个井号 获取参数 $0获取程序文件名 $n第n个参数 算术运算符 bash原生不支持简单的算术运算的，可以利用expr，(()),[],bc等实现 前三者只能进行整数运算，后者可以小数。 expr v=expr 2 + 2 表达式和运算符之间要有空格，例如 2+2 是不对的，必须写成 2 + 2 完整的表达式要被 包含 乘号需要转义* $(()) echo $(( 2 + 5 )) 不需要转义乘号 $[] r=$[ 4 + 5 ] bc bc可以看做是linux计算器,支持小数。 浮点数的小数位数是由内建变量scale控制的。scala变量的默认值是0，在scala值被设置之前，bc计算结果不包含小数位。 在脚本中我们可以这样使用bc: variable=$(echo “options; expression” | bc) var1=$(echo “scale=4; 3.44 /5” | bc) 注意：小于1的时候，结果是没有整数的0的，可以这样解决： res1=$(printf “%.2f” echo \"scale=2;1/3\"|bc) 关系运算符 [a -eq b] 判断相等 相当于= [a -ne b] 判断不相等 相当与!= [a -lt b] 小于 [a -gt b] 大于 #!/bin/bash a=2 b=2 if [ ${a} = ${b} ] then echo \"yes\" fi if [ ${a} -eq ${b} ] then echo \"yes\" else echo \"no\" fi if [ ${a} -eq 34 ] then echo \"yes\" elif [ ${a} -lt 20 ] then echo \"no\" fi 非与或 -o 或 -a 与 ！非 #!/bin/bash a=2 b=3 if [ ${a} = ${b} -o ${a} -lt ${b} ] then echo \"yes\" fi 字符串运算符 # 判断是否相等 if [ $str = $str2 ] then echo \"yes\" fi # 判断是否为空 if [ -z $str ] then echo \"zero\" fi 文件运算符 文件测试运算符用于检测 Unix 文件的各种属性。 f=\"./hello\" if [ -f $f ] then echo \"file\" fi -f 是否为文件 -d是否为目录 -e文件是否存在 -x是否可执行 流程控制 if判断: num1=$[2*3] num2=$[1+5] if test $[num1] -eq $[num2] then echo '两个数字相等!' else echo '两个数字不相等!' fi for循环: for loop in 1 2 3 4 5 do echo \"The value is: $loop\" done #第二种： for (( i=0; i \u003c 3; ++i )) do echo $i done while循环： #!/bin/bash int=1 while(( $int\u003c=5 )) do echo $int let \"int++\" done 命令执行结果赋值 shell编程中把命令的执行结果赋值给变量： 第一种方法： v=$(ls ) 第二种： v=`ls` ","date":"2019-03-22","objectID":"/posts/linux/:0:10","tags":["linux"],"title":"linux","uri":"/posts/linux/"},{"categories":["笔记"],"content":"压缩文件 gzip 压缩后的格式为：*.gz 这种压缩方式不能保存原文件；且不能压缩目录 命令举例： gzip buodo gunzip buodo.gz zip 与gzip相比：可以压缩目录； 可以保留原文件； 选项： -r(recursive) 递归压缩目录内的所有文件和目录 命令举例： zip boduo.zip boduo unzip boduo.zip tar 命令选项： -z(gzip) 用gzip来压缩/解压缩文件 -j(bzip2) 用bzip2来压缩/解压缩文件 -v(verbose) 详细报告tar处理的文件信息 -c(create) 创建新的档案文件 -x(extract) 解压缩文件或目录 -f(file) 使用档案文件或设备，这个选项通常是必选的。 命令举例： tar -zvcf buodo.tar.gz buodo tar -zvxf buodo.tar.gz ","date":"2019-03-22","objectID":"/posts/linux/:0:11","tags":["linux"],"title":"linux","uri":"/posts/linux/"},{"categories":["笔记"],"content":"awk awk是一个强大的文本分析工具，awk在其对数据分析并生成报告时，显得尤为强大。简单来说awk就是把文件逐行的读入，以空格为默认分隔符将每行切片，切开的部分再进行各种分析处理。 调用awk有多种方法，常用命令行方法： awk [-F field-separator] ‘commands’ input-file(s) 其中，commands 是真正awk命令，[-F域分隔符]是可选的。 input-file(s) 是待处理的文件。 在awk中，文件的每一行中，由域分隔符分开的每一项称为一个域。通常，在不指名-F域分隔符的情况下，默认的域分隔符是空格 命令举例： # 查看ls -l结果的第一列 ls -l ./ | awk '{print $1}' ","date":"2019-03-22","objectID":"/posts/linux/:0:12","tags":["linux"],"title":"linux","uri":"/posts/linux/"},{"categories":["笔记"],"content":"案例 读取一个文件每行内容 利用管道 #!/bin/bash f=\"./hello_test.go\" cat $f | while read line do echo $line done ","date":"2019-03-22","objectID":"/posts/linux/:0:13","tags":["linux"],"title":"linux","uri":"/posts/linux/"},{"categories":["笔记"],"content":"linux io模型","date":"2019-03-22","objectID":"/posts/linux_io/","tags":["linux"],"title":"linux io模型","uri":"/posts/linux_io/"},{"categories":["笔记"],"content":"用户空间与内核空间 现在操作系统都是采用虚拟存储器，那么对32位操作系统而言，它的寻址空间（虚拟存储空间）为4G（2的32次方）。操作系统的核心是内核，独立于普通的应用程序，可以访问受保护的内存空间，也有访问底层硬件设备的所有权限。为了保证用户进程不能直接操作内核（kernel），保证内核的安全，操心系统将虚拟空间划分为两部分，一部分为内核空间，一部分为用户空间。针对linux操作系统而言，将最高的1G字节（从虚拟地址0xC0000000到0xFFFFFFFF），供内核使用，称为内核空间，而将较低的3G字节（从虚拟地址0x00000000到0xBFFFFFFF），供各个进程使用，称为用户空间。 ","date":"2019-03-22","objectID":"/posts/linux_io/:0:1","tags":["linux"],"title":"linux io模型","uri":"/posts/linux_io/"},{"categories":["笔记"],"content":"进程切换 为了控制进程的执行，内核必须有能力挂起正在CPU上运行的进程，并恢复以前挂起的某个进程的执行。这种行为被称为进程切换。因此可以说，任何进程都是在操作系统内核的支持下运行的，是与内核紧密相关的。 ","date":"2019-03-22","objectID":"/posts/linux_io/:0:2","tags":["linux"],"title":"linux io模型","uri":"/posts/linux_io/"},{"categories":["笔记"],"content":"进程的阻塞 正在执行的进程，由于期待的某些事件未发生，如请求系统资源失败、等待某种操作的完成、新数据尚未到达或无新工作做等，则由系统自动执行阻塞原语(Block)，使自己由运行状态变为阻塞状态。可见，进程的阻塞是进程自身的一种主动行为，也因此只有处于运行态的进程（获得CPU），才可能将其转为阻塞状态。当进程进入阻塞状态，是不占用CPU资源的。 ","date":"2019-03-22","objectID":"/posts/linux_io/:0:3","tags":["linux"],"title":"linux io模型","uri":"/posts/linux_io/"},{"categories":["笔记"],"content":"文件描述符 我们都知道unix(like)世界里，一切皆文件，而文件是什么呢？文件就是一串二进制流而已，不管socket,还是FIFO、管道、终端，对我们来说，一切都是文件，一切都是流。在信息 交换的过程中，我们都是对这些流进行数据的收发操作，简称为I/O操作(input and output)，往流中读出数据，系统调用read，写入数据，系统调用write。 计算机里有这么多的流，我怎么知道要操作哪个流呢？对，就是文件描述符，即通常所说的fd，一个fd就是一个整数，所以，对这个整数的操作，就是对这个文件（流）的操作。我们创建一个socket,通过系统调用会返回一个文件描述符，那么剩下对socket的操作就会转化为对这个描述符的操作。 ","date":"2019-03-22","objectID":"/posts/linux_io/:0:4","tags":["linux"],"title":"linux io模型","uri":"/posts/linux_io/"},{"categories":["笔记"],"content":"缓存 I/O 缓存 I/O 又被称作标准 I/O，大多数文件系统的默认 I/O 操作都是缓存 I/O。在 Linux 的缓存 I/O 机制中，操作系统会将 I/O 的数据缓存在文件系统的页缓存（ page cache ）中，也就是说，数据会先被拷贝到操作系统内核的缓冲区中，然后才会从操作系统内核的缓冲区拷贝到应用程序的地址空间。 缓存 I/O 的缺点： 数据在传输过程中需要在应用程序地址空间和内核进行多次数据拷贝操作，这些数据拷贝操作所带来的 CPU 以及内存开销是非常大的。 ","date":"2019-03-22","objectID":"/posts/linux_io/:0:5","tags":["linux"],"title":"linux io模型","uri":"/posts/linux_io/"},{"categories":["笔记"],"content":"IO模式 刚才说了，对于一次IO访问（以read举例），数据会先被拷贝到操作系统内核的缓冲区中，然后才会从操作系统内核的缓冲区拷贝到应用程序的地址空间。所以说，当一个read操作发生时，它会经历两个阶段： 等待数据准备 (Waiting for the data to be ready) 将数据从内核拷贝到进程中 (Copying the data from the kernel to the process) 正式因为这两个阶段，linux系统产生了下面五种网络模式的方案。 阻塞 I/O（blocking IO） 非阻塞 I/O（nonblocking IO） I/O 多路复用（ IO multiplexing） 信号驱动 I/O（ signal driven IO） 异步 I/O（asynchronous IO） 注：由于signal driven IO在实际中并不常用，所以我这只提及剩下的四种IO Model。 ","date":"2019-03-22","objectID":"/posts/linux_io/:0:6","tags":["linux"],"title":"linux io模型","uri":"/posts/linux_io/"},{"categories":["笔记"],"content":"阻塞IO 当用户进程调用了recvfrom这个系统调用，kernel就开始了IO的第一个阶段：准备数据（对于网络IO来说，很多时候数据在一开始还没有到达。比如，还没有收到一个完整的UDP包。这个时候kernel就要等待足够的数据到来）。这个过程需要等待，也就是说数据被拷贝到操作系统内核的缓冲区中是需要一个过程的。而在用户进程这边，整个进程会被阻塞（当然，是进程自己选择的阻塞）。当kernel一直等到数据准备好了，它就会将数据从kernel中拷贝到用户内存，然后kernel返回结果，用户进程才解除block的状态，重新运行起来。 所以，blocking IO的特点就是在IO执行的两个阶段都被block了。 ","date":"2019-03-22","objectID":"/posts/linux_io/:0:7","tags":["linux"],"title":"linux io模型","uri":"/posts/linux_io/"},{"categories":["笔记"],"content":"非阻塞io 当用户进程发出read操作时，如果kernel中的数据还没有准备好，那么它并不会block用户进程，而是立刻返回一个error。从用户进程角度讲 ，它发起一个read操作后，并不需要等待，而是马上就得到了一个结果。用户进程判断结果是一个error时，它就知道数据还没有准备好，于是它可以再次发送read操作。一旦kernel中的数据准备好了，并且又再次收到了用户进程的system call，那么它马上就将数据拷贝到了用户内存，然后返回。 所以，nonblocking IO的特点是用户进程需要不断的主动询问kernel数据好了没有。 ","date":"2019-03-22","objectID":"/posts/linux_io/:0:8","tags":["linux"],"title":"linux io模型","uri":"/posts/linux_io/"},{"categories":["笔记"],"content":"io多路复用 IO multiplexing就是我们说的select，poll，epoll，有些地方也称这种IO方式为event driven IO。select/epoll的好处就在于单个process就可以同时处理多个网络连接的IO。它的基本原理就是select，poll，epoll这个function会不断的轮询所负责的所有socket，当某个socket有数据到达了，就通知用户进程。 当用户进程调用了select，那么整个进程会被block，而同时，kernel会“监视”所有select负责的socket，当任何一个socket中的数据准备好了，select就会返回。这个时候用户进程再调用read操作，将数据从kernel拷贝到用户进程。 所以，I/O 多路复用的特点是通过一种机制一个进程能同时等待多个文件描述符，而这些文件描述符（套接字描述符）其中的任意一个进入读就绪状态，select()函数就可以返回。 I/O多路复用就通过一种机制，可以监视多个描述符，一旦某个描述符就绪（一般是读就绪或者写就绪），能够通知程序进行相应的读写操作。但select，poll，epoll本质上都是同步I/O，因为他们都需要在读写事件就绪后自己负责进行读写，也就是说这个读写过程是阻塞的，而异步I/O则无需自己负责进行读写，异步I/O的实现会负责把数据从内核拷贝到用户空间。 在IO multiplexing Model中，实际中，对于每一个socket，一般都设置成为non-blocking，但是，如上图所示，整个用户的process其实是一直被block的。只不过process是被select这个函数block，而不是被socket IO给block。 select，poll，epoll都是IO多路复用的机制。I/O多路复用就是通过一种机制，一个进程可以监视多个描述符，一旦某个描述符就绪（一般是读就绪或者写就绪），能够通知程序进行相应的读写操作。但select，poll，epoll本质上都是同步I/O，因为他们都需要在读写事件就绪后自己负责进行读写，也就是说这个读写过程是阻塞的，而异步I/O则无需自己负责进行读写，异步I/O的实现会负责把数据从内核拷贝到用户空间。 ","date":"2019-03-22","objectID":"/posts/linux_io/:0:9","tags":["linux"],"title":"linux io模型","uri":"/posts/linux_io/"},{"categories":["笔记"],"content":"异步io linux下的asynchronous IO其实用得很少。 用户进程发起read操作之后，立刻就可以开始去做其它的事。而另一方面，从kernel的角度，当它受到一个asynchronous read之后，首先它会立刻返回，所以不会对用户进程产生任何block。然后，kernel会等待数据准备完成，然后将数据拷贝到用户内存，当这一切都完成之后，kernel会给用户进程发送一个signal，告诉它read操作完成了。 ","date":"2019-03-22","objectID":"/posts/linux_io/:0:10","tags":["linux"],"title":"linux io模型","uri":"/posts/linux_io/"},{"categories":["笔记"],"content":"blocking和non-blocking的区别 调用blocking IO会一直block住对应的进程直到操作完成，而non-blocking IO在kernel还准备数据的情况下会立刻返回。 ","date":"2019-03-22","objectID":"/posts/linux_io/:0:11","tags":["linux"],"title":"linux io模型","uri":"/posts/linux_io/"},{"categories":["笔记"],"content":"synchronous IO和asynchronous IO的区别 在说明synchronous IO和asynchronous IO的区别之前，需要先给出两者的定义。POSIX的定义是这样子的： A synchronous I/O operation causes the requesting process to be blocked until that I/O operation completes; An asynchronous I/O operation does not cause the requesting process to be blocked; 两者的区别就在于synchronous IO做”IO operation”的时候会将process阻塞。按照这个定义，之前所述的blocking IO，non-blocking IO，IO multiplexing都属于synchronous IO。 有人会说，non-blocking IO并没有被block啊。这里有个非常“狡猾”的地方，定义中所指的”IO operation”是指真实的IO操作，就是例子中的recvfrom这个system call。non-blocking IO在执行recvfrom这个system call的时候，如果kernel的数据没有准备好，这时候不会block进程。但是，当kernel中数据准备好的时候，recvfrom会将数据从kernel拷贝到用户内存中，这个时候进程是被block了，在这段时间内，进程是被block的。 而asynchronous IO则不一样，当进程发起IO 操作之后，就直接返回再也不理睬了，直到kernel发送一个信号，告诉进程说IO完成。在这整个过程中，进程完全没有被block。 ","date":"2019-03-22","objectID":"/posts/linux_io/:0:12","tags":["linux"],"title":"linux io模型","uri":"/posts/linux_io/"},{"categories":["笔记"],"content":"同步异步，阻塞非阻塞区别联系 实际上同步与异步是针对应用程序与内核的交互而言的。 同步过程中进程触发IO操作并等待(也就是我们说的阻塞)或者轮询的去查看IO操作(也就是我们说的非阻塞)是否完成。 异步过程中进程触发IO操作以后，直接返回，做自己的事情，IO交给内核来处理，完成后内核通知进程IO完成。 同步和异步针对应用程序来，关注的是程序中间的协作关系；阻塞与非阻塞更关注的是单个进程的执行状态。 阻塞、非阻塞、多路IO复用，都是同步IO，异步必定是非阻塞的，所以不存在异步阻塞和异步非阻塞的说法。真正的异步IO需要CPU的深度参与。换句话说，只有用户线程在操作IO的时候根本不去考虑IO的执行全部都交给CPU去完成，而自己只等待一个完成信号的时候，才是真正的异步IO。所以，拉一个子线程去轮询、去死循环，或者使用select、poll、epool，都不是异步。 一个IO操作其实分成了两个步骤：发起IO请求和实际的IO操作，同步IO和异步IO的区别就在于第二个步骤是否阻塞，如果实际的IO读写阻塞请求进程，那么就是同步IO，因此阻塞IO、非阻塞IO、IO复用、信号驱动IO都是同步IO，如果不阻塞，而是操作系统帮你做完IO操作再将结果返回给你，那么就是异步IO。阻塞IO和非阻塞IO的区别在于第一步，发起IO请求是否会被阻塞，如果阻塞直到完成那么就是传统的阻塞IO，如果不阻塞，那么就是非阻塞IO。 ","date":"2019-03-22","objectID":"/posts/linux_io/:0:13","tags":["linux"],"title":"linux io模型","uri":"/posts/linux_io/"},{"categories":["笔记"],"content":"select int select (int n, fd_set *readfds, fd_set *writefds, fd_set *exceptfds, struct timeval *timeout); select 函数监视的文件描述符分3类，分别是writefds、readfds、和exceptfds。调用后select函数会阻塞，直到有描述副就绪（有数据 可读、可写、或者有except），或者超时（timeout指定等待时间，如果立即返回设为null即可），函数返回。当select函数返回后，可以 通过遍历fdset，来找到就绪的描述符。 select目前几乎在所有的平台上支持，其良好跨平台支持也是它的一个优点。select的一 个缺点在于单个进程能够监视的文件描述符的数量存在最大限制，在Linux上一般为1024，可以通过修改宏定义甚至重新编译内核的方式提升这一限制，但 是这样也会造成效率的降低。 ","date":"2019-03-22","objectID":"/posts/linux_io/:0:14","tags":["linux"],"title":"linux io模型","uri":"/posts/linux_io/"},{"categories":["笔记"],"content":"poll int poll (struct pollfd *fds, unsigned int nfds, int timeout); 不同与select使用三个位图来表示三个fdset的方式，poll使用一个 pollfd的指针实现。 struct pollfd { int fd; /* file descriptor */ short events; /* requested events to watch */ short revents; /* returned events witnessed */ }; pollfd结构包含了要监视的event和发生的event，不再使用select“参数-值”传递的方式。同时，pollfd并没有最大数量限制（但是数量过大后性能也是会下降）。 和select函数一样，poll返回后，需要轮询pollfd来获取就绪的描述符。 从上面看，select和poll都需要在返回后，通过遍历文件描述符来获取已经就绪的socket。事实上，同时连接的大量客户端在一时刻可能只有很少的处于就绪状态，因此随着监视的描述符数量的增长，其效率也会线性下降。 ","date":"2019-03-22","objectID":"/posts/linux_io/:0:15","tags":["linux"],"title":"linux io模型","uri":"/posts/linux_io/"},{"categories":["笔记"],"content":"epoll epoll是在2.6内核中提出的，是之前的select和poll的增强版本。相对于select和poll来说，epoll更加灵活，没有描述符限制。epoll使用一个文件描述符管理多个描述符，将用户关系的文件描述符的事件存放到内核的一个事件表中，这样在用户空间和内核空间的copy只需一次。 epoll操作过程需要三个接口，分别如下： int epoll_create(int size)；//创建一个epoll的句柄，size用来告诉内核这个监听的数目一共有多大 int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event)； int epoll_wait(int epfd, struct epoll_event * events, int maxevents, int timeout); epoll工作模式 　epoll对文件描述符的操作有两种模式：LT（水平触发，level trigger）和ET（边缘触发，edge trigger）。LT模式是默认模式，LT模式与ET模式的区别如下： * LT模式：当epoll_wait检测到描述符事件发生并将此事件通知应用程序，应用程序可以不立即处理该事件。下次调用epoll_wait时，会再次响应应用程序并通知此事件。 * ET模式：当epoll_wait检测到描述符事件发生并将此事件通知应用程序，应用程序必须立即处理该事件。如果不处理，下次调用epoll_wait时，不会再次响应应用程序并通知此事件。 1. LT模式 LT(level triggered)是缺省的工作方式，并且同时支持block和no-block socket.在这种做法中，内核告诉你一个文件描述符是否就绪了，然后你可以对这个就绪的fd进行IO操作。如果你不作任何操作，内核还是会继续通知你的。 2. ET模式 ET(edge-triggered)是高速工作方式，只支持no-block socket。在这种模式下，当描述符从未就绪变为就绪时，内核通过epoll告诉你。然后它会假设你知道文件描述符已经就绪，并且不会再为那个文件描述符发送更多的就绪通知，直到你做了某些操作导致那个文件描述符不再为就绪状态了(比如，你在发送，接收或者接收请求，或者发送接收的数据少于一定量时导致了一个EWOULDBLOCK 错误）。但是请注意，如果一直不对这个fd作IO操作(从而导致它再次变成未就绪)，内核不会发送更多的通知(only once) ET模式在很大程度上减少了epoll事件被重复触发的次数，因此效率要比LT模式高。epoll工作在ET模式的时候，必须使用非阻塞套接口，以避免由于一个文件句柄的阻塞读/阻塞写操作把处理多个文件描述符的任务饿死。 epoll总结 在 select/poll中，进程只有在调用一定的方法后，内核才对所有监视的文件描述符进行扫描，而epoll事先通过epoll_ctl()来注册一 个文件描述符，一旦基于某个文件描述符就绪时，内核会采用类似callback的回调机制，迅速激活这个文件描述符，当进程调用epoll_wait() 时便得到通知。(此处去掉了遍历文件描述符，而是通过监听回调的的机制。这正是epoll的魅力所在。) ","date":"2019-03-22","objectID":"/posts/linux_io/:0:16","tags":["linux"],"title":"linux io模型","uri":"/posts/linux_io/"},{"categories":["笔记"],"content":"epoll比select高效 监视的描述符数量不受限制，它所支持的FD上限是最大可以打开文件的数目，这个数字一般远大于2048,举个例子,在1GB内存的机器上大约是10万左 右，具体数目可以cat /proc/sys/fs/file-max察看,一般来说这个数目和系统内存关系很大。select的最大缺点就是进程打开的fd是有数量限制的。这对 于连接数量比较大的服务器来说根本不能满足。虽然也可以选择多进程的解决方案( Apache就是这样实现的)，不过虽然linux上面创建进程的代价比较小，但仍旧是不可忽视的，加上进程间数据同步远比不上线程间同步的高效，所以也不是一种完美的方案。 IO的效率不会随着监视fd的数量的增长而下降。epoll不同于select和poll轮询的方式，而是通过每个fd定义的回调函数来实现的。只有就绪的fd才会执行回调函数。 如果没有大量的idle -connection或者dead-connection，epoll的效率并不会比select/poll高很多，但是当遇到大量的idle- connection，就会发现epoll的效率大大高于select/poll。 ","date":"2019-03-22","objectID":"/posts/linux_io/:0:17","tags":["linux"],"title":"linux io模型","uri":"/posts/linux_io/"},{"categories":["笔记"],"content":"总结 形象的例子： 有多个公司快递要来，比如申通、中通、韵达等。 同步非阻塞：打电话给申通，问申通的来没有，如果没有，挂电话，再打过去，问来没有，一直这样。毫无疑问，这样会严重浪费快递员的时间，也就是CPU的资源。 同步阻塞：网购后在家等着快递（自己被阻塞了，被“定”住了），申通快递到了会给我打电话（唤醒），我在也不用一直打扰快递员了（CPU）。 同步阻塞虽然比非阻塞忙轮询好，但是同一时间只能接收一个人快递员的电话，如果同时给我打电话的话，我需要分身（创建多个线程），各分身负责等一个电话。 io多路复用：可以找一个快递接收点，接收员叫select,有快递到时候他会联系我，但是select只是告诉我有快递，具体哪个快递来了，我还要再依次跟各快递员打电话问一遍哪个快递到了。epoll接收员比较负责任，他直接就能告诉我是哪个快递来了。 异步：快递送到家并给我拆开，我什么都不用管。只需要给我发个短信说他干完了就行了。 参考资料： https://segmentfault.com/a/1190000003063859 ","date":"2019-03-22","objectID":"/posts/linux_io/:0:18","tags":["linux"],"title":"linux io模型","uri":"/posts/linux_io/"},{"categories":["笔记"],"content":"linux","date":"2019-03-22","objectID":"/posts/linux_%E5%8E%8B%E7%BC%A9/","tags":["linux"],"title":"linux-压缩命令","uri":"/posts/linux_%E5%8E%8B%E7%BC%A9/"},{"categories":["笔记"],"content":"gzip ","date":"2019-03-22","objectID":"/posts/linux_%E5%8E%8B%E7%BC%A9/:0:0","tags":["linux"],"title":"linux-压缩命令","uri":"/posts/linux_%E5%8E%8B%E7%BC%A9/"},{"categories":["笔记"],"content":"压缩单个文件 gzip命令的使用方式很简单，命令后直接跟输入文件即可，gzip命令压缩后默认会覆盖源文件，生成以.gz为后缀的文件。 命令加-k参数，表示keep保留源文件 gzip -k a.txt ","date":"2019-03-22","objectID":"/posts/linux_%E5%8E%8B%E7%BC%A9/:1:0","tags":["linux"],"title":"linux-压缩命令","uri":"/posts/linux_%E5%8E%8B%E7%BC%A9/"},{"categories":["笔记"],"content":"压缩目录下所有文件 -r参数表示递归压缩目录下每一个文件的作用，gzip命令只能压缩单个文件，即使压缩目录，也只是压缩目录下的每一个文件。(这里讲gzip只能压缩单个文件，并不是一次只能压缩一个文件，而是压缩的单位是单个文件，即并不能将多个文件压缩成为一个文件。) ls directory a b gzip -r directory ls directory a.gz b.gz ","date":"2019-03-22","objectID":"/posts/linux_%E5%8E%8B%E7%BC%A9/:2:0","tags":["linux"],"title":"linux-压缩命令","uri":"/posts/linux_%E5%8E%8B%E7%BC%A9/"},{"categories":["笔记"],"content":"解压缩单个文件 解压缩有两种方式，可以使用gzip -d或者gunzip完成解压缩操作。 同理，可以使用-k参数保留源文件 gunzip -k dd.txt.gz bzip2 相对于gzip，bzip2是一个压缩效率更高的命令，压缩后文件占据的空间更小，所以需要的压缩时间要比gzip更久，bzip2的使用方式与gzip基本相同。 bzip2 a // 压缩 bzip2 -dk a.bz2 // 解压 bunzip2 a.bz2 // 解压 bzip2命令的压缩和解压方式与gzip相同，且同样通过-k参数保留源文件。压缩后生成以.bz2为后缀的文件。 bzip2命令对目录的压缩同样是压缩目录下每一个文件，不过bzip2命令并没有提供-r参数，所以无法递归的对目录下文件进行压缩与解压操作。 zip ","date":"2019-03-22","objectID":"/posts/linux_%E5%8E%8B%E7%BC%A9/:3:0","tags":["linux"],"title":"linux-压缩命令","uri":"/posts/linux_%E5%8E%8B%E7%BC%A9/"},{"categories":["笔记"],"content":"zip压缩 zip命令的压缩率要低于bzip2和gzip，不过使用较为广泛，且兼容性较好。 zip dest.txt.zip src.txt unzip test.zip zip命令生成以.zip为后缀的压缩文件，使用-r参数完成对目录的递归压缩，且默认情况下不删除源文件。zip命令提供有-m参数，用于删除源文件，-m表示move移动源文件到压缩包中的意思。 ","date":"2019-03-22","objectID":"/posts/linux_%E5%8E%8B%E7%BC%A9/:4:0","tags":["linux"],"title":"linux-压缩命令","uri":"/posts/linux_%E5%8E%8B%E7%BC%A9/"},{"categories":["笔记"],"content":"unzip解压缩 与压缩操作相同，解压缩操作同样不删除源文件，所以若目录下存在同名文件时，会出现是否更新文件的提示。 使用-o参数忽略提示，直接更新同名文件，-o表示overwrite覆盖同名文件的意思。unzip同时提供-n参数，忽略提示并不更新同名文件，-n表示never覆盖同名文件的意思。 打包和压缩 打包是指将一大堆文件或目录什么的变成一个总的文件，压缩则是将一个大的文件通过一些压缩算法变成一个小文件。 Linux中的很多压缩程序只能针对一个文件进行压缩，这样当你想要压缩一大堆文件时，你就得先借助另它的工具将这一大堆文件先打成一个包，然后再就原来的压缩程序进行压缩。 Linux下最常用的打包程序就是tar了，使用tar程序打出来的包我们常称为tar包，tar包文件的命令通常都是以.tar结尾的。生成tar包后，就可以用其它的程序来进行压缩了，所以首先就来讲讲tar命令的基本用法： tar命令的选项有很多(用man tar可以查看到)，但常用的就那么几个选项，下面来举例说明一下： *tar -cf all.tar .jpg 这条命令是将所有.jpg的文件打成一个名为all.tar的包。-c是表示产生新的包，-f指定包的文件名。 tar -xf all.tar 这条命令是解出all.tar包中所有文件，-x是解开的意思。 以上就是tar的最基本的用法。为了方便用户在打包解包的同时可以压缩或解压文件，tar提供了一种特殊的功能。这就是tar可以在打包或解包的同时调用其它的压缩程序，比如调用gzip、bzip2等。 ","date":"2019-03-22","objectID":"/posts/linux_%E5%8E%8B%E7%BC%A9/:5:0","tags":["linux"],"title":"linux-压缩命令","uri":"/posts/linux_%E5%8E%8B%E7%BC%A9/"},{"categories":["笔记"],"content":"tar调用压缩程序 tar中使用-z这个参数来调用gzip。下面来举例说明一下： *tar -czf all.tar.gz .jpg 这条命令是将所有.jpg的文件打成一个tar包，并且将其用gzip压缩，生成一个gzip压缩过的包，包名为all.tar.gz tar -xzf all.tar.gz 这条命令是将上面产生的包解开。 tar中使用-j这个参数来调用gzip。下面来举例说明一下： *tar -cjf all.tar.bz2 .jpg 这条命令是将所有.jpg的文件打成一个tar包，并且将其用bzip2压缩，生成一个bzip2压缩过的包，包名为all.tar.bz2 tar -xjf all.tar.bz2 这条命令是将上面产生的包解开。 ","date":"2019-03-22","objectID":"/posts/linux_%E5%8E%8B%E7%BC%A9/:6:0","tags":["linux"],"title":"linux-压缩命令","uri":"/posts/linux_%E5%8E%8B%E7%BC%A9/"},{"categories":["笔记"],"content":"nsq消息队列","date":"2019-03-22","objectID":"/posts/mq_nsq/","tags":["mq"],"title":"nsq消息队列","uri":"/posts/mq_nsq/"},{"categories":["笔记"],"content":"nsq消息队列 NSQ是一个基于Go语言的分布式实时消息平台，它基于MIT开源协议发布，由bitly公司开源出来的一款简单易用的消息中间件。 ","date":"2019-03-22","objectID":"/posts/mq_nsq/:0:1","tags":["mq"],"title":"nsq消息队列","uri":"/posts/mq_nsq/"},{"categories":["笔记"],"content":"Features 特点 Distributed NSQ提供了分布式的，去中心化，且没有单点故障的拓扑结构，稳定的消息传输发布保障，能够具有高容错和HA（高可用）特性。 Scalable易于扩展 NSQ支持水平扩展，没有中心化的brokers。内置的发现服务简化了在集群中增加节点。同时支持pub-sub和load-balanced 的消息分发。 Ops Friendly NSQ非常容易配置和部署，生来就绑定了一个管理界面。二进制包没有运行时依赖。官方有Docker image。 Integrated高度集成 官方的 Go 和 Python库都有提供。而且为大多数语言提供了库。 可延时接收 ","date":"2019-03-22","objectID":"/posts/mq_nsq/:0:2","tags":["mq"],"title":"nsq消息队列","uri":"/posts/mq_nsq/"},{"categories":["笔记"],"content":"组件 Topic ：一个topic就是程序发布消息的一个逻辑键，当程序第一次发布消息时就会创建topic。 Channels ：channel与消费者相关，是消费者之间的负载均衡，channel在某种意义上来说是一个“队列”。每当一个发布者发送一条消息到一个topic，消息会被复制到所有消费者连接的channel上，消费者通过这个特殊的channel读取消息，实际上，在消费者第一次订阅时就会创建channel。Channel会将消息进行排列，如果没有消费者读取消息，消息首先会在内存中排队，当量太大时就会被保存到磁盘中。 Messages：消息构成了我们数据流的中坚力量，消费者可以选择结束消息，表明它们正在被正常处理，或者重新将他们排队待到后面再进行处理。每个消息包含传递尝试的次数，当消息传递超过一定的阀值次数时，我们应该放弃这些消息，或者作为额外消息进行处理。 nsqd：nsqd 是一个守护进程，负责接收，排队，投递消息给客户端。它可以独立运行，不过通常它是由 nsqlookupd 实例所在集群配置的（它在这能声明 topics 和 channels，以便大家能找到）。 nsqlookupd：nsqlookupd 是守护进程负责管理拓扑信息。客户端通过查询 nsqlookupd 来发现指定话题（topic）的生产者，并且 nsqd 节点广播话题（topic）和通道（channel）信息。有两个接口：TCP 接口，nsqd 用它来广播。HTTP 接口，客户端用它来发现和管理。 nsqadmin：nsqadmin 是一套 WEB UI，用来汇集集群的实时统计，并执行不同的管理任务。 常用工具类： nsq_to _file：消费指定的话题（topic）/通道（channel），并写到文件中，有选择的滚动和/或压缩文件。 nsq_to _http：消费指定的话题（topic）/通道（channel）和执行 HTTP requests (GET/POST) 到指定的端点。 nsq_to _nsq：消费者指定的话题/通道和重发布消息到目的地 nsqd 通过 TCP。 组件通讯 [文件] nsqd 分发数据 这是官方的图，第一个channel(meteics)因为有多个消费者，所以触发了负载均衡机制。后面两个channel由于没有消费者，所有的message均会被缓存在相应的队列里，直到消费者出现 ","date":"2019-03-22","objectID":"/posts/mq_nsq/:0:3","tags":["mq"],"title":"nsq消息队列","uri":"/posts/mq_nsq/"},{"categories":["笔记"],"content":"运行服务 首先启动nsqlookud nsqlookupd 启动nsqd，并接入刚刚启动的nsqlookud。这里为了方便接下来的测试，启动了两个nsqd nsqd –lookupd-tcp-address=127.0.0.1:4160 nsqd –lookupd-tcp-address=127.0.0.1:4160 -tcp-address=0.0.0.0:4152 -http-address=0.0.0.0:4153 启动nqsadmin（不是必须的） nsqadmin –lookupd-http-address=127.0.0.1:4161 go使用nsq nsq提供了go客户端库 生产者： var producer *nsq.Producer func main() { nsqd := \"127.0.0.1:4150\" producer, err := nsq.NewProducer(nsqd, nsq.NewConfig()) for i := 0; i \u003c 5; i++ { producer.Publish(\"test\", []byte(\"nihao\")) } if err != nil { panic(err) } } 消费者： import ( \"fmt\" \"github.com/nsqio/go-nsq\" \"sync\" ) type NSQHandler struct { id int } func (this *NSQHandler) HandleMessage(msg *nsq.Message) error { fmt.Println(this.id,\"receive\", msg.NSQDAddress, \"message:\", string(msg.Body)) return nil } func testNSQ() { waiter := sync.WaitGroup{} waiter.Add(1) go func() { defer waiter.Done() config:=nsq.NewConfig() config.MaxInFlight=9 //建立多个连接 for i := 0; i\u003c10; i++ { consumer, err := nsq.NewConsumer(\"test\", \"struggle\", config) if nil != err { fmt.Println(\"err\", err) return } consumer.AddHandler(\u0026NSQHandler{i}) err = consumer.ConnectToNSQD(\"127.0.0.1:4150\") if nil != err { fmt.Println(\"err\", err) return } } select{} }() waiter.Wait() } func main() { testNSQ() } ","date":"2019-03-22","objectID":"/posts/mq_nsq/:0:4","tags":["mq"],"title":"nsq消息队列","uri":"/posts/mq_nsq/"},{"categories":["笔记"],"content":"总结 事实上，简单性是我们决定使用NSQ的首要因素，这方便与我们的许多其他软件一起维护，通过引入队列使我们得到了堪称完美的表现，通过队列甚至让我们增加了几个数量级的吞吐量。越来越多的consumer需要一套严格可靠性和顺序性保障，这已经超过了NSQ提供的简单功能。 NSQ 的 TCP 协议是面向 push 的。另外它是无序的，可能有重复数据，这个要根据实际需求考虑。 ","date":"2019-03-22","objectID":"/posts/mq_nsq/:0:5","tags":["mq"],"title":"nsq消息队列","uri":"/posts/mq_nsq/"},{"categories":["笔记"],"content":"rabbitMQ","date":"2019-03-22","objectID":"/posts/mq_rabbitmq/","tags":["mq"],"title":"rabbitMQ","uri":"/posts/mq_rabbitmq/"},{"categories":["笔记"],"content":"简介 RabbitMQ是一个实现了AMQP（Advanced Message Queuing Protocol）高级消息队列协议的消息队列服务，用Erlang语言的。 在我们秒杀抢购商品的时候，系统会提醒我们稍等排队中，而不是像几年前一样页面卡死或报错给用户。 像这种排队结算就用到了消息队列机制，放入通道里面一个一个结算处理，而不是某个时间断突然涌入大批量的查询新增把数据库给搞宕机，所以RabbitMQ本质上起到的作用就是削峰填谷，为业务保驾护航。 ","date":"2019-03-22","objectID":"/posts/mq_rabbitmq/:0:1","tags":["mq"],"title":"rabbitMQ","uri":"/posts/mq_rabbitmq/"},{"categories":["笔记"],"content":"为什么选择RabbitMQ 现在的市面上有很多MQ可以选择，比如ActiveMQ、ZeroMQ、Appche Qpid，那问题来了为什么要选择RabbitMQ？ 除了Qpid，RabbitMQ是唯一一个实现了AMQP标准的消息服务器； 可靠性，RabbitMQ的持久化支持，保证了消息的稳定性； 高并发，RabbitMQ使用了Erlang开发语言，Erlang是为电话交换机开发的语言，天生自带高并发光环，和高可用特性； 集群部署简单，正是应为Erlang使得RabbitMQ集群部署变的超级简单； 社区活跃度高，根据网上资料来看，RabbitMQ也是首选； ","date":"2019-03-22","objectID":"/posts/mq_rabbitmq/:0:2","tags":["mq"],"title":"rabbitMQ","uri":"/posts/mq_rabbitmq/"},{"categories":["笔记"],"content":"工作机制 生产者、消费者和代理 在了解消息通讯之前首先要了解3个概念：生产者、消费者和代理。 生产者：消息的创建者，负责创建和推送数据到消息服务器； 消费者：消息的接收方，用于处理数据和确认消息； 代理：就是RabbitMQ本身，用于扮演“快递”的角色，本身不生产消息，只是扮演“快递”的角色。 消息发送原理 首先你必须连接到Rabbit才能发布和消费消息，那怎么连接和发送消息的呢？ 你的应用程序和Rabbit Server之间会创建一个TCP连接，一旦TCP打开，并通过了认证，认证就是你试图连接Rabbit之前发送的Rabbit服务器连接信息和用户名和密码，有点像程序连接数据库，一旦认证通过你的应用程序和Rabbit就创建了一条AMQP信道（Channel）。 信道是创建在“真实”TCP上的虚拟连接，AMQP命令都是通过信道发送出去的，每个信道都会有一个唯一的ID，不论是发布消息，订阅队列或者介绍消息都是通过信道完成的。 为什么不通过TCP直接发送命令？ 对于操作系统来说创建和销毁TCP会话是非常昂贵的开销，假设高峰期每秒有成千上万条连接，每个连接都要创建一条TCP会话，这就造成了TCP连接的巨大浪费，而且操作系统每秒能创建的TCP也是有限的，因此很快就会遇到系统瓶颈。 如果我们每个请求都使用一条TCP连接，既满足了性能的需要，又能确保每个连接的私密性，这就是引入信道概念的原因。 ","date":"2019-03-22","objectID":"/posts/mq_rabbitmq/:0:3","tags":["mq"],"title":"rabbitMQ","uri":"/posts/mq_rabbitmq/"},{"categories":["笔记"],"content":"Rabbit的名词 想要真正的了解Rabbit有些名词是你必须知道的。 包括：ConnectionFactory（连接管理器）、Channel（信道）、Exchange（交换器）、Queue（队列）、RoutingKey（路由键）、BindingKey（绑定键）、message、Broker、虚拟主机 ConnectionFactory（连接管理器）：应用程序与Rabbit之间建立连接的管理器，程序代码中使用； Channel（信道）：消息推送使用的通道； Exchange（交换器）：用于接受、分配消息；可以将交换器理解成一个由绑定构成的路由表。 Queue（队列）：用于存储生产者的消息；一个消息可投入一个或多个队列。消息一直在队列里面，等待消费者连接到这个队列将其取走。 RoutingKey（路由键）：用于把生成者的数据分配到交换器上； BindingKey（绑定键）：用于把交换器的消息绑定到队列上； Message消息：消息是不具名的，它由消息头和消息体组成。消息体是不透明的，而消息头则由一系列的可选属性组成，这些属性包括routing-key（路由键）、priority（相对于其他消息的优先权）、delivery-mode（指出该消息可能需要持久性存储）等。 Broker 表示消息队列服务器实体。 虚拟主机，每个Rabbit都能创建很多vhost，我们称之为虚拟主机，每个虚拟主机其实都是mini版的RabbitMQ，拥有自己的队列，交换器和绑定，拥有自己的权限机制。 RabbitMQ默认的vhost是“/”开箱即用； 多个vhost是隔离的，多个vhost无法通讯，并且不用担心命名冲突（队列和交换器和绑定），实现了多层分离； 创建用户的时候必须指定vhost； ","date":"2019-03-22","objectID":"/posts/mq_rabbitmq/:0:4","tags":["mq"],"title":"rabbitMQ","uri":"/posts/mq_rabbitmq/"},{"categories":["笔记"],"content":"Exchange 类型 Exchange分发消息时根据类型的不同分发策略有区别，目前共四种类型：direct、fanout、topic、headers 。headers 匹配 AMQP 消息的 header 而不是路由键，此外 headers 交换器和 direct 交换器完全一致，但性能差很多，目前几乎用不到了，所以直接看另外三种类型： direct类型 消息中的路由键（routing key）如果和 Binding 中的 binding key 一致， 交换器就将消息发到对应的队列中。路由键与队列名完全匹配，如果一个队列绑定到交换机要求路由键为“dog”，则只转发 routing key 标记为“dog”的消息，不会转发“dog.puppy”，也不会转发“dog.guard”等等。它是完全匹配、单播的模式。 接收相关 当接收端订阅者有多个的时候，direct会轮询公平的分发给每个订阅者（订阅者消息确认正常 消息接收到之后必须使用channel.basicAck()方法手动确认（非自动确认删除模式下），那么问题来了。 消息收到未确认会怎么样？ 如果应用程序接收了消息，因为bug忘记确认接收的话，消息在队列的状态会从“Ready”变为“Unacked”。 如果消息收到却未确认，Rabbit将不会再给这个应用程序发送更多的消息了，这是因为Rabbit认为你没有准备好接收下一条消息。 此条消息会一直保持Unacked的状态，直到你确认了消息，或者断开与Rabbit的连接，Rabbit会自动把消息改完Ready状态，分发给其他订阅者。 消息拒绝 消息在确认之前，可以有两个选择： 选择1：断开与Rabbit的连接，这样Rabbit会重新把消息分派给另一个消费者； 选择2：拒绝Rabbit发送的消息使用channel.basicReject(long deliveryTag, boolean requeue)，参数1：消息的id；参数2：处理消息的方式，如果是true，Rabbib会重新分配这个消息给其他订阅者，如果设置成false的话，Rabbit会把消息发送到一个特殊的“死信”队列，用来存放被拒绝而不重新放入队列的消息。 fanout类型 每个发到 fanout 类型交换器的消息都会分到所有绑定的队列上去。fanout 交换器不处理路由键，只是简单的将队列绑定到交换器上，每个发送到交换器的消息都会被转发到与该交换器绑定的所有队列上。很像子网广播，每台子网内的主机都获得了一份复制的消息。fanout 类型转发消息是最快的。 当你发送一条消息的时候，交换器会把消息广播到所有附加到这个交换器的队列上。 比如用户上传了自己的头像，这个时候图片需要清除缓存，同时用户应该得到积分奖励，你可以把这两个队列绑定到图片上传的交换器上，这样当有第三个、第四个上传完图片需要处理的需求的时候，原来的代码可以不变，只需要添加一个订阅消息即可，这样发送方和消费者的代码完全解耦，并可以轻而易举的添加新功能了。 接收相关 接受消息不同于direct，我们需要声明fanout路由器，并使用默认的队列绑定到fanout交换器上。 topic类型 topic 交换器通过模式匹配分配消息的路由键属性，将路由键和某个模式进行匹配，此时队列需要绑定到一个模式上。它将路由键和绑定键的字符串切分成单词，这些单词之间用点隔开。 假设我们现在有一个日志系统，会把所有日志级别的日志发送到交换器，warning、log、error、fatal，但我们只想处理error以上的日志，要怎么处理？这就需要使用topic路由器了。 topic路由器的关键在于定义路由键，定义routingKey名称不能超过255字节，使用“.”作为分隔符，例如：com.mq.rabbit.error。 消费消息的时候routingKey可以使用下面字符匹配消息： “*“匹配一个分段(用“.”分割)的内容； “#“匹配0和多个字符； 例如发布了一个“com.mq.rabbit.error”的消息： 能匹配上的路由键： cn.mq.rabbit.* cn.mq.rabbit.# #.error cn.mq.# # 不能匹配上的路由键： cn.mq.* *.error * 所以如果想要订阅所有消息，可以使用“#”匹配。 fanout、topic交换器是没有历史数据的，也就是说对于中途创建的队列，获取不到之前的消息。 ","date":"2019-03-22","objectID":"/posts/mq_rabbitmq/:0:5","tags":["mq"],"title":"rabbitMQ","uri":"/posts/mq_rabbitmq/"},{"categories":["笔记"],"content":"RabbitMQ 集群 RabbitMQ 最优秀的功能之一就是内建集群，这个功能设计的目的是允许消费者和生产者在节点崩溃的情况下继续运行，以及通过添加更多的节点来线性扩展消息通信吞吐量。RabbitMQ 内部利用 Erlang 提供的分布式通信框架 OTP 来满足上述需求，使客户端在失去一个 RabbitMQ 节点连接的情况下，还是能够重新连接到集群中的任何其他节点继续生产、消费消息。 RabbitMQ 集群中的一些概念 RabbitMQ 会始终记录以下四种类型的内部元数据： 队列元数据 包括队列名称和它们的属性，比如是否可持久化，是否自动删除 交换器元数据 交换器名称、类型、属性 绑定元数据 内部是一张表格记录如何将消息路由到队列 vhost 元数据 为 vhost 内部的队列、交换器、绑定提供命名空间和安全属性 在单一节点中，RabbitMQ 会将所有这些信息存储在内存中，同时将标记为可持久化的队列、交换器、绑定存储到硬盘上。存到硬盘上可以确保队列和交换器在节点重启后能够重建。而在集群模式下同样也提供两种选择：存到硬盘上（独立节点的默认设置），存在内存中。 如果在集群中创建队列，集群只会在单个节点而不是所有节点上创建完整的队列信息（元数据、状态、内容）。结果是只有队列的所有者节点知道有关队列的所有信息，因此当集群节点崩溃时，该节点的队列和绑定就消失了，并且任何匹配该队列的绑定的新消息也丢失了。还好RabbitMQ 2.6.0之后提供了镜像队列以避免集群节点故障导致的队列内容不可用。 RabbitMQ 集群中可以共享 user、vhost、exchange等，所有的数据和状态都是必须在所有节点上复制的，例外就是上面所说的消息队列。RabbitMQ 节点可以动态的加入到集群中。 当在集群中声明队列、交换器、绑定的时候，这些操作会直到所有集群节点都成功提交元数据变更后才返回。集群中有内存节点和磁盘节点两种类型，内存节点虽然不写入磁盘，但是它的执行比磁盘节点要好。内存节点可以提供出色的性能，磁盘节点能保障配置信息在节点重启后仍然可用，那集群中如何平衡这两者呢？ RabbitMQ 只要求集群中至少有一个磁盘节点，所有其他节点可以是内存节点，当节点加入火离开集群时，它们必须要将该变更通知到至少一个磁盘节点。如果只有一个磁盘节点，刚好又是该节点崩溃了，那么集群可以继续路由消息，但不能创建队列、创建交换器、创建绑定、添加用户、更改权限、添加或删除集群节点。换句话说集群中的唯一磁盘节点崩溃的话，集群仍然可以运行，但知道该节点恢复，否则无法更改任何东西。 ","date":"2019-03-22","objectID":"/posts/mq_rabbitmq/:0:6","tags":["mq"],"title":"rabbitMQ","uri":"/posts/mq_rabbitmq/"},{"categories":["笔记"],"content":"消息持久化 当你把消息发送到Rabbit服务器的时候，你需要选择你是否要进行持久化，但这并不能保证Rabbit能从崩溃中恢复，想要Rabbit消息能恢复必须满足条件： 投递消息的时候durable设置为true，消息持久化，代码：channel.queueDeclare(x, true, false, false, null)，参数2设置为true持久化； 设置投递模式deliveryMode设置为2（持久），代码：channel.basicPublish(x, x, MessageProperties.PERSISTENT_TEXT_PLAIN,x)，参数3设置为存储纯文本到磁盘； 消息已经到达持久化交换器上； 消息已经到达持久化的队列； Rabbit会将你的持久化消息写入磁盘上的持久化日志文件，等消息被消费之后，Rabbit会把这条消息标识为等待垃圾回收。 持久化的缺点 消息持久化的优点显而易见，但缺点也很明显，那就是性能，因为要写入硬盘要比写入内存性能较低很多，从而降低了服务器的吞吐量，尽管使用SSD硬盘可以使事情得到缓解，但他仍然吸干了Rabbit的性能，当消息成千上万条要写入磁盘的时候，性能是很低的。 如果要保证消息的可靠性，需要对消息进行持久化处理，然而消息持久化除了需要代码的设置之外，还有一个重要步骤是至关重要的，那就是保证你的消息顺利进入Broker（代理服务器），如图所示： 正常情况下，如果消息经过交换器进入队列就可以完成消息的持久化，但如果消息在没有到达broker之前出现意外，那就造成消息丢失，有没有办法可以解决这个问题？ RabbitMQ有两种方式来解决这个问题： 通过AMQP提供的事务机制实现； 使用发送者确认模式实现； ","date":"2019-03-22","objectID":"/posts/mq_rabbitmq/:0:7","tags":["mq"],"title":"rabbitMQ","uri":"/posts/mq_rabbitmq/"},{"categories":["笔记"],"content":"面试中的一些问题 如何确保消息正确地发送至 RabbitMQ 发送方确认模式将信道设置成 confirm 模式（发送方确认模式），则所有在信道上发布的消息都会被指派一个唯一的 ID。一旦消息被投递到目的队列后，或者消息被写入磁盘后（可持久化的消息），信道会发送一个确认给生产者（包含消息唯一 ID）。如果 RabbitMQ 发生内部错误从而导致消息丢失，会发送一条 nack（notacknowledged，未确认）消息。发送方确认模式是异步的，生产者应用程序在等待确认的同时，可以继续发送消息。当确认消息到达生产者应用程序，生产者应用程序的回调方法就会被触发来处理确认消息。 如何确保消息接收方消费了消息？ 消费者接收每一条消息后都必须进行确认（消息接收和消息确认是两个不同操作）。只有消费者确认了消息，RabbitMQ 才能安全地把消息从队列中删除。这里并没有用到超时机制，RabbitMQ 仅通过 Consumer 的连接中断来确认是否需要重新发送消息。也就是说，只要连接不中断，RabbitMQ 给了 Consumer 足够长的时间来处理消息。保证数据的最终一致性；下面罗列几种特殊情况（1）如果消费者接收到消息，在确认之前断开了连接或取消订阅，RabbitMQ 会认为消息没有被分发，然后重新分发给下一个订阅的消费者。（可能存在消息重复消费的隐患，需要去重）（1）2如果消费者接收到消息却没有确认消息，连接也未断开，则 RabbitMQ 认为该消费者繁忙，将不会给该消费者分发更多的消息。 如何避免消息重复投递或重复消费？ 在消息生产时，MQ 内部针对每条生产者发送的消息生成一个 inner-msg-id，作为去重的依据（消息投递失败并重传），避免重复的消息进入队列；在消息消费时，要求消息体中必须要有一个 bizId（对于同一业务全局唯一，如支付 ID、订单 ID、帖子 ID 等）作为去重的依据，避免同一条消息被重复消费。 消息基于什么传输？ 由于 TCP 连接的创建和销毁开销较大，且并发数受系统资源限制，会造成性能瓶颈。RabbitMQ 使用信道的方式来传输数据。信道是建立在真实的 TCP 连接内的虚拟连接，且每条 TCP 连接上的信道数量没有限制。 消息如何分发？ 若该队列至少有一个消费者订阅，消息将以循环（round-robin）的方式发送给消费者。每条消息只会分发给一个订阅的消费者（前提是消费者能够正常处理消息并进行确认）。通过路由可实现多消费的功能 消息怎么路由？ 消息提供方-\u003e路由-\u003e一至多个队列消息发布到交换器时，消息将拥有一个路由键（routing key），在消息创建时设定。通过队列路由键，可以把队列绑定到交换器上。消息到达交换器后，RabbitMQ 会将消息的路由键与队列的路由键进行匹配（针对不同的交换器有不同的路由规则）；常用的交换器主要分为一下三种：fanout：如果交换器收到消息，将会广播到所有绑定的队列上direct：如果路由键完全匹配，消息就被投递到相应的队列topic：可以使来自不同源头的消息能够到达同一个队列。 使用 topic 交换器时，可以使用通配符 如何确保消息不丢失？ 消息持久化，当然前提是队列必须持久化RabbitMQ 确保持久性消息能从服务器重启中恢复的方式是，将它们写入磁盘上的一个持久化日志文件，当发布一条持久性消息到持久交换器上时，Rabbit 会在消息提交到日志文件后才发送响应。一旦消费者从持久队列中消费了一条持久化消息，RabbitMQ 会在持久化日志中把这条消息标记为等待垃圾收集。如果持久化消息在被消费之前 RabbitMQ 重启，那么 Rabbit 会自动重建交换器和队列（以及绑定），并重新发布持久化日志文件中的消息到合适的队列。 实现支付超时 订单30分钟未支付,系统自动超时关闭 原理:当我们在下单的时候,往MQ投递一个消息设置有效期为30分钟,但该消息失效的时候(没有被消费的情况下), 执行我们客户端一个方法告诉我们该消息已经失效,这时候查询这笔订单是否有支付. ","date":"2019-03-22","objectID":"/posts/mq_rabbitmq/:0:8","tags":["mq"],"title":"rabbitMQ","uri":"/posts/mq_rabbitmq/"},{"categories":["笔记"],"content":"xorm笔记","date":"2019-03-22","objectID":"/posts/go_xorm%E7%AC%94%E8%AE%B0/","tags":["golang"],"title":"xorm笔记","uri":"/posts/go_xorm%E7%AC%94%E8%AE%B0/"},{"categories":["笔记"],"content":"连接池 xorm可以指定数据库连接缓存池的空闲和活动连接大小。 engine.SetMaxOpenConns() engine.SetMaxIdleConns() 映射表和结构体 名字的映射 结构体和数据表映射 实现接口Conversion，自己实现映射规则 用xorm提供的驼峰SnakeMapper、同名SameMapper、GonicMapper。第一种是默认的，第三种是第一的升级版，可以对特定单词处理的更好。 我们还可以在上面三种的前提上再指定前缀、后缀 结构体拥有TableName() string的成员方法。还有字段tag 通过engine.Table()方法可以改变struct对应的数据库表的名称 上面几种方法都是指定表名或字段名映射的。他们是有优先级的： 对于表名：engine.Table() \u003e TableName() \u003e 映射器 对于字段名：tag \u003e 映射器 注意：字段名首字母必须大写，否则报错 字段类型及约束映射 type Person struct { Id int64 `xorm:\"pk autoincr\"` //主键自增，如果多个字段带pk就是联合主键 Name string `xorm:\"notnull unique\"` //唯一 Age int `xorm:\"index\"` //单字段索引 Y string `xorm:\"index(uindex)\"` //X Y为联合索引 X string `xorm:\"index(uindex)\"` LastTime time.Time `xorm:\"created updated\"` Anim `xorm:\"extends\"` //此结构体的所有成员也映射到数据库中，extends可加载无限级 Like Like `xorm:\"json\"` //内容将先转成Json格式，然后存储到数据库中 Version int `xorm:\"version\"` //会在insert时默认为1，每次更新自动加1 Group Group //表中对应group的主键id，group没主键就会报错，查询的时候会自动关联查找group的信息 } type Group struct { Id int `xorm:\"pk\"` Name string } type Anim struct { Height int `xorm:\"notnull\"` } type Like struct { LikeType string LikeName string } string对应的是varchar（255）如果觉得浪费，可以自己指定 查询 所有的查询条件不区分调用顺序，但必须在调用Get，Exist, Sum, Find，Count, Iterate, Rows这几个函数之前调用。同时需要注意的一点是，在调用的参数中，如果采用默认的SnakeMapper所有的字符字段名均为映射后的数据库的字段名，而不是field的名字。 临时开关 NoAutoTime() 此次操作禁止自动更新时间 NoCache() 此次非缓存 NoAutoCondition()禁用自动根据结构体中的值来生成条件 UseBool(…string) 默认bool类型不参与查询条件或更新内容 Get方法 查询单条数据使用Get方法，在调用Get方法时需要传入一个对应结构体的指针，同时结构体中的非空field自动成为查询的条件和前面的方法条件组合在一起查询。 Exist系列方法 判断某个记录是否存在可以使用Exist, 相比Get，Exist性能更好。exist不会把查询的结果赋值到参数中的字段。 Find方法 查询多条数据使用Find方法，Find方法的第一个参数为slice的指针或Map指针，即为查询后返回的结果，第二个参数可选，为查询的条件struct的指针。 传入Slice用于返回数据 everyone := make([]Userinfo, 0)//切片的指针 err := engine.Find(\u0026everyone) pEveryOne := make([]*Userinfo, 0)//元素为指针的切片 err := engine.Find(\u0026pEveryOne) 传入Map用户返回数据，map必须为map[int64]Userinfo的形式，map的key为id，因此对于复合主键无法使用这种方式。 users := make(map[int64]Userinfo) err := engine.Find(\u0026users) pUsers := make(map[int64]*Userinfo) err := engine.Find(\u0026pUsers) Join方法 join方法用于多表关联查询 Join(string,interface{},string) 第一个参数为连接类型，当前支持INNER, LEFT OUTER, CROSS中的一个值， 第二个参数为string类型的表名，表对应的结构体指针或者为两个值的[]string，表示表名和别名， 第三个参数为连接条件。 Sum系列方法 求和数据可以使用Sum, SumInt, Sums 和 SumsInt 四个方法，Sums系列方法的参数为struct的指针并且成为查询条件。 update更新 update如果有version会自动对version+1并比对version。 更新数据使用Update方法，Update方法的第一个参数为需要更新的内容，可以为一个结构体指针或者一个Map[string]interface{}类型。当传入的为结构体指针时，只有非空和0的field才会被作为更新的字段。当传入的为Map类型时，key为数据库Column的名字，value为要更新的内容。 这里需要注意，Update会自动从user结构体中提取非0和非nil得值作为需要更新的内容，因此，如果需要更新一个值为0，则此种方法将无法实现，因此有两种选择： 1.通过添加Cols函数指定需要更新结构体中的哪些值，未指定的将不更新，指定了的即使为0也会更新。 affected, err := engine.Id(id).Cols(“age”).Update(\u0026user) 2.通过传入map[string]interface{}来进行更新，但这时需要额外指定更新到哪个表，因为通过map是无法自动检测更新哪个表的。 删除 xorm可以软删除（逻辑删除） type User struct { Id int64 Name string DeletedAt time.Time `xorm:\"deleted\"` } 那么如果记录已经被标记为删除后，要真正的获得该条记录或者真正的删除该条记录，需要启用Unscoped。 原生sql查询 sql := “select * from userinfo” results, err := engine.Query(sql) 当调用Query时，第一个返回值results为[]map[string][]byte的形式。 这样再设置给结构体就麻烦了，可以用下面的这个方法： engine.SQL(\"\").Get() 原生sql增删改 也可以直接执行一个SQL命令，即执行Insert， Update， Delete 等操作。此时不管数据库是何种类型，都可以使用 ` 和 ? 符号。 sql = “update userinfo set username=? where id=?” res, err := engine.Exec(sql, “xiaolun”, 1) 事务 大多数情况下我们可以使用简单事务模型来进行事务处理。当使用简单事务模型进行事务处理时，需要创建Session对象。 session := engine.NewSession() defer session.Close() // add Begin() before any action err := session.Begin() user1 := Userinfo{Username: \"xiaoxiao\", Departname: \"dev\", Alias: \"lunny\", Created: time.Now()} _, err = session.Insert(\u0026user1) if err != nil { session.Rollback() return } user2 := Userinfo{Username: \"yyy\"} _, err = session.Where(\"id = ?\", 2).Update(\u0026user2) if err != nil { session.Rollback() return } _, err = session.Exec(\"delete from userinfo where username = ?\", user2.Username) if err != nil { session.Rollback() return } // add Commit() after all actions err = session.Commit() if err != nil { return } 如果我们没有提交，close（）时会自动rollback。 什么是嵌套事务？ 嵌套事务是一个外部事务的一个子事务，是一个外部事务的一个组成部分，当嵌套事务发生异常，而回滚，则会回复到嵌套事务的执行前的状态，相当于嵌套事务未执行。如果外部事务回滚，则嵌套事务也会回滚！！！外部事务提交的时候，它才会被提交。 ","date":"2019-03-22","objectID":"/posts/go_xorm%E7%AC%94%E8%AE%B0/:0:0","tags":["golang"],"title":"xorm笔记","uri":"/posts/go_xorm%E7%AC%94%E8%AE%B0/"},{"categories":["笔记"],"content":"mysql分库分表分区","date":"2019-03-07","objectID":"/posts/mysql_%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8%E5%88%86%E5%8C%BA/","tags":["mysql"],"title":"mysql分库分表分区","uri":"/posts/mysql_%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8%E5%88%86%E5%8C%BA/"},{"categories":["笔记"],"content":"传统的分库分表都是通过应用层逻辑实现的，对于数据库层面来说，都是普通的表和库。分区是数据库层面的。 ","date":"2019-03-07","objectID":"/posts/mysql_%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8%E5%88%86%E5%8C%BA/:0:0","tags":["mysql"],"title":"mysql分库分表分区","uri":"/posts/mysql_%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8%E5%88%86%E5%8C%BA/"},{"categories":["笔记"],"content":"分库 database不是文件，只起到namespace的作用，所以MySQL对database大小当然也是没有限制的，而且对里面的表数量也没有限制。为什么要分库呢？ 为了解决单台服务器的性能问题，当单台数据库服务器无法支撑当前的数据量时，就需要根据业务逻辑紧密程度把表分成几撮，分别放在不同的数据库服务器中以降低单台服务器的负载。 比如一个论坛系统的数据库因当前服务器性能无法满足需要进行分库。先垂直切分，按业务逻辑把用户相关数据表比如用户信息、积分、用户间私信等放入user数据库；论坛相关数据表比如板块，帖子，回复等放入forum数据库，两个数据库放在不同服务器上。 ","date":"2019-03-07","objectID":"/posts/mysql_%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8%E5%88%86%E5%8C%BA/:0:1","tags":["mysql"],"title":"mysql分库分表分区","uri":"/posts/mysql_%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8%E5%88%86%E5%8C%BA/"},{"categories":["笔记"],"content":"分表 当表中的数据库巨大的时候就要考虑分表了，不然这将产生大量随机I/O，随之，数据库的响应时间将大到不可接受的程度。另外，索引维护（磁盘空间、I/O操作）的代价也非常高。 竖向分表 顾名思义，就是竖着开刀，把表中的一些字段分出去。 我们知道innodb主索引叶子节点存储着当前行的所有信息，减少字段可以让内存加载更多的数据。 受操作系统中文件大小限制。 横向分表 顾名思义，就是横着开刀，这样就把表一些行分出去。 随着数据量的增大，table行数巨大，查询的效率越来越低。 同样受限于操作系统中的文件大小限制，数据量不能无限增加，当到达一定容量时，需要水平切分以降低单表（文件）的大小。 至于怎么横着切要看需求了，可以根据主键范围，可以对主键进行hash等。 ","date":"2019-03-07","objectID":"/posts/mysql_%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8%E5%88%86%E5%8C%BA/:0:2","tags":["mysql"],"title":"mysql分库分表分区","uri":"/posts/mysql_%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8%E5%88%86%E5%8C%BA/"},{"categories":["笔记"],"content":"表分区 上面的分库、分表都是在应用层实现，拆分要对应用层有很大的调整，比如要实现全局主键生成器，无法再做关联查询等。 但是表分区完全不用考虑这些东西，因为它是在数据库层，对应用层是透明的。 表分区其实也是横向拿刀，切法有下面几种 range分区 根据范围分区，范围应该连续但是不重叠，使用PARTITION BY RANGE, VALUES LESS THAN关键字。 注意：不使用COLUMNS关键字时RANGE括号内必须为整数字段名或返回确定整数的函数。 CREATE TABLE employees ( id INT NOT NULL, fname VARCHAR(30), lname VARCHAR(30), hired DATE NOT NULL DEFAULT '1970-01-01', separated DATE NOT NULL DEFAULT '9999-12-31', job_code INT NOT NULL, store_id INT NOT NULL ) PARTITION BY RANGE (store_id) ( PARTITION p0 VALUES LESS THAN (6), PARTITION p1 VALUES LESS THAN (11), PARTITION p2 VALUES LESS THAN (16), PARTITION p3 VALUES LESS THAN MAXVALUE ); 根据TIMESTAMP范围： CREATE TABLE quarterly_report_status ( report_id INT NOT NULL, report_status VARCHAR(20) NOT NULL, report_updated TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP ) PARTITION BY RANGE ( UNIX_TIMESTAMP(report_updated) ) ( PARTITION p0 VALUES LESS THAN ( UNIX_TIMESTAMP('2008-01-01 00:00:00') ), PARTITION p1 VALUES LESS THAN ( UNIX_TIMESTAMP('2008-04-01 00:00:00') ), PARTITION p2 VALUES LESS THAN ( UNIX_TIMESTAMP('2008-07-01 00:00:00') ), PARTITION p3 VALUES LESS THAN ( UNIX_TIMESTAMP('2008-10-01 00:00:00') ), PARTITION p4 VALUES LESS THAN ( UNIX_TIMESTAMP('2009-01-01 00:00:00') ), PARTITION p5 VALUES LESS THAN ( UNIX_TIMESTAMP('2009-04-01 00:00:00') ), PARTITION p6 VALUES LESS THAN ( UNIX_TIMESTAMP('2009-07-01 00:00:00') ), PARTITION p7 VALUES LESS THAN ( UNIX_TIMESTAMP('2009-10-01 00:00:00') ), PARTITION p8 VALUES LESS THAN ( UNIX_TIMESTAMP('2010-01-01 00:00:00') ), PARTITION p9 VALUES LESS THAN (MAXVALUE) ); List分区 根据具体数值分区，每个分区数值不重叠，使用PARTITION BY LIST、VALUES IN关键字。跟Range分区类似，不使用COLUMNS关键字时List括号内必须为整数字段名或返回确定整数的函数。 数值必须被所有分区覆盖，否则插入一个不属于任何一个分区的数值会报错。 Hash分区 Hash分区主要用来确保数据在预先确定数目的分区中平均分布，Hash括号内只能是整数列或返回确定整数的函数，实际上就是使用返回的整数对分区数取模。 CREATE TABLE employees ( id INT NOT NULL, fname VARCHAR(30), lname VARCHAR(30), hired DATE NOT NULL DEFAULT '1970-01-01', separated DATE NOT NULL DEFAULT '9999-12-31', job_code INT, store_id INT ) PARTITION BY HASH(store_id) PARTITIONS 4; Hash分区也存在与传统Hash分表一样的问题，可扩展性差。MySQL也提供了一个类似于一致Hash的分区方法－线性Hash分区，只需要在定义分区时添加LINEAR关键字 Key分区 Key分区与Hash分区很相似，只是Hash函数不同，定义时把Hash关键字替换成Key即可 CREATE TABLE tk ( col1 INT NOT NULL, col2 CHAR(5), col3 DATE ) PARTITION BY LINEAR KEY (col1) PARTITIONS 3; 子分区 子分区是分区表中每个分区的再次分割。创建子分区方法： CREATE TABLE ts (id INT, purchased DATE) PARTITION BY RANGE( YEAR(purchased) ) SUBPARTITION BY HASH( TO_DAYS(purchased) ) SUBPARTITIONS 2 ( PARTITION p0 VALUES LESS THAN (1990), PARTITION p1 VALUES LESS THAN (2000), PARTITION p2 VALUES LESS THAN MAXVALUE ); 分区的使用 分区的目的是为了提高查询效率，如果查询范围是所有分区那么就说明分区没有起到作用，我们用explain partitions命令来查看SQL对于分区的使用情况。 一般来说，就是在where条件中加入分区列。 ","date":"2019-03-07","objectID":"/posts/mysql_%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8%E5%88%86%E5%8C%BA/:0:3","tags":["mysql"],"title":"mysql分库分表分区","uri":"/posts/mysql_%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8%E5%88%86%E5%8C%BA/"},{"categories":["笔记"],"content":"总结 分库分表是应用层面的，分区是数据库层面的。 分区对程序改动最小。 分表有横向、竖向，但是分区只有横向。 ","date":"2019-03-07","objectID":"/posts/mysql_%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8%E5%88%86%E5%8C%BA/:0:4","tags":["mysql"],"title":"mysql分库分表分区","uri":"/posts/mysql_%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8%E5%88%86%E5%8C%BA/"},{"categories":["笔记"],"content":"mysql索引原理","date":"2019-03-05","objectID":"/posts/mysql_%E7%B4%A2%E5%BC%95%E5%8E%9F%E7%90%86/","tags":["mysql"],"title":"mysql索引原理","uri":"/posts/mysql_%E7%B4%A2%E5%BC%95%E5%8E%9F%E7%90%86/"},{"categories":["笔记"],"content":"页 mysql中的页和操作系统的页有点类似，都是逻辑单位。都是假设数据在磁盘上是一起的，我们读取磁盘的时候一次读一页，而不是一条一条的数据来取，一页上可能会有多条数据，再取后面的数据的时候就先去已读取的页中查看，提高效率。操作系统的页是4KB，mysql的页默认是16KB。 ","date":"2019-03-05","objectID":"/posts/mysql_%E7%B4%A2%E5%BC%95%E5%8E%9F%E7%90%86/:0:1","tags":["mysql"],"title":"mysql索引原理","uri":"/posts/mysql_%E7%B4%A2%E5%BC%95%E5%8E%9F%E7%90%86/"},{"categories":["笔记"],"content":"innodb数据页结构及其中的目录 每条记录中有个next_record字段，这玩意儿非常重要，它表示从当前记录的真实数据到下一条记录的真实数据的地址偏移量，所以数据页每条记录是个单向链表。 既然是单向链表，如果页里数据比较多，用遍历效率是不高的。所以我们在数据页里再做个目录。 怎么做目录？分槽。 把16k数据分成若干槽，每个槽里面有若干条记录。由于记录都是主键递增的，每个槽都取的最后一条的偏移量，那么槽也是递增的。查找的时候先对槽进行二分查找，再对槽里面的数据进行遍历。 ","date":"2019-03-05","objectID":"/posts/mysql_%E7%B4%A2%E5%BC%95%E5%8E%9F%E7%90%86/:0:2","tags":["mysql"],"title":"mysql索引原理","uri":"/posts/mysql_%E7%B4%A2%E5%BC%95%E5%8E%9F%E7%90%86/"},{"categories":["笔记"],"content":"索引实现 不同的存储引擎采用不同的实现方式。 MyISAM 非聚集索引 MyISAM引擎使用B+Tree作为索引结构，叶节点的data域存放的是数据记录的地址。 我们看到，索引文件本身和数据表的文件是分离的，这也是非聚集索引的由来。叶子节点的data区域存放的是数据表每条记录的地址。 其实叶子节点都是页，里面又很多数据，当页数据满的时候在加一个页。 InnoDB聚集索引 InnoDB的主键索引也使用B+Tree作为索引结构，但这里表数据文件本身就是B+树的一个结构，也就是说叶子节点的data区域保存了完整的数据表的一条记录。索引的key就是表的主键，这就是聚集索引的由来。 由此看来，InnoDB的数据表必须要有一个主键，如果没有指定，mysql就会自动选择一个唯一标识记录的作为主键。那这样的也不存在怎么办？mysql就会生成一个隐含的6字节长整型作为主键。 现在看下如何定位一个Record： 1 通过根节点开始遍历一个索引的B+树，通过各层非叶子节点最终到达一个Page，这个Page里存放的都是叶子节点。 2 在Page内从”Infimum”节点开始遍历单链表（这种遍历往往会被优化），如果找到该键则成功返回。如果记录到达了”supremum”，说明当前Page里没有合适的键，这时要借助Page的Next Page指针，跳转到下一个Page继续从”Infimum”开始逐个查找。 B+树本身就是一个目录，或者说本身就是一个索引。 它有两个特点: 使用记录主键值的大小进行记录和页的排序，这包括三个方面的含义: 页内的记录是按照主键的大小顺序排成一个单向链表。 各个存放用户记录的页也是根据页中用户记录的主键大小顺序排成一个双向链表。 存放目录项记录的页分为不同的层次，在同一层次中的页也是根据页中目录项记录的主键大小顺序排成一个双向链表。 B+树的叶子节点存储的是完整的用户记录。 所谓完整的用户记录，就是指这个记录中存储了所有列的值(包括隐藏列)。 InnoDB的次级索引 InnoDB还有个地方与MyISAM不同，就是辅助索引data记录的是主键的值而不是数据表记录的地址。此时，索引文件和数据文件是分开的。 这样就要注意了，InnoDB主键不要太大，因为所有辅助索引都引用主索引，过长的主索引会令辅助索引变得过大。 当通过辅助索引来寻找数据时，Innodb存储引擎会遍历辅助索引并通过叶级别的指针获得指向主键索引的主键，然后再通过主键索引来找到一个完整的行记录。这种在二级索引中不能找到所有需要的数据列的现象，被称为非覆盖索引，反之称为覆盖索引。 这个B+树与上边介绍的聚簇索引有几处不同: 使用记录c2列的大小进行记录和页的排序，这包括三个方面的含义: 页内的记录是按照c2列的大小顺序排成一个单向链表。 各个存放用户记录的页也是根据页中记录的c2列大小顺序排成一个双向链表。 存放目录项记录的页分为不同的层次，在同一层次中的页也是根据页中目录项记录的c2列大小顺序排成一个双向链表。 B+树的叶子节点存储的并不是完整的用户记录，而只是c2列+主键这两个列的值。 目录项记录中不再是主键+页号的搭配，而变成了c2列+页号的搭配。 ","date":"2019-03-05","objectID":"/posts/mysql_%E7%B4%A2%E5%BC%95%E5%8E%9F%E7%90%86/:0:3","tags":["mysql"],"title":"mysql索引原理","uri":"/posts/mysql_%E7%B4%A2%E5%BC%95%E5%8E%9F%E7%90%86/"},{"categories":["笔记"],"content":"采用B+树的原因 B+树的特点决定的。查询一般为log(n)。选择B+树而不是其他数据结构的原因主要是因为数据是保存在硬盘上而不是内存中，所以减少磁盘IO次数才是提升效率的关键。 B+树的磁盘读写代价更低：B+树的内部节点并没有指向关键字具体信息的指针，因此其内部节点相对B树更小，如果把所有同一内部节点的关键字存放在同一盘块中，那么盘块所能容纳的关键字数量也越多，一次性读入内存的需要查找的关键字也就越多，相对IO读写次数就降低了。 B+树的查询效率更加稳定：由于非终结点并不是最终指向文件内容的结点，而只是叶子结点中关键字的索引。所以任何关键字的查找必须走一条从根结点到叶子结点的路。所有关键字查询的路径长度相同，导致每一个数据的查询效率相当。 由于B+树的数据都存储在叶子结点中，分支结点均为索引，方便扫库，只需要扫一遍叶子结点即可，但是B树因为其分支结点同样存储着数据，我们要找到具体的数据，需要进行一次中序遍历按序来扫，所以B+树更加适合在区间查询的情况，所以通常B+树用于数据库索引。 总结： Hash索引查询是O(1),应该是更快，但是为啥不用呢？因为大多数情况下并不是每次只查询一个，而是多个，比如前10条，b+树叶子节点有链接，所以能快速查询。另外，文件或数据库索引数据比较大，也不做不到一次加载到内存，但是B树可以一个一个节点的加载，进行查询。 ","date":"2019-03-05","objectID":"/posts/mysql_%E7%B4%A2%E5%BC%95%E5%8E%9F%E7%90%86/:0:4","tags":["mysql"],"title":"mysql索引原理","uri":"/posts/mysql_%E7%B4%A2%E5%BC%95%E5%8E%9F%E7%90%86/"},{"categories":["笔记"],"content":"mysql explain","date":"2019-03-03","objectID":"/posts/mysql_explain/","tags":["mysql"],"title":"mysql explain","uri":"/posts/mysql_explain/"},{"categories":["笔记"],"content":"一条查询语句在经过MySQL查询优化器的各种基于成本和规则的优化会后生成一个所谓的执行计划，这个执行计划展示了接下来具体执行查询的方式，比如多表连接的顺序是什么，对于每个表采用什么访 问方法来具体执行查询等等。 其实除了以SELECT开头的查询语句，其余的DELETE、INSERT、REPLACE以及UPDATE语句前边都可以加上EXPLAIN这个词儿。 列名 描述 id 在一个大的查询语句每个SELECT关键字都对应一个唯一的id select_type SELECT关键字对应的那个查询的类型 table 表名 partitions 匹配的分区信息 type 针对单表的访问方法 possible_keys 可能用到的索引 key 实际上使用的索引 key_len 实际使用到的索引长度 ref 当使用索引列等值查询时，与索引列进行等值匹配的对象信息 rows 预估的需要读取的记录条数 filtered 某个表经过搜索条件过滤后剩余记录条数的百分比 Extra 一些额外的信息 table 不论我们的查询语句有多复杂，里边儿包含了多少个表，到最后也是需要对每个表进行单表访问的，所以设计MySQL的大叔规定EXPLAIN语句输出的每条记录都对应着某个单表的访问方法，该条记录的 table列代表着该表的表名. #id 查询语句中每出现一个SELECT关键字，设计MySQL的大叔就会为它分配一个唯一的id值。 一个SELECT关键字后边的FROM子句中可以跟随多个表，所以在连接查询的执行计划中，每个表都会对应一条记录，但是这些记录的id值都是相同的。出现在前边的表表示驱动表，出现在后边的表表示被驱动表。 查询优化器可能对涉及子查询的查询语句进行重写，从而转换为连接查询。所以如果我们想知道查询优化器对某个包含子查询的语句是否进行了重写，直接查看执行计划就好了。重写后两个ID是相同的。 select_type 一条大的查询语句里边可以包含若干个SELECT关键字，每个SELECT关键字代表着一个小的查询语句，而每个SELECT关键字的FROM子句中都可以包含若干张表(这些表用来做连 接查询)，每一张表都对应着执行计划输出中的一条记录，对于在同一个SELECT关键字中的表来说，它们的id值是相同的。 设计MySQL的大叔为每一个SELECT关键字代表的小查询都定义了一个称之为select_type的属性，意思是我们只要知道了某个小查询的select_type属性，就知道了这个小查询在整个大查询中扮演了一个什么角色。 SIMPLE (简单SELECT,不使用UNION或子查询等) PRIMARY 对于包含UNION、UNION ALL或者子查询的大查询来说，它是由几个小查询组成的，其中最左边的那个查询的select_type值就是PRIMARY。查询中若包含任何复杂的子部分,最外层的select被标记为PRIMARY。 EXPLAIN SELECT * FROM s1 UNION SELECT * FROM s2; 最左边的小查询SELECT * FROM s1对应的是执行计划中的第一条记录，它的select_type值就是PRIMARY。 UNION 对于包含UNION或者UNION ALL的大查询来说，它是由几个小查询组成的，其中除了最左边的那个小查询以外，其余的小查询的select_type值就是UNION UNION RESULT MySQL选择使用临时表来完成UNION查询的去重工作，针对该临时表的查询的select_type就是UNION RESULT。 SUBQUERY 如果包含子查询的查询语句不能够转为对应的semi-join的形式，并且该子查询是不相关子查询，并且查询优化器决定采用将该子查询物化的方案来执行该子查询时，该子查询的第一个SELECT关键 字代表的那个查询的select_type就是SUBQUERY。 mysql\u003e EXPLAIN SELECT * FROM s1 WHERE key1 IN (SELECT key1 FROM s2) OR key3 = 'a'; +----+-------------+-------+------------+-------+---------------+----------+---------+------+------+----------+-------------+ | id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra | +----+-------------+-------+------------+-------+---------------+----------+---------+------+------+----------+-------------+ | 1 | PRIMARY | s1 | NULL | ALL | idx_key3 | NULL | NULL | NULL | 9688 | 100.00 | Using where | | 2 | SUBQUERY | s2 | NULL | index | idx_key1 | idx_key1 | 303 | NULL | 9954 | 100.00 | Using index | +----+-------------+-------+------------+-------+---------------+----------+---------+------+------+----------+-------------+ 外层查询的select_type就是PRIMARY，子查询的select_type就是SUBQUERY。需要大家注意的是，由于select_type为SUBQUERY的子查询由于会被物化，所以只需要执行一遍。 DEPENDENT SUBQUERY 如果包含子查询的查询语句不能够转为对应的semi-join的形式，并且该子查询是相关子查询，则该子查询的第一个SELECT关键字代表的那个查询的select_type就是DEPENDENT SUBQUERY。 EXPLAIN SELECT * FROM s1 WHERE key1 IN (SELECT key1 FROM s2 WHERE s1.key2 = s2.key2) OR key3 = 'a'; select_type为DEPENDENT SUBQUERY的查询可能会被执行多次。 DEPENDENT UNION 在包含UNION或者UNION ALL的大查询中，如果各个小查询都依赖于外层查询的话，那除了最左边的那个小查询之外，其余的小查询的select_type的值就是DEPENDENT UNION。说的有些绕哈，比方说 下边这个查询: mysql\u003e EXPLAIN SELECT * FROM s1 WHERE key1 IN (SELECT key1 FROM s2 WHERE key1 = 'a' UNION SELECT key1 FROM s1 WHERE key1 = 'b'); DERIVED 对于采用物化的方式执行的包含派生表的查询，该派生表对应的子查询的select_type就是DERIVED， EXPLAIN SELECT * FROM (SELECT key1, count(*) as c FROM s1 GROUP BY key1) AS derived_s1 where c \u003e 1; +----+-------------+------------+------------+-------+---------------+----------+---------+------+------+----------+-------------+ | id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra | +----+-------------+------------+------------+-------+---------------+----------+---------+------+------+----------+-------------+ | 1 | PRIMARY | \u003cderived2\u003e | NULL | ALL | NULL | NULL | NULL | NULL | 9688 | 33.33 | Using where | | 2 | DERIVED | s1 | NULL | index | idx_key1 | idx_key1 | 303 | NULL | 9688 | 100.00 | Using index | +----+-------------+------------+------------+-------+---------------+----------+---------+------+------+----------+-------------+ 从执行计划中可以看出，id为2的记录就代表子查询的执行方式，它的select_type是DERIVED，说明该子查询是以物化的方式执行的。id为1的记录代表外层查询，大家注意看它的t","date":"2019-03-03","objectID":"/posts/mysql_explain/:0:0","tags":["mysql"],"title":"mysql explain","uri":"/posts/mysql_explain/"},{"categories":["笔记"],"content":"mysql 索引下推","date":"2019-03-03","objectID":"/posts/mysql_%E7%B4%A2%E5%BC%95%E4%B8%8B%E6%8E%A8/","tags":["mysql"],"title":"mysql 索引下推","uri":"/posts/mysql_%E7%B4%A2%E5%BC%95%E4%B8%8B%E6%8E%A8/"},{"categories":["笔记"],"content":"索引下推（INDEX CONDITION PUSHDOWN，简称 ICP）是 MySQL 5.6 发布后针对扫描二级索引的一项优化改进。总的来说是通过把索引过滤条件下推到存储引擎，来减少 MySQL 存储引擎访问基表的次数以及 MySQL 服务层访问存储引擎的次数。ICP 适用于 MYISAM 和 INNODB，本篇的内容只基于 INNODB。ICP的实质就是通过二级索引尽可能的过滤不符合条件的记录，哪怕不符合最左匹配原则，减少回表，降低执行成本。 MySQL ICP 里涉及到的知识点如下： MySQL 服务层：也就是 SERVER 层，用来解析 SQL 的语法、语义、生成查询计划、接管从 MySQL 存储引擎层上推的数据进行二次过滤等等。 MySQL 存储引擎层：按照 MySQL 服务层下发的请求，通过索引或者全表扫描等方式把数据上传到 MySQL 服务层。 MySQL 索引扫描：根据指定索引过滤条件（比如 where id = 1) ，遍历索引找到索引键对应的主键值后回表过滤剩余过滤条件。 MySQL 索引过滤：通过索引扫描并且基于索引进行二次条件过滤后再回表。 ICP 就是把以上索引扫描和索引过滤合并在一起处理，过滤后的记录数据下推到存储引擎后的一种索引优化策略。这样做的优点如下： 减少了回表的操作次数。 减少了上传到 MySQL SERVER 层的数据。 ICP 默认开启，可通过优化器开关参数关闭 ICP：optimizer_switch='index_condition_pushdown=off’ 或者是在 SQL 层面通过 HINT 来关闭。 在不使用 ICP 索引扫描的过程： MySQL 存储引擎层只把满足索引键值对应的整行表记录一条一条取出，并且上传给 MySQL 服务层。 MySQL 服务层对接收到的数据，使用 SQL 语句后面的 where 条件过滤，直到处理完最后一行记录，再一起返回给客户端。 使用 ICP 扫描的过程： MySQL 存储引擎层，先根据过滤条件中包含的索引键确定索引记录区间，再在这个区间的记录上使用包含索引键的其他过滤条件进行过滤，之后规避掉不满足的索引记录，只根据满足条件的索引记录回表取回数据上传到 MySQL 服务层。 explain中查看 查看语句是否用了 ICP，只需要对语句进行 EXPLAIN，在 EXTRA 信息里可以看到 ICP 相关信息。其中 extra 里显示 “Using index condition” 就代表用了 ICP。 使用时限制条件 任何需要下推到底层存储层的操作一般都有诸多限制，MySQL ICP 也不例外，ICP 限制如下： ICP 仅用于需要访问基表所有记录时使用，适用的访问方法为：range、ref、eq_ref、ref_or_null。我上面举的例子即是 ref 类型，ICP 尤其是对联合索引的部分列模糊查找非常有效。 ICP 同样适用于分区表。 ICP 的目标是减少全行记录读取，从而减少 I/O 操作，仅用于二级索引。主键索引本身即是表数据，不存在下推操作。 ICP 不支持基于虚拟列上建立的索引，比如函数索引。 ICP 不支持引用子查询的条件。 ","date":"2019-03-03","objectID":"/posts/mysql_%E7%B4%A2%E5%BC%95%E4%B8%8B%E6%8E%A8/:0:0","tags":["mysql"],"title":"mysql 索引下推","uri":"/posts/mysql_%E7%B4%A2%E5%BC%95%E4%B8%8B%E6%8E%A8/"},{"categories":["笔记"],"content":"mysql 锁","date":"2019-03-03","objectID":"/posts/mysql_%E9%94%81/","tags":["mysql"],"title":"mysql 锁","uri":"/posts/mysql_%E9%94%81/"},{"categories":["笔记"],"content":"意向锁 当我们在对使用InnoDB存储引擎的表的某些记录加S锁之前，那就需要先在表级别加一个IS锁，当我们在对使用InnoDB存储引擎的表的某些记录加X锁之前，那就需要先在表级别加一个IX锁。 IS、IX锁是表级锁，它们的提出仅仅为了在之后加表级别的S锁和X锁时可以快速判断表中的记录是否被上锁，以避免用遍历的方式来查看表中有没有上锁的记录，也就是说其实IS锁和IX锁是 兼容的，IX锁和IX锁是兼容的。 表级别的S锁、X锁 在对某个表执行SELECT、INSERT、DELETE、UPDATE语句时，InnoDB存储引擎是不会为这个表添加表级别的S锁或者X锁的。 另外，在对某个表执行一些诸如ALTER TABLE、DROP TABLE这类的DDL语句时，其他事务对这个表并发执行诸如SELECT、INSERT、DELETE、UPDATE的语句会发生阻塞，同理，某个事务中对某个表执 行SELECT、INSERT、DELETE、UPDATE语句时，在其他会话中对这个表执行DDL语句也会发生阻塞。这个过程其实是通过在server层使用一种称之为元数据锁(英文名:Metadata Locks，简称MDL)东东 来实现的，一般情况下也不会使用InnoDB存储引擎自己提供的表级别的S锁和X锁。 其实这个InnoDB存储引擎提供的表级S锁或者X锁是相当鸡肋，只会在一些特殊情况下，比方说崩溃恢复过程中用到。 表级别的AUTO-INC锁 在使用MySQL过程中，我们可以为表的某个列添加AUTO_INCREMENT属性，之后在插入记录时，可以不指定该列的值，系统会自动为它赋上递增的值。 系统实现这种自动给AUTO_INCREMENT修饰的列递增赋值的原理主要是两个: 采用AUTO-INC锁 也就是在执行插入语句时就在表级别加一个AUTO-INC锁，然后为每条待插入记录的AUTO_INCREMENT修饰的列分配递增的值，在该语句执行结束后，再把AUTO-INC锁释放掉。 这样一个事务在持有AUTO-INC锁的过程中，其他事务的插入语句都要被阻塞，可以保证一个语句中分配的递增值是连续的。 如果我们的插入语句在执行前不可以确定具体要插入多少条记录(无法预计即将插入记录的数量)，比方说使用INSERT … SELECT、REPLACE … SELECT或者LOAD DATA这种插入语句，一般 是使用AUTO-INC锁为AUTO_INCREMENT修饰的列生成对应的值。 小贴士: 需要注意一下的是，这个AUTO-INC锁的作用范围只是单个插入语句，插入语句执行完成后，这个锁就被释放了，跟我们之前介绍的锁在事务结束时释放是不一样的。 采用一个轻量级的锁 在为插入语句生成AUTO_INCREMENT修饰的列的值时获取一下这个轻量级锁，然后生成本次插入语句需要用到的AUTO_INCREMENT列的值之后，就把该轻量级锁释放掉，并 不需要等到整个插入语句执行完才释放锁。 如果我们的插入语句在执行前就可以确定具体要插入多少条记录，比方说我们上边举的关于表t的例子中，在语句执行前就可以确定要插入2条记录，那么一般采用轻量级锁的方式 对AUTO_INCREMENT修饰的列进行赋值。这种方式可以避免锁定表，可以提升插入性能。 小贴士: 设计InnoDB的大叔提供了一个称之为innodb_autoinc_lock_mode的系统变量来控制到底使用上述两种方式中的哪种来为AUTO_INCREMENT修饰的列进行赋值，当 innodb_autoinc_lock_mode值为0时，一律采用AUTO-INC锁;当innodb_autoinc_lock_mode值为2时，一律采用轻量级锁;当innodb_autoinc_lock_mode值为1时，两种方式混着来(也就是在插入 记录数量确定时采用轻量级锁，不确定时使用AUTO-INC锁)。不过当innodb_autoinc_lock_mode值为2时，可能会造成不同事务中的插入语句为AUTO_INCREMENT修饰的列生成的值是交 叉的，在有主从复制的场景中是不安全的。 ","date":"2019-03-03","objectID":"/posts/mysql_%E9%94%81/:0:0","tags":["mysql"],"title":"mysql 锁","uri":"/posts/mysql_%E9%94%81/"},{"categories":["笔记"],"content":"mysql优化","date":"2019-03-03","objectID":"/posts/mysql_%E4%BC%98%E5%8C%96/","tags":["mysql"],"title":"mysql优化","uri":"/posts/mysql_%E4%BC%98%E5%8C%96/"},{"categories":["笔记"],"content":"分析效率 show status like “xxx” 查询一些msyql性能参数 connections: 连接数据库服务端的次数 slow_queries：慢查询次数 com_select：查询次数 com_insert：插入次数等 慢查询日志分析 MySQL的慢查询日志是MySQL提供的一种日志记录，用来记录在MySQL中响应时间超过阈值的语句，具体指运行时间超过long_query_time值的SQL，则会被记录到慢查询日志中（日志可以写入文件或者数据库表，如果对性能要求高的话，建议写文件）。默认情况下，MySQL数据库是不开启慢查询日志的，long_query_time的默认值为10（即10秒，通常设置为1秒），即运行10秒以上的语句是慢查询语句。 修改my.cnf文件，增加或修改参数slow_query_log 和slow_query_log_file后，然后重启MySQL服务器，如下所示 slow_query_log =1 slow_query_log_file=/tmp/mysql_slow.log ","date":"2019-03-03","objectID":"/posts/mysql_%E4%BC%98%E5%8C%96/:0:1","tags":["mysql"],"title":"mysql优化","uri":"/posts/mysql_%E4%BC%98%E5%8C%96/"},{"categories":["笔记"],"content":"优化查询 联合索引最左前缀原则 复合索引遵守「最左前缀」原则，查询条件中，使用了复合索引前面的字段，索引才会被使用，如果不是按照索引的最左列开始查找，则无法使用索引。 比如在(a,b,c)三个字段上建立联合索引，那么它能够加快a|(a,b)|(a,b,c)三组查询的速度，而不能加快b|(b,a)这种查询顺序。 另外，建联合索引的时候，区分度最高的字段在最左边。 不要在列上使用函数和进行运算 不要在列上使用函数，这将导致索引失效而进行全表扫描。 例如下面的 SQL 语句： select * from artile where YEAR(create_time) \u003c= ‘2018’; 即使 date 上建立了索引，也会全表扫描，可以把计算放到业务层，这样做不仅可以节省数据库的 CPU，还可以起到查询缓存优化效果。 负向条件查询不能使用索引 负向条件有：!=、\u003c\u003e、not in、not exists、not like 等。 select * from artile where status != 1 and status != 2; 可以使用in进行优化： select * from artile where status in (0,3) 使用覆盖索引 所谓覆盖索引，是指被查询的列，数据能从索引中取得，而不用通过行定位符再到数据表上获取，能够极大的提高性能。 可以定义一个让索引包含的额外的列，即使这个列对于索引而言是无用的。 避免强制类型转换 当查询条件左右两侧类型不匹配的时候会发生强制转换，强制转换可能导致索引失效而进行全表扫描。 如果phone字段是varchar类型，则下面的SQL不能命中索引： select * from user where phone=12345678901; 复制代码可以优化为： select * from user where phone='12345678901’; 范围列可以用到索引 范围条件有：\u003c、\u003c=、\u003e、\u003e=、between等。 范围列可以用到索引，但是范围列后面的列无法用到索引，索引最多用于一个范围列，如果查询条件中有两个范围列则无法全用到索引 更新频繁、数据区分度不高的字段上不宜建立索引 更新会变更B+树，更新频繁的字段建立索引会大大降低数据库性能。 「性别」这种区分度不大的属性，建立索引没有意义，不能有效过滤数据，性能与全表扫描类似。 区分度可以使用 count(distinct(列名))/count(*) 来计算，在80%以上的时候就可以建立索引。 避免使用or来连接条件 应该尽量避免在 where 子句中使用 or 来连接条件，因为这会导致索引失效而进行全表扫描，虽然新版的MySQL能够命中索引，但查询优化耗费的 CPU比in多。 Or的时候只有or前后都是索引才有效。 模糊查询 前导模糊查询不能使用索引，非前导查询可以。 分页优化 大的分页数据效率比较差，可以使用子查询先获得对于的Id,然后再查。或者通过id的返回。不过这些都是要id是有序的才行。 如果明确知道只有一条结果返回，limit 1 能够提高效率 虽然自己知道只有一条结果，但数据库并不知道，明确告诉它，让它主动停止游标移动。 旧版本中可以使用limit n,-1;来获取偏移量到最后的数据，新版本中不能这样了，官方建议使用一个较大的数字来实现。 ","date":"2019-03-03","objectID":"/posts/mysql_%E4%BC%98%E5%8C%96/:0:2","tags":["mysql"],"title":"mysql优化","uri":"/posts/mysql_%E4%BC%98%E5%8C%96/"},{"categories":["笔记"],"content":"插入的优化 如果要插入大量的数据可以在插入前暂停索引、唯一校验、外键检查。不过看存储引擎是否支持。 load data infile比insert语句快。 ","date":"2019-03-03","objectID":"/posts/mysql_%E4%BC%98%E5%8C%96/:1:0","tags":["mysql"],"title":"mysql优化","uri":"/posts/mysql_%E4%BC%98%E5%8C%96/"},{"categories":["笔记"],"content":"mysql备份、主从","date":"2019-03-02","objectID":"/posts/mysql_%E5%A4%87%E4%BB%BD%E4%B8%BB%E4%BB%8E/","tags":["mysql"],"title":"mysql备份、主从","uri":"/posts/mysql_%E5%A4%87%E4%BB%BD%E4%B8%BB%E4%BB%8E/"},{"categories":["笔记"],"content":"备份和恢复 ","date":"2019-03-02","objectID":"/posts/mysql_%E5%A4%87%E4%BB%BD%E4%B8%BB%E4%BB%8E/:0:0","tags":["mysql"],"title":"mysql备份、主从","uri":"/posts/mysql_%E5%A4%87%E4%BB%BD%E4%B8%BB%E4%BB%8E/"},{"categories":["笔记"],"content":"mysqldump命令备份 mysqldump命令把数据库备份为一个文本文件，包含了很多create和insert语句，使用这些语句就可以重新插入和备份。 我们可以直接对数据库备份，也可以对具体某些表进行导出。 mysqldump -u user -h host -p password dbname [tablename…] \u003e filename.sql ","date":"2019-03-02","objectID":"/posts/mysql_%E5%A4%87%E4%BB%BD%E4%B8%BB%E4%BB%8E/:1:0","tags":["mysql"],"title":"mysql备份、主从","uri":"/posts/mysql_%E5%A4%87%E4%BB%BD%E4%B8%BB%E4%BB%8E/"},{"categories":["笔记"],"content":"数据恢复 mysql -u user -p [dbname] \u003c filename.sql 如果导出的语句中包含创建数据库的语句就不用指定数据库名了 ","date":"2019-03-02","objectID":"/posts/mysql_%E5%A4%87%E4%BB%BD%E4%B8%BB%E4%BB%8E/:2:0","tags":["mysql"],"title":"mysql备份、主从","uri":"/posts/mysql_%E5%A4%87%E4%BB%BD%E4%B8%BB%E4%BB%8E/"},{"categories":["笔记"],"content":"数据库迁移 我们可能需要安装新的数据库、mysql版本更新等原因而迁移数据库。 迁移其实就是导出和恢复的过程，当然如果主版本号相同我们还可以直接拷贝数据库文件（只适用于MyISAM引擎） Mysql Binlog格式介绍 Mysql binlog日志有三种格式，分别为Statement,MiXED,以及ROW！ Mysql默认是使用Statement日志格式，推荐使用MIXED。 由于一些特殊使用，可以考虑使用ROWED，如自己通过binlog日志来同步数据的修改，这样会节省很多相关操作。 ","date":"2019-03-02","objectID":"/posts/mysql_%E5%A4%87%E4%BB%BD%E4%B8%BB%E4%BB%8E/:3:0","tags":["mysql"],"title":"mysql备份、主从","uri":"/posts/mysql_%E5%A4%87%E4%BB%BD%E4%B8%BB%E4%BB%8E/"},{"categories":["笔记"],"content":"Statement 每一条会修改数据的sql都会记录在binlog中。 **优点：**不需要记录每一行的变化，减少了binlog日志量，节约了IO，提高性能。 **缺点：**由于记录的只是执行语句，为了这些语句能在slave上正确运行，因此还必须记录每条语句在执行的时候的一些相关信息，以保证所有语句能在slave得到和在master端执行时候相同 的结果。另外mysql 的复制,像一些特定函数功能，slave可与master上要保持一致会有很多相关问题。 ","date":"2019-03-02","objectID":"/posts/mysql_%E5%A4%87%E4%BB%BD%E4%B8%BB%E4%BB%8E/:4:0","tags":["mysql"],"title":"mysql备份、主从","uri":"/posts/mysql_%E5%A4%87%E4%BB%BD%E4%B8%BB%E4%BB%8E/"},{"categories":["笔记"],"content":"Row: 不记录sql语句上下文相关信息，仅保存哪条记录被修改。 优点： binlog中可以不记录执行的sql语句的上下文相关的信息，仅需要记录那一条记录被修改成什么了。所以rowlevel的日志内容会非常清楚的记录下每一行数据修改的细节。而且不会出现某些特定情况下的存储过程，或function，以及trigger的调用和触发无法被正确复制的问题。 **缺点：**所有的执行的语句当记录到日志中的时候，都将以每行记录的修改来记录，这样可能会产生大量的日志内容,比如一条update语句，修改多条记录，则binlog中每一条修改都会有记录，这样造成binlog日志量会很大，特别是当执行alter table之类的语句的时候，由于表结构修改，每条记录都发生改变，那么该表每一条记录都会记录到日志中。 ","date":"2019-03-02","objectID":"/posts/mysql_%E5%A4%87%E4%BB%BD%E4%B8%BB%E4%BB%8E/:5:0","tags":["mysql"],"title":"mysql备份、主从","uri":"/posts/mysql_%E5%A4%87%E4%BB%BD%E4%B8%BB%E4%BB%8E/"},{"categories":["笔记"],"content":"Mixedlevel: 是以上两种level的混合使用，一般的语句修改使用statment格式保存binlog，如一些函数，statement无法完成主从复制的操作，则采用row格式保存binlog 主从复制 主服务器master把数据复制到多台从服务器slaves。 主从一般是一个主，多个从。也可以是链式的a-\u003eb-\u003ec ,这样b既是主又是从。 主从复制主要分以下步骤： 主服务器 将数据的更新记录到 二进制日志（Binary log）中，用于记录二进制日志事件，这一步由 主库线程 完成； 从库 将 主库 的 二进制日志 复制到本地的 中继日志（Relay log），这一步由 从库 I/O 线程 完成； 从库 读取 中继日志 中的 事件，将其重放到数据中，这一步由 从库 SQL 线程 完成。 ","date":"2019-03-02","objectID":"/posts/mysql_%E5%A4%87%E4%BB%BD%E4%B8%BB%E4%BB%8E/:6:0","tags":["mysql"],"title":"mysql备份、主从","uri":"/posts/mysql_%E5%A4%87%E4%BB%BD%E4%B8%BB%E4%BB%8E/"},{"categories":["笔记"],"content":"流程 可以看到：主库接收到客户端的更新请求后，执行内部事务的更新逻辑，同时写 binlog。 备库 B 跟主库 A 之间维持了一个长连接。主库 A 内部有一个线程，专门用于服务备库 B 的这个长连接。一个事务日志同步的完整过程是这样的： 在备库 B 上通过 change master 命令，设置主库 A 的 IP、端口、用户名、密码，以及要从哪个位置开始请求 binlog，这个位置包含文件名和日志偏移量。 在备库 B 上执行 start slave 命令，这时候备库会启动两个线程，就是图中的 io_thread 和 sql_thread。其中 io_thread 负责与主库建立连接。 主库 A 校验完用户名、密码后，开始按照备库 B 传过来的位置，从本地读取 binlog，发给 B。 备库 B 拿到 binlog 后，写到本地文件，称为中转日志（relay log）。 sql_thread 读取中转日志，解析出日志里的命令，并执行。 备库设置成只读了，还怎么跟主库保持同步更新呢？ 这个问题，你不用担心。因为 readonly 设置对超级 (super) 权限用户是无效的，而用于同步更新的线程，就拥有超级权限。 ","date":"2019-03-02","objectID":"/posts/mysql_%E5%A4%87%E4%BB%BD%E4%B8%BB%E4%BB%8E/:7:0","tags":["mysql"],"title":"mysql备份、主从","uri":"/posts/mysql_%E5%A4%87%E4%BB%BD%E4%B8%BB%E4%BB%8E/"},{"categories":["笔记"],"content":"操作步骤 ","date":"2019-03-02","objectID":"/posts/mysql_%E5%A4%87%E4%BB%BD%E4%B8%BB%E4%BB%8E/:8:0","tags":["mysql"],"title":"mysql备份、主从","uri":"/posts/mysql_%E5%A4%87%E4%BB%BD%E4%B8%BB%E4%BB%8E/"},{"categories":["笔记"],"content":"master配置 [mysqld] ## 设置server_id，一般设置为IP,注意要唯一 server_id=100 ## 复制过滤：也就是指定哪个数据库不用同步（mysql库一般不同步） binlog-ignore-db=mysql ## 开启二进制日志功能，可以随便取，最好有含义（关键就是这里了） log-bin=edu-mysql-bin ## 为每个session 分配的内存，在事务过程中用来存储二进制日志的缓存 binlog_cache_size=1M ## 主从复制的格式（mixed,statement,row，默认格式是statement） binlog_format=mixed ## 二进制日志自动删除/过期的天数。默认值为0，表示不自动删除。 expire_logs_days=7 ## 跳过主从复制中遇到的所有错误或指定类型的错误，避免slave端复制中断。 ## 如：1062错误是指一些主键重复，1032错误是因为主从数据库数据不一致 slave_skip_errors=1062 ","date":"2019-03-02","objectID":"/posts/mysql_%E5%A4%87%E4%BB%BD%E4%B8%BB%E4%BB%8E/:8:1","tags":["mysql"],"title":"mysql备份、主从","uri":"/posts/mysql_%E5%A4%87%E4%BB%BD%E4%B8%BB%E4%BB%8E/"},{"categories":["笔记"],"content":"创建复制账户，授予权限 CREATE USER 'slave'@'%' IDENTIFIED BY '123456'; GRANT REPLICATION SLAVE, REPLICATION CLIENT ON *.* TO 'slave'@'%'; 这里主要是要授予用户REPLICATION SLAVE权限和REPLICATION CLIENT权限 ","date":"2019-03-02","objectID":"/posts/mysql_%E5%A4%87%E4%BB%BD%E4%B8%BB%E4%BB%8E/:8:2","tags":["mysql"],"title":"mysql备份、主从","uri":"/posts/mysql_%E5%A4%87%E4%BB%BD%E4%B8%BB%E4%BB%8E/"},{"categories":["笔记"],"content":"slave配置 [mysqld] ## 设置server_id，一般设置为IP,注意要唯一 server_id=101 ## 复制过滤：也就是指定哪个数据库不用同步（mysql库一般不同步） binlog-ignore-db=mysql ## 开启二进制日志功能，以备Slave作为其它Slave的Master时使用 log-bin=edu-mysql-slave1-bin ## 为每个session 分配的内存，在事务过程中用来存储二进制日志的缓存 binlog_cache_size=1M ## 主从复制的格式（mixed,statement,row，默认格式是statement） binlog_format=mixed ## 二进制日志自动删除/过期的天数。默认值为0，表示不自动删除。 expire_logs_days=7 ## 跳过主从复制中遇到的所有错误或指定类型的错误，避免slave端复制中断。 ## 如：1062错误是指一些主键重复，1032错误是因为主从数据库数据不一致 slave_skip_errors=1062 ## relay_log配置中继日志 relay_log=edu-mysql-relay-bin ## log_slave_updates表示slave将复制事件写进自己的二进制日志 log_slave_updates=1 ## 防止改变数据(除了特殊的线程) read_only=1 ","date":"2019-03-02","objectID":"/posts/mysql_%E5%A4%87%E4%BB%BD%E4%B8%BB%E4%BB%8E/:8:3","tags":["mysql"],"title":"mysql备份、主从","uri":"/posts/mysql_%E5%A4%87%E4%BB%BD%E4%B8%BB%E4%BB%8E/"},{"categories":["笔记"],"content":"查看master status show master status; 记录下返回结果的File列和Position列的值 ","date":"2019-03-02","objectID":"/posts/mysql_%E5%A4%87%E4%BB%BD%E4%B8%BB%E4%BB%8E/:8:4","tags":["mysql"],"title":"mysql备份、主从","uri":"/posts/mysql_%E5%A4%87%E4%BB%BD%E4%B8%BB%E4%BB%8E/"},{"categories":["笔记"],"content":"Slave中设置Master信息 change master to master_host='192.168.1.100', master_user='slave', master_password='123456', master_port=3306, master_log_file='edu-mysql-bin.000001', master_log_pos=1389, master_connect_retry=30; 上面执行的命令的解释： master_host=’192.168.1.100′ ## Master的IP地址 master_user=’slave’ ## 用于同步数据的用户（在Master中授权的用户） master_password=’123456′ ## 同步数据用户的密码 master_port=3306 ## Master数据库服务的端口 masterlogfile=’edu-mysql-bin.000001′ ##指定Slave从哪个日志文件开始读复制数据（Master上执行命令的结果的File字段） masterlogpos=429 ## 从哪个POSITION号开始读（Master上执行命令的结果的Position字段） masterconnectretry=30 ##当重新建立主从连接时，如果连接建立失败，间隔多久后重试。单位为秒，默认设置为60秒，同步延迟调优参数。 ","date":"2019-03-02","objectID":"/posts/mysql_%E5%A4%87%E4%BB%BD%E4%B8%BB%E4%BB%8E/:8:5","tags":["mysql"],"title":"mysql备份、主从","uri":"/posts/mysql_%E5%A4%87%E4%BB%BD%E4%B8%BB%E4%BB%8E/"},{"categories":["笔记"],"content":"开始同步 start slave; show slave status; 查询查看主从同步状态，会发现SlaveIORunning和SlaveSQLRunning是Yes了，表明开启成功 读写分离 主服务只负责写，从服务器只负责读，可以达到负载均衡的效果。 我们可以在程序中指定连接地址，区分读写数据库。 这里用第三方mysql proxy来读写分离。它是位于客户端与mysql数据库之间的程序。 我们可以指定主、从的地址，它能自动的做负载均衡，对程序是透明的。 ","date":"2019-03-02","objectID":"/posts/mysql_%E5%A4%87%E4%BB%BD%E4%B8%BB%E4%BB%8E/:8:6","tags":["mysql"],"title":"mysql备份、主从","uri":"/posts/mysql_%E5%A4%87%E4%BB%BD%E4%B8%BB%E4%BB%8E/"},{"categories":["笔记"],"content":"mysql基础","date":"2019-03-01","objectID":"/posts/mysql_%E5%9F%BA%E7%A1%80/","tags":["mysql"],"title":"mysql基础","uri":"/posts/mysql_%E5%9F%BA%E7%A1%80/"},{"categories":["笔记"],"content":"概述 sql包含4个部分： 数据定义语言DDL:create,drop,alter 数据操作语言DML:insert,update,delete 数据查询语言DQL:select 数据控制语言DCL:grant,revoke,commit,rollback ","date":"2019-03-01","objectID":"/posts/mysql_%E5%9F%BA%E7%A1%80/:0:1","tags":["mysql"],"title":"mysql基础","uri":"/posts/mysql_%E5%9F%BA%E7%A1%80/"},{"categories":["笔记"],"content":"数据引擎 mysql有InnoDB,MyISAM,Merge等存储引擎，我们可以针对每一个表指定引擎。 CREATE TABLE parent ( id INT NOT NULL, PRIMARY KEY (id) ) ENGINE=INNODB; 查看支持的引擎。show engines InnoDB和MyISAM对比： InnoDB 支持事务，MyISAM 不支持事务。这是 MySQL 将默认存储引擎从 MyISAM 变成 InnoDB 的重要原因之一； InnoDB 支持外键，而 MyISAM 不支持。对一个包含外键的 InnoDB 表转为 MYISAM 会失败； InnoDB 是聚集索引，MyISAM 是非聚集索引。聚簇索引的文件存放在主键索引的叶子节点上，因此 InnoDB 必须要有主键，通过主键索引效率很高。但是辅助索引需要两次查询，先查询到主键，然后再通过主键查询到数据。因此，主键不应该过大，因为主键太大，其他索引也都会很大。而 MyISAM 是非聚集索引，数据文件是分离的，索引保存的是数据文件的指针。主键索引和辅助索引是独立的。 InnoDB 不保存表的具体行数，执行 select count(*) from table 时需要全表扫描。而MyISAM 用一个变量保存了整个表的行数，执行上述语句时只需要读出该变量即可，速度很快； InnoDB 最小的锁粒度是行锁，MyISAM 最小的锁粒度是表锁。一个更新语句会锁住整张表，导致其他查询和更新都会被阻塞，因此并发访问受限。这也是 MySQL 将默认存储引擎从 MyISAM 变成 InnoDB 的重要原因之一； 如何选择： 是否要支持事务，如果要请选择 InnoDB，如果不需要可以考虑 MyISAM； 如果表中绝大多数都只是读查询，可以考虑 MyISAM，如果既有读写也挺频繁，请使用InnoDB。 系统奔溃后，MyISAM恢复起来更困难，能否接受，不能接受就选 InnoDB； MySQL5.5版本开始Innodb已经成为Mysql的默认引擎(之前是MyISAM) ","date":"2019-03-01","objectID":"/posts/mysql_%E5%9F%BA%E7%A1%80/:0:2","tags":["mysql"],"title":"mysql基础","uri":"/posts/mysql_%E5%9F%BA%E7%A1%80/"},{"categories":["笔记"],"content":"字符集 字符集可以针对全局、数据库、表、列进行设置 show variables like ‘character%'; 查看当前字符集设置 /etc/mysql/mysql.conf.d/mysqld.cnf 文件[musqld]下增加：character-set-server =utf8 重启服务 sudo service mysql restart ","date":"2019-03-01","objectID":"/posts/mysql_%E5%9F%BA%E7%A1%80/:0:3","tags":["mysql"],"title":"mysql基础","uri":"/posts/mysql_%E5%9F%BA%E7%A1%80/"},{"categories":["笔记"],"content":"数据类型 mysql数据类型主要包含整数、浮点数、日期、时间及字符串。 整数 小数 字段名 float(M,N)，M代表总共位数；N代表小数位数。若不知道M，N则有硬件和操作系统决定。 日期时间 datetime输入什么时间取出就是什么时间，但是timestamp就是会根据数据库的时区变化。 我们还可以利用timestamp来指定字段插入或更新的时候自动生成/修改值 alter table user add last_time timestamp default current_timestamp on update current_timestamp; 字符串 CHAR(M), VARCHAR(M)不同之处 CHAR(M)定义的列的长度为固定的，M取值可以为0～255之间，当保存CHAR值时，在它们的右边填充空格以达到指定的长度。当检 索到CHAR值时，尾部的空格被删除掉。在存储或检索过程中不进行大小写转换。CHAR存储定长数据很方便，CHAR字段上的索引效率比较高。 VARCHAR(M)定义的列的长度为可变长字符串，M取值可以为0~65535之间(旧版本255），(VARCHAR的最大有效长度由最大行大小和使用 的字符集确定。整体最大长度是65,532字节）。VARCHAR值保存时只保存需要的字符数，另加一个字节来记录长度(如果列声明的长度超过255，则 使用两个字节)。VARCHAR值保存时不进行填充。 enum只能选用其中的一个，而set可以是其中几个的联合，set会把值的末尾空格去除。 二进制 数据类型的选择 在长度一定的情况下，浮点数float能表示更大的范围，但是定点数decimal更准确 char固定长度，自动删除尾部空格，varchar是可变长度，不会删除尾部空格。 ","date":"2019-03-01","objectID":"/posts/mysql_%E5%9F%BA%E7%A1%80/:0:4","tags":["mysql"],"title":"mysql基础","uri":"/posts/mysql_%E5%9F%BA%E7%A1%80/"},{"categories":["笔记"],"content":"数据库基本操作 showdatabases查看所有数据库,其中MySQL是必须的，描述用户访问权限 创建数据库。create database db1; 查看数据库创建信息。show create database db1; 删除数据库。drop database db1; 切换数据库。use db1; ","date":"2019-03-01","objectID":"/posts/mysql_%E5%9F%BA%E7%A1%80/:0:5","tags":["mysql"],"title":"mysql基础","uri":"/posts/mysql_%E5%9F%BA%E7%A1%80/"},{"categories":["笔记"],"content":"数据表的基本操作 创建数据表。 create table tb_name ( 字段名1，数据类型[列级别约束条件][默认值]， 字段名2，数据类型[列级别约束条件][默认值] ) 约束 主键约束 主键约束。要求唯一，不能为空。 可以直接在类型后指定，也可以定义完所有的列之后指定。 create table tb1 ( id int primary key, name varchar(25) ) create table tb2 ( id int, name varchar(25), primary key(id) ) 联合主键约束。 联合主键要用第二种声明方式了。 create table tb2 ( id int, name varchar(25), primary key(id,name) ) 外键约束 只有InnoDB引擎才能使用外键 外键约束用在表之间建立链接。可以是一列或多列。外键可以为空值，如果不为空则必须等于另一个表中的主键。 外键不一定必须是另一个表的主键，只要满足唯一性就好。 外键中主键所在的那个表是主表，想关联外表的表是从表。 create table if not exists user ( id int, name varchar(25), classid int, cityid int, constraint fk_user_class foreign key (classid) references class(id) on delete cascade on update cascade, constraint fk_user_city foreign key (cityid) references city(id) ) user表classid，cityid分别链接class，city表中的id。 另外我们还自定义了一个on，当我们没指定on的时候，删除和修改主表中的主键都是不可以的。 on delete，on update后面都可以跟参数，有4种参数： restrict方式：严格模式，同no action，都是立即检查外键约束；不能删除和改 cascade方式：也叫级联方式，在父表上update/delete记录时，同步update/delete子表的匹配记录 No action方式：如果子表中有匹配的记录,则不允许对父表对应候选键进行update/delete set null方式：在父表上update/delete记录时，将子表上匹配记录的列设为null 要注意子表的外键列不能为not null 外键默认是严格模式。 非空约束 create table class ( id int primary key, name varchar(20) not null ) 唯一约束 create table person ( id int primary key, name varchar(20) unique ) 要求表中该值唯一，但只能出现一个空值。但是mysql是可以多个null的。 默认约束 字段名 数据类型 default 默认值 比如用户表男性较多，那么性别默认值可以设置为男 自增约束 自增类型可以是任何整数类型 字段名 数据类型 auto_increment 查看数据表结构 desc tb1 查看表的结构 show create table tb1 查看表的创建语句 修改表 重命名表 alter table 旧表名 rename 新表名 修改表引擎 alter table 表名 engine=\"InnoDB” 修改字段类型 alter table 表名 change 旧字段 新字段 数据类型 添加字段 alter table 表名 add 字段名 类型（这里我们可以指定字段的位置） 删除字段 alter table 表名 drop 字段名 添加外键 alter table 表名 add constraint 外键名 foreign key （字段名）references 主表名（字段名） 删除外键 alter table 表名 drop foreign key 外键名 删除表 删除表 drop table 表名1，表名2… 如果有字段被其他表关联，要先删除外键 ","date":"2019-03-01","objectID":"/posts/mysql_%E5%9F%BA%E7%A1%80/:0:6","tags":["mysql"],"title":"mysql基础","uri":"/posts/mysql_%E5%9F%BA%E7%A1%80/"},{"categories":["笔记"],"content":"数据库函数 数学函数 常见的有: 绝对值函数abs(x) 三角函数 随机函数 rand(),rand(x) 0\u003c=结果\u003c=1，x是种子 向上取整 ceil(x) 向下取整 floor(x) 平均值 avg(x) select avg(price) from user; 字符串函数 长度 char_length(“xxx”) 小写 lcase(“XXX”) 大写 ucase(“xxx”) 去除空格 trim(”x”)，ltrim(“xx”)，rtrim(“xx”) 取子串 substring(“str”,start,len) start从1开始，负数表示从后面开始 子串位置 instr(“abcd”,“c”) 字符串翻转 reverse(“abc”) 日期时间函数 当前时间 now() 当前日期 current_date() 当前时间 current_time() 获取年月日year(now())，month(now())，day(now()) 格式化 date_format(date,format) 日期时间加减 date_add()，addtime()等 加密函数 md加密 md5(str) 聚合函数 AVG()函数忽略列值为NULL的行。 MAX()函数忽略列值为NULL的行。 MIN()函数忽略列值为NULL的行。 SUM()函数忽略列值为NULL的行。 COUNT()函数有两种使用方式： 使用COUNT(*)对表中行的数目进行计数，不管表列中包含的是空值（NULL）还是非空值。 使用COUNT(column)对特定列中具有值的行进行计数，忽略NULL值。 其他函数 返回最近一次插入的数据的id last_insert_id() 这里需要注意，last_insert_id是与表无关的，向a插入，再向b插入，获取的是b表的最新id ","date":"2019-03-01","objectID":"/posts/mysql_%E5%9F%BA%E7%A1%80/:0:7","tags":["mysql"],"title":"mysql基础","uri":"/posts/mysql_%E5%9F%BA%E7%A1%80/"},{"categories":["笔记"],"content":"增、删，改表中的数据 插入数据 insert into 表名（字段名，字段名）values(值，值)； 将查询结果插入数据表 insert into 表名 （字段名）select …. 修改数据 update 表名 set 字段名=value，字段名=value [条件] 删除数据 delete from 表名 [条件] ","date":"2019-03-01","objectID":"/posts/mysql_%E5%9F%BA%E7%A1%80/:0:8","tags":["mysql"],"title":"mysql基础","uri":"/posts/mysql_%E5%9F%BA%E7%A1%80/"},{"categories":["笔记"],"content":"单表的查询 select {* |字段列表} [ from 表1，表2 [where] [group by] [having] [order by] [limit [offet,]rowcount] ] where in操作 where id in (1,2,3)或者where id not in(1,2,3) 位于两者之间 where price between 1 and 2或者where price not between 1 and 2 通配符 where name like ‘%xxx%'。_只能通配一个字符 is null判断 where name is null或者where name not is null 多个条件 用and连接 条件或用or连接 去重 select distinct 字段名 分组 分组通常和count(),max()等函数配合使用 比如： select s_id,count(*) from fruits group by s_id 另外group_concat(字段名)可以把分组中想要的各字段名显示出来 多字段分组 分组可以指定多个字段，先按第一个字段分组，再按第二个分组，一次类推。 having过滤分组 group by 和having配合，只有满足条件的分组才能被显示。 select s_id,count(*) from fruits group by s_id having count(*)\u003e2 排序 order by 字段名，字段名 [asc/desc] 默认升序 如果有分组是对分组的排序 limit limit[offset] rows 如果想跳过n行取所有，可以指定一个很大的数字 ","date":"2019-03-01","objectID":"/posts/mysql_%E5%9F%BA%E7%A1%80/:0:9","tags":["mysql"],"title":"mysql基础","uri":"/posts/mysql_%E5%9F%BA%E7%A1%80/"},{"categories":["笔记"],"content":"连接查询 连接查询就是对2个或2个以上的表连接为一个表进行查询。 from 表1 [链接方式] join 表2 [on 条件] 链接方式常有交叉链接cross,内连接inner，左外连接left,右外连接right 交叉连接 select * from user cross join class; 这样的话，左表每条记录分别和右表每条记录连接，造成很大的无用的数据，实际意义不大。 内连接 select * from user inner join class on user.classid=class.id; 保留的都是满足条件的记录，使用最多。 除此之外，内连接还有where形式的： select * from user,class where user.classid=class.id; 但是where相比inner join on效率低 左外连接 select * from user left join class on user.classid=class.id; 在进行连接后，以左边的表为主，左边表所有的数据都会显示在结果中，哪怕他们不符合on后边的条件，此时右边表的数据显示null 右连接 右连接和左连接类似，只不过是以右边为主 联合查询 联合查询就是把多个查询语句结果集中在一起。 SELECT column_name(s) FROM table_name1 UNION SELECT column_name(s) FROM table_name2 联合的时候两个结果集的字段名不要求一样，但是数量和类型要求一样。 union联合的时候不包括重复行 union all联合的时候包括重复行 ","date":"2019-03-01","objectID":"/posts/mysql_%E5%9F%BA%E7%A1%80/:0:10","tags":["mysql"],"title":"mysql基础","uri":"/posts/mysql_%E5%9F%BA%E7%A1%80/"},{"categories":["笔记"],"content":"子查询 子查询是指一个查询语句嵌套在另一个查询语句中，首先内部子查询的结果作为外部查询的输入。 any 只要满足子查询随意的一个结果即可 all 满足所有子查询的结果 exists 子查询如果查到了一行，那么exists就是true,否则为false外查询不进行查询 in 存在和子查询中的结果一样的数据 not in 参考in ","date":"2019-03-01","objectID":"/posts/mysql_%E5%9F%BA%E7%A1%80/:0:11","tags":["mysql"],"title":"mysql基础","uri":"/posts/mysql_%E5%9F%BA%E7%A1%80/"},{"categories":["笔记"],"content":"起别名 字段 [as] 别名 表 [as] 别名 ","date":"2019-03-01","objectID":"/posts/mysql_%E5%9F%BA%E7%A1%80/:0:12","tags":["mysql"],"title":"mysql基础","uri":"/posts/mysql_%E5%9F%BA%E7%A1%80/"},{"categories":["笔记"],"content":"利用正则查询 利用正则匹配查询 select * from user where name regexp \"^h\"; select * from user where name regexp \"a$\" 查询name以h开头的数据，name以a结尾的数据。 ","date":"2019-03-01","objectID":"/posts/mysql_%E5%9F%BA%E7%A1%80/:0:13","tags":["mysql"],"title":"mysql基础","uri":"/posts/mysql_%E5%9F%BA%E7%A1%80/"},{"categories":["笔记"],"content":"索引 索引是一个独立的、存在在磁盘上的数据库结构，它包含着表里所有记录的引用指针。有了索引查询的时候就不用从头到尾一行一行的判断了，速度更快。比如两万条数据，select * from user where id =30必须遍历表。有了索引直接找到id=30的行。 mysql的索引类型有btree和hash，具体和存储引擎相关。 索引优点： 通过创建唯一索引，保证每一条数据的唯一性 大大提升查询速度，这是主要原因 索引缺点 创建和维护需要时间，尤其是数据量大以后，每次插入要排序 占用空间 索引分类 普通索引和唯一索引 普通索引允许索引列有重复值和空值。 唯一索引必须唯一，可以为空。主键索引是特殊的唯一索引，不能为空。 mysql创建外键的时候会自动为该列创建普通索引 单列索引和组合索引 单列索引顾名思义就是只包含一个列。 组合索引包含多个列，联合索引又叫复合索引。对于复合索引:Mysql从左到右的使用索引中的字段，一个查询可以只使用索引中的一部份，但只能是最左侧部分。例如索引是key index (a,b,c). 可以支持a | a,b| a,b,c 3种组合进行查找，但不支持 b,c进行查找 .当最左侧字段是常量引用时，索引就十分有效。 全文索引 全文索引可以在char、varchar或者text类型上创建。 常用的全文检索模式有两种： 自然语言的全文索引 自然语言模式是MySQL 默认的全文检索模式。自然语言模式不能使用操作符，不能指定关键词必须出现或者必须不能出现等复杂查询。 默认情况下，或者使用 in natural language mode 修饰符时，match() 函数对文本集合执行自然语言搜索，上面的例子都是自然语言的全文索引。 自然语言搜索引擎将计算每一个文档对象和查询的相关度。这里，相关度是基于匹配的关键词的个数，以及关键词在文档中出现的次数。在整个索引中出现次数越少的词语，匹配时的相关度就越高。相反，非常常见的单词将不会被搜索，如果一个词语的在超过 50% 的记录中都出现了，那么自然语言的搜索将不会搜索这类词语。上面提到的，测试表中必须有 4 条以上的记录，就是这个原因。 这个机制也比较好理解，比如说，一个数据表存储的是一篇篇的文章，文章中的常见词、语气词等等，出现的肯定比较多，搜索这些词语就没什么意义了，需要搜索的是那些文章中有特殊意义的词，这样才能把文章区分开。 布尔全文索引 在布尔搜索中，我们可以在查询中自定义某个被搜索的词语的相关性，当编写一个布尔搜索查询时，可以通过一些前缀修饰符来定制搜索。 MySQL 内置的修饰符，上面查询最小搜索长度时，搜索结果 ft_boolean_syntax 变量的值就是内置的修饰符，下面简单解释几个，更多修饰符的作用可以查手册 必须包含该词 必须不包含该词 提高该词的相关性，查询的结果靠前 \u003c 降低该词的相关性，查询的结果靠后 (*)星号 通配符，只能接在词后面 对于上面提到的问题，可以使用布尔全文索引查询来解决，使用下面的命令，a、aa、aaa、aaaa 就都被查询出来了。 select * test where match(content) against(‘a*’ in boolean mode); MySQL 中的全文索引，有两个变量，最小搜索长度和最大搜索长度，对于长度小于最小搜索长度和大于最大搜索长度的词语，都不会被索引。通俗点就是说，想对一个词语使用全文索引搜索，那么这个词语的长度必须在以上两个变量的区间内。 最小搜索长度 MyISAM 引擎下默认是 4，InnoDB 引擎下是 3，也即，MySQL 的全文索引只会对长度大于等于 4 或者 3 的词语建立索引。 创建索引 创建表的时候，修改表的时候或者直接create index都可以创建索引。 一个列上可以创建多个索引 创建表时建索引 create table 表名 [字段名 类型] [unionque|fulltext|spatial] [index|key] [索引名] (字段名[length]) [asc|desc] 其中unique、fulltext、spatial分别代表唯一、全文、空间索引，不指定时为普通索引；index和key意义一样；索引名可以为空，此时索引名默认为列名；asc或desc代表升序或降序的索引。 **explain select * from user where id=30;**利用explain可以查看执行的时候是不是用到了索引。 修改表时增索引 alter table 表名 add [unionque|fulltext|spatial] [index|key] [索引名] (字段名[length]) [asc|desc] 直接increate index create [unionque|fulltext|spatial] [index|key] [索引名] on 表名 (字段名[length]) [asc|desc] 删除索引 drop index 索引名 on 表名 alter table 表名 drop index 索引名 索引涉及原则 索引并不是越多越好 避免经常更新的表进行索引，并且索引尽可能少。经常查询的表可以建索引。 数据量少的话不用建索引，不然适得其反。 如果列上相同的值很多，不适合建索引，比如性别列 频繁进行排序order by和分组group by的列上适合创建索引 尽量使用短索引，比如有的字段0-255，实际上前10个字符就能判断出唯一，可指定索引长度10 索引无用的情况 利用索引可以直接定位，免去了遍历，是最有效的优化查询方案。但是有些情况虽然字段带索引，是不起作用的： 利用like，而且%在第一个位置，索引无效。 一个索引可以有16个字段，只有查询条件是索引的第一个字段时才有效。 查询条件有or时，or前后条件都是索引才有效。 ","date":"2019-03-01","objectID":"/posts/mysql_%E5%9F%BA%E7%A1%80/:0:14","tags":["mysql"],"title":"mysql基础","uri":"/posts/mysql_%E5%9F%BA%E7%A1%80/"},{"categories":["笔记"],"content":"视图 视图（view）是一种虚拟存在的表，是一个逻辑表，本身并不包含数据。作为一个select语句保存在数据字典中的。 通过视图，可以展现基表的部分数据；视图数据来自定义视图的查询中使用的表，使用视图动态生成。 视图本身没有数据，数据的变化是修改的数据表。 不是所有的视图都可以做DML操作。 有下列内容之一，视图不能做DML操作： 　①select子句中包含distinct 　②select子句中包含组函数 　③select语句中包含group by子句 　④select语句中包含order by子句 　⑤select语句中包含union 、union all等集合运算符 　⑥where子句中包含相关子查询 　⑦from子句中包含多个表 　⑧如果视图中有计算列，则不能更新 　⑨如果基表中有某个具有非空约束的列未出现在视图定义中，则不能做insert操作 ","date":"2019-03-01","objectID":"/posts/mysql_%E5%9F%BA%E7%A1%80/:0:15","tags":["mysql"],"title":"mysql基础","uri":"/posts/mysql_%E5%9F%BA%E7%A1%80/"},{"categories":["笔记"],"content":"存储过程和函数 存储过程可以简单的看成一条或多条sql语句的集合。 创建存储过程 create procedure sp_name ([param]) [characteristics ...] routine_body 其中参数可以是 IN OUT INOUT三种类型 routine_body是sql代码的内容，可以用begin…end开始和结束。 示例： delimiter // create procedure pdtest() begin select * from user; end // delimiter //指定语句用//来结束，防止与sql语句冲突，定义完存储过程后，在回到原来的delimiter ; 创建函数 create function func_name([param]) returns type [characteristic...] routine_body 参数和存储过程一样，也是那三种，默认是IN。 returns types是必须要的。不需要begin end 示例： delimiter // create function functest() returns varchar(10) return (select name from user where id=2); // 变量及流程控制 我们可以声明变量、使用流程控制、光标等。 调用存储过程或函数 call pd_name() 调用存储过程 select func_name() 调用函数 查看存储过程和函数 show {procedure | function} status [like “pattern”] show create {procedure | function} name 修改、删除存储过程和函数 alter {procedure | function} name… drop procedure name; drop function name; ","date":"2019-03-01","objectID":"/posts/mysql_%E5%9F%BA%E7%A1%80/:0:16","tags":["mysql"],"title":"mysql基础","uri":"/posts/mysql_%E5%9F%BA%E7%A1%80/"},{"categories":["笔记"],"content":"触发器 触发器和存储过程类似，都是嵌入到mysql的一段程序。 触发器是由事件触发某个操作，包括insert、update、delete。 创建触发器 create trigger trigger_name trigger_time trigger_event on ta_name for each row trigger_stmt 当有多条sql语句时，用begin end create trigger trigger_name trigger_time trigger_event on ta_name for each row begin trigger_stmt... end 示例： create trigger tgtest after update on user for each row begin ...//此处小心插入死循环 end 查看、删除触发器 show triggers; 直接从information_schema数据库triggers表中查，另外还有存储过程和函数表 drop trigger name ","date":"2019-03-01","objectID":"/posts/mysql_%E5%9F%BA%E7%A1%80/:0:17","tags":["mysql"],"title":"mysql基础","uri":"/posts/mysql_%E5%9F%BA%E7%A1%80/"},{"categories":["笔记"],"content":"事务 MyISAM 不支持事务，InnoDB支持事务。 在innodb里面， 所有的活动都是运行在事务里面的。innodb默认autocommit=1的，意思就是MySQL会在每个语句执行的时候自动提交事务，当然是语句没有报错，如果报错了，那就会自动回滚rollback。 查看当前autocommit模式 show variables like 'autocommit'; 设置事务级别 开启autocommit模式 SET [SESSION | GLOBAL] TRANSACTION ISOLATION LEVEL {READ UNCOMMITTED | READ COMMITTED | REPEATABLE READ | SERIALIZABLE} 这里session是指对当前连接上执行的事务设置默认事务级别。默认是对下一个（未开始）事务设置隔离级别 查看事务隔离级别 SELECT @@global.tx_isolation; SELECT @@session.tx_isolation; SELECT @@tx_isolation; 各种读问题 未提交读（Read uncommitted）就是能读取到其他回话中未提交事务的修改，造成脏读问题。 脏读。就是一个事务读取到了另一个事务修改还未提交的数据变化，因为这个变化可能被恢复。 提交读(Read Committed) 只能读取到别人已经提提交的数据。避免了脏读的问题。但是在自己的事务中多次读取，数据可能不一致也就是不可重复读。 不可重复读。就是在同一事务中多次读一个数据，但是不一致，因为期间哟其他事务修改并提交了。 可重复读(Repeated Read) InnoDB默认级别，同一个事务中多次读取都是一致的，解决了不可重复读的问题。但是有幻读问题。 幻读。可重复读级别虽然外部的修改和插入不会影响本事务查看数据，但是可能影响本事务修改或插入。 串行读(Serializable) 现在好了，完全串行化的读，每次读都需要获得表级共享锁，读写相互都会阻塞。 事务使用 start transaction; ... ... commit;//rollback; ","date":"2019-03-01","objectID":"/posts/mysql_%E5%9F%BA%E7%A1%80/:0:18","tags":["mysql"],"title":"mysql基础","uri":"/posts/mysql_%E5%9F%BA%E7%A1%80/"},{"categories":["笔记"],"content":"三大范式 第一范式:确保每列的原子性. 第一范式是最基本的范式。 数据库表中的字段都是单一属性的，不可再分。 只要是关系数据库都满足第一范式 如果每列(或者每个属性)都是不可再分的最小数据单元(也称为最小的原子单元),则满足第一范式. 例如:顾客表(姓名、编号、地址、……)其中\"地址\"列还可以细分为国家、省、市、区等。 第二范式(确保表中的每列都和主键相关). 如果一个关系满足第一范式,并且除了主键以外的其它列,都依赖于该主键,则满足第二范式. 例如:订单表(订单编号、产品编号、定购日期、价格、……)，“订单编号\"为主键，“产品编号\"和主键列没有直接的关系，即\"产品编号\"列不依赖于主键列，应删除该列。 第三范式(确保每列都和主键列直接相关,而不是间接相关). 如果一个关系满足第二范式,并且除了主键以外的其它列都不依赖于主键列,则满足第三范式. 为了理解第三范式，需要根据Armstrong公里之一定义传递依赖。假设A、B和C是关系R的三个属性，如果A-〉B且B-〉C，则从这些函数依赖中，可以得出A-〉C，如上所述，依赖A-〉C是传递依赖。 例如:订单表(订单编号，定购日期，顾客编号，顾客姓名，……)，初看该表没有问题，满足第二范式，每列都和主键列\"订单编号\"相关，再细看你会发现\"顾客姓名\"和\"顾客编号\"相关，“顾客编号\"和\"订单编号\"又相关，最后经过传递依赖，“顾客姓名\"也和\"订单编号\"相关。为了满足第三范式，应去掉\"顾客姓名\"列，放入客户表中。 ","date":"2019-03-01","objectID":"/posts/mysql_%E5%9F%BA%E7%A1%80/:0:19","tags":["mysql"],"title":"mysql基础","uri":"/posts/mysql_%E5%9F%BA%E7%A1%80/"},{"categories":["笔记"],"content":"案例 先分组取分组内的n条 查询男、女生前两名 select a.* from student a where (select count(distinct b.score) from student b where a.sex=b.sex and b.score\u003ea.score)\u003c2; 其中子查询的select取决于外面的查询，每次拿外面的一个数据带入到里面进行判断，如果比他分数大的人少于两个，那么不用说他在前两名，满足条件。 为啥要distinct？因为分数可能有重复！ 看比他大的在两个以内那就是前两名，注意条件中\u003c=和\u003c的区别 ","date":"2019-03-01","objectID":"/posts/mysql_%E5%9F%BA%E7%A1%80/:0:20","tags":["mysql"],"title":"mysql基础","uri":"/posts/mysql_%E5%9F%BA%E7%A1%80/"},{"categories":["笔记"],"content":"go http流程","date":"2019-02-18","objectID":"/posts/go_http%E6%B5%81%E7%A8%8B/","tags":["golang"],"title":"go http流程","uri":"/posts/go_http%E6%B5%81%E7%A8%8B/"},{"categories":["笔记"],"content":"go http流程 go中使用web非常简单，因为api封装很完美，一般我们会写下面代码： http.Handle(\"/\",handler) http.HandleFunc(\"/user\",HandlerFunc) http.ListenAndServe(\":8080\",nil) 还有下面的： mux := http.ServeMux{} mux.Handle(\"\",handler) http.ListenAndServe(\"\",\u0026mux) 前者没有自动路由器，那么会使用默认的serveMux。 还会有疑问，为什么有的是handler有的是方法？ 其实方法也是实现了handler接口，传进去后被强转为handler即可。 证据如下： type HandlerFunc func(ResponseWriter, *Request) // ServeHTTP calls f(w, r). func (f HandlerFunc) ServeHTTP(w ResponseWriter, r *Request) { f(w, r) } // HandleFunc registers the handler function for the given pattern. func (mux *ServeMux) HandleFunc(pattern string, handler func(ResponseWriter, *Request)) { if handler == nil { panic(\"http: nil handler\") } mux.Handle(pattern, HandlerFunc(handler)) } Go代码的执行流程 通过对http包的分析之后，现在让我们来梳理一下整个的代码执行过程。 首先调用Http.HandleFunc 按顺序做了几件事： 1 调用了DefaultServeMux的HandleFunc 2 调用了DefaultServeMux的Handle 3 往DefaultServeMux的map[string]muxEntry中增加对应的handler和路由规则 其次调用http.ListenAndServe(\":9090”, nil) 按顺序做了几件事情： 实例化Server 调用Server的ListenAndServe() 调用net.Listen(“tcp”, addr)监听端口 启动一个for循环，在循环体中Accept请求 对每个请求实例化一个Conn，并且开启一个goroutine为这个请求进行服务go c.serve() 读取每个请求的内容w, err := c.readRequest() 判断handler是否为空，如果没有设置handler（这个例子就没有设置handler），handler就设置为DefaultServeMux 调用handler的ServeHttp 在这个例子中，下面就进入到DefaultServeMux.ServeHttp 根据request选择handler，并且进入到这个handler的ServeHTTP mux.handler(r).ServeHTTP(w, r) 选择handler： A 判断是否有路由能满足这个request（循环遍历ServerMux的muxEntry） B 如果有路由满足，调用这个路由handler的ServeHttp C 如果没有路由满足，调用NotFoundHandler的ServeHttp ","date":"2019-02-18","objectID":"/posts/go_http%E6%B5%81%E7%A8%8B/:0:1","tags":["golang"],"title":"go http流程","uri":"/posts/go_http%E6%B5%81%E7%A8%8B/"},{"categories":["笔记"],"content":"defer语句","date":"2019-01-17","objectID":"/posts/go_defer/","tags":["golang"],"title":"defer语句","uri":"/posts/go_defer/"},{"categories":["笔记"],"content":"defer语句 defer及defer函数的执行顺序分2步： 执行defer语句，计算函数的入参的值，并传递给函数，但不执行函数，而是将函数压入栈。 函数return语句后，或panic后，执行压入栈的函数，函数中变量的值，此时会被计算。 Each time a “defer” statement executes, the function value and parameters to the call are evaluated as usual and saved anew but the actual function is not invoked. ","date":"2019-01-17","objectID":"/posts/go_defer/:0:1","tags":["golang"],"title":"defer语句","uri":"/posts/go_defer/"},{"categories":["笔记"],"content":"defer语句的引用问题 注意： defer 后面常跟匿名函数，函数内用到外面的变量，属于闭包，变量是引用 func main() { fmt.Println(test()) //5 fmt.Println(test2()) //4 } func test()(x int){ x=2 defer func() { x=5 fmt.Println(x)//5 }() return 4 } func test2()(x int){ x=2 defer func(i int) { i=5 }(x) return 4 } ","date":"2019-01-17","objectID":"/posts/go_defer/:0:2","tags":["golang"],"title":"defer语句","uri":"/posts/go_defer/"},{"categories":["笔记"],"content":"defer与return 编译器将 defer 处理成两个函数调用，deferproc 定义一个延迟调用对象，然后在函数结束 前通过 deferreturn 完成最终调用。 大致表达为： step 1 : 在defer表达式的地方，会调用runtime.deferproc(size int32, fn *funcval)保存延时调用，注意这里保存了延时调用的参数 step 2 : 在return时，先将返回值保存起来 step 3 : 按FILO顺序调用runtime.deferreturn，即延时调用 step 4 : RET指令 func namedReturn() (r int) { defer func() { r++ fmt.Println(\"defer in namedReturn : r = \", r) }() return } func unnamedReturn() int { var r int defer func() { r++ fmt.Println(\"defer in unnamedReturn : r = \", r) }() return r } func main() { //1 fmt.Println(\"namedReturn : r = \", namedReturn()) //0 fmt.Println(\"unnamedReturn : r = \", unnamedReturn()) } 原因就是return会将返回值先保存起来，对于无名返回值来说，保存在一个临时对象中，defer是看不到这个临时对象的；而对于有名返回值来说，就保存在已命名的变量中。 匿名返回值是在return执行时被声明，有名返回值则是在函数声明的同时被声明，因此在defer语句中只能访问有名返回值，而不能直接访问匿名返回值； ","date":"2019-01-17","objectID":"/posts/go_defer/:0:3","tags":["golang"],"title":"defer语句","uri":"/posts/go_defer/"},{"categories":["笔记"],"content":"认证鉴权","date":"2018-12-17","objectID":"/posts/%E8%AE%A4%E8%AF%81%E9%89%B4%E6%9D%83/","tags":["golang"],"title":"认证鉴权","uri":"/posts/%E8%AE%A4%E8%AF%81%E9%89%B4%E6%9D%83/"},{"categories":["笔记"],"content":"认证鉴权 HTTP协议是无状态的和Connection: keep-alive的区别： 无状态是指协议对于事务处理没有记忆能力，服务器不知道客户端是什么状态。从另一方面讲，打开一个服务器上的网页和你之前打开这个服务器上的网页之间没有任何联系。 HTTP是一个无状态的面向连接的协议，无状态不代表HTTP不能保持TCP连接，更不能代表HTTP使用的是UDP协议（无连接）。 从HTTP/1.1起，默认都开启了Keep-Alive，保持连接特性，简单地说，当一个网页打开完成后，客户端和服务器之间用于传输HTTP数据的TCP连接不会关闭，如果客户端再次访问这个服务器上的网页，会继续使用这一条已经建立的连接。 Keep-Alive不会永久保持连接，它有一个保持时间，可以在不同的服务器软件（如Apache）中设定这个时间。 http协议本身是无状态的，所以服务端不能知道用户的状态，需要借助些技术手段来实现。 basic认证 basic认证是基于http协议。 访问受保护的资源，没有登录 服务端返回401，并响应头设置WWW-Authenticate：Basic realm=”localhost 浏览器弹出用户名密码框，输入后再次访问资源通过，后续的访问都会请求头带上用户名和密码，格式为Authorization：Basic base64(name:password) http.HandleFunc(\"/user\", func(writer http.ResponseWriter, request *http.Request) { authHeader := request.Header.Get(\"Authorization\") fmt.Println(authHeader) //Basic emhhbmdzYW46MTIz if authHeader == \"\" { //请求没有头信息，返回401 writer.Header().Add(\"WWW-Authenticate\", \"Basic realm=”localhost\") writer.WriteHeader(401) } else { //请求有头信息，base64解码，并校验用户名密码 authHeader = authHeader[6:] bytes, _ := base64.StdEncoding.DecodeString(authHeader) fmt.Println(string(bytes)) //zhangsan:123 nameAndPwd := strings.Split(string(bytes), \":\") if nameAndPwd[0] == \"zhangsan\" \u0026\u0026 nameAndPwd[1] == \"123\" { writer.Write([]byte(\"login success\")) writer.WriteHeader(200) } else { writer.Header().Add(\"WWW-Authenticate\", \"Basic realm=”localhost\") writer.WriteHeader(401) } } }) 这种basic认证是不安全的，因为都知道是base64编码，相当于铭文。 另外，登录后每次都带上密码，要想注销很麻烦，一般采用约定用户名密码表明要注销，比如：logout:logout cookie session cookie是http协议中的，在请求头和响应头中，但是session并不是http协议中的一部分。 如果不设置Expires的属性那么Cookie的存活时间就是在关闭浏览器的时候。默认cookies失效时间是直到关闭浏览器，cookies失效，也可以指定cookies时间。 服务端通过Set-Cookie头来指导浏览器保存cookie，指导过期时间、域名 cookie保存在浏览器是不安全的，通过嵌入脚本就能拿到cookie，比如xss（跨网站脚本），可以通过设置cookie的httponly属性 + https来避免。 session保存在服务端，但是容易被劫持，也就是通过监听网络拿到sessionid，有效的手段就是https；另外如果session生成规则比较简单，也是可以暴力尝试的。 jwt jwt全称是json web token，它是保存在客户端的。传统的session是保存在服务端，每次拿到sessionid后还要查询数据库等来获取用户信息并校验，服务端压力比较大。jwt通过把用户信息通过一定的格式并加密后返回给客户端，每次请求传给服务端（一般放在header中），服务端进行解密校验即可自动用户的登录态和数据，避免了查询数据库。 jwt有三部分：base64url(header) +”.” +base64url(payload) +”.“+ 加密(base64url(header) +”.” +base64url(payload) +盐) header： { \"alg\": \"HS256\", \"typ\": \"JWT\" } 它指定了加密方法 Claims 或者叫payload，指定了用户的信息及jwt的其他信息，比如userId、过期时间等 { \"sub\": \"1234567890\", \"name\": \"John Doe\", \"admin\": true } golang中有jwt-go等框架，方便我们生成校验jwt 生成jwt type Person struct { Name string Age int } type MyCustomClaims struct { Person Person jwt.StandardClaims } func jwtTest() string { mySigningKey := []byte(\"AllYourBase\") // Create the Claims claims := MyCustomClaims{ Person: Person{ Name: \"zhang san\", Age: 12, }, StandardClaims: jwt.StandardClaims{ NotBefore: int64(time.Now().Unix()), ExpiresAt: time.Now().Unix() + 3, Issuer: \"test\", Id: \"12345\", }, } token := jwt.NewWithClaims(jwt.SigningMethodHS256, claims) ss, err := token.SignedString(mySigningKey) fmt.Printf(\"%v %v\\n\", ss, err) return ss } 校验jwt func jwtCheck(str string) { token, err := jwt.Parse(str, func(token *jwt.Token) (interface{}, error) { return []byte(\"AllYourBase\"), nil }) if token.Valid { fmt.Println(\"You look nice today\") } else if ve, ok := err.(*jwt.ValidationError); ok { if ve.Errors\u0026jwt.ValidationErrorMalformed != 0 { fmt.Println(\"That's not even a token\") } else if ve.Errors\u0026(jwt.ValidationErrorExpired|jwt.ValidationErrorNotValidYet) != 0 { // Token is either expired or not active yet fmt.Println(\"Timing is everything\") } else { fmt.Println(\"Couldn't handle this token:\", err) } } else { fmt.Println(\"Couldn't handle this token:\", err) } c, ok := token.Claims.(jwt.MapClaims) fmt.Println(c[\"Person\"], ok) } ","date":"2018-12-17","objectID":"/posts/%E8%AE%A4%E8%AF%81%E9%89%B4%E6%9D%83/:0:1","tags":["golang"],"title":"认证鉴权","uri":"/posts/%E8%AE%A4%E8%AF%81%E9%89%B4%E6%9D%83/"},{"categories":["笔记"],"content":"godoc","date":"2018-01-01","objectID":"/posts/go_doc/","tags":["golang"],"title":"godoc","uri":"/posts/go_doc/"},{"categories":["笔记"],"content":"生成文档 生成文档是很简单的，主要在package,方法等想要注释的地方写上注释即可。可以使用//或者/**/ 举例： // this is model comment package model import \"fmt\" // user struct comment type User struct { } func (u User) Run() { fmt.Println(\" user run\") } // return the maxvalue between a and b func GetMax(a, b int) int { if a \u003e b { return a } return b } 上面的代码中我们为包、结构体，函数都写了注释，这样即可。 ","date":"2018-01-01","objectID":"/posts/go_doc/:0:1","tags":["golang"],"title":"godoc","uri":"/posts/go_doc/"},{"categories":["笔记"],"content":"查看文档 查看文档可以使用 godoc -http=:8080生产一个web服务，在localhost:8080即可查看文档 ","date":"2018-01-01","objectID":"/posts/go_doc/:0:2","tags":["golang"],"title":"godoc","uri":"/posts/go_doc/"},{"categories":["笔记"],"content":"生产示例文档 除了以上的注释文档，我们还可以在文档中查看示例； 创建 example_test.go文件 为函数创建示例代码。创建ExampleGetMax函数，规则是Example+示例函数名 为方法创建示例代码。创建ExampleUser_Run函数，规则是Example + 结构体 + _方法名 在创建示例代码的时候还可以指定Output:标签 example_test.go func ExampleUser_Run() { user := User{} user.Run() } func ExampleGetMax() { max:=GetMax(1,2) fmt.Println(max) //Output: //2 } 文档如下： ","date":"2018-01-01","objectID":"/posts/go_doc/:0:3","tags":["golang"],"title":"godoc","uri":"/posts/go_doc/"},{"categories":["笔记"],"content":"网络—链路层","date":"2017-12-31","objectID":"/posts/network_link_layer/","tags":["网络"],"title":"网络—链路层","uri":"/posts/network_link_layer/"},{"categories":["笔记"],"content":"链路层 数据链路层要完成3个目标： 封装成帧，透明传输，差错控制 帧格式 其中末尾的crc生成fcs是校验位，校验数据传输中是否出错，类似于数字签名。 链路层协议 以太网（Ethernet）协议；PPPoE（ADSL）协议；等 mac地址 以太网规协议定，接入网络的设备都必须安装网络适配器，即网卡， 数据包必须是从一块网卡传送到另一块网卡。而网卡地址就是数据包的发送地址和接收地址，也就是帧首部所包含的MAC地址，MAC地址是每块网卡的身份标识，就如同我们身份证上的身份证号码，具有全球唯一性。MAC地址采用十六进制标识，共6个字节， 前三个字节是厂商编号，后三个字节是网卡流水号，例如 4C-0F-6E-12-D2-19 有了MAC地址以后，以太网采用广播形式，把数据包发给该子网内所有主机，子网内每台主机在接收到这个包以后，都会读取首部里的目标MAC地址，然后和自己的MAC地址进行对比，如果相同就做下一步处理，如果不同，就丢弃这个包。 ","date":"2017-12-31","objectID":"/posts/network_link_layer/:0:1","tags":["网络"],"title":"网络—链路层","uri":"/posts/network_link_layer/"},{"categories":["笔记"],"content":"go fmt","date":"2017-07-15","objectID":"/posts/go_fmt/","tags":["golang"],"title":"go fmt","uri":"/posts/go_fmt/"},{"categories":["笔记"],"content":"布尔 %t 单词true或false ","date":"2017-07-15","objectID":"/posts/go_fmt/:0:1","tags":["golang"],"title":"go fmt","uri":"/posts/go_fmt/"},{"categories":["笔记"],"content":"整型 %b 表示为二进制 %c 该值对应的unicode码值 %d 表示为十进制 %o 表示为八进制 %q 该值对应的单引号括起来的go语法字符字面值，必要时会采用安全的转义表示 %x 表示为十六进制，使用a-f %X 表示为十六进制，使用A-F %U 表示为Unicode格式：U+1234，等价于\"U+%04X” %g 根据情况选择 %e 或 %f 以产生更紧凑的（无末尾的0）输出 %G 根据情况选择 %E 或 %f 以产生更紧凑的（无末尾的0）输出 ","date":"2017-07-15","objectID":"/posts/go_fmt/:0:2","tags":["golang"],"title":"go fmt","uri":"/posts/go_fmt/"},{"categories":["笔记"],"content":"浮点数 %f: 默认宽度，默认精度 %9f 宽度9，默认精度 %.2f 默认宽度，精度2 %9.2f 宽度9，精度2 %9.f 宽度9，精度0 宽度是在％之后的值，如果没有指定，则使用该值的默认值，精度是跟在宽度之后的值，如果没有指定，也是使用要打印的值的默认精度．例如：％９.２f，宽度９，精度２,如果宽度不足9就补0。 对数值而言，宽度为该数值占用区域的最小宽度；精度为小数点之后的位数。但对于 %g/%G 而言，精度为所有数字的总数。例如，对于123.45，格式 %6.2f会打印123.45，而 %.4g 会打印123.5。%e 和 %f 的默认精度为6；但对于 %g 而言，它的默认精度为确定该值所必须的最小位数 ","date":"2017-07-15","objectID":"/posts/go_fmt/:0:3","tags":["golang"],"title":"go fmt","uri":"/posts/go_fmt/"},{"categories":["笔记"],"content":"sync包","date":"2017-05-15","objectID":"/posts/go_sync/","tags":["golang"],"title":"sync包","uri":"/posts/go_sync/"},{"categories":["笔记"],"content":"锁 共享内存的并发问题 在很多语言中并发编程都有对一个内存进行读写的情况，比如下面的测试： func test() { count++ println(count) } var count int func main() { for i := 0; i \u003c 30; i++ { go test() } time.Sleep(3 * time.Second) } 很简单，我们开启30个协程对count变量进行增加写操作，而后又进行读取打印操作。 结果并不是顺序的打出1-30，因为协程的执行时机是随机的。但是有重复的结果打印出，那就是多协程并发读写的原因了。 java语言有锁，golang中也有，有了锁，就可以安全的读写了。 互斥锁 sync.Mutex sync.Mutex是一个结构体，它是开箱即用的。 它实现了Locker接口： type Locker interface { Lock() Unlock() } 通过sync.Mutex来对上面示例进行改造： func test(mutex *sync.Mutex) { mutex.Lock() defer mutex.Unlock()//lock unlock要成对出现，直接使用defer更安全 count++ println(count) } var count int func main() { mutex := sync.Mutex{} for i := 0; i \u003c 30; i++ { go test(\u0026mutex)//这里要使用指针，若发生复制便不再是一个锁了 } time.Sleep(3 * time.Second) } 注意： 未lock直接unlock报错 重复lock报错 读写锁 sync.RWMutex sync.RWMutex也是一个结构体，内部嵌入sync.Mutex，也是开箱即用的。 读写锁顾名思义就是可以加只读锁和写锁 注意： RWMutex 是单写多读锁，该锁可以加多个读锁或者一个写锁 读锁占用的情况下会阻止写，不会阻止读，多个 goroutine 可以同时获取读锁 写锁会阻止其他 goroutine（无论读和写）进来，整个锁由该 goroutine 独占适用于读多写少的场景 总结起来就是读和写是互斥的，但是读之间是不互斥的。 读写锁使用的例子： func testRead(group *sync.WaitGroup, mutex *sync.RWMutex) { mutex.RLock() defer func() { mutex.RUnlock() group.Done() }() fmt.Println(\"读取开始\") time.Sleep(time.Second) println(count) time.Sleep(time.Second) fmt.Println(\"读取结束\") } func testWrite(group *sync.WaitGroup, mutex *sync.RWMutex) { mutex.Lock() defer func() { mutex.Unlock() group.Done() }() fmt.Println(\"写开始\") time.Sleep(time.Second) count++ time.Sleep(time.Second) fmt.Println(\"写结束\") } var count int func main() { mutex := sync.RWMutex{} group := sync.WaitGroup{} group.Add(10) for i := 0; i \u003c 5; i++ { go testRead(\u0026group, \u0026mutex) //这里要使用指针，若发生复制便不再是一个锁了 go testWrite(\u0026group, \u0026mutex) } group.Wait() } 打印结果： 可以看到，读操作是有重叠的，即他们之间不互斥，但是写操作之间，读写操作之间都没有交叉，他们是互斥的。 ","date":"2017-05-15","objectID":"/posts/go_sync/:0:1","tags":["golang"],"title":"sync包","uri":"/posts/go_sync/"},{"categories":["笔记"],"content":"条件 sync.Cond 条件变量的作用并不是保证在同一时刻仅有一个线程访问某一个共享数据，而是在对应的共享数据的状态发生变化时，通知其他因此而被阻塞的线程。 条件变量要和配合锁来使用 unc testWait(group *sync.WaitGroup, mutex *sync.Cond) { mutex.L.Lock()//wait前必须先加锁 defer func() { mutex.L.Unlock() group.Done() }() fmt.Println(\"读取开始\") mutex.Wait()//内部先解锁，再加锁 time.Sleep(time.Second) fmt.Println(\"读取结束\") } func testSignal(group *sync.WaitGroup, mutex *sync.Cond) { fmt.Println(\"唤醒其他协程\") mutex.Signal()//唤醒一个 mutex.Broadcast()//唤醒所有等待该条件变量的协程 group.Done() } func main() { mutex := sync.Mutex{} cond := sync.NewCond(\u0026mutex) group := sync.WaitGroup{} group.Add(2) go testWait(\u0026group, cond) time.Sleep(time.Second) go testSignal(\u0026group, cond) group.Wait() } wait会阻塞当前goroutine，内部会先解锁，再加锁，所以在wait之前要先加锁，否则会报错。 ","date":"2017-05-15","objectID":"/posts/go_sync/:0:2","tags":["golang"],"title":"sync包","uri":"/posts/go_sync/"},{"categories":["笔记"],"content":"sync.Once 这个相对就比较简单了，直接看示例： func testOnce() { once := sync.Once{} for i := 0; i \u003c 5; i++ { once.Do(func() { fmt.Println(\"once do\") }) } time.Sleep(3 * time.Second) } once.do中的函数只会执行一次，其实原理也很简单，once结构体中有个互斥锁及标签位 type Once struct { m Mutex done uint32 } ","date":"2017-05-15","objectID":"/posts/go_sync/:0:3","tags":["golang"],"title":"sync包","uri":"/posts/go_sync/"},{"categories":["笔记"],"content":"原子操作 通过对互斥锁的合理使用，我们可以使一个 goroutine 在执行临界区中的代码时，不被其他的goroutine 打扰。不过，虽然不会被打扰，但是它仍然可能会被中断（interruption）。 当协程正执行临界区的时候还是会可能失去CPU的。换句话说，互斥锁可以保证串行执行，但是不能保证原子执行。 在底层，由 CPU 提供芯片级别的支持，原子操作可以实现。这使得原子操作可以完全地消除竞态条件，并能够绝对地保证并发安全性。并且，它的执行速度要比其他的同步工具快得多，通常会高出好几个数量级。不过，它的缺点也很明显。 更具体地说，正是因为原子操作不能被中断，所以它需要足够简单，并且要求快速。 Go 语言的原子操作当然是基于 CPU 和操作系统的，所以它也只针对少数数据类型的值提供了原子 操作函数。这些函数都存在于标准库代码包sync/atomic中。 sync/atomic包中的函数可以做的原子操作有： 加法（add） 比较并交换（compare andswap，简称 CAS） 加载（load） 存储（store） 交换（swap） 这些函数针对的数据类型并不多。但是，对这些类型中的每一个，sync/atomic包都会有一套函 数给予支持。这些数据类型有：int32、int64、uint32、uint64、uintptr，以及unsafe包中的Pointer。 示例： func testAtmic() { atomic.AddInt32(\u0026count, 10) val := atomic.LoadInt32(\u0026count) fmt.Println(val) atomic.AddInt32(\u0026count, -1) fmt.Println(count) atomic.SwapInt32(\u0026count, 33) fmt.Println(count) atomic.StoreInt32(\u0026count, 11) fmt.Println(count) atomic.CompareAndSwapInt32(\u0026count, 11, 22) fmt.Println(count) } 原子操作任意类型 此外，sync/atomic包还提供了一个名为Value的类型，它可以被用来存储任意类型的值 func testAtmicValue() { value := atomic.Value{} u := user{ name: \"tom\", age: 12, } value.Store(u) u2 := value.Load().(user) fmt.Println(u2) } ","date":"2017-05-15","objectID":"/posts/go_sync/:0:4","tags":["golang"],"title":"sync包","uri":"/posts/go_sync/"},{"categories":["笔记"],"content":"对象缓存池 sync.Pool func testPool() { pool := sync.Pool{ New: func() interface{} {//生成新的对象 return 0 }, } value1 := pool.Get()//取 fmt.Println(value1) pool.Put(1)//存 value2 := pool.Get() fmt.Println(value2) } 对象缓存池和普通意义的缓存池作用一样，我们可以指定生成新对象的方式，get时，缓存池中没有数据便调用改函数生成。 缓存池并没有数量的限制，每次gc时都会清除缓存池中的数据，所以要看清使用场景。比如当做数据连接池显然就不合适。 ","date":"2017-05-15","objectID":"/posts/go_sync/:0:5","tags":["golang"],"title":"sync包","uri":"/posts/go_sync/"},{"categories":["笔记"],"content":"安全的map sync.Map 普通的map并不是并发安全的，于是提供了一个并发安全的。 简单使用示例： func testMap(){ m := sync.Map{} u := user{ name: \"tom\", age: 12, } m.Store(\"zhangsan\",u) value, _ := m.Load(\"zhangsan\") u2 := value.(user) fmt.Println(u2) } sync.Map主要使用方法： m.Load() m.Store() m.Delete() m.LoadOrStore() m.Range() sync.map的操作方法key，value都是interface{}格式的，所以要自己进行类型判断，这显然有点麻烦。这里有个方案： type mapWrapper struct { M sync.Map keyType reflect.Type valType reflect.Type } func (m *mapWrapper) load(key interface{}) (value interface{}, ok bool) { if reflect.TypeOf(key) != m.keyType { return nil, false } return m.M.Load(key) } func (m *mapWrapper) store(key interface{}, value interface{}) (ok bool) { if reflect.TypeOf(key) != m.keyType { //校验key和value类型 return false } if reflect.TypeOf(value) != m.valType { return false } m.M.Store(key, value) return true } func testMap() { m := mapWrapper{ keyType: reflect.TypeOf(\"\"), valType: reflect.TypeOf(user{}), } u := user{ name: \"tom\", age: 12, } ok := m.store(1, 2) fmt.Println(ok) ok = m.store(\"zhangsan\", u) fmt.Println(ok) value, ok := m.load(\"zhangsan\") fmt.Println(value, ok) } 主要思路是对sync.Map封装，操作的时候校验类型，类型由初始化的时候传入。 注意： 和原来的map一样，虽然key是interface{}类型，但是一定是可以比较类型的，func,map等作为key就不别想了 ","date":"2017-05-15","objectID":"/posts/go_sync/:0:6","tags":["golang"],"title":"sync包","uri":"/posts/go_sync/"},{"categories":["笔记"],"content":"总结 互斥锁是一个很有用的同步工具，它可以保证每一时刻进入临界区的 goroutine 只有一个。读写锁对共享资源的写操作和读操作则区别看待，并消除了读操作之间的互斥。 条件变量主要是用于协调想要访问共享资源的那些线程。 当共享资源的状态发生变化时，它可以被用来通知被互斥锁阻塞的线程，它既可以基于互斥锁，也可以基于读写锁。当然了，读写锁也是一种互斥锁，前者是对后者的扩展。 原子操作不光能保证并发安全(不使用锁)，还能保证临界区操作不被中断而原子执行，但是只支持少数的数据类型。当然也有atomic.Value支持任意类型 缓存池提供了一个api，可以提高重复使用率，但是要注意gc时会清空。 ","date":"2017-05-15","objectID":"/posts/go_sync/:0:7","tags":["golang"],"title":"sync包","uri":"/posts/go_sync/"},{"categories":["笔记"],"content":"排序算法","date":"2017-05-10","objectID":"/posts/sort/","tags":["算法"],"title":"排序算法","uri":"/posts/sort/"},{"categories":["笔记"],"content":"排序 我们通常所说的排序算法往往指的是内部排序算法，即数据记录在内存中进行排序。 排序算法大体可分为两种： 一种是比较排序，时间复杂度O(nlogn) ~ O(n^2)，主要有：冒泡排序，选择排序，插入排序，归并排序，堆排序，快速排序等。 另一种是非比较排序，时间复杂度可以达到O(n)，主要有：计数排序，基数排序，桶排序等。 稳定性 稳定性的简单形式化定义为：如果Ai = Aj，排序前Ai在Aj之前，排序后Ai还在Aj之前，则称这种排序算法是稳定的。通俗地讲就是保证排序前后两个相等的数的相对顺序不变。 需要注意的是，排序算法是否为稳定的是由具体算法决定的，不稳定的算法在某种条件下可以变为稳定的算法，而稳定的算法在某种条件下也可以变为不稳定的算法。 例如，对于冒泡排序，原本是稳定的排序算法，如果将记录交换的条件改成A[i] \u003e= A[i + 1]，则两个相等的记录就会交换位置，从而变成不稳定的排序算法。 冒泡排序 它重复地走访过要排序的元素，依次比较相邻两个元素，如果他们的顺序错误就把他们调换过来，直到没有元素再需要交换，排序完成。这个算法的名字由来是因为越小(或越大)的元素会经由交换慢慢“浮”到数列的顶端。 冒泡排序算法的运作如下： 比较相邻的元素，如果前一个比后一个大，就把它们两个调换位置。 对每一对相邻元素作同样的工作，从开始第一对到结尾的最后一对。这步做完后，最后的元素会是最大的数。 针对所有的元素重复以上的步骤，除了最后一个。 持续每次对越来越少的元素重复上面的步骤，直到没有任何一对数字需要比较。 // 分类 -------------- 内部比较排序 // 数据结构 ---------- 数组 // 最差时间复杂度 ---- O(n^2) // 最优时间复杂度 ---- 如果能在内部循环第一次运行时,使用一个旗标来表示有无需要交换的可能,可以把最优时间复杂度降低到O(n) // 平均时间复杂度 ---- O(n^2) // 所需辅助空间 ------ O(1) // 稳定性 ------------ 稳定 func bubbleSort(arr []int) { for i := 0; i \u003c len(arr); i++ { for j := 0; j \u003c len(arr)-1-i; j++ { if arr[j] \u003e arr[j+1] { arr[j], arr[j+1] = arr[j+1], arr[j] } } } } 冒泡改进（鸡尾酒排序） 鸡尾酒排序，也叫定向冒泡排序，是冒泡排序的一种改进。此算法与冒泡排序的不同处在于从低到高然后从高到低，而冒泡排序则仅从低到高去比较序列里的每个元素。他可以得到比冒泡排序稍微好一点的效能。 func bubbleSort2(arr []int) { l, r := 0, len(arr)-1 for ; l \u003c r; { for i := l; i \u003c r; i++ { if arr[i] \u003e arr[i+1] { arr[i], arr[i+1] = arr[i+1], arr[i] } } r-- for i := r; i \u003e l; i-- { if arr[i] \u003c arr[i-1] { arr[i], arr[i-1] = arr[i-1], arr[i] } } l++ } } 快速选择排序 它的工作原理很容易理解：初始时在序列中找到最小（大）元素，放到序列的起始位置作为已排序序列；然后，再从剩余未排序元素中继续寻找最小（大）元素，放到已排序序列的末尾。以此类推，直到所有元素均排序完毕。 // 分类 -------------- 内部比较排序 // 数据结构 ---------- 数组 // 最差时间复杂度 ---- O(n^2) // 最优时间复杂度 ---- O(n^2) // 平均时间复杂度 ---- O(n^2) // 所需辅助空间 ------ O(1) // 稳定性 ------------ 不稳定 func selectionSort(arr []int) { for i := 0; i \u003c len(arr); i++ { minValeIndex := i for j := i + 1; j \u003c len(arr); j++ { if arr[j] \u003c arr[minValeIndex] { minValeIndex = j } } arr[i], arr[minValeIndex] = arr[minValeIndex], arr[i] } } 选择排序是不稳定的排序算法，不稳定发生在最小元素与A[i]交换的时刻。 比如序列：{ 5, 8, 5, 2, 9 }，一次选择的最小元素是2，然后把2和第一个5进行交换，从而改变了两个元素5的相对次序。 插入排序 插入排序是一种简单直观的排序算法。它的工作原理非常类似于我们抓扑克牌。 对于未排序数据(右手抓到的牌)，在已排序序列(左手已经排好序的手牌)中从后向前扫描，找到相应位置并插入。 插入排序在实现上，通常采用in-place排序（即只需用到O(1)的额外空间的排序），因而在从后向前扫描过程中，需要反复把已排序元素逐步向后挪位，为最新元素提供插入空间。 具体算法描述如下： 从第一个元素开始，该元素可以认为已经被排序 取出下一个元素，在已经排序的元素序列中从后向前扫描 如果该元素（已排序）大于新元素，将该元素移到下一位置 重复步骤3，直到找到已排序的元素小于或者等于新元素的位置 将新元素插入到该位置后 重复步骤2~5 // 分类 ------------- 内部比较排序 // 数据结构 ---------- 数组 // 最差时间复杂度 ---- 最坏情况为输入序列是降序排列的,此时时间复杂度O(n^2) // 最优时间复杂度 ---- 最好情况为输入序列是升序排列的,此时时间复杂度O(n) // 平均时间复杂度 ---- O(n^2) // 所需辅助空间 ------ O(1) // 稳定性 ------------ 稳定 func insertSort(arr []int) { for i := 1; i \u003c len(arr); i++ { toInsertValue := arr[i] var j int for j = i - 1; j \u003e= 0; j-- { if arr[j] \u003c= toInsertValue { break } arr[j+1] = arr[j] } arr[j+1] = toInsertValue } } 插入排序升级（二分查找插入） 对于插入排序，如果比较操作的代价比交换操作大的话，可以采用二分查找法来减少比较操作的次数，我们称为二分插入排序，换句话说：原来从右往左老老实实的找，现在通过二分查找来找到要插入的位置。 // 分类 -------------- 内部比较排序 // 数据结构 ---------- 数组 // 最差时间复杂度 ---- O(n^2) // 最优时间复杂度 ---- O(nlogn) // 平均时间复杂度 ---- O(n^2) // 所需辅助空间 ------ O(1) // 稳定性 ------------ 稳定 func insertSort2(arr []int) { for i := 1; i \u003c len(arr); i++ { toInsertValue := arr[i] l, r := 0, i-1 for ; l \u003c= r; { mid := l + (r-l)/2 if arr[mid] \u003c toInsertValue { l = mid + 1 } else { r = mid - 1 } } var j int for j = i - 1; j \u003e= l; j-- { arr[j+1] = arr[j] } arr[j+1] = toInsertValue } } 插入排序升级（希尔排序） 回想一下直接插入排序过程，排序过程中，我们可以设置一条线，左边是排好序的，右边则是一个一个等待排序， 如果最小的那个值在最右边，那么排这个最小值的时候，需要将所有元素向右边移动一位。 是否能够减少这样的移位呢？ 我们不希望它是一步一步的移动，而是大步大步的移动。刚开始的时候步长比较大，逐渐缩小至1（这时就是普通的插入排序了）。最后还是插入排序，那快在哪里？意义何在？其实最后一遍已经基本有序了，几乎就是O(n)，别忘了，插入排序最适合的场景就是排基本有序的数据。 分组，步长incre初始化为len/2(当然也可以设为其他) 从incre处开始依次进行插入 和普通的插入排序一样，把插入的值一一和左边已经排序好的进行比较，只不过步长不再为1，而是incre 一遍排完，缩小步长incre=incre/2,循环进行步骤1-3，知道incre=1 func insertSort3(arr []int) { for incre := len(arr) / 2; incre \u003e= 1; incre = incre / 2 { for i := incre; i \u003c len(arr); i++ { toInsertValue := arr[i] var j int for j = i - incre; j \u003e= 0; j -= incre { if arr[j] \u003c= toInsertValue { break } arr[j+incre] = arr[j] } arr[j+incre] = toInsertValue } } } 归并排序 使用分而治之的思想，先把数组分成两","date":"2017-05-10","objectID":"/posts/sort/:0:1","tags":["算法"],"title":"排序算法","uri":"/posts/sort/"},{"categories":["笔记"],"content":"golang slice \u0026 map","date":"2017-05-05","objectID":"/posts/go_slice/","tags":["golang"],"title":"golang slice \u0026 map","uri":"/posts/go_slice/"},{"categories":["笔记"],"content":"slice slice是一个数组某个部分的引用，非协程安全的。 创建 通过数组创建 切片的初始化格式是：var slice1 []type = arr1[start:end]，start和end都是角标，含头不含尾 s:=arr[0:3] s:=arr[:3] s:=arr[0:]都是合法的 make声明 make([]type,len,cap)cap为非必填 数据结构 type slice struct { array unsafe.Pointer len int cap int } 其中array就是指向数组的指针 从Go1.2开始slice支持了三个参数的slice，之前我们一直采用这种方式在slice或者array基础上来获取一个slice var array [10]int slice := array[2:4] 这个例子里面slice的容量是8，新版本里面可以指定这个容量 slice = array[2:4:7] 上面这个的容量就是7-2，即5。这样这个产生的新的slice就没办法访问最后的三个元素 扩容 先看几个示例： s := make([]int, 2) s2 := make([]int, 1) fmt.Println(len(s), cap(s), s) s = append(s, s2...) fmt.Println(len(s), cap(s), s) 打印： 2 2 [0 0] 3 4 [0 0 0] s := make([]int, 2) s2 := make([]int, 3) fmt.Println(len(s), cap(s), s) s = append(s, s2...) fmt.Println(len(s), cap(s), s) 打印： 2 2 [0 0] 5 6 [0 0 0 0 0] s := make([]int, 1024) s2 := make([]int, 3) fmt.Println(len(s), cap(s)) s = append(s, s2...) fmt.Println(len(s), cap(s)) 打印： 1024 1024 1027 1280 扩容策略 If 当前长度\u003c1024{ If 当追加后的len \u003c 当前的2倍{ 直接翻倍 }else{ 新的cap=追加后的长度 } }else{ 每次扩容为当前1.25倍，循环，直到装下所有元素 } 计算出了新容量之后，还没有完，出于内存的高效利用考虑，还要进行内存对齐。通过内存对齐后才能知道最终的cap值。 slice删除 arr[low:high] For arrays or strings, the indices are in range if 0 \u003c= low \u003c= high \u003c= len(a), otherwise they are out of range. For slices, the upper index bound is the slice capacity cap(a) rather than the length. ","date":"2017-05-05","objectID":"/posts/go_slice/:0:1","tags":["golang"],"title":"golang slice \u0026 map","uri":"/posts/go_slice/"},{"categories":["笔记"],"content":"补充 slice的最大cap是多少？ 源码中slice的cap 为int类型，int和平台有关。其次cap也和能申请的内存大小有关，简单来说max = maxmemory / element.size slice没有缩容，必须自己注意内存泄漏 小心slice用的是同一个数组的坑 func main() { s := []int{1, 2, 3, 4, 5} s2 := append(s[0:1], 6) fmt.Printf(\"%p %v %d \\n\", s, s, cap(s))// 0xc04200a240 [1 6 3 4 5] 5 fmt.Printf(\"%p %v %d\", s2, s2, cap(s)) // 0xc04200a240 [1 6] 5 } ","date":"2017-05-05","objectID":"/posts/go_slice/:0:2","tags":["golang"],"title":"golang slice \u0026 map","uri":"/posts/go_slice/"},{"categories":["笔记"],"content":"Redis学习-redis底层数据结构","date":"2017-04-16","objectID":"/posts/redis_%E5%BA%95%E5%B1%82%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/","tags":["redis"],"title":"Redis学习-redis底层数据结构","uri":"/posts/redis_%E5%BA%95%E5%B1%82%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"},{"categories":["笔记"],"content":"动态字符串 redis是C语言编写，但是字符串并不是简单地的C字符串。它是SDS（simple dynamic string）。 /* * 保存字符串对象的结构 */ struct sdshdr { // buf 中已占用空间的长度 int len; // buf 中剩余可用空间的长度 int free; // 数据空间 char buf[]; }; 1、len 变量，用于记录buf 中已经使用的空间长度（这里指出Redis 的长度为5） 2、free 变量，用于记录buf 中还空余的空间（初次分配空间，一般没有空余，在对字符串修改的时候，会有剩余空间出现） 3、buf 字符数组，用于记录我们的字符串（记录Redis） SDS 与 C 字符串的区别 获取字符串长度（SDS O（1）/C 字符串 O(n)） 传统的C 字符串 使用长度为N+1 的字符串数组来表示长度为N 的字符串，所以为了获取一个长度为C字符串的长度，必须遍历整个字符串。 和C 字符串不同，SDS 的数据结构中，有专门用于保存字符串长度的变量，我们可以通过获取len 属性的值，直接知道字符串长度。 杜绝缓冲区溢出 C 字符串 不记录字符串长度，除了获取的时候复杂度高以外，还容易导致缓冲区溢出。 假设程序中有两个在内存中紧邻着的 字符串 s1 和 s2，其中s1 保存了字符串“redis”，二s2 则保存了字符串“MongoDb” 如果我们现在将s1 的内容修改为redis cluster，但是又忘了重新为s1 分配足够的空间，这时候就会出现以下问题： 原本s2 中的内容已经被S1的内容给占领了，s2 现在为 cluster，而不是“Mongodb”。 Redis 中SDS 的空间分配策略完全杜绝了发生缓冲区溢出的可能性： 当我们需要对一个SDS 进行修改的时候，redis 会在执行拼接操作之前，预先检查给定SDS 空间是否足够，如果不够，会先拓展SDS 的空间，然后再执行拼接操作。 减少修改字符串时带来的内存重分配次数　 预分配内存，提前申请空间。 C语言字符串在进行字符串的扩充和收缩的时候，都会面临着内存空间的重新分配问题。 字符串拼接会产生字符串的内存空间的扩充，在拼接的过程中，原来的字符串的大小很可能小于拼接后的字符串的大小，那么这样的话，就会导致一旦忘记申请分配空间，就会导致内存的溢出。 字符串在进行收缩的时候，内存空间会相应的收缩，而如果在进行字符串的切割的时候，没有对内存的空间进行一个重新分配，那么这部分多出来的空间就成为了内存泄露。 举个例子：我们需要对下面的SDS进行拓展，则需要进行空间的拓展，这时候redis 会将SDS的长度修改为13字节，并且将未使用空间同样修改为13字节 。这样下次扩展字符串的时候可能就不需要申请了，因为上一次多申请了13个。 惰性空间释放 我们在观察SDS 的结构的时候可以看到里面的free 属性，是用于记录空余空间的。我们除了在拓展字符串的时候会使用到free 来进行记录空余空间以外，在对字符串进行收缩的时候，我们也可以使用free 属性来进行记录剩余空间，这样做的好处就是避免下次对字符串进行再次修改的时候，需要对字符串的空间进行拓展。 然而，我们并不是说不能释放SDS 中空余的空间，SDS 提供了相应的API，让我们可以在有需要的时候，自行释放SDS 的空余空间。 通过惰性空间释放，SDS 避免了缩短字符串时所需的内存重分配操作，并未将来可能有的增长操作提供了优化 二进制安全 C 字符串中的字符必须符合某种编码，并且除了字符串的末尾之外，字符串里面不能包含空字符，否则最先被程序读入的空字符将被误认为是字符串结尾，这些限制使得C字符串只能保存文本数据，而不能保存想图片，音频，视频，压缩文件这样的二进制数据。 但是在Redis中，不是靠空字符来判断字符串的结束的，而是通过len这个属性。那么，即便是中间出现了空字符对于SDS来说，读取该字符仍然是可以的。 总结 SDS相比C字符串好处多多。 获取字符串长度不用再遍历。 字符串变长会校验空间，不会溢出。 扩容的时候会多申请空间，减少内存分配次数。 可以保持二进制数据。图片、视频、空格都不怕了。 ","date":"2017-04-16","objectID":"/posts/redis_%E5%BA%95%E5%B1%82%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/:0:1","tags":["redis"],"title":"Redis学习-redis底层数据结构","uri":"/posts/redis_%E5%BA%95%E5%B1%82%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"},{"categories":["笔记"],"content":"链表 redis的list是一个双向链表。 链表的特性 双端：链表节点带有prev 和next 指针，获取某个节点的前置节点和后置节点的时间复杂度都是O（N） 无环：表头节点的 prev 指针和表尾节点的next 都指向NULL，对立案表的访问时以NULL为截止 表头和表尾：因为链表带有head指针和tail 指针，程序获取链表头结点和尾节点的时间复杂度为O(1) 长度计数器：链表中存有记录链表长度的属性 len 多态：链表节点使用 void* 指针来保存节点值，并且可以通过list 结构的dup 、 free、 match三个属性为节点值设置类型特定函数。 ","date":"2017-04-16","objectID":"/posts/redis_%E5%BA%95%E5%B1%82%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/:0:2","tags":["redis"],"title":"Redis学习-redis底层数据结构","uri":"/posts/redis_%E5%BA%95%E5%B1%82%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"},{"categories":["笔记"],"content":"字典 在字典中，一个键（key）可以和一个值（value）进行关联，字典中的每个键都是独一无二的。在C语言中，并没有这种数据结构，但是Redis 中构建了自己的字典实现。 SET msg “hello world” 创建这样的键值对（“msg”，“hello world”）在数据库中就是以字典的形式存储. 如果出现hash 值相同的情况怎么办？Redis 采用了链地址法： 当k1 和k0 的hash 值相同时，将k1中的next 指向k0 想成一个链表。 Rehash rehash就是扩容和缩容。 加载因子（load factor） = ht[0].used / ht[0].size 也就是所有的元素个数 / 桶的个数 扩容： 没有执行BGSAVE和BGREWRITEAOF指令的情况下，哈希表的加载因子大于等于1。 正在执行BGSAVE和BGREWRITEAOF指令的情况下，哈希表的加载因子大于等于5。 收缩: 加载因子小于0.1时，程序自动开始对哈希表进行收缩操作。 扩容和收缩的数量 扩容： 第一个大于等于ht[0].used * 2的2^n(2的n次方幂)。 收缩： 第一个大于等于ht[0].used的2^n(2的n次方幂)。 hash的扩容和缩容都不是一下就完成的。而是·渐进式的，这样可以避免暂时的卡住。 甚至在进行期间，每次对哈希表的增删改查操作，除了正常执行之外，还会顺带将ht[0]哈希表相关键值对rehash到ht[1]。 ","date":"2017-04-16","objectID":"/posts/redis_%E5%BA%95%E5%B1%82%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/:0:3","tags":["redis"],"title":"Redis学习-redis底层数据结构","uri":"/posts/redis_%E5%BA%95%E5%B1%82%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"},{"categories":["笔记"],"content":"ziplist 压缩列表。 redis的列表键和哈希键的底层实现之一。此数据结构是为了节约内存而开发的。和各种语言的数组类似，它是由连续的内存块组成的，这样一来，由于内存是连续的，就减少了很多内存碎片和指针的内存占用，进而节约了内存。 然后文中的entry的结构是这样的： 元素的遍历 先找到列表尾部元素, 然后再根据ziplist节点元素中的previous_entry_length属性，来逐个遍历。 ","date":"2017-04-16","objectID":"/posts/redis_%E5%BA%95%E5%B1%82%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/:0:4","tags":["redis"],"title":"Redis学习-redis底层数据结构","uri":"/posts/redis_%E5%BA%95%E5%B1%82%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"},{"categories":["笔记"],"content":"整数集合 整数集合（intset）是Redis用于保存整数值的集合抽象数据结构，可以保存类型为int16_t、int32_t、int64_t的整数值，并且保证集合中不会出现重复元素 整数集合是集合（Set）的底层实现之一，如果一个集合只包含整数值元素，且元素数量不多时，会使用整数集合作为底层实现 typedef struct intset { // 编码方式 uint32_t encoding; // 集合包含的元素数量 uint32_t length; // 保存元素的数组 int8_t contents[]; } intset; contents数组：整数集合的每个元素在数组中按值的大小从小到大排序，且不包含重复项 length记录整数集合的元素数量，即contents数组长度 encoding决定contents数组的真正类型，如INTSET_ENC_INT16、INTSET_ENC_INT32、INTSET_ENC_INT64 ","date":"2017-04-16","objectID":"/posts/redis_%E5%BA%95%E5%B1%82%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/:0:5","tags":["redis"],"title":"Redis学习-redis底层数据结构","uri":"/posts/redis_%E5%BA%95%E5%B1%82%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"},{"categories":["笔记"],"content":"跳跃表 个人理解有点像mysql索引的B+树，为的就是zset快速查找。 一个普通的单链表查询一个元素的时间复杂度为O(N)，即便该单链表是有序的。使用跳跃表（SkipList）是来解决查找问题的，它是一种有序的数据结构，不属于平衡树结构，也不属于Hash结构，它通过在每个节点维持多个指向其他节点的指针，而达到快速访问节点的目的。 跳跃表是有序集合（Sorted Set）的底层实现之一，如果有序集合包含的元素比较多，或者元素的成员是比较长的字符串时，Redis会使用跳跃表做有序集合的底层实现 跳跃表其实可以把它理解为多层的链表，它有如下的性质 多层的结构组成，每层是一个有序的链表 最底层（level 1）的链表包含所有的元素 跳跃表的查找次数近似于层数，时间复杂度为O(logn)，插入、删除也为 O(logn) 跳跃表是一种随机化的数据结构(通过抛硬币来决定层数) 那么如何来理解跳跃表呢，我们从最底层的包含所有元素的链表开始，给定如下的链表 然后我们每隔一个元素，把它放到上一层的链表当中，这里我把它叫做上浮（注意，科学的办法是抛硬币的方式，来决定元素是否上浮到上一层链表，我这里先简单每隔一个元素上浮到上一层链表，便于理解），操作完成之后的结构如下 查找元素的方法是这样，从上层开始查找，大数向右找到头，小数向左找到头，例如我要查找17，查询的顺序是：13 -\u003e 46 -\u003e 22 -\u003e 17；如果是查找35，则是 13 -\u003e 46 -\u003e 22 -\u003e 46 -\u003e 35；如果是54，则是 13 -\u003e 46 -\u003e 54 跳跃表节点的删除和添加都是不可预测的，很难保证跳表的索引是始终均匀的，抛硬币的方式可以让大体上是趋于均匀的 ","date":"2017-04-16","objectID":"/posts/redis_%E5%BA%95%E5%B1%82%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/:0:6","tags":["redis"],"title":"Redis学习-redis底层数据结构","uri":"/posts/redis_%E5%BA%95%E5%B1%82%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"},{"categories":["笔记"],"content":"redis五种数据结构的实现 redis对象 redis中并没有直接使用以上所说的各种数据结构来实现键值数据库，而是基于一种对象，对象底层再间接的引用上文所说的具体的数据结构。 根据对象的类型可以判断一个对象是否可以执行给定的命令，也可针对不同的使用场景，对象设置有多种不同的数据结构实现，从而优化对象在不同场景下的使用效率 字符串 其中：embstr和raw都是由SDS动态字符串构成的。唯一区别是：raw是分配内存的时候，redisobject和 sds 各分配一块内存，而embstr是redisobject和raw在一块儿内存中。 列表 hash set zset ","date":"2017-04-16","objectID":"/posts/redis_%E5%BA%95%E5%B1%82%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/:0:7","tags":["redis"],"title":"Redis学习-redis底层数据结构","uri":"/posts/redis_%E5%BA%95%E5%B1%82%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"},{"categories":["笔记"],"content":"Redis学习-主从、哨兵、集群","date":"2017-04-16","objectID":"/posts/redis_%E4%B8%BB%E4%BB%8E%E5%93%A8%E5%85%B5%E9%9B%86%E7%BE%A4/","tags":["redis"],"title":"Redis学习-主从、哨兵、集群","uri":"/posts/redis_%E4%B8%BB%E4%BB%8E%E5%93%A8%E5%85%B5%E9%9B%86%E7%BE%A4/"},{"categories":["笔记"],"content":"主从 需要注意，主从复制的开启，完全是在从节点发起的；不需要我们在主节点做任何事情。 从节点开启主从复制，有3种方式： （1）配置文件 在从服务器的配置文件中加入：slaveof \u003cmasterip\u003e \u003cmasterport\u003e （2）启动命令 redis-server启动命令后加入 --slaveof \u003cmasterip\u003e \u003cmasterport\u003e （3）客户端命令 Redis服务器启动后，直接通过客户端执行命令：slaveof \u003cmasterip\u003e \u003cmasterport\u003e，则该Redis实例成为从节点。 复制 psync命令的执行会执行全量/增量的复制。 主从复制原理 当从数据库启动时，会向主数据库发送sync命令，主数据库接收到sync后开始在后台保存快照rdb，在保存快照期间受到的命名缓存起来，当快照完成时，主数据库会将快照和缓存的命令一块发送给从。复制初始化结束。 之后，主每受到1个命令就同步发送给从。 当出现断开重连后，2.8之后的版本会将断线期间的命令传给重数据库。增量复制 主从复制是乐观复制，当客户端发送写执行给主，主执行完立即将结果返回客户端，并异步的把命令发送给从，从而不影响性能。也可以设置至少同步给多少个从主才可写。 ","date":"2017-04-16","objectID":"/posts/redis_%E4%B8%BB%E4%BB%8E%E5%93%A8%E5%85%B5%E9%9B%86%E7%BE%A4/:0:1","tags":["redis"],"title":"Redis学习-主从、哨兵、集群","uri":"/posts/redis_%E4%B8%BB%E4%BB%8E%E5%93%A8%E5%85%B5%E9%9B%86%E7%BE%A4/"},{"categories":["笔记"],"content":"哨兵 当主数据库遇到异常中断服务后，开发者可以通过手动的方式选择一个从数据库来升格为主数据库，以使得系统能够继续提供服务。然而整个过程相对麻烦且需要人工介入，难以实现自动化。 为此，Redis 2.8中提供了哨兵工具来实现自动化的系统监控和故障恢复功能。 哨兵的作用就是监控redis主、从数据库是否正常运行，主出现故障自动将从数据库转换为主数据库。 哨兵节点本质还是一个redis实例，与主从节点不同的是sentinel节点作用是用于监控redis数据节点的，可以有多个哨兵节点。 上图就是一主二从多个哨兵。 哨兵原理 哨兵节点之间相互发消息，来检测其余哨兵是否正常工作。 哨兵也会向主从节点发送消息来检测主从节点是否在正常工作。如果主节点发送故障，那么某一个哨兵向它发送数据后就收不到回复，该哨兵会再向其他哨兵发消息来询问是否也认为该主节点异常，如果有一半的哨兵认为主节点异常，那么就进行主从节点的故障转移。 故障转移的基本思路是在从节点中选取某个从节点向其发送slaveof no one（假设选取的从节点为127.0.0.1:6380），使其称为独立的节点（也就是新的主节点），然后sentinel向其余的从节点发送slaveof 127.0.0.1 6380命令使它们重新成为新的主节点的从节点。重新分配之后sentinel节点集合还会继续监控已经下线的主节点（假设为127.0.0.1:6379），如果其重新上线，那么sentinel会向其发送slaveof命令，使其成为新的主机点的从节点，如此故障转移工作完成。 哨兵配置 在redis安装目录下有个默认的sentinel配置文件sentinel.conf。 每个sentinel的myid参数也要进行修改，因为sentinel之间是通过该属性来唯一区分其他sentinel节点的。 参数中sentinel monitor mymaster 127.0.0.1 6379 2，因为sentinel是通过检测主节点的状态来得知当前主节点的从节点有哪些的，因而设置为主节点的端口号即可 配置完成后我们首先启动三个主从节点，然后分别使用三个配置文件使用如下命令启用sentinel： ./src/redis-sentinel sentinel-26379.conf 哨兵节点本身也是redis实例，我们可以连接上，并查看主从节点的状态。 ","date":"2017-04-16","objectID":"/posts/redis_%E4%B8%BB%E4%BB%8E%E5%93%A8%E5%85%B5%E9%9B%86%E7%BE%A4/:0:2","tags":["redis"],"title":"Redis学习-主从、哨兵、集群","uri":"/posts/redis_%E4%B8%BB%E4%BB%8E%E5%93%A8%E5%85%B5%E9%9B%86%E7%BE%A4/"},{"categories":["笔记"],"content":"集群 主从模式可以解决读写分离，哨兵可以保证主从问题的容错切换，但是他们还是在单机，主从节点都要保存所有的数据。 当单机内存、并发和流量瓶颈的时候，需要使用集群方法来解决。 集群可以多个节点，每个节点可以有一个master，当master异常时，它的从节点切换。如果从节点也失败了，那么集群也就失败了。 Keys分布模型 redis集群中数据是和槽（slot）挂钩的，其总共定义了16384个槽，所有的数据根据一致哈希算法会被映射到这16384个槽中的某个槽中；另一方面，这16384个槽是按照设置被分配到不同的redis节点上的，比如启动了三个redis实例：cluster-A，cluster-B和cluster-C，这里将0-5460号槽分配给cluster-A，将5461-10922号槽分配给cluster-B，将10923-16383号槽分配给cluster-C。 redis集群投票机制 redis集群中有多台redis服务器不可避免会有服务器挂掉。redis集群服务器之间通过互相的ping-pong判断是否节点可以连接上。如果有一半以上的节点去ping一个节点的时候没有回应，集群就认为这个节点宕机了。 其他 在redis的官方文档中，对redis-cluster架构上，有这样的说明：在cluster架构下，默认的，一般redis-master用于接收读写，而redis-slave则用于备份，当有请求是在向slave发起时，会直接重定向到对应key所在的master来处理。但如果不介意读取的是redis-cluster中有可能过期的数据并且对写请求不感兴趣时，则亦可通过readonly命令，将slave设置成可读，然后通过slave获取相关的key，达到读写分离。 redigo库本身不支持redis哨兵和集群，redis-go-cluster库在此基础上在本地缓存了slot，并自动更新，为每一个节点管理连接池。 go-sentinel库也是支持redigo库来实现哨兵模式 ","date":"2017-04-16","objectID":"/posts/redis_%E4%B8%BB%E4%BB%8E%E5%93%A8%E5%85%B5%E9%9B%86%E7%BE%A4/:0:3","tags":["redis"],"title":"Redis学习-主从、哨兵、集群","uri":"/posts/redis_%E4%B8%BB%E4%BB%8E%E5%93%A8%E5%85%B5%E9%9B%86%E7%BE%A4/"},{"categories":["笔记"],"content":"Redis学习（一）-redis基础","date":"2017-04-16","objectID":"/posts/redis_%E5%9F%BA%E7%A1%80/","tags":["redis"],"title":"Redis学习（一）-redis基础","uri":"/posts/redis_%E5%9F%BA%E7%A1%80/"},{"categories":["笔记"],"content":"redis简介 Redis 是一个开源的使用 ANSI C 语言编写、支持网络、可基于内存亦可持久化的日志 型、Key-Value 数据库。 redis会周期性的把更新的数据写入磁盘或者把修改操作写入追加的记录文件，并且在此基础上实现了master-slave(主从)同步，redis在3.0版本推出集群模式。 redis优势 1.k、v键值存储以及数据结构存储（如列表、字典） 2.所有数据(包括数据的存储)操作均在内存中完成 3.单线程服务(这意味着会有较多的阻塞情况)，采用epoll模型进行请求响应，对比nginx 4.支持主从复制模式，更提供高可用主从复制模式（哨兵） 5.去中心化分布式集群 6.丰富的编程接口支持，如Python、Golang、Java、php、Ruby、Lua、Node.js 7.功能丰富，除了支持多种数据结构之外，还支持事务、发布/订阅、消息队列等功能 8.支持数据持久化(AOF、RDB) ","date":"2017-04-16","objectID":"/posts/redis_%E5%9F%BA%E7%A1%80/:0:1","tags":["redis"],"title":"Redis学习（一）-redis基础","uri":"/posts/redis_%E5%9F%BA%E7%A1%80/"},{"categories":["笔记"],"content":"redis的服务端启动与客户端连接 启动服务器 redis-server redis.windows.conf 关闭服务器 redis-cli shutdown 客户端连接 redis-cli 参数： -h \u003chostname\u003e Server hostname (default: 127.0.0.1). -p \u003cport\u003e Server port (default: 6379). -a \u003cpassword\u003e Password to use when connecting to the server. --version Output version and exit. ","date":"2017-04-16","objectID":"/posts/redis_%E5%9F%BA%E7%A1%80/:0:2","tags":["redis"],"title":"Redis学习（一）-redis基础","uri":"/posts/redis_%E5%9F%BA%E7%A1%80/"},{"categories":["笔记"],"content":"redis的key操作 redis的key均为字符串，而且不能包含有空格（通过命令行客户端不能，但是通过redigo却是可以的，建议大家还是不要使用空格了），区分大小写。 redis key常用命令 exists key 检测指定 key 是否存在，返回 1 表示存在，0 不存在 时间复杂度：O(1) del key1 key2 …… keyN 删除给定 key,返回删除 key 的数目，0 表示给定 key 都不存在。 时间复杂度： 当删除一个key的时候，若value为string复杂度为O(1)，否则为O(m),m为value的元素个数 当删除的value为string的时候,复杂度为O(n)，n为key的个数 type key 返回给定 key 值的类型。返回 none 表示 key 不存在 类型 set 无序集合类型…… keys pattern 返回匹配指定模式的所有 key,keys * 即可返回所有的key randomkey 返回从当前数据库中随机选择的一个 key,如果当前数据库是空的，返回空串,个人感觉用处不大 rename oldkey newkey 重命名一个 key,如果 newkey 存在，将会被覆盖，返回 1 表示成功，0 失败。可能是 oldkey 不存在或者和 newkey 相同。 renamenx oldkey newkey 同上，但是如果 newkey 存在返回失败。nx后缀为可作not exist理解，即新key不存在才成功 expire key seconds 为 key 指定过期时间，单位是秒。返回 1 成功，0 表示失败；设置-1则删除key;如果已经设置了过期时间，则覆盖原来的值（这里部分手册上说重复设置时间返回0，亲测证明可以重复设置） 时间复杂度O(1) persist key 去除key的有效期 ttl key 返回设置过过期时间key的剩余过期秒数。-2表示key不存在，-1未设置过期时间。（这里部分手册上说重复设置时间返回0，亲测证明可以重复设置） expireat key timestamp和上面的expire类似，这里传的是unix timestamp，也就是1970年至今的秒数，设置一个已经过去的时间，相当于删除key pexpire key milliseconds和expire类似，但是是毫秒，与之对应的是pttl,pexpireat命令（2.6.0以后可用） select db-index 通过索引选择数据库，默认连接的数据库是 0,默认数据库数是 16 个。返回 1表示成功，0 失败。 move key db-index 将 key 从当前数据库移动到指定数据库。返回 1 表示成功。0 表示 key不存在或者已经在指定数据库中。 **SORT key [BY pattern] [LIMIT offset count] [GET pattern [GET pattern …]] [ASC|DESC] [ALPHA] [STORE destination]**对list,set,sorted set的元素进行排序，可返回排序后的结果或把结果进行保存到指定的key中 时间复杂度为O(N+M*log(M))，其中N为集合中的个数，M为返回的元素个数 默认是把元素按照双精度的小数进行排序，如果我们想按字典顺序排序字符串可以使用ALPHA 默认是把排序后的结果返回给客户端，但是集合本身数据位置是没有变的 ","date":"2017-04-16","objectID":"/posts/redis_%E5%9F%BA%E7%A1%80/:0:3","tags":["redis"],"title":"Redis学习（一）-redis基础","uri":"/posts/redis_%E5%9F%BA%E7%A1%80/"},{"categories":["笔记"],"content":"redis的value操作 redis的value共有5中类型：string，list，hash，set和sorted set string类型相关命令 string字符串最多可以存512M字节。 string 是最基本的类型，而且 string 类型是二进制安全的。意思是 redis 的 string 可以 包含任何数据。比如 jpg 图片或者序列化的对象。从内部实现来看其实 string 可以看作 byte数组，最大上限是 1G 字节。 set key value 设置 key 对应 string 类型的值，返回 1 表示成功，0 失败。 setnx key value 如果 key 不存在，设置 key 对应 string 类型的值。如果 key 已经存在，返回 0。 setex key seconds value 设置值并设置过期时间 get key 获取 key 对应的 string 值,如果 key 不存在返回 nil getset key value 先获取 key 的值，再设置 key 的值。如果 key 不存在返回 nil。 mget key1 key2 …… keyN 一次获取多个 key 的值，如果对应 key 不存在，则对应返回 nil，m可以做multy理解 mset key1 value1 …… keyN valueN 一次设置多个 key 的值，成功返回 1 表示所有的值都设置了，失败返回 0 表示没有任何值被设置。 msetnx key1 value1 …… keyN valueN 一次设置多个 key 的值，但是不会覆盖已经存在的 key incr key 对 key 的值做加1操作，并返回新的值。注意 incr 一个不是 int 的 value 会返回错误，incr 一个不存在的 key，则设置 key 值为 1，当超出范围时也会返回错误 decr key 对 key 的值做减1操作，decr 一个不存在 key，则设置 key 值为-1。 incrby key integer 对 key 加上指定值 ，key 不存在时候会设置 key，并认为原来的 value是 0。 decrby key integer 对 key 减去指定值。decrby 完全是为了可读性，我们完全可以通过 incrby一个负值来实现同样效果，反之一样。 从 Redis 2.6.12 版本开始， SET 命令的行为可以通过一系列参数来修改： EX seconds ： 将键的过期时间设置为 seconds 秒。 执行 SET key value EX seconds 的效果等同于执行 SETEX key seconds value 。 PX milliseconds ： 将键的过期时间设置为 milliseconds 毫秒。 执行 SET key value PX milliseconds 的效果等同于执行 PSETEX key milliseconds value 。 NX ： 只在键不存在时， 才对键进行设置操作。 执行 * SET key value NX 的效果等同于执行 SETNX key value 。 XX ： 只在键已经存在时， 才对键进行设置操作。 我们在用redis实现分布式锁的时候可以保证setnx 和key过期时间的原子性，就是利用set key value EX time NX。 list类型相关命令 ist的元素个数最多为2^32-1个，也就是4294967295个。 这里的list可看做双向链表，key就是一个list的名字，不通过的key即为不同的list lpush key value [value…] 在 key 对应 list 的左边添加字符串元素，返回 list的长度 表示成功，0 表示 key 存在且不是 list 类型。(2.4版本以后可以一次添加多个value) lpushx key value 和上面的类似，但是要求key已经存在且里面含有元素 rpush key string 在 key 对应 list 的右边添加字符串元素。 llen key 返回 key 对应 list 的长度，如果 key 不存在返回 0，如果 key 对应类型不是 list返回错误。 lrange key start end 返回指定区间内的元素，下标从 0 开始，包含end，负值表示从后面计算，-1 表示倒数第一个元素 ，key 不存在返回空。 ltrim key start end 截取 list 指定区间内元素，成功返回 1，key 不存在返回错误。 时间复杂度O(N)，N为要删除的元素个数 lset key index value 设置 list 中指定下标的元素值，成功返回 1，key 或者下标不存在返回错误。 lindex key index 返回index位置的值 lrem key count value从 List 的头部（count正数）或尾部（count负数）删除一定数量（count ）匹配 value 的元素，返回删除的元素数量。count 为 0 时候删除全部；count\u003e0从头部开始；count\u003c0从尾部开始 lpop key 从 list 的头部删除并返回删除元素。如果 key 对应 list 不存在或者是空返回 nil ，如果 key 对应值不是 list 返回错误。rpop key 从 list 的尾部删除并返回删除元素。 rpop key从list的右边开始移除并返回元素。 **BLPOP key [key …] timeout ** 阻塞式弹出，设置 timeout参数为0表示永远阻塞。但把它用在 MULTI / EXEC 块当中没有意义。 rpoplpush key1 key2把key1右边的元素删除，并放到key2的左边。key1 key2可以为同一个。 linsert key BEFORE|AFTER pivot value在指定的value前/后插入新的value LINSERT mylist BEFORE “World” “There” hash类型相关命令 键值对个数最多为2^32-1个，也就是4294967295个。 Redis Hash对应Value内部实际就是一个HashMap，实际这里会有2种不同实现，这个Hash的成员比较少时Redis为了节省内存会采用类似一维数组的方式来紧凑存储，而不会采用真正的HashMap结构，对应的value redisObject的encoding为zipmap,当成员数量增大时会自动转成真正的HashMap hset key field value 设置hash字段的值 hget key field 根据字段key获取hash的值 **hdel key field [field …]**根据key删除hash容器的值，2.4版本以后支持多个key hexists key field 判断是否存在key及hash字段，key或字段不存在返回0 hgetall key 获取所有的字段key和value，value紧跟key一同返回 hkeys key返回hash集合的所有字段key hvals key返回包含所有字段value的list set类型相关命令 元素个数最多为2^32-1个，也就是4294967295个。 set 是无序集合，最大可以包含(2 的 32 次方-1)个元素。set 的是通过 hash table 实现的 ，所以添加，删除，查找的复杂度都是 O(1)。关于 set 集合类型除了基本的添加删除操作，其它有用的操作还包含集合的取并集(union)，交集(intersection) ，差集(difference)。 sadd key member 添加一个 string 元素到 key 对应 set 集合中，成功返回 1,如果元素以及在集合中则返回 0，key 对应的 set 不存在则返回错误。（2.4版本以后支持多个元素） srem key member 从 key 对应 set 中移除指定元素，成功返回 1，如果 member 在集合中不存在或者 key 不存在返回 0，如果 key 对应的不是 set 类型的值返回错误。 spop key 删除并返回 key 对应 set 中随机的一个元素,如果 set 是空或者 key 不存在返回nil。 srandmember随机获取一个元素，但是不从集合中移除。 scard key 返回 set 的元素个数，如果 set 是空或者 key 不存在返回 0。 sismember key member 判断 member 是否在 set 中，存在返回 1，0 表示不存在或者 key 不存在。 sinter key1 key2 …… keyN 返回所有给定 key 的交集。 sinterstore dstkey key1 ……. keyN 返回所有给定 key 的交集，并保存交集存到 dstkey 下。 sunion key1 key2 …… keyN 返回所有给定 key 的并集。 sunionstore dstkey key1 …… keyN 返回所有给定 key 的并集，并保存并集到 dstkey 下。 sdiff key1 key2 …… keyN 返回所有给定 key 的差集。 sdiffstore dstkey key1 …… keyN 返回所有给定 key 的差集，并保存差集到 dstkey 下。 smembers key 返回 key 对应 set 的所有元素，结","date":"2017-04-16","objectID":"/posts/redis_%E5%9F%BA%E7%A1%80/:0:4","tags":["redis"],"title":"Redis学习（一）-redis基础","uri":"/posts/redis_%E5%9F%BA%E7%A1%80/"},{"categories":["笔记"],"content":"其他命令 清除redis的expire时间： 对key的内容进行修改，包括set,del,lpush,getset等都可以 对一个key进行rename，则expire会转移到新的key PERSIST key(2.2.0以后可用) list没有提供by index删除元素的方法，可以使用 下面的方法迂回删除 lset listkey 1 del lrem listkey 0 del ","date":"2017-04-16","objectID":"/posts/redis_%E5%9F%BA%E7%A1%80/:0:5","tags":["redis"],"title":"Redis学习（一）-redis基础","uri":"/posts/redis_%E5%9F%BA%E7%A1%80/"},{"categories":["笔记"],"content":"Redis学习（三）-redis内部探索","date":"2017-04-16","objectID":"/posts/redis_%E5%8E%9F%E7%90%86/","tags":["redis"],"title":"Redis学习（三）-redis内部探索","uri":"/posts/redis_%E5%8E%9F%E7%90%86/"},{"categories":["笔记"],"content":"此篇主要介绍redis的一些机制 ","date":"2017-04-16","objectID":"/posts/redis_%E5%8E%9F%E7%90%86/:0:0","tags":["redis"],"title":"Redis学习（三）-redis内部探索","uri":"/posts/redis_%E5%8E%9F%E7%90%86/"},{"categories":["笔记"],"content":"expire删除机制 大家有没有想过redis是怎么实现key的过期删除的，作为有经验的开发人员，也许大家很容易想到以下三个方案： 每设置一个过期时间就启动一个定时器，时间到的时候删除即可，但是当有大量的key需要过期的时候就显得效率低下。 懒汉式检查，当我们取值的时候再进行判断，此种方案节约了不少检查的开支，但是如果长期没有进行取值那就行造成存储资源的浪费，而且同一时间进行大量的取值要进行的判断次数很多，cpu的压力陡增。 定期检查并删除，这样可以解决懒汉式的缺点，但是检查的周期和每次执行的时间要合适，不然会给cpu过多的压力 这里我们看下redis官网上的介绍 Redis keys are expired in two ways: a passive way, and an active way. A key is passively expired simply when some client tries to access it, and the key is found to be timed out. Of course this is not enough as there are expired keys that will never be accessed again. These keys should be expired anyway, so periodically Redis tests a few keys at random among keys with an expire set. All the keys that are already expired are deleted from the keyspace. Specifically this is what Redis does 10 times per second: Test 20 random keys from the set of keys with an associated expire. Delete all the keys found expired. If more than 25% of keys were expired, start again from step 1. 大概意思是说有两个方案可以选择。消极的方法就是客户端请求访问数据的时候再检查是否超时，当然这是不够的，因为有些keys可能再也不会访问到。redis从设置过期的keys中随机的选择一些进行检查。所有过期的key都要从空间中删除。 具体内容，每秒做10次以下步骤： 随机选择20个key 删除过期的key 如果超过25%的key是过期的，继续循环，执行第一步 ","date":"2017-04-16","objectID":"/posts/redis_%E5%8E%9F%E7%90%86/:1:0","tags":["redis"],"title":"Redis学习（三）-redis内部探索","uri":"/posts/redis_%E5%8E%9F%E7%90%86/"},{"categories":["笔记"],"content":"Redis学习（二）-redis进阶","date":"2017-04-16","objectID":"/posts/redis_%E8%BF%9B%E9%98%B6/","tags":["redis"],"title":"Redis学习（二）-redis进阶","uri":"/posts/redis_%E8%BF%9B%E9%98%B6/"},{"categories":["笔记"],"content":"上篇主要讲到redis的基础命令，这篇涉及到redis的配置、持久化、主从、事务、内存淘汰策略 ","date":"2017-04-16","objectID":"/posts/redis_%E8%BF%9B%E9%98%B6/:0:0","tags":["redis"],"title":"Redis学习（二）-redis进阶","uri":"/posts/redis_%E8%BF%9B%E9%98%B6/"},{"categories":["笔记"],"content":"redis的配置 启动redis服务器的时候会指定配置文件，主要参数如下： daemonize： #是否以后台守护进程方式运行 pidfile： #pid 文件位置 port： #监听的端口号 timeout： #请求超时时间 loglevel： #log 信息级别，总共支持四个级别：debug、verbose、notice、warning ， 默认为 verbose logfile： #默认为标准输出（stdout），如果配置为守护进程方式运行，而这里又配 置为日志记录方式为标准输出，则日志将会发送给/dev/null databases： #开启数据库的数量。使用“SELECT 库 ID”方式切换操作各个数据库 save：保存快照的频率，第一个表示多长时间，第二个表示执行多少次写操作。在一定时间内执行一定数量的写操作时，自动保存快照。可设置多个条件。 rdbcompression：#保存快照是否使用压缩 dbfilename： #数据快照文件名（只是文件名，不包括目录）。默认值为 dump.rdb dir： #数据快照的保存目录（这个是目录） requirepass： #设置 Redis 连接密码，如果配置了连接密码，客户端在连接 Redis 时需 要通过 AUTH 命令提供密码，默认关闭。 ","date":"2017-04-16","objectID":"/posts/redis_%E8%BF%9B%E9%98%B6/:1:0","tags":["redis"],"title":"Redis学习（二）-redis进阶","uri":"/posts/redis_%E8%BF%9B%E9%98%B6/"},{"categories":["笔记"],"content":"redis持久化 redis在内存中存储数据，但重启的时候我们还是希望能恢复数据，那么可以把数据持久化到数据库中。你也可以同时开启两种持久化方式， 在这种情况下, 当redis重启的时候会优先载入AOF文件来恢复原始的数据,因为在通常情况下AOF文件保存的数据集要比RDB文件保存的数据集要完整. ","date":"2017-04-16","objectID":"/posts/redis_%E8%BF%9B%E9%98%B6/:2:0","tags":["redis"],"title":"Redis学习（二）-redis进阶","uri":"/posts/redis_%E8%BF%9B%E9%98%B6/"},{"categories":["笔记"],"content":"快照方式RDB (Redis Database) 快照方式也是redis默认使用方式，这种方式就是将内存中数据以快照的方式写入到二进制文件中，默认的文件名为dump.rdb。当然我们也可以主动调用命令来手动持久化，使用save 或者bgsave 命令通知 redis 做一次快照持久化。 配置参数 save 900 1 save 300 10 save 60 10000 分别表示900秒（15分钟）内有1个更改，300秒（5分钟）内有10个更改以及60秒内有10000个更改。 RDB文件通过两个命令来生成： SAVE:阻塞redis的服务器进程，直到RDB文件被创建完毕。在主线程中保存快照，redis是用一个主线程来处理所有请求的，这种方式会阻塞所有客户端的请求。 BGSAVE:Fork出一个子进程来创建RDB文件，不阻塞服务器进程，记录接收BGSAVE当时的数据库状态，父进程继续处理接收到的命令，子进程完成文件的创建之后，会发送信号给父进程。 自动化触发RDB持久化的方式 1\u003e根据配置redis.conf的save就可以(用的bgsave) 2\u003e主从复制时，主节点自动触发 3\u003e执行Debug Reload 4\u003e执行shutdown且没有开启AOF持久化 注意： save 操作是在主线程中保存快照的，由于 redis 是用一个主线程来处理所有客户端的请求，这种方式会阻塞所有客户端请求。所以不推荐使用。 每次快照持久化都是将内存数据完整写入到磁盘一次，并不是增量的只同步增量数据。如果数据量大的话，写操作会比较多，必然会引起大量的磁盘 IO 操作，可能会严重影响性能。 ","date":"2017-04-16","objectID":"/posts/redis_%E8%BF%9B%E9%98%B6/:2:1","tags":["redis"],"title":"Redis学习（二）-redis进阶","uri":"/posts/redis_%E8%BF%9B%E9%98%B6/"},{"categories":["笔记"],"content":"日志追加方式AOF(Append Only File) 这种方式 redis 会将每一个收到的写命令都通过 write 函数追加到文件中(默认appendonly.aof)。当 redis 重启时会通过重新执行文件中保存的写命令来在内存中重建整个数据库的内容。当然由于操作系统会在内核中缓存 write 做的修改，所以可能不是立即写到磁盘上。这样的持久化还是有可能会丢失部分修改。不过我们可以通过配置文件告诉redis 我们想要通过 fsync 函数强制操作系统写入到磁盘的时机。有三种方式如下（默认是 ：每秒 fsync 一次） appendonly yes //启用日志追加持久化方式 appendfsync always //每次收到写命令就立即强制写入磁盘，最慢的，但是保证完全 的持久化，不推荐使用 appendfsync everysec //每秒钟强制写入磁盘一次，在性能和持久化方面做了很好的折 中，推荐 appendfsync no //完全依赖操作系统，性能最好,持久化没保证 日志追加方式同时带来了另一个问题。持久化文件会变的越来越大。为了压缩这种持久化方式的日志文件 。redis 提供了bgrewriteaof 命令。收到此命令 redis 将使用与快照类似的方式将内存中的数据以命令的方式保存到临时文件中，而不是把原来那种所有的操作记录，最后替换原来的持久化日志文件。 我们可以配置aof文件的大小扩张倍数即重写。 比如说redis现在做一个定时器，轮询100下，那其实我们想要的结果是最后的数据，但是AOF会把整个过程记录下来，所以AOF文件大小会不断增大。怎么办呢？ BGREWRITEAOF命令来重写 1\u003e调用fork()，创建一个子进程 2\u003e子进程把新的AOF写到一个临时文件里，不依赖原来的AOF文件 3\u003e主进程持续将新的变动同时写到内存和原来的AOF里 4\u003e主进程获取子进程重写AOF的信号之后，往新的AOF同步增量变动 5\u003e使用新的AOF文件替换旧的AOF文件 ","date":"2017-04-16","objectID":"/posts/redis_%E8%BF%9B%E9%98%B6/:2:2","tags":["redis"],"title":"Redis学习（二）-redis进阶","uri":"/posts/redis_%E8%BF%9B%E9%98%B6/"},{"categories":["笔记"],"content":"RDB和AOF的优缺点比较 1\u003e RDB优点：全量数据快照，文件小，恢复快 RDB缺点：无法保存最近一次快照之后的数据 2\u003e AOF优点：可读性高，适合保存增量数据，数据不易丢失 AOF缺点：文件体积大，恢复时间长 RDB-AOF混合 redis4.0之后推出RDB-AOF混合持久化方式，并且是默认配置。 BGSAVE做镜像全量持久化，AOF做增量持久化。 在redis实例重启时，会使用BGSAVE持久化文件重新构建内容，再使用AOF重放近期的操作指令。 ","date":"2017-04-16","objectID":"/posts/redis_%E8%BF%9B%E9%98%B6/:2:3","tags":["redis"],"title":"Redis学习（二）-redis进阶","uri":"/posts/redis_%E8%BF%9B%E9%98%B6/"},{"categories":["笔记"],"content":"redis主从简介 Redis 支持将数据同步到多台从库上，这种特性对提高 读取性能非常有益。 master 可以有多个 slave。 除了多个 slave 连到相同的 master 外，slave 也可以连接其它 slave 形成图状结构。 主从复制不会阻塞 master。也就是说当一个或多个 slave 与 master 进行初次同步数据 时，master 可以继续处理客户端发来的请求。 主从复制可以用来提高系统的可伸缩性,我们可以用多个 slave 专门用于客户端的读请求，比如 sort 操作可以使用 slave 来处理。也可以用来做简单的数据冗余。 可以在 master 禁用数据持久化，只需要注释掉 master 配置文件中的所有 save 配置，然 后只在 slave 上配置数据持久化 ","date":"2017-04-16","objectID":"/posts/redis_%E8%BF%9B%E9%98%B6/:3:0","tags":["redis"],"title":"Redis学习（二）-redis进阶","uri":"/posts/redis_%E8%BF%9B%E9%98%B6/"},{"categories":["笔记"],"content":"redis主从原理 当设置好 slave 服务器后，slave 会建立和 master 的连接，然后发送 sync 命令。无论是第一次同步建立的连接还是连接断开后的重新连接，master 都会启动一个后台进程，将数据库快照保存到文件中，同时 master 主进程会开始收集新的写命令并缓存起来。后台进程完成写文件后，master 就发送文件给 slave，slave 将文件保存到磁盘上，然后加载到内存恢复数据库快照到 slave 上。接着 master 就会把缓存的命令转发给 slave。而且后续 master 收到的写命令都会通过开始建立的连接发送给slave。从master到slave的同步数据的命令和从客户端发送的命令使用相同的协议格式。当 master 和 slave 的连接断开时 slave 可以自动重新建立连接。如果 master 同时收到多个 slave 发来的同步连接命令，只会启动一个进程来写数据库镜像，然后发送给所有 slave。 配置 slave 服务器很简单，只需要在配置文件中加入如下配置： slaveof 192.168.1.1 6379 #指定 master 的 ip 和端口 ","date":"2017-04-16","objectID":"/posts/redis_%E8%BF%9B%E9%98%B6/:3:1","tags":["redis"],"title":"Redis学习（二）-redis进阶","uri":"/posts/redis_%E8%BF%9B%E9%98%B6/"},{"categories":["笔记"],"content":"redis事务 如果是在入队时报错，那么都不会执行；在非入队时报错，那么成功的就会成功执行。 Redis 通过 MULTI 、DISCARD 、EXEC 和 WATCH 四个命令来实现事务功能。 事务提供了一种“将多个命令打包，然后一次性、按顺序地执行”的机制，并且事务在执行的期间不会主动中断——服务器在执行完事务中的所有命令之后，才会继续处理其他客户端的其他命令。 另外，Redis的事务是不可嵌套的，当客户端已经处于事务状态，而客户端又再向服务器发送 MULTI 时，服务器只是简单地向客户端发送一个错误，然后继续等待其他命令的入队。 ","date":"2017-04-16","objectID":"/posts/redis_%E8%BF%9B%E9%98%B6/:4:0","tags":["redis"],"title":"Redis学习（二）-redis进阶","uri":"/posts/redis_%E8%BF%9B%E9%98%B6/"},{"categories":["笔记"],"content":"redis事务流程 一个事务从开始到执行会经历以下三个阶段： 开始事务。//MUTI命令，这个命令唯一做的就是，将客户端的 REDIS_MULTI 选项打开，让客户端从非事务状态切换到事务状态。 命令入队。//…要执行的命令，当客户端处于非事务状态下时，所有发送给服务器端的命令都会立即被服务器执行，但是当客户端进入事务状态之后，服务器在收到来自客户端的命令时，不会立即执行命令，而是将这些命令全部放进一个事务队列里，然后返回 QUEUED ，表示命令已入队。 执行事务。//EXEC命令，前面说到，当客户端进入事务状态之后，客户端发送的命令就会被放进事务队列里。但其实并不是所有的命令都会被放进事务队列，其中的例外就是 EXEC 、DISCARD 、MULTI 和 WATCH 这四个命令——当这四个命令从客户端发送到服务器时，它们会像客户端处于非事务状态一样，直接被服务器执行 ","date":"2017-04-16","objectID":"/posts/redis_%E8%BF%9B%E9%98%B6/:4:1","tags":["redis"],"title":"Redis学习（二）-redis进阶","uri":"/posts/redis_%E8%BF%9B%E9%98%B6/"},{"categories":["笔记"],"content":"DISCARD、WATCH命令 DISCARD代表取消事务，它清空客户端的整个事务队列，然后将客户端从事务状态调整回非事务状态，最后返回字符串 OK 给客户端，说明事务已被取消。 WATCH 命令用于在事务开始之前监视任意数量的键：当调用 EXEC 命令执行事务时，如果任意一个被监视的键已经被其他客户端修改了，那么整个事务不再执行，直接返回失败。 redis\u003e WATCH name OK redis\u003e MULTI OK redis\u003e SET name peter QUEUED redis\u003e EXEC 当一个客户端结束它的事务时，无论事务是成功执行，还是失败,watch内容都会被清空。 ","date":"2017-04-16","objectID":"/posts/redis_%E8%BF%9B%E9%98%B6/:4:2","tags":["redis"],"title":"Redis学习（二）-redis进阶","uri":"/posts/redis_%E8%BF%9B%E9%98%B6/"},{"categories":["笔记"],"content":"事务的 ACID 性质 Redis 事务保证了其中的一致性（C）和隔离性（I），但并不保证原子性（A）和持久性（D）。 原子性（Atomicity 单个 Redis 命令的执行是原子性的，但 Redis 没有在事务上增加任何维持原子性的机制，所以 Redis 事务的执行并不是原子性的。 如果一个事务队列中的所有命令都被成功地执行，那么称这个事务执行成功。另一方面，如果 Redis 服务器进程在执行事务的过程中被停止——比如接到 KILL 信号、宿主机器停机，等等，那么事务执行失败。当事务失败时，Redis 也不会进行任何的重试或者回滚动作。 一致性（Consistency Redis 的一致性问题可以分为三部分来讨论：入队错误、执行错误、Redis 进程被终结。 入队错误 在命令入队的过程中，如果客户端向服务器发送了错误的命令，比如命令的参数数量不对，等等，那么服务器将向客户端返回一个出错信息，并且将客户端的事务状态设为REDIS_DIRTY_EXEC，因此，带有不正确入队命令的事务不会被执行，也不会影响数据库的一致性。 执行错误 如果命令在事务执行的过程中发生错误，比如说，对一个不同类型的 key 执行了错误的操作， 那么 Redis 只会将错误包含在事务的结果中，这不会引起事务中断或整个失败，不会影响已执 行事务命令的结果，也不会影响后面要执行的事务命令，所以它对事务的一致性也没有影响。 Redis 进程被终结 如果 Redis 服务器进程在执行事务的过程中被其他进程终结，或者被管理员强制杀死，那么根 据 Redis 所使用的持久化模式，可能有以下情况出现： • 内存模式：如果 Redis 没有采取任何持久化机制，那么重启之后的数据库总是空白的，所 以数据总是一致的。 • RDB 模式：在执行事务时，Redis 不会中断事务去执行保存 RDB 的工作，只有在事务执 行之后，保存 RDB 的工作才有可能开始。所以当 RDB 模式下的 Redis 服务器进程在事 务中途被杀死时，事务内执行的命令，不管成功了多少，都不会被保存到 RDB 文件里。 恢复数据库需要使用现有的 RDB 文件，而这个 RDB 文件的数据保存的是最近一次的数 据库快照（snapshot），所以它的数据可能不是最新的，但只要 RDB 文件本身没有因为 其他问题而出错，那么还原后的数据库就是一致的。 • AOF 模式：因为保存 AOF 文件的工作在后台线程进行，所以即使是在事务执行的中途， 保存 AOF 文件的工作也可以继续进行，因此，根据事务语句是否被写入并保存到 AOF 文件，有以下两种情况发生： 如果事务语句未写入到 AOF 文件，或 AOF 未被 SYNC 调用保存到磁盘，那么当进 程被杀死之后，Redis 可以根据最近一次成功保存到磁盘的 AOF 文件来还原数据库，只 要 AOF 文件本身没有因为其他问题而出错，那么还原后的数据库总是一致的，但其中的 数据不一定是最新的。 如果事务的部分语句被写入到 AOF 文件，并且 AOF 文件被成功保存，那么不完整的 事务执行信息就会遗留在 AOF 文件里，当重启 Redis 时，程序会检测到 AOF 文件并不 完整，Redis 会退出，并报告错误。需要使用 redis-check-aof 工具将部分成功的事务命令 移除之后，才能再次启动服务器。还原之后的数据总是一致的，而且数据也是最新的（直 到事务执行之前为止）。 隔离性（Isolation） Redis 是单进程程序，并且它保证在执行事务时，不会对事务进行中断，事务可以运行直到执 行完所有事务队列中的命令为止。因此，Redis 的事务是总是带有隔离性的。 持久性（Durability） 因为事务不过是用队列包裹起了一组 Redis 命令，并没有提供任何额外的持久性功能，所以事 务的持久性由 Redis 所使用的持久化模式决定 ","date":"2017-04-16","objectID":"/posts/redis_%E8%BF%9B%E9%98%B6/:4:3","tags":["redis"],"title":"Redis学习（二）-redis进阶","uri":"/posts/redis_%E8%BF%9B%E9%98%B6/"},{"categories":["笔记"],"content":"redis淘汰策略 相关知识：redis 内存数据集大小上升到一定大小的时候，就会施行数据淘汰策略（回收策略）。redis 提供 6种数据淘汰策略： volatile-lru：从已设置过期时间的数据集（server.db[i].expires）中挑选最近最少使用的数据淘汰 volatile-ttl：从已设置过期时间的数据集（server.db[i].expires）中挑选将要过期的数据淘汰 volatile-random：从已设置过期时间的数据集（server.db[i].expires）中任意选择数据淘汰 allkeys-lru：从数据集（server.db[i].dict）中挑选最近最少使用的数据淘汰 allkeys-random：从数据集（server.db[i].dict）中任意选择数据淘汰 no-enviction（驱逐）：禁止驱逐数据，内存不足就报错（默认的策略） ","date":"2017-04-16","objectID":"/posts/redis_%E8%BF%9B%E9%98%B6/:5:0","tags":["redis"],"title":"Redis学习（二）-redis进阶","uri":"/posts/redis_%E8%BF%9B%E9%98%B6/"},{"categories":["笔记"],"content":"Redis学习（五）-缓存三大问题","date":"2017-04-16","objectID":"/posts/redis_%E7%BC%93%E5%AD%98%E4%B8%89%E5%A4%A7%E9%97%AE%E9%A2%98/","tags":["redis"],"title":"Redis学习（五）-缓存三大问题","uri":"/posts/redis_%E7%BC%93%E5%AD%98%E4%B8%89%E5%A4%A7%E9%97%AE%E9%A2%98/"},{"categories":["笔记"],"content":"缓存穿透 概念 访问一个不存在的key，缓存不起作用，请求会穿透到DB，流量大时DB会挂掉。 解决方案 第一种：采用布隆过滤器，使用一个足够大的bitmap，用于存储可能访问的key，不存在的key直接被过滤； 第二种：访问key未在DB查询到值，也将空值写进缓存，但可以设置较短过期时间。 ","date":"2017-04-16","objectID":"/posts/redis_%E7%BC%93%E5%AD%98%E4%B8%89%E5%A4%A7%E9%97%AE%E9%A2%98/:0:1","tags":["redis"],"title":"Redis学习（五）-缓存三大问题","uri":"/posts/redis_%E7%BC%93%E5%AD%98%E4%B8%89%E5%A4%A7%E9%97%AE%E9%A2%98/"},{"categories":["笔记"],"content":"缓存雪崩 概念 大量的key设置了相同的过期时间，导致在缓存在同一时刻全部失效，造成瞬时DB请求量大、压力骤增，引起雪崩。 解决方案 可以给缓存设置过期时间时加上一个随机值时间，使得每个key的过期时间分布开来，不会集中在同一时刻失效。 ","date":"2017-04-16","objectID":"/posts/redis_%E7%BC%93%E5%AD%98%E4%B8%89%E5%A4%A7%E9%97%AE%E9%A2%98/:0:2","tags":["redis"],"title":"Redis学习（五）-缓存三大问题","uri":"/posts/redis_%E7%BC%93%E5%AD%98%E4%B8%89%E5%A4%A7%E9%97%AE%E9%A2%98/"},{"categories":["笔记"],"content":"缓存击穿 概念 一个存在的key，在缓存过期的一刻，同时有大量的请求，这些请求都会击穿到DB，造成瞬时DB请求量大、压力骤增。 解决方案 当失效的时候有一个去读数据即可，其他的先等着。 在访问key之前，采用SETNX（set if not exists）来设置另一个短期key，设置成功去读数据库。如果没有设置成功就先等等（比如1s）再重试读缓存，读数据的成功后回设缓存，并访问结束再删除该短期key。 ","date":"2017-04-16","objectID":"/posts/redis_%E7%BC%93%E5%AD%98%E4%B8%89%E5%A4%A7%E9%97%AE%E9%A2%98/:0:3","tags":["redis"],"title":"Redis学习（五）-缓存三大问题","uri":"/posts/redis_%E7%BC%93%E5%AD%98%E4%B8%89%E5%A4%A7%E9%97%AE%E9%A2%98/"},{"categories":["笔记"],"content":"Redis学习（四）-golang使用redis","date":"2017-04-15","objectID":"/posts/redis_go/","tags":["redis"],"title":"Redis学习（四）-golang使用redis","uri":"/posts/redis_go/"},{"categories":["笔记"],"content":"该文章讲解redigo的使用。 talk is cheap show me the code ","date":"2017-04-15","objectID":"/posts/redis_go/:0:0","tags":["redis"],"title":"Redis学习（四）-golang使用redis","uri":"/posts/redis_go/"},{"categories":["笔记"],"content":"redigo基本使用 opt := redis.DialPassword(\"root\") c, err := redis.Dial(\"tcp\", \"127.0.0.1:6379\", opt) if err != nil { fmt.Println(\"Connect to redis error\", err) return } defer c.Close() userName, err := redis.String(c.Do(\"GET\", \"userName\")) if err != nil { fmt.Println(\"redis get failed:\", err) } else { fmt.Printf(\"Get mykey: %v \\n\", userName) } 获取连接的时候可以提供配置文件，上面代码中指定了密码，此外还有redis.DialConnectTimeout()等。 Do方法执行命令，与客户端使用类似。 reids提供了一系列方法从返回值interface{}转换为我们需要的类型。 ","date":"2017-04-15","objectID":"/posts/redis_go/:1:0","tags":["redis"],"title":"Redis学习（四）-golang使用redis","uri":"/posts/redis_go/"},{"categories":["笔记"],"content":"redigo事务 redis还支持管道 c.Send(\"SET\", \"foo\", \"bar\") c.Send(\"GET\", \"foo\") c.Flush() c.Receive() // reply from SET v, err = c.Receive() // reply from GET 命令写在连接的缓冲区中，当flush后发送到服务器，receive方法从服务器读取一行命令的结果。 其实Do方法是上面系列方法的组合，Do以send命令并flush至缓冲区，后面接收所有的结果，其中一个命令的发生错误，那么Do方法就返回该error，所有的命令正常执行完， 则返回最后一条命令的结果。 我们使用该方案实现redis的事务： c.Send(\"MULTI\") c.Send(\"INCR\", \"foo\") c.Send(\"INCR\", \"bar\") r, err := c.Do(\"EXEC\") fmt.Println(r) // prints [1, 1] ","date":"2017-04-15","objectID":"/posts/redis_go/:2:0","tags":["redis"],"title":"Redis学习（四）-golang使用redis","uri":"/posts/redis_go/"},{"categories":["笔记"],"content":"redigo订阅、发布 redigo中pub/sub用法 psc := redis.PubSubConn{Conn: c} psc.Subscribe(\"example\") for { switch v := psc.Receive().(type) { case redis.Message: fmt.Printf(\"%s: message: %s\\n\", v.Channel, v.Data) case redis.Subscription: fmt.Printf(\"%s: %s %d\\n\", v.Channel, v.Kind, v.Count) case error: return } } ","date":"2017-04-15","objectID":"/posts/redis_go/:3:0","tags":["redis"],"title":"Redis学习（四）-golang使用redis","uri":"/posts/redis_go/"},{"categories":["笔记"],"content":"redigo连接池 在实际的项目中要频繁的使用redis，如果每次连接，使用完毕断开连接，势必会造成效率低下。推荐使用redigo自带的连接池。 我们先来看下线程池结构体的信息： type Pool struct { //Dial 是创建链接的方法 Dial func() (Conn, error) //TestOnBorrow 是一个测试链接可用性的方法 TestOnBorrow func(c Conn, t time.Time) error // 最大的空闲连接数，表示即使没有redis连接时依然可以保持N个空闲的连接，而不被清除，随时处于待命状态 MaxIdle int // 最大的激活连接数，表示同时最多有N个连接 ，为0事表示没有限制 MaxActive int //最大的空闲连接等待时间，超过此时间后，空闲连接将被关闭 IdleTimeout time.Duration // 当链接数达到最大后是否阻塞，如果不的话，达到最大后返回错误 Wait bool } 如果使用： RedisClient = \u0026redis.Pool{ // 从配置文件获取maxidle以及maxactive，取不到则用后面的默认值 IdleTimeout: 180 * time.Second, Dial: func() (redis.Conn, error) { c, err := redis.Dial(\"tcp\", REDIS_HOST) if err != nil { return nil, err } return c, nil }, } // 从池里获取连接 rc := RedisClient.Get() // 用完后将连接放回连接池 defer rc.Close() ","date":"2017-04-15","objectID":"/posts/redis_go/:4:0","tags":["redis"],"title":"Redis学习（四）-golang使用redis","uri":"/posts/redis_go/"},{"categories":["笔记"],"content":"android加载html","date":"2017-04-08","objectID":"/posts/android_%E5%8A%A0%E8%BD%BDhtml/","tags":["android"],"title":"android加载html","uri":"/posts/android_%E5%8A%A0%E8%BD%BDhtml/"},{"categories":["笔记"],"content":"android加载html Android中是通过webView控件来加载html⻚页⾯面的。在Android \u003c 4.4 （KitKat）上的WebView是基于 Webkit实现的，⽽而Android \u003e= 4.4 的WebView 则是基于Chrome for Android version 30实现的。 andorid代码示例例： 加载html，很简单，⼀一般我们还会设置WebViewClient，来进⾏行行更更复杂的操作。 设置WebViewClient 除此之外，我们在WebViewClient的回调⽅方法⾥里里可以监听html的开始、结束等信息。 mWebView.loadUrl(\"http://www.baidu.com\" mWebView.setWebViewClient(new WebViewClient() { @Override public boolean shouldOverrideUrlLoading(WebView view, String url) { //我们可以在这⾥里里拦截特定的rl请求，然后进⾏行行⾃自⼰己要的操作 if (url.equals(\"http://www.baidu.com\")) { Log.e(TAG, \"shouldOverrideUrlLoading: \" + url); startActivity(new Intent(MainActivity.this,Main2Activity.class)); return true; } else { //这⾥里里我们⾃自⼰己重新加载新的url⻚页⾯面，防⽌止点击链接跳转到系统浏览器器 mWebView.loadUrl(url); return true; } } } }); 设置WebChromeClient 为webView控件指定WebChromeClient。 mWebView.setWebChromeClient(new WebChromeClient() { @Override public void onProgressChanged(WebView view, int newProgress) { } }); WebChromeClient与webViewClient的区别 webViewClient处理理偏界⾯面的操作：打开新界⾯面，界⾯面打 开，界⾯面打开，界⾯面放⼤大，结束等。 WebChromeClient处理理偏js的操作。 webview基本设置 WebSettings webSettings = mWebView.getSettings(); //如果访问的⻚页⾯面中要与Javascript交互，则webview必须设置⽀支持Javascript webSettings.setJavaScriptEnabled(true); // 若加载的 html ⾥里里有JS 在执⾏行行动画等操作，会造成资源浪费（CPU、电量量） // 在 onStop 和 onResume ⾥里里分别把 setJavaScriptEnabled() 给设置成 false 和 true 即 可 //⽀支持插件 webSettings.setPluginsEnabled(true); //设置⾃自适应屏幕，两者合⽤用 webSettings.setUseWideViewPort(true); //将图⽚片调整到适合webview的⼤大⼩小 webSettings.setLoadWithOverviewMode(true); // 缩放⾄至屏幕的⼤大⼩小 //缩放操作 webSettings.setSupportZoom(true); //⽀支持缩放，默认为true。是下⾯面那个的前提。 webSettings.setBuiltInZoomControls(true); //设置内置的缩放控件。若为false，则该 WebView不不可缩放 webSettings.setDisplayZoomControls(false); //隐藏原⽣生的缩放控件 //其他细节操作 webSettings.setCacheMode(WebSettings.LOAD_CACHE_ELSE_NETWORK); //关闭webview中缓 存 webSettings.setAllowFileAccess(true); //设置可以访问⽂文件 webSettings.setJavaScriptCanOpenWindowsAutomatically(true); //⽀支持通过JS打开新窗 ⼝口 webSettings.setLoadsImagesAutomatically(true); //⽀支持⾃自动加载图⽚片 webSettings.setDefaultTextEncodingName(\"utf-8\");//设置编码格式 基本设置api根据实际需要来改动。 ","date":"2017-04-08","objectID":"/posts/android_%E5%8A%A0%E8%BD%BDhtml/:0:1","tags":["android"],"title":"android加载html","uri":"/posts/android_%E5%8A%A0%E8%BD%BDhtml/"},{"categories":["笔记"],"content":"android调⽤用js android要想调⽤用js⼀一定要让webView⽀支持 有两种⽅方式： 调⽤用的js⽅方法没有返回值，则直接可以调⽤用mWebView.loadUrl(“javascript:do()\");其中do是js中的 ⽅方法； 有返回值时我们可以调⽤用mWebView.evaluateJavascript()⽅方法。 实例例代码： //⾸首先设置Webview⽀支持JS代码 webView.getSettings().setJavaScriptEnabled(true); @TargetApi(Build.VERSION_CODES.KITKAT) public void onSum(View view){ webView.evaluateJavascript(\"sum(1,2)\", new ValueCallback\u003cString\u003e() { @Override public void onReceiveValue(String value) { Toast.makeText(getApplicationContext(), } }); } \"相加结果:\"+value, Toast.LENGTH_SHORT).show(); public void onDoing(View view){ String msg = \"测试\"; webView.loadUrl(\"javascript:showInfoFromJava('\"+msg+\"')\"); } 对比： 优点：该⽅方法⽐比第⼀一种⽅方法效率更更⾼高、使⽤用更更简洁。 缺点：因为该⽅方法的执⾏行行不不会使⻚页⾯面刷新，⽽而第⼀一种⽅方法（loadUrl ）的执⾏行行则会。Android 4.4 后才可 使⽤用。（开发中第⼀一种较多） ","date":"2017-04-08","objectID":"/posts/android_%E5%8A%A0%E8%BD%BDhtml/:0:2","tags":["android"],"title":"android加载html","uri":"/posts/android_%E5%8A%A0%E8%BD%BDhtml/"},{"categories":["笔记"],"content":"js调⽤用android 对于JS调⽤用Android代码的⽅方法有3种： 通过WebView的addJavascriptInterface（）进⾏行行对象映射。 通过 WebViewClient 的shouldOverrideUrlLoading ()⽅方法回调拦截 url。 通过 WebChromeClient 的onJsAlert()、onJsConfirm()、onJsPrompt（）⽅方法回调拦截JS对话框 alert()、confirm()、prompt（） 消息。 addJavascriptInterface⽅方案 通过webview的api注册本地⽅方法: public void addJavascriptInterface(Object object, String name); js⽂文件实例例代码: function jsJava(){ //调⽤用java的⽅方法，顶级对象，java⽅方法 //可以直接访问JSTest，这是因为JSTest挂载到js的window对象下了了 JSTest.showToast(\"我是被JS执⾏行行的Android代码\"); } 对应的Java代码: //java与js回调，⾃自定义⽅方法 //1.java调⽤用js //2.js调⽤用java //⾸首先java暴暴露露接⼝口，供js调⽤用 /** * obj:暴暴露露的要调⽤用的对象 * interfaceName:对象的映射名称 ,object的对象名，在js中可以直接调⽤用 * 在html的js中:JSTest.showToast(msg) * 可以直接访问JSTest，这是因为JSTest挂载到js的window对象下了了 */ webView.addJavascriptInterface(new Object() { //定义要调⽤用的⽅方法 //msg由js调⽤用的时候传递 @JavascriptInterface public void showToast(String msg) { Toast.makeText(getApplicationContext(), msg, Toast.LENGTH_SHORT).show(); } }, \"JSTest\"); 这样就实现了了js调⽤用android原⽣生的toast效果。 注意: 该⽅方案在部分操作系统版本中存在漏漏洞洞: Android API 16.0及之前的版本中存在安全漏漏洞洞，该漏漏洞洞源于程序没有正确限制使⽤用 WebView.addJavascriptInterface⽅方法。远程攻击者可通过使⽤用Java Reflection API利利⽤用该漏漏洞洞执 ⾏行行任意Java对象的⽅方法。API\u003c=16 java/android/webkit/BrowserFrame.java 使⽤用addJavascriptInterface API并创建了了 SearchBoxImpl类的对象。攻击者可通过访问searchBoxJavaBridge_接⼝口利利⽤用该漏漏洞洞执⾏行行任意Java 代码。API\u003c=4.3.1 所有由系统提供的WebView都会被加⼊入两个JS objects，分别为是accessibility和 accessibilityTraversal。恶意攻击者就可以使⽤用accessibility和accessibilityTraversal这两个Java Bridge来执⾏行行远程攻击代码. API\u003c4.4 升级系统API level 17后，只有显示添加 @JavascriptInterface的⽅方法才能被JavaScript调⽤用，这样反射 就失去作⽤用了了。但对于更更低版本则还是会存在。 拦截url⽅方案（schema） 具体原理理： Android通过 WebViewClient 的回调⽅方法 shouldOverrideUrlLoading ()拦截 url 解析该 url 的协议 如果检测到是预先约定好的协议，就调⽤用相应⽅方法 mWebView.setWebViewClient(new WebViewClient() { @Override { public boolean shouldOverrideUrlLoading(WebView view, String url) // 步骤2:根据协议的参数，判断是否是所需要的url // ⼀一般根据scheme(协议格式) \u0026 authority(协议名)判断(前两个参数) 定好的需要拦截的) 协议 Uri uri = Uri.parse(url); // 如果url的协议 = 预先约定的 js 协议 // 就解析往下解析参数 if (uri.getScheme().equals(\"js\")) { // 如果 authority = 预先约定协议⾥里里的 webview，即代表都符合约定的 // 所以拦截url,下⾯面JS开始调⽤用Android需要的⽅方法 if (uri.getAuthority().equals(\"webview\")) { // 步骤3: // 执⾏行行JS所需要调⽤用的逻辑 System.out.println(\"js调⽤用了了Android的⽅方法\"); // 可以在协议上带有参数并传递到Android上 HashMap\u003cString, String\u003e params = new HashMap\u003c\u003e(); Set\u003cString\u003e collection = uri.getQueryParameterNames(); } return true; } return super.shouldOverrideUrlLoading(view, url); } }); 拦截对话框 该开发中使⽤用不不多。 Android通过 WebChromeClient 的onJsAlert()、onJsConfirm()、 onJsPrompt（）⽅方法回调分别拦截JS对话框（即上述三个⽅方法），得到他们的消息内容，然后解析即 可。 ","date":"2017-04-08","objectID":"/posts/android_%E5%8A%A0%E8%BD%BDhtml/:0:3","tags":["android"],"title":"android加载html","uri":"/posts/android_%E5%8A%A0%E8%BD%BDhtml/"},{"categories":["笔记"],"content":"总结 android调⽤用js 不不需要返回值：loadUrl()法 需要返回值：evaluateJavascript()⽅方法 js调⽤用android addJavascriptInterface⽅方案: 需要注册js⽅方法，使⽤用简单，但是有些安卓版本存在漏漏洞洞。 schema协议⽅方案: 兼容性好，⽐比较流⾏行行(JSBridge库就是利利⽤用该⽅方案进⾏行行封装)。但是接⼝口太多， android要对url进⾏行行⼤大量量的解析判断。 项⽬目中遇到的问题 https加载问题 很多HTTPs⻚页⾯面加载不不出来的主要原因是SSL证书不不能被正确识别，所以我们这⾥里里要做的，就是重载 WebViewClient中的OnReceiveSslError⽅方法: /** * Description: handle https * Created by Michael Lee on 12/6/16 08:38 */ @Override public void onReceivedSslError(WebView view, SslErrorHandler handler, SslError error) { handler.proceed(); } 这样，当收到证书错误时，忽略略掉，直接继续处理理就⾏行行了了;相当于信任了了所有的证书⼀一样。 webview销毁问题 在某些⼿手机上，Webview有视频时，activity销毁后，视频资源没有被销毁，甚⾄至还 能听到在后台播放。即便便是像刚才那样各种销毁webview也⽆无济于事，解决办法:在onDestory之前修 改url为空地址。 WebView硬件加速导致⻚页⾯面渲染闪烁问题 关于Android硬件加速 开始于Android 3.0 (API level 11),开 启硬件加速后，WebView渲染⻚页⾯面更更加快速，拖动也更更加顺滑。但有个副作⽤用就是容易易会出现⻚页⾯面加载 ⽩白块同时界⾯面闪烁现象。解决这个问题的⽅方法是设置WebView暂时关闭硬件加速 代码如下: if (Build.VERSION.SDK_INT \u003e= Build.VERSION_CODES.HONEYCOMB) { webview.setLayerType(View.LAYER_TYPE_SOFTWARE, null); } h5加载百度地图不不显示问题 //启⽤用数据库 webSettings.setDatabaseEnabled(true); String dir = this.getApplicationContext().getDir(“database”, Context.MODE_PRIVATE).getPath(); //启⽤用地理理定位 webSettings.setGeolocationEnabled(true); ","date":"2017-04-08","objectID":"/posts/android_%E5%8A%A0%E8%BD%BDhtml/:0:4","tags":["android"],"title":"android加载html","uri":"/posts/android_%E5%8A%A0%E8%BD%BDhtml/"},{"categories":["笔记"],"content":"android安全","date":"2017-04-08","objectID":"/posts/android_%E5%AE%89%E5%85%A8/","tags":["android"],"title":"android安全","uri":"/posts/android_%E5%AE%89%E5%85%A8/"},{"categories":["笔记"],"content":"混淆 针对项⽬目代码，代码混淆通常将代码中的各种元素（变量量、函数、类名等）改为⽆无意义的名字，使得阅 读的⼈人⽆无法通过名称猜测其⽤用途，增⼤大反编译者的理理解难度。 虽然代码混淆可以提⾼高反编译的⻔门槛，但 是对开发者本身也增⼤大了了调试除错的难度。开发⼈人员通常需要保留留原始未混淆代码⽤用于调试。 ","date":"2017-04-08","objectID":"/posts/android_%E5%AE%89%E5%85%A8/:0:1","tags":["android"],"title":"android安全","uri":"/posts/android_%E5%AE%89%E5%85%A8/"},{"categories":["笔记"],"content":"加固 针对apk，加固是多维度的安全防护⽅方案，包括反破解、反逆向、防篡改等，可以防⽌止应⽤用被各类常⻅见 破解⼯工具逆向，安全性要远⼤大于单纯的代码混淆。 对App进⾏行行加固，可以有效防⽌止移动应⽤用被破解、盗 版、⼆二次打包、注⼊入、反编译等，保障程序的安全性、稳定性。对于⾦金金融类App，尤其重要。 加固原理理： 对App dex进⾏行行加固的基本步骤如下： 从App原始apk⽂文件⾥里里获取到原始dex⽂文件 对原始dex⽂文件进⾏行行加密，并将加密后的dex⽂文件和相关的存放到assert⽬目录⾥里里 ⽤用脱壳dex⽂文件替换原始apk⽂文件⾥里里的dex⽂文件；脱壳dex⽂文件的作⽤用主要有两个，⼀一个是解密加密 后的dex⽂文件；⼆二是基于dexclassloader动态加载解密后的dex⽂文件 因为原始apk⽂文件已经被修改，所以需要删除原始apk的签名信息，即删除META-INF⽬目录下 的.RSA、.SF 和MANIFEST.MF⽂文件 ⽣生成加固后的apk⽂文件 对加固后的apk⽂文件进⾏行行签名，apk加固完成。 原理理分析： 为什什么要对原始dex进⾏行行加密，同时⽤用脱壳dex⽂文件替换原始dex⽂文件？⼤大部分的apk反编译⼯工具 （dex2jar、apktools、jui等）都是对dex⽂文件进⾏行行反编译，将dex⽂文件反编译成smail，然后再转 化成class⽂文件进⾏行行阅读和修改。⽤用脱壳dex替换原始dex⽂文件之后，⽤用上⾯面的反编译⼯工具反编译 apk⽂文件，只能看到脱壳程序的class⽂文件，看不不到apk本身的class⽂文件。对dex⽂文件进⾏行行加密，这 样即使第三⽅方拿到了了dex⽂文件，以为⽆无法解密，也就⽆无法对其进⾏行行解析和分析。 怎么确保apk功能正常运⾏行行？加固后的apk启动之后，脱壳dex⽂文件会对加密后的dex⽂文件进⾏行行解 密，然后机遇dexclassload动态加载解密后的dex⽂文件。从⽤用户的⻆角度，加固前后App的功能和体 验基本是⼀一样的。 常⽤用的加固⼯工具有腾讯加固，360加固等。 ","date":"2017-04-08","objectID":"/posts/android_%E5%AE%89%E5%85%A8/:0:2","tags":["android"],"title":"android安全","uri":"/posts/android_%E5%AE%89%E5%85%A8/"},{"categories":["笔记"],"content":"秘钥保存 移动app需要在app端保存⼀一些静态字符串串常量量，其可能是静态秘钥、第三⽅方appId等。在保存这些字符 串串常量量的时候就涉及到了了如何保证秘钥的安全性问题。如何保证在App中静态秘钥唯⼀一且正确安全，这 是⼀一个很重要的问题。 常⻅见保存⽅方式： 密钥直接明⽂文存在sharedprefs⽂文件中，这是最不不安全的。 密钥直接硬编码在Java代码中，这很不不安全，dex⽂文件很容易易被逆向成java代码。 将密钥分成不不同的⼏几段，有的存储在⽂文件中、有的存储在代码中，最后将他们拼接起来，可以将整 个操作写的很复杂，这因为还是在java层，逆向者只要花点时间，也很容易易被逆向。 ⽤用ndk开发，将密钥放在so⽂文件，加密解密操作都在so⽂文件⾥里里，这从⼀一定程度上提⾼高了了的安全性， 挡住了了⼀一些逆向者，但是有经验的逆向者还是会使⽤用IDA破解的。 在so⽂文件中不不存储密钥，so⽂文件中对密钥进⾏行行加解密操作，将密钥加密后的密钥命名为其他普通 ⽂文件，存放在assets⽬目录下或者其他⽬目录下，接着在so⽂文件⾥里里⾯面添加⽆无关代码（花指令），虽然可 以增加静态分析难度，但是可以使⽤用动态调式的⽅方法，追踪加密解密函数，也可以查找到密钥内 容。 ","date":"2017-04-08","objectID":"/posts/android_%E5%AE%89%E5%85%A8/:0:3","tags":["android"],"title":"android安全","uri":"/posts/android_%E5%AE%89%E5%85%A8/"},{"categories":["笔记"],"content":"使用Android Keystore 保存密码等敏敏感信息，有版本限制。 Android Keystore 利利⽤用 Android 密钥库系统，您可以在容器器中存储加密密钥，从⽽而提⾼高从设备中提取密钥的难度。在密钥 进⼊入密钥库后，可以将它们⽤用于加密操作，⽽而密钥材料料仍不不可导出。此外，它提供了了密钥使⽤用的时间和 ⽅方式限制措施，例例如要求进⾏行行⽤用户身份验证才能使⽤用密钥，或者限制为只能在某些加密模式中使⽤用。 ⼀一个应⽤用程式只能编辑、保存、取出⾃自⼰己的密钥。这个概念很简单，但是功能很强⼤大。App可以⽣生成或 者接收⼀一个公私密钥对，并存储在Android的Keystore系统中。公钥可以⽤用于在应⽤用数据放置到特定⽂文 件夹前对数据进⾏行行加密，私钥可以在需要的时候解密相应的数据。 Android 4.3 以下系统并不不⽀支持 KeyStore。 ","date":"2017-04-08","objectID":"/posts/android_%E5%AE%89%E5%85%A8/:0:4","tags":["android"],"title":"android安全","uri":"/posts/android_%E5%AE%89%E5%85%A8/"},{"categories":["笔记"],"content":"ndk开发保存 利利⽤用jni把秘钥字符串串放到native⾥里里⾯面，但是破解者直接调⽤用so⾥里里⾯面的⽅方法，就能绕过，我们可以在jni中 校验apk的签名，来防⽌止⼆二次打包。 ","date":"2017-04-08","objectID":"/posts/android_%E5%AE%89%E5%85%A8/:0:5","tags":["android"],"title":"android安全","uri":"/posts/android_%E5%AE%89%E5%85%A8/"},{"categories":["笔记"],"content":"android适配","date":"2017-04-08","objectID":"/posts/android_%E9%80%82%E9%85%8D/","tags":["android"],"title":"android适配","uri":"/posts/android_%E9%80%82%E9%85%8D/"},{"categories":["笔记"],"content":"适配难点 Android的UI的适配⼀直是开发环节中最重要的问题，最核⼼的问题有两个： 其⼀，就是适配的效率，即把设计图转化为App界⾯的过程是否⾼效 其⼆如何保证实现UI界⾯在不同尺⼨和分辨率的⼿机中UI的⼀致性。 这两个问题都很重要，⼀个是保证我们开发的⾼效，⼀个是保证我们适配的成效。 在标识尺⼨的时候，Android并不推荐我们使⽤px这个真实像素单位，因为不同的⼿机之间，分辨率是不同的，⽐如⼀个96*96像素的控件在分 辨率越来越⾼的⼿机上会在整体UI中看起来越来越⼩。 android⼿机碎⽚化严重，各种尺⼨，各种屏幕密度。但是设计图⼀般⽤px标注，转化及其困难，想要在各⼿机正确显⽰更困难。 ","date":"2017-04-08","objectID":"/posts/android_%E9%80%82%E9%85%8D/:0:1","tags":["android"],"title":"android适配","uri":"/posts/android_%E9%80%82%E9%85%8D/"},{"categories":["笔记"],"content":"android适配基本概念 Android推荐使⽤dp作为尺⼨单位来适配UI。 dp指的是设备独⽴像素，以dp为尺⼨单位的控件，在不同分辨率和尺⼨的⼿机上代表了不同的真实像素，⽐如在分辨率较低的⼿机中，可能 1dp=1px,⽽在分辨率较⾼的⼿机中，可能1dp=2px，这样的话，⼀个96*96dp的控件，在不同的⼿机中就能表现出差不多的⼤⼩了。 这个dp是如何计算的呢？ 我们都知道⼀个公式： px = dp(dpi/160) 系统都是通过这个来判断px和dp的数学关系。 dpi是什么呢？ dpi是像素密度，指的是在系统软件上指定的单位尺⼨的像素数量，它往往是写在系统出⼚配置⽂件的⼀个固定值。 android中的dp在渲染前会将dp转为px，计算公式： 2019/6/27 android适配难点 - 马克飞象 - 专为印象笔记打造的Markdown编辑器 chrome-extension://kidnkfckhbdkfgbicccmdggmpgogehop/index_zh.html 2/5 px = density * dp; density = dpi / 160; px = dp * (dpi / 160); 举个例⼦：屏幕分辨率为：1920*1080，屏幕尺⼨为5吋的话，那么dpi为440。 实际中很多种dpi，其实，每部安卓⼿机屏幕都有⼀个初始的固定密度，这些数值是120、160、240、320、480、640，权且称为“系统密度”。 分别对应dpi、mdpi、hdpi、xhdpi、xxhdpi、xxxhdpi，Density取值为0.75，1，1.5，2，3，4。安卓对界⾯元素进⾏缩放的⽐例依据正是系统 密度，⽽不是实际密度，根据实际密度⾃动选择相近的系统密度。 2019/6/27 android适配难点 - 马克飞象 - 专为印象笔记打造的Markdown编辑器 chrome-extension://kidnkfckhbdkfgbicccmdggmpgogehop/index_zh.html 3/5 ","date":"2017-04-08","objectID":"/posts/android_%E9%80%82%E9%85%8D/:0:2","tags":["android"],"title":"android适配","uri":"/posts/android_%E9%80%82%E9%85%8D/"},{"categories":["笔记"],"content":"常见适配⽅案 多套布局⽂件 在layout⽂件夹中，根据不同分辨率，创建不同的布局⽂件夹： layout-800 * 480 layout-1280 * 720 ⼿机会根据分辨率去找设定的不同⼤⼩的layout的布局。实际开发中这种使⽤的情况⾮常少，因为占⽤太多资源 利⽤技巧 在开发中尽量使⽤线性布局、相对布局、wrap_content、match_parent、layout_weight、约束布局等技巧。 缺点： ⽅式⽆法快速⾼效的把设计师的设计稿实现到布局代码中，通过dp直接适配，我们只能让UI基本适配不同的⼿机,但是在设计图和UI代码之间的 鸿沟，dp是⽆法解决的，因为dp不是真实像素。 在把设计稿向UI代码转换的过程中，我们需要耗费相当的精⼒去转换尺⼨，这会极⼤的降低我们的⽣产⼒，拉低开发效率。 宽⾼限定符 根据市⾯上⼿机分辨率的占⽐分析，我们选定⼀个占⽐例值⼤的（⽐如1280720）设定为⼀个基准，然后其他分辨率根据这个基准做适配。 基准的意思（⽐如320480的分辨率为基准）是： 宽为320，将任何分辨率的宽度分为320份，取值为x1到x320 长为480，将任何分辨率的⾼度分为480份，取值为y1到y480 例如对于800 * 480的分辨率设备来讲，需要在项⽬中values-800x480⽬录下的dimens.xml⽂件中的如下设置： \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e \u003cresources\u003e \u003cdimen name=\"x1\"\u003e1.5px\u003c/dimen\u003e \u003cdimen name=\"x2\"\u003e3.0px\u003c/dimen\u003e \u003cdimen name=\"x3\"\u003e4.5px\u003c/dimen\u003e \u003cdimen name=\"x4\"\u003e6.0px\u003c/dimen\u003e \u003cdimen name=\"x5\"\u003e7.5px\u003c/dimen\u003e px会根据我们指定的不同values⽂件夹⾃动适配为合适的⼤⼩。 缺点: Android不同分辨率的⼿机实在太多了，可能你说主流就可以，的确⼩公司主流就可以，淘宝这种App肯定不能只适配主流⼿机。 控件在设计图上显⽰的⼤⼩以及控件之间的间隙在⼩分辨率和⼤分辨率⼿机上天壤之别，你会发现⼤屏幕⼿机上控件超级⼤。可能你会觉 得正常，毕竟分辨率不同。但实际效果⼤的有些夸张。 最⼩宽度限定符 这种适配依据的是最⼩宽度限定符。指的是Android会识别屏幕可⽤⾼度和宽度的最⼩尺⼨的dp值（其实就是⼿机的宽度值），然后根据识别到 的结果去资源⽂件中寻找对应限定符的⽂件夹下的资源⽂件。这种机制和上⽂提到的宽⾼限定符适配原理上是⼀样的，都是系统通过特定的规 则来选择对应的⽂件。 ⽐如设计图宽度为750px，对于360dp宽度那么就是分为了750份，每份750/360dp。同理，对于440dp宽度就是分为440份。我们只要在 values-sw360dp等⽂件夹中创建dimens⽂件即可。 smallestWidth限定符适配和宽⾼限定符适配最⼤的区别在于，有很好的容错机制，如果没有value-sw360dp⽂件夹，系统会向下寻找，⽐如离 360dp最近的只有value-sw350dp，那么Android就会选择value-sw350dp⽂件夹下⾯的资源⽂件。这个特性就完美的解决了上⽂提到的宽⾼限 定符的容错问题。 缺点： Android 私⼈订制的原因，宽度⽅⾯参差不齐，不可能适配所有的⼿机。（能覆盖绝⼤多数） sp和dp值有些值不全（这个应该是可以解决的），姑且算是⼀个⼩问题。 项⽬中增加了N个⽂件夹，安装包会增⼤（⽐较⼩，可以忽略）。 通过宽度限定符就近查找的原理，可以看出来匹配出来的⼤⼩不够准确。 是在Android 3.2 以后引⼊的，Google的本意是⽤它来适配平板的布局⽂件（但是实际上显然⽤于diemns适配的效果更好）。基本都能在 3.2以上，这问题其实也不重要了。 以后想改适配⽅案及其困难。 今⽇头条⽅案 通过修改density值，强⾏把所有不同尺⼨分辨率的⼿机的宽度dp值改成⼀个统⼀的值，这样就解决了所有的适配问题。 ⽐如，设计稿宽度是360px，那么开发这边就会把⽬标dp值设为360dp，在不同的设备中，动态修改density值，从⽽保证(⼿机像素宽 度)px/density这个值始终是360dp,这样的话，就能保证UI在不同的设备上表现⼀致了。 缺点： 项⽬中的所有地⽅都会⾃动适配，那就是只能⼀⼑切的将整个项⽬进⾏适配，但适配范围是不可控的。这个⽅案依赖于设计图尺⼨，但是 项⽬中的系统控件、三⽅库控件、等⾮我们项⽬⾃⾝设计的控件，它们的设计图尺⼨并不会和我们项⽬⾃⾝的设计图尺⼨⼀样。 对⽼项⽬不够兼容，系统的density值改变后，那么整个布局的实际尺⼨都会改变，在⽼项⽬中，以前的布局⽅式都要重新按照设计图进⾏ 修改。 注意适配字体。 ","date":"2017-04-08","objectID":"/posts/android_%E9%80%82%E9%85%8D/:0:3","tags":["android"],"title":"android适配","uri":"/posts/android_%E9%80%82%E9%85%8D/"},{"categories":["笔记"],"content":"总结 多套布局开发中是不可取的。 最⼩宽度限定符是宽⾼限定符的进阶版，缺点是可以接受的，基本满⾜开头的适配需求。实际开发中可以结合适配技巧，效果更佳。 今⽇头条适配⽅案对⽼项⽬不好过渡，要注意字体，系统和第三⽅控件的适配。 SmallestWidth 限定符适配⽅案 主打的是稳定性，在运⾏过程中极少会出现安全隐患，适配范围也可控，不会产⽣其他未知的影响，⽽ 今⽇头 条适配⽅案 主打的是降低开发成本、提⾼开发效率，使⽤上更灵活，也能满⾜更多的扩展需求。 ","date":"2017-04-08","objectID":"/posts/android_%E9%80%82%E9%85%8D/:0:4","tags":["android"],"title":"android适配","uri":"/posts/android_%E9%80%82%E9%85%8D/"},{"categories":["笔记"],"content":"golang seelog使用","date":"2017-03-20","objectID":"/posts/go_seelog/","tags":["golang"],"title":"golang seelog使用","uri":"/posts/go_seelog/"},{"categories":["笔记"],"content":"golang中自带的有log包，但是功能并不能满足我们。很多人推荐seelog，我们今天一起学习下。 ","date":"2017-03-20","objectID":"/posts/go_seelog/:0:0","tags":["golang"],"title":"golang seelog使用","uri":"/posts/go_seelog/"},{"categories":["笔记"],"content":"安装 go get github.com/cihub/seelog ","date":"2017-03-20","objectID":"/posts/go_seelog/:1:0","tags":["golang"],"title":"golang seelog使用","uri":"/posts/go_seelog/"},{"categories":["笔记"],"content":"快速开始 引用seelog wiki上的代码 package main import log \"github.com/cihub/seelog\" func main() { defer log.Flush() log.Info(\"Hello from Seelog!\") } 这样就能在控制台打印了，但是它是那么的丑陋！这里Info方法代表的是日志的级别，常用的有Trace, Debug, Info, Warn, Error, Critical 方法。 我们一般都会自定义格式等，seelog为我们提供了xml和代码两种方式来配置。 先看最简单的xml配置使用方法 \u003cseelog\u003e \u003coutputs\u003e \u003cconsole /\u003e \u003c/outputs\u003e \u003c/seelog\u003e 在代码中读取配置文件 logger, err := seelog.LoggerFromConfigAsFile(\"./config/seelog.xml\") if err != nil { log.Fatal(err) } seelog.ReplaceLogger(logger) defer seelog.Flush() seelog.Error(\"seelog from config xml\") 这样我们就使用的默认的配置，将信息打印到控制台，同样它是如此的丑陋！ 我们可以在任何时候调用LoggerFromConfigAsFile，官网上有句英文很形象\"Changing config on the fly\"除了AsFile方法之外，还有LoggerFromConfigAsBytes’, ‘LoggerFromConfigAsString，但是本人还是配置文件可读性更好。 Flush方法很重要。一般的我们不会在main协程中写日志文件。logger应该异步并保持顺序的吞掉日志message放到队列中，而flush就是用来把队列中的数据写到文件中。在crash的时候也应该保证日志数据不会丢失，我们应该用defer语句来达到这个效果，并且在没有写日志操作之前就调用，这样我们后面就不用操心了。 在这里有个ReplaceLogger方法,我们来指定logger，类似的还有个UseLogger方法，前者会flush日志并关闭原来的logger,而后者只flush日志。 ","date":"2017-03-20","objectID":"/posts/go_seelog/:2:0","tags":["golang"],"title":"golang seelog使用","uri":"/posts/go_seelog/"},{"categories":["笔记"],"content":"配置文件 先看下官网的示范配置文件 \u003cseelog type=\"asynctimer\" asyncinterval=\"5000000\" minlevel=\"debug\" maxlevel=\"error\"\u003e \u003cexceptions\u003e \u003cexception funcpattern=\"*main.test*Something*\" minlevel=\"info\"/\u003e \u003cexception filepattern=\"*main.go\" minlevel=\"error\"/\u003e \u003c/exceptions\u003e \u003coutputs formatid=\"main\"\u003e \u003cconsole/\u003e \u003csplitter formatid=\"format1\"\u003e \u003cfile path=\"log.log\"/\u003e \u003cfile path=\"log2.log\"/\u003e \u003c/splitter\u003e \u003csplitter formatid=\"format2\"\u003e \u003cfile path=\"log3.log\"/\u003e \u003cfile path=\"log4.log\"/\u003e \u003c/splitter\u003e \u003crollingfile formatid=\"someformat\" type=\"size\" filename=\"./log/roll.log\" maxsize=\"100\" maxrolls=\"5\" /\u003e \u003cbuffered formatid=\"testlevels\" size=\"10000\" flushperiod=\"1000\"\u003e \u003cfile path=\"./log/bufFileFlush.log\"/\u003e \u003c/buffered\u003e \u003cfilter levels=\"error\"\u003e \u003cfile path=\"./log/error.log\"/\u003e \u003csmtp senderaddress=\"noreply-notification-service@none.org\" sendername=\"Automatic notification service\" hostname=\"mail.none.org\" hostport=\"587\" username=\"nns\" password=\"123\"\u003e \u003crecipient address=\"john-smith@none.com\"/\u003e \u003crecipient address=\"hans-meier@none.com\"/\u003e \u003c/smtp\u003e \u003cconn net=\"tcp4\" addr=\"server.address:5514\" tls=\"true\" insecureskipverify=\"true\" /\u003e \u003c/filter\u003e \u003c/outputs\u003e \u003cformats\u003e \u003cformat id=\"main\" format=\"%Date(2006 Jan 02/3:04:05.000000000 PM MST) [%Level] %Msg%n\"/\u003e \u003cformat id=\"someformat\" format=\"%Ns [%Level] %Msg%n\"/\u003e \u003cformat id=\"testlevels\" format=\"%Level %Lev %LEVEL %LEV %l %Msg%n\"/\u003e \u003cformat id=\"usetags\" format=\"\u0026lt;msg\u0026gt;%Msg\u0026lt;/time\u0026gt;\"/\u003e \u003cformat id=\"format1\" format=\"%Date/%Time [%LEV] %Msg%n\"/\u003e \u003cformat id=\"format2\" format=\"%File %FullPath %RelFile %Msg%n\"/\u003e \u003c/formats\u003e \u003c/seelog\u003e 我们并不必把所有的配置项都了解清楚，只需要根据我们的需求配置即可。 ","date":"2017-03-20","objectID":"/posts/go_seelog/:3:0","tags":["golang"],"title":"golang seelog使用","uri":"/posts/go_seelog/"},{"categories":["笔记"],"content":"formart配置 日志格式是配置文件中最重要的之一 \u003cformats\u003e \u003cformat id=\"common\" format=\"[%LEV] %Msg\"/\u003e \u003cformat id=\"critical\" format=\"%Time %Date %RelFile %Func %Msg\"/\u003e \u003cformat id=\"criticalemail\" format=\"Critical error on our server!\\n %Time %Date %RelFile %Func %Msg \\nSent by Seelog\"/\u003e \u003c/formats\u003e 我们可以指定一个format，每个format都有一个唯一的id方便在别的地方引用它，format属性就是格式了，我们可以指定颜色、日期及调用的地方等。以%开始，后跟特定的字符串 %Level log level (Trace, Debug, Info, Warn, Error, Critical) %Lev short log level (Trc, Dbg, Inf, Wrn, Err, Crt) %LEVEL capitalized log level (TRACE, DEBUG, INFO, WARN, ERROR, CRITICAL) %LEV short capitalized log level (TRC, DBG, INF, WRN, ERR, CRT) %l super compact log level (t, d, i, w, e, c) %Msg message text (string) %FullPath full caller file path %File caller filename only %RelFile caller path relative to the application runtime directory %Func caller function name %FuncShort caller function name part after the last dot %Line line number where logger was called Date and time %Ns - time.Now().UnixNano() %Date - shortcut for ‘2006-01-02’ %Time - shortcut for ‘15:04:05’ %Date(…) - date with format, specified in parentheses. Uses standard time.Format, so check http://golang.org/src/pkg/time/format.go for identifiers list. Use it like that: “%Date(2006-01-02)” (or any other format) %UTCNs - time.Now().UTC().UnixNano() %UTCDate - shortcut for ‘2006-01-02’ (UTC) %UTCTime - shortcut for ‘15:04:05’ (UTC) %UTCDate(…) - UTC date with format, specified in parentheses. Uses standard time.Format, so check http://golang.org/src/pkg/time/format.go for identifiers list. Use it like that: “%UTCDate(2006-01-02)” (or any other format) Special symbols %EscN - terminal ANSI CSI n [;k] m escape. Check Colored output for details %n - newline %t - tab ","date":"2017-03-20","objectID":"/posts/go_seelog/:3:1","tags":["golang"],"title":"golang seelog使用","uri":"/posts/go_seelog/"},{"categories":["笔记"],"content":"自定义formatter 上面这些特殊规则基本都能满足我们的需求，如果想自定义规则要注册： func createMyFormatter(params string) seelog.FormatterFunc { fmt.Println(params)//打印配置文件传入的参数10 return func(message string, level seelog.LogLevel, context seelog.LogContextInterface) interface{} { return \"hello\" } } //注册 seelog.RegisterCustomFormatter(\"myFormat\",createMyFormatter) 在配置文件中使用 \u003cformats\u003e \u003cformat id=\"test\" format=\"%myFormat(10) %Date %Time %LEV %RelFile %Line:%Msg%n\"/\u003e \u003c/formats\u003e 这样每次都会在日志钱加上\"hello” ","date":"2017-03-20","objectID":"/posts/go_seelog/:3:2","tags":["golang"],"title":"golang seelog使用","uri":"/posts/go_seelog/"},{"categories":["笔记"],"content":"约束配置 约束是为了配置保存的日志级别的，分为全局约束和异常 全局约束 全局约束是应用到整个应用的。可以在根节点配置，可以设置最大、最小级别，或者直接列举出所有支持的级别。 \u003cseelog minlevel=\"info\" maxlevel=\"error\"\u003e \u003cseelog levels=\"trace,info,critical\"\u003e 异常约束 这个主要是指定特殊的方法或文件中使用异常级别，感觉用处不大，可以去官网看详情。 ","date":"2017-03-20","objectID":"/posts/go_seelog/:3:3","tags":["golang"],"title":"golang seelog使用","uri":"/posts/go_seelog/"},{"categories":["笔记"],"content":"Dispatchers and receivers seelog为我们提供了主流的接收方式：文件、控制台、网络通道、邮件等。 举个栗子： \u003cseelog\u003e \u003coutputs\u003e \u003csplitter formatid=\"common\"\u003e \u003cconsole/\u003e \u003cfile path=\"file.log\"/\u003e \u003cconn addr=\"192.168.0.2:8123\"/\u003e \u003c/splitter\u003e \u003cfilter levels=\"critical\"\u003e \u003cfile path=\"critical.log\" formatid=\"critical\"/\u003e \u003csmtp formatid=\"criticalemail\" senderaddress=\"noreply-notification-service@none.org\" sendername=\"Automatic notification service\" hostname=\"mail.none.org\" hostport=\"587\" username=\"nns\" password=\"123\"\u003e \u003crecipient address=\"john-smith@none.com\"/\u003e \u003crecipient address=\"hans-meier@none.com\"/\u003e \u003c/smtp\u003e \u003c/filter\u003e \u003c/outputs\u003e \u003cformats\u003e \u003cformat id=\"common\" format=\"[%LEV] %Msg\"/\u003e \u003cformat id=\"critical\" format=\"%Time %Date %RelFile %Func %Msg\"/\u003e \u003cformat id=\"criticalemail\" format=\"Critical error on our server!\\n %Time %Date %RelFile %Func %Msg \\nSent by Seelog\"/\u003e \u003c/formats\u003e \u003c/seelog\u003e 在这里我们配置输出信息，splitter把数据分为三个组，而filter根据日志级别分为了两个组。具体分组后的流向很容易明白，不再做解释。 问题来了：我们怎么知道接收者和分发的更多属性细节？且听我细细道来： ","date":"2017-03-20","objectID":"/posts/go_seelog/:3:4","tags":["golang"],"title":"golang seelog使用","uri":"/posts/go_seelog/"},{"categories":["笔记"],"content":"receivers细节 File writer 这个就比较简单了 \u003cseelog\u003e \u003coutputs\u003e \u003cfile path=\"log.log\"/\u003e \u003c/outputs\u003e \u003c/seelog\u003e 注意不要使用特殊符号作为文件名即可 Console writer 这个更简单 \u003cseelog\u003e \u003coutputs\u003e \u003cconsole/\u003e \u003c/outputs\u003e \u003c/seelog\u003e Rolling file writer 通过这个节点我们在日期发生变化或者日志文件达到限制的时候切换另一个新的文件。下面是对属性的解释： filename 指定日志文件路径，当切换新的日志文件的时候，文件名会有一定格式后面会讲解到 type “date\"或者\"size” namemode “postfix”, “prefix\"文件名前缀或后缀格式 maxrolls 最多的文件数目，当超过这个现在就回把原来的给删除掉 存档格式，当超过文件数目时可以指定存档格式\"none”, “zip”, “gzip”。如果设置为\"none\"那么就会删除 archivepath 日志存档的目录 maxsize 当type为size类型是，每个文件最大的限制（单位是字节） datepattern 日志文件名的日期格式，type为date时有效 fullname boolean型的值，是否设置当前日志文件就按过期格式命名 Buffered writer 顾名思义，先把数据存到内存中，当缓冲区满的时候再刷到文件中 \u003cbuffered size=\"10000\" flushperiod=\"1000\"\u003e \u003cfile path=\"bufFile.log\"/\u003e \u003c/buffered\u003e 除了以上各种writer外还有邮件，网络这里暂时不讲了 ","date":"2017-03-20","objectID":"/posts/go_seelog/:3:5","tags":["golang"],"title":"golang seelog使用","uri":"/posts/go_seelog/"},{"categories":["笔记"],"content":"自定义receiver 除了控制台、文件、网络、邮件外，seelog还支持自定义接收者，但是比较复杂，如果有需求可以详细的看文档，此处暂不深究 ","date":"2017-03-20","objectID":"/posts/go_seelog/:3:6","tags":["golang"],"title":"golang seelog使用","uri":"/posts/go_seelog/"},{"categories":["笔记"],"content":"总结 seelog功能的确很强大！ ","date":"2017-03-20","objectID":"/posts/go_seelog/:4:0","tags":["golang"],"title":"golang seelog使用","uri":"/posts/go_seelog/"},{"categories":["笔记"],"content":"golang包管理","date":"2017-03-10","objectID":"/posts/go_package_manage/","tags":["golang"],"title":"golang包管理","uri":"/posts/go_package_manage/"},{"categories":["笔记"],"content":"什么是vendor go vendor 是go 1.5 官方引入管理包依赖的方式，1.6正式引入。其基本思路是，将引用的外部包的源代码放在当前工程的vendor目录下面，go 1.6以后编译go代码会优先从vendor目录先寻找依赖包。 在Go 1.6之前，你需要手动的设置环境变量GO15VENDOREXPERIMENT=1才可以使Go找到Vendor目录，然而在Go 1.6之后，这个功能已经不需要配置环境变量就可以实现了。 引入vendor机制后查找依赖包路径的解决方案如下： 当前包下的vendor目录。 向上级目录查找，直到找到src下的vendor目录。 在GOPATH下面查找依赖包。 在GOROOT目录下查找。 ","date":"2017-03-10","objectID":"/posts/go_package_manage/:0:1","tags":["golang"],"title":"golang包管理","uri":"/posts/go_package_manage/"},{"categories":["笔记"],"content":"为什么要引入vendor golang通过go get命令获取的代码是在gopath/src下的，如果我们有多个项目都引用同一个package就会出现一些问题，比如：项目迁移或新同事加入时，多次go get引入的package版本不一致，导致项目编译不通过。 ","date":"2017-03-10","objectID":"/posts/go_package_manage/:0:2","tags":["golang"],"title":"golang包管理","uri":"/posts/go_package_manage/"},{"categories":["笔记"],"content":"dep的使用 vendor 机制虽然解决了上面的一些问题，但是无法精确的引用外部包进行版本控制，不能指定引用某个特定版本的外部包；只是在开发时，将其拷贝过来，但是一旦外部包升级,vendor下的代码不会跟着升级，而且vendor下面并没有文件记录引用包的版本信息。 目前为止，golang官方并没有指定的工具，一些第三方开源帮我们解决这个问题。比如：govendor、glide、godep，dep等。dep和godep其实作者是同一人，只不过dep是golang官方非正式版。 dep is a prototype dependency management tool for Go. It requires Go 1.9 or newer to compile. dep is safe for production use. dep is the official experiment, but not yet the official tool. 这是dep官网上的简介，它要求至少1.9的版本。 安装dep: go get -u github.com/golang/dep/cmd/dep dep命令： dep init set up a new project dep ensure install the project's dependencies dep ensure -update update the locked versions of all dependencies dep ensure -add github.com/pkg/errors add a dependency to the project 执行init命令后，项目根目录中新增了Gopkg.lock、Gopkg.toml和一个目录vendor。 Gopkg.toml可以灵活地描述用户的意图，包括依赖的 source、branch、version等。Gopkg.lock仅仅描述依赖的具体状态，例如各依赖的revision。Gopkg.toml可以通过命令生产，也可以被用户根据 需要手动修改，Gopkg.lock是自动生成的，不可以修改。 要想添加新的依赖可以使用命令 dep ensure -add github.com/bitly/go-simplejson@=0.4.3 也可以在代码中import，然后dep ensure命令即可。 ","date":"2017-03-10","objectID":"/posts/go_package_manage/:0:3","tags":["golang"],"title":"golang包管理","uri":"/posts/go_package_manage/"},{"categories":["笔记"],"content":"gomod 另外使用 vendor 后，每个项目都完整拷贝一份依赖包，既不方便管理又浪费了本地空间。 此外，Go 项目中的 import 指令后面的 package 路径与项目代码的存放路径相关，项目目录不能随意移动，必须安分守己地趴在 $GOPATH/src 中，否则 import 会找不到项目中的 package，虽然可以通过在容器中编译或者为每个项目准备一套 Go 环境的方式解决，但是麻烦且有额外开销。 Go1.11 和 Go1.12 引入的 Go Modules 机制，提供了统一的依赖包管理工具 go mod，依赖包统一下载在 GOPATH/pkg/mod 中进行集中管理，统一包后面有版本号，所以不会出现问题。 GO111MODULE 有三个值：off, on和auto（默认值）。 GO111MODULE=off，go命令行将不会支持module功能，寻找依赖包的方式将会沿用旧版本那种通过vendor目录或者GOPATH模式来查找。 GO111MODULE=on，go命令行会使用modules，而一点也不会去GOPATH目录下查找。 GO111MODULE=auto，默认值，go命令行将会根据当前目录来决定是否启用module功能。这种情况下可以分为两种情形： 当前目录在GOPATH/src之外且该目录包含go.mod文件 当前文件在包含go.mod文件的目录下面。 gomod 依赖包下载存放目录在GOPATH/pkg/mod Go Modules 将成为 Go1.13 默认的依赖包管理方法，在 Go1.11 和 Go1.12 中， Go Modules 只能在 $GOPATH 外部使用。 gomod初始化 在 GOPATH 外部创建一个目录，然后初始化，项目的路径设置为 exampe.com/hello： go mod init example.com/hello # 该项目代码的引用路径是 example.com/hello 增加了 go.mod 和 go.sum 文件。 go.mod 提供了module, require、replace和exclude 四个命令 module 语句指定包的名字（路径） require 语句指定的依赖项模块 replace 语句可以替换依赖项模块 exclude 语句可以忽略依赖项模块 依赖包添加 我们可以直接在代码中import包，然后go build时就会自动更新go.mod文件。这个时候下载的是最新的版本。 go get命令会下载指定版本，并更新到go.mod，不过它是indirect，因为它还没有用到。 go get github.com/lijiaocn/glib@v0.0.2 依赖包的修改 依赖代码的版本更新很简单，直接用 go get 获取指定版本的依赖代码即可。 删除依赖 不需要的依赖必须手动清除，执行 go mod tidy，清除所有未使用的依赖。 ","date":"2017-03-10","objectID":"/posts/go_package_manage/:0:4","tags":["golang"],"title":"golang包管理","uri":"/posts/go_package_manage/"},{"categories":["笔记"],"content":"goproxy 默认情况下，go 命令直接从 VCS 下载模块。GOPROXY 环境变量允许进一步控制下载源。环境变量将 go 命令配置为使用 Go 模 块代理。 不怕依赖的库被作者删除了。 您不再需要任何 VSC 工具来下载依赖项，因为依赖项是通过 HTTP 提供的 (Go 代理在后台使用 HTTP)。 下载和构建 Go 模块的速度明显加快，因为 Go 代理通过 HTTP 分别提供了源代码 ( .zip 存档) 和 go.mod。与从 VCS 进行提取相比，这导致下载花费更少的时间和更快的时间 (由于更少的开销)。 国内不用梯子了。 在 Go 1.13 中如何使用 goproxy.cn？ 答：一条 go env -w GOPROXY=https://goproxy.cn,direct 即可。之所以在后面拼接一个 ,direct，是因为通过这样做我们可以在一定程度上解决私有库的问题（当然， goproxy.cn 无法访问你的私有库）。这个 GOPROXY 设定的工作原理是：当 go 在抓取目标模块时，若遇见了 404 错误，那么就回退到 direct 也就是直接去目标模块的源头（比如 GitHub） 去抓取。而恰好，GitHub 等类似的代码托管网站的原则基本都是“你无权访问的你来说就是不存在的”，所以我才说通过这样设定可以在一定程度上解决私有库无法通过模块代理访问的问题。 在 Go 1.13 之前如何使用 goproxy.cn？ 答：同样也是设置环境变量即可，但是得你手动配置，而且还不能使用上述的那个 ,direct 后缀，因为那是 Go 1.13 刚加的特性。详细配置方法可以参见 goproxy.cn 的 README 文件。 GOPRIVATE 前面也说到对于一些内部的 package，GoProxy 并不能很好的处理，Go 1.13 推出了 GOPRIVATE 机制。只需要设置这个环境变量，然后标识出哪些 package 是 private 的，那么对于这个 package 的处理将不会从 proxy 下载。GOPRIVATE 的值是一个以逗号分隔的列表，支持正则（正则语法遵守 Golang 的 包 path.Match） GOPRIVATE=*.corp.example.com,rsc.io/private ","date":"2017-03-10","objectID":"/posts/go_package_manage/:0:5","tags":["golang"],"title":"golang包管理","uri":"/posts/go_package_manage/"},{"categories":["笔记"],"content":"context","date":"2017-03-01","objectID":"/posts/go_context/","tags":["golang"],"title":"context","uri":"/posts/go_context/"},{"categories":["笔记"],"content":"为什么要有context 为了在协程之间传递信息，比如网络服务，每个请求都是一个协程，在协程中还可能开启新的子协程，子协程中还有新的协程，我们如果要取消请求的计算，所有的子协程都应该停止。 Go 1.7 标准库引入 context，中文译作“上下文”，准确说它是 goroutine 的上下文，包含 goroutine 的运行状态、环境、现场等信息。 context 主要用来在 goroutine 之间传递上下文信息，包括：取消信号、超时时间、截止时间、k-v 等。 context使用 我们知道context是可以形成一个树形结构，context是可以有父和多个子的。 我们一般用context.Background()和context.TODO()返回值来作为根root。 //一般用于取消子协程 func WithCancel(parent Context) (ctx Context, cancel CancelFunc){} //一般用于给定时间点取消子协程，当然也可以在deadline前主动去取消 WithDeadline(parent Context, d time.Time) (Context, CancelFunc){} //和上面的类似，只不过不是特定的时间点，而是一段时间后取消 func WithTimeout(parent Context, timeout time.Duration) (Context, CancelFunc){} //一般用于在协程间传递数据 func WithValue(parent Context, key, val interface{}) Context{} 上面的4个方法返回context，主要分为取消用和传值用，context一般用于参数传给协程 func main() { deadline, cancelFunc := context.WithDeadline(context.Background(), time.Now().Add(2*time.Second)) go test1(deadline) cancelFunc() time.Sleep(7 * time.Second) } func test1(ctx context.Context) { for { select { case \u003c-ctx.Done(): //context到期或主动取消时，channel就被关闭，零值被取出，语句得到执行 fmt.Println(ctx.Err() == context.DeadlineExceeded) fmt.Println(\"test1 stop\") return default: fmt.Println(\"test1\") time.Sleep(time.Second) } } } 这个演示固定时间点自动取消子协程的代码。 context原理 context.Context是一个接口，比较简单： type Context interface { //获取设置的截止时间 Deadline() (deadline time.Time, ok bool) //返回只读通道，取消后即可从通道中读数据 Done() \u003c-chan struct{} //返回取消原因 Err() error //根据key取到conetext中设置的值 Value(key interface{}) interface{} } Context接口并不需要我们实现，Go内置emptyCtx结构体已经帮我们实现了，我们代码中最开始都是以这两个内置的作为最顶层的partent context，衍生出更多的子Context。 context.Background()和context.TODO()返回的都是emptyCtx。 Done()方法返回的是一个只读的channel，被取消的时候，channel会被父关闭，这样\u003c-channel会返回零值，在此之前一直被阻塞。 父被取消的时候会调用所有子的取消方法，哪怕子还没有到预定的时间。 context通过withX操作可以嵌套，形成一个树，新生成的context继承父的属性和特点，对父进行取消，父及子context均取消。 取值的时候先在当前context中查询，若没有找到，则依次向上查找，通过查看代码得知，是采用深度遍历。 ","date":"2017-03-01","objectID":"/posts/go_context/:0:0","tags":["golang"],"title":"context","uri":"/posts/go_context/"},{"categories":["笔记"],"content":"golang strings bytes","date":"2017-02-10","objectID":"/posts/go_bufio/","tags":["golang"],"title":"golang strings bytes","uri":"/posts/go_bufio/"},{"categories":["笔记"],"content":"bytes包使用 bytes包提供了很多操作tyes切片的方法。多次的连接字符串+=效率低下，应该使用bytes.buffer func main() { bufferString := bytes.NewBufferString(\"a\") bufferString.WriteString(\"b\") fmt.Println(bufferString.String()) } 除此之外，还有比较、查找、前缀、后缀，分割等等。 ","date":"2017-02-10","objectID":"/posts/go_bufio/:0:1","tags":["golang"],"title":"golang strings bytes","uri":"/posts/go_bufio/"},{"categories":["笔记"],"content":"strings包使用 strings包和bytes包类似，对应的，它提供的是操作string的方法。 修改string的值 首先、string本质是一个结构体，有个指针指向不可变的一块区域，这块区域就是实际的字符数据。string本身可以修改即指向其他的数组，但是string里面的元素不可以修改。 可以把string转为[]byte,修改切片后，再转为string string长度 如果直接用len来获取字符串的长度是不准确的，因为它获取的是字节长度 func test() { str:=\"中a\" println(utf8.RuneCountInString(str))//2 } 获取substring golang中没有类似java subString的方法，但是可以直接通过index像slice一样来获取 func test() { str:=\"abc\" println(str[0:1]) } 但是 func test() { str:=\"中国人\" println(str[0:1])//� } 如果有中文，会有乱码的情况，原因和上面的类似，应该先转为[]rune func test() { str:=\"中国人\" runes:=[]rune(str) println(string(runes[0:1]))//中 } 去除左前缀 strings.TrimPrefix(s,“ab”) 去除字符串s中的左前缀ab 去除字符集中字符 strings.TrimLeft(s,“xy”) 从左边开始匹配，发现在\"xy“中的字符就删除，发现不在字符集的字符就停止，并返回后面的字符串。 strings其他 除上面的以外，strings包还提供了分割，大小写转换，统计，包含判断等等操作 ","date":"2017-02-10","objectID":"/posts/go_bufio/:0:2","tags":["golang"],"title":"golang strings bytes","uri":"/posts/go_bufio/"},{"categories":["笔记"],"content":"golang strings bytes","date":"2017-02-10","objectID":"/posts/go_bytes_strings/","tags":["golang"],"title":"golang strings bytes","uri":"/posts/go_bytes_strings/"},{"categories":["笔记"],"content":"bytes包使用 bytes包提供了很多操作tyes切片的方法。多次的连接字符串+=效率低下，应该使用bytes.buffer func main() { bufferString := bytes.NewBufferString(\"a\") bufferString.WriteString(\"b\") fmt.Println(bufferString.String()) } 除此之外，还有比较、查找、前缀、后缀，分割等等。 ","date":"2017-02-10","objectID":"/posts/go_bytes_strings/:0:1","tags":["golang"],"title":"golang strings bytes","uri":"/posts/go_bytes_strings/"},{"categories":["笔记"],"content":"strings包使用 strings包和bytes包类似，对应的，它提供的是操作string的方法。 修改string的值 首先、string本质是一个结构体，有个指针指向不可变的一块区域，这块区域就是实际的字符数据。string本身可以修改即指向其他的数组，但是string里面的元素不可以修改。 可以把string转为[]byte,修改切片后，再转为string string长度 如果直接用len来获取字符串的长度是不准确的，因为它获取的是字节长度 func test() { str:=\"中a\" println(utf8.RuneCountInString(str))//2 } 获取substring golang中没有类似java subString的方法，但是可以直接通过index像slice一样来获取 func test() { str:=\"abc\" println(str[0:1]) } 但是 func test() { str:=\"中国人\" println(str[0:1])//� } 如果有中文，会有乱码的情况，原因和上面的类似，应该先转为[]rune func test() { str:=\"中国人\" runes:=[]rune(str) println(string(runes[0:1]))//中 } 去除左前缀 strings.TrimPrefix(s,“ab”) 去除字符串s中的左前缀ab 去除字符集中字符 strings.TrimLeft(s,“xy”) 从左边开始匹配，发现在\"xy“中的字符就删除，发现不在字符集的字符就停止，并返回后面的字符串。 strings其他 除上面的以外，strings包还提供了分割，大小写转换，统计，包含判断等等操作 ","date":"2017-02-10","objectID":"/posts/go_bytes_strings/:0:2","tags":["golang"],"title":"golang strings bytes","uri":"/posts/go_bytes_strings/"},{"categories":["笔记"],"content":"IP，子网掩码，网关","date":"2017-02-01","objectID":"/posts/network_%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/","tags":["网络"],"title":"IP，子网掩码，网关","uri":"/posts/network_%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/"},{"categories":["笔记"],"content":"经常看到下图，有些概念还是要弄清楚的。 IP 电脑之间要实现网络通信，就必须要有一个合法的ip地址。IP地址 = 网络地址 + 主机地址，（又称：主机号和网络号组成）ip地址的结构使我们可以在Internet上很方便的寻址。ip地址通常用更直观的，以圆点分隔号的四个十进制数字表示，每个数字从0到255，如某一台主机的ip地址为：128.20.4.1在局域网里，同样也需要ip地址，一般内网的ip地址是以192.168开头的，这样很容易区分公网和内网的ip地址。 ip对于一台计算记得的意义可以举个形象的例子： “幸福小区”有若干住户，每个住户都有门牌号，范围是0-255，我们若在小区里要找5号，显然很简单就找到了。但是如果我们在大街上找“幸福小区”肯定就蒙了，因为我们压根不知道去哪个小区的地址。所以我们给小区找个地址，一般就要第一个住户的地址。 网络地址：小区地址，一般是第一个用户的地址。比如192.168.0.0 主机地址：门牌号，比如0.0.0.1 IP地址：网络地址+主机地址，192.168.0.1 子网掩码 子网掩码(subnet mask)又叫网络掩码、地址掩码、子网络遮罩，它是一种用来指明一个IP地址的哪些位标识的是主机所在的子网，以及哪些位标识的是主机的位掩码。子网掩码不能单独存在，它必须结合IP地址一起使用。子网掩码只有一个作用，就是将某个IP地址划分成网络地址和主机地址两部分。 最为简单的理解就是两台计算机各自的ip地址与子网掩码进行\u0026运算后，得出的结果是相同的，则说明这两台计算机是处于同一个子网络上的，可以进行直接的通讯。 举例： A的ip为192.168.0.1，子网掩码为255.255.255.0 B的ip为192.168.0.200，子网掩码为255.255.255.0 ip，子网掩码分别换算为2进制，进行\u0026运算，结果一致则在同一网络，可以进行互连。 网关 上面讲到只有在同一个网络才能进行通信，利用的是子网掩码。那就是需要不在同一个网络的计算器进行互连可怎么办？网关的作用就来了。 ","date":"2017-02-01","objectID":"/posts/network_%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/:0:0","tags":["网络"],"title":"IP，子网掩码，网关","uri":"/posts/network_%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/"},{"categories":["笔记"],"content":"那么网关到底是什么呢？ 网关实质上是一个网络通向其他网络的IP地址。比如有网络A和网络B，网络A的IP地址范围为“192.168.1.1~192. 168.1.254”，子网掩码为255.255.255.0；网络B的IP地址范围为“192.168.2.1~192.168.2.254”，子网掩码为255.255.255.0。在没有路由器的情况下，两个网络之间是不能进行TCP/IP通信的，即使是两个网络连接在同一台交换机（或集线器）上，TCP/IP协议也会根据子网掩码（255.255.255.0）判定两个网络中的主机处在不同的网络里。而要实现这两个网络之间的通信，则必须通过网关。如果网络A中的主机发现数据包的目的主机不在本地网络中，就把数据包转发给它自己的网关，再由网关转发给网络B的网关，网络B的网关再转发给网络B的某个主机。网络B向网络A转发数据包的过程也是如此。所以说，只有设置好网关的IP地址，TCP/IP协议才能实现不同网络之间的相互通信。那么这个IP地址是哪台机器的IP地址呢？网关的IP地址是具有路由功能的设备的IP地址，具有路由功能的设备有路由器、启用了路由协议的服务器（实质上相当于一台路由器）、代理服务器（也相当于一台路由器）。 简单来说，tcp/ip发现两台计算机不在同一个网络，别怕，先发给自己所在网络的网关，利用网关传到另一网络中去，进而传到目标主机。 一台主机可以有多个网关，还会有一个默认网关，当不知道利用那个网关传数据的时候就用这个默认网关。 ","date":"2017-02-01","objectID":"/posts/network_%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/:1:0","tags":["网络"],"title":"IP，子网掩码，网关","uri":"/posts/network_%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/"},{"categories":["笔记"],"content":"设置默认网关 设置默认网关可以手动和自动设置 手动设置不利于迁移，因为迁移时ip地址会改变 自动设置就是利用DHCP服务器来自动给网络中的电脑分配IP地址、子网掩码和默认网关。这样做的好处是一旦网络的默认网关发生了变化时，只要更改了DHCP服务器中默认网关的设置，那么网络中所有的电脑均获得了新的默认网关的IP地址。 私有地址 绝大部分计算机都是在一个内网中，而不是直接分配一个公网ipv4地址，我们可以用ipconfig查看一下本地的吧ip地址，然后对比一下公网ip地址： ipconfig查出来的是你本机的IP地址，也就是内网私有地址，此类地址仅在局域网使用，不能联通外网。百度在线查出来的地址是你上网的公网地址。 总结： 网络通信就好比送快递，商品外面的一层层包裹就是各种协议，协议包含了商品信息、收货地址、收件人、联系方式等，然后还需要配送车、配送站、快递员，商品才能最终到达用户手中。 一般情况下，快递是不能直达的，需要先转发到对应的配送站，然后由配送站再进行派件。 配送车就是物理介质，配送站就是网关， 快递员就是路由器，收货地址就是IP地址，联系方式就是MAC地址。 ","date":"2017-02-01","objectID":"/posts/network_%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/:2:0","tags":["网络"],"title":"IP，子网掩码，网关","uri":"/posts/network_%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/"},{"categories":["笔记"],"content":"快递员负责把包裹转发到各个配送站，配送站根据收获地址里的省市区，确认是否需要继续转发到其他配送站，当包裹到达了目标配送站以后，配送站再根据联系方式找到收件人进行派件。 参考文章： https://www.cnblogs.com/songQQ/archive/2009/05/27/1490612.html ","date":"2017-02-01","objectID":"/posts/network_%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/:3:0","tags":["网络"],"title":"IP，子网掩码，网关","uri":"/posts/network_%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/"},{"categories":["笔记"],"content":"TCP UDP HTTP SOCKET","date":"2017-02-01","objectID":"/posts/network_tcp_udp_http_socket/","tags":["网络"],"title":"TCP UDP HTTP SOCKET","uri":"/posts/network_tcp_udp_http_socket/"},{"categories":["笔记"],"content":"网络可以划分为7层，有时候也会看成4层 各层协议 ","date":"2017-02-01","objectID":"/posts/network_tcp_udp_http_socket/:0:0","tags":["网络"],"title":"TCP UDP HTTP SOCKET","uri":"/posts/network_tcp_udp_http_socket/"},{"categories":["笔记"],"content":"TCP （传输控制协议，Transmission Control Protocol）：(类似打电话) 面向连接、传输可靠（保证数据正确性）、有序（保证数据顺序）、传输大量数据（流模式）、速度慢、对系统资源的要求多，程序结构较复杂， 每一条TCP连接只能是点到点的， TCP首部开销20字节。 ","date":"2017-02-01","objectID":"/posts/network_tcp_udp_http_socket/:0:1","tags":["网络"],"title":"TCP UDP HTTP SOCKET","uri":"/posts/network_tcp_udp_http_socket/"},{"categories":["笔记"],"content":"UDP (用户数据报协议，User Data Protocol)：（类似发短信） 面向非连接 、传输不可靠（可能丢包）、无序、传输少量数据（数据报模式）、速度快，对系统资源的要求少，程序结构较简单 ，不支持重试， UDP支持一对一，一对多，多对一和多对多的交互通信， UDP的首部开销小，只有8个字节。 ","date":"2017-02-01","objectID":"/posts/network_tcp_udp_http_socket/:0:2","tags":["网络"],"title":"TCP UDP HTTP SOCKET","uri":"/posts/network_tcp_udp_http_socket/"},{"categories":["笔记"],"content":"TCP UDP区别 Socket UDP不需要链接，客户端知道服务端的IP和端口号直接发送数据就可以了，Socket TCP由于需要链接所以使用的时候需要心跳机制来确保链接没有断开。 ","date":"2017-02-01","objectID":"/posts/network_tcp_udp_http_socket/:0:3","tags":["网络"],"title":"TCP UDP HTTP SOCKET","uri":"/posts/network_tcp_udp_http_socket/"},{"categories":["笔记"],"content":"HTTP HTTP是应用层协议，它是基于tcp的。 http无状态，即客户端主动请求后断开连接，服务端无法记住上一次客户端的状态。但是可以利用cookie和session来实现。 http有keep-alive字段来标示是否支持长连接。http 1.0默认是短连接，1.1及以后默认是长连接，这样就tcp在http断开后是不断开的，下次http就可以利用tcp连接，减少握手时间。 ","date":"2017-02-01","objectID":"/posts/network_tcp_udp_http_socket/:0:4","tags":["网络"],"title":"TCP UDP HTTP SOCKET","uri":"/posts/network_tcp_udp_http_socket/"},{"categories":["笔记"],"content":"SOCKET 套接字，它并不是协议，而是TCP，UDP网络的API，每个语言基本都有对应的实现。Socket是应用层与TCP/IP协议族通信的中间软件抽象层，它是一组接口。socket是在应用层和传输层之间的一个抽象层，它把TCP/IP层复杂的操作抽象为几个简单的接口供应用层调用已实现进程在网络中通信。 ","date":"2017-02-01","objectID":"/posts/network_tcp_udp_http_socket/:0:5","tags":["网络"],"title":"TCP UDP HTTP SOCKET","uri":"/posts/network_tcp_udp_http_socket/"},{"categories":["笔记"],"content":"Websocket Websocket协议解决了服务器与客户端全双工通信的问题。 **注:什么是单工、半双工、全工通信？ ** 信息只能单向传送为单工； 信息能双向传送但不能同时双向传送称为半双工； 信息能够同时双向传送则称为全双工。 websocket协议解析 wensocket协议包含两部分:一部分是“握手”，一部分是“数据传输”。 ","date":"2017-02-01","objectID":"/posts/network_tcp_udp_http_socket/:0:6","tags":["网络"],"title":"TCP UDP HTTP SOCKET","uri":"/posts/network_tcp_udp_http_socket/"},{"categories":["笔记"],"content":"WebSocket和Socket区别 可以把WebSocket想象成HTTP(应用层)，HTTP和Socket什么关系，WebSocket和Socket就是什么关系。 HTTP 协议有一个缺陷：通信只能由客户端发起，做不到服务器主动向客户端推送信息。 WebSocket 协议在2008年诞生，2011年成为国际标准。所有浏览器都已经支持了。 它的最大特点就是，服务器可以主动向客户端推送信息，客户端也可以主动向服务器发送信息，是真正的双向平等对话，属于服务器推送技术的一种。 参考文章： https://www.cnblogs.com/ghj1976/p/4295346.html https://blog.csdn.net/sinat_31057219/article/details/72872359 https://juejin.im/entry/5a04153751882554b836ff50 ","date":"2017-02-01","objectID":"/posts/network_tcp_udp_http_socket/:0:7","tags":["网络"],"title":"TCP UDP HTTP SOCKET","uri":"/posts/network_tcp_udp_http_socket/"},{"categories":["笔记"],"content":"网络 DNS","date":"2017-02-01","objectID":"/posts/network_dns/","tags":["网络"],"title":"网络 DNS","uri":"/posts/network_dns/"},{"categories":["笔记"],"content":"DNS查询过程 ","date":"2017-02-01","objectID":"/posts/network_dns/:0:0","tags":["网络"],"title":"网络 DNS","uri":"/posts/network_dns/"},{"categories":["笔记"],"content":"迭代查询 迭代查询就是你问我，我不知道，我知道谁知道，我把他的地址告诉你，你去问他。 终端用户DNS客户端向LocalDNS发起递归查询。 LocalDNS检查本地对应缓存无数据后，则向根DNS服务器发起迭代查询请求。任何LocalDNS都需知道根DNS服务器的IP地址（全球共13台）。迭代查询指域名服务器或者给出最终结果，或者告诉DNS客户（此处指LocalDNS）应去哪些DNS服务器查询。 根DNS服务器告诉LocalDNS该域名已授权给com区管理，应去com DNS服务器查询，并给出了com DNS服务器的IP地址。LocalDNS将com DNS服务器及其IP地址加入到缓存，下次DNS请求，在缓存未过期的情况下，授权给com 区的域名需向com DNS服务器请求查询时，直接往com DNS服务器发查询请求，不再向根DNS服务器请求。 LocalDNS服务器根据根DNS服务器响应的结果向com DNS服务器发起迭代查询请求。 com DNS服务器告诉LocalDNS服务器该域名已授权给http://wangsu.com区管理，应去http://wangsu.com DNS服务器查询，并给出http://wangsu.com DNS服务器的IP地址。LocalDNS将http://wangsu.com DNS服务器及其IP地址加入到缓存，下次DNS请求，在缓存未过期的情况下，授权给http://wangsu.com区的域名需向http://wangsu.com DNS服务器请求查询时，直接往对应 DNS服务器发查询请求，不再向根DNS服务器及com DNS服务器请求。 LocalDNS服务器根据 com DNS服务器响应的结果向http://wangsu.com DNS服务器发起迭代查询请求。 http://wangsu.com DNS服务器给出域名的IP地址。LocalDNS将www.wangsu.com IP地址加入到缓存，下次相同的DNS请求，在缓存未过期的情况下，LocalDNS直接给出该域名的IP地址，不再向权威DNS服务器查询。 LocalDNS服务器将该域名对应的IP地址返回给终端用户，DNS客户端将域名对应的IP地址接入缓存。下次请求该域名时，若缓存未过期，DNS客户端直接从缓存取出IP，不再向LocalDNS发起迭代查询。 ","date":"2017-02-01","objectID":"/posts/network_dns/:1:0","tags":["网络"],"title":"网络 DNS","uri":"/posts/network_dns/"},{"categories":["笔记"],"content":"递归查询 你问我，我不知道，我替你去问其他人，问出来了告诉你最终的结果。 递归查询指如果终端用户所请求的LocalDNS服务器不知道被查询的域名的IP地址，则以DNS客户端的身份，向其它域名服务器继续发出查询请求报文(即替主机继续查询)，而不是让主机自己进行下一步查询。因此，递归查询返回的查询结果或者是所要查询的IP地址，或者是报错，表示无法查询到所需的IP地址。\\ 总结 我们知道了DNS的两种查询方法，但实际上，在DNS查询过程中，客户端和服务器也都会加入缓存的机制，这样可以减少查询的次数，加快域名解析过程。当我们在浏览器中输入一个网站时，会发生如下过程 1、浏览器中输入想要访问的网站的域名，操作系统会先检查本地的hosts文件是否有这个网址映射关系，如果有，就先调用这个IP地址映射，完成域名解析。 2、如果hosts里没有这个域名的映射，客户端会向本地DNS服务器发起查询。本地DNS服务器收到查询时，如果要查询的域名包含在本地配置区域资源中，则返回解析结果给客户机，完成域名解析。 3、如果本地DNS服务器本地区域文件与缓存解析都失效，则根据本地DNS服务器的设置，采用递归或者迭代查询，直至解析完成。 ","date":"2017-02-01","objectID":"/posts/network_dns/:2:0","tags":["网络"],"title":"网络 DNS","uri":"/posts/network_dns/"},{"categories":["笔记"],"content":"网络_分层","date":"2017-02-01","objectID":"/posts/network_%E5%88%86%E5%B1%82/","tags":["网络"],"title":"网络_分层","uri":"/posts/network_%E5%88%86%E5%B1%82/"},{"categories":["笔记"],"content":"网络分层 第7层 应用层 应用层（Application Layer）提供为应用软件而设的接口，以设置与另一应用软件之间的通信。例如: HTTP，HTTPS，FTP，TELNET，SSH，SMTP，POP3.HTML.等。 第6层 表示层 表示层（Presentation Layer）把数据转换为能与接收者的系统格式兼容并适合传输的格式。在传输之前是否进行加密或压缩处理。 第5层 会话层 会话层（Session Layer）负责在数据传输中设置和维护计算机网络中两台计算机之间的通信连接。查木马。 第4层 传输层 传输层（Transport Layer）把传输表头（TH）加至数据以形成数据包。传输表头包含了所使用的协议等发送信息。例如:传输控制协议（TCP）等。 第3层 网络层 网络层（Network Layer）决定数据的路径选择和转寄，将网络表头（NH）加至数据包，以形成报文。网络表头包含了网络数据。例如:互联网协议（IP）等。负责规划最佳路径，规划IP地址。 整个包最大65535字节。 第2层 数据链路层 数据链路层（Data Link Layer）负责网络寻址、错误侦测和改错。当表头和表尾被加至数据包时，会形成帧。帧的开始和结束，透明传输，差错校验（但不能纠错，纠错由传输层）。 分为两个子层：逻辑链路控制（logical link control，LLC）子层和介质访问控制（Medium access control，MAC）子层。 这个帧最大1500字节。网络层数据过大就分片。网络层头部分有个“标识”字段，在链路层分片后是可以看到的，相同的“标识”值是同一个ip数据包分割而来的。 第1层 物理层 物理层（Physical Layer）在局部局域网上传送数据帧（data frame），它负责管理计算机通信设备和网络媒体之间的互通。包括了针脚、电压、线缆规范、集线器、中继器、网卡、主机接口卡等。 ","date":"2017-02-01","objectID":"/posts/network_%E5%88%86%E5%B1%82/:0:1","tags":["网络"],"title":"网络_分层","uri":"/posts/network_%E5%88%86%E5%B1%82/"},{"categories":["笔记"],"content":"网络—TCP","date":"2017-02-01","objectID":"/posts/network_tcp/","tags":["网络"],"title":"网络—TCP","uri":"/posts/network_tcp/"},{"categories":["笔记"],"content":"TCP是什么？ TCP（Transmission Control Protocol 传输控制协议）是一种面向连接的、可靠的、基于字节流的传输层通信协议。 首先来看看OSI的七层模型： 在OSI七层模型中，每一层的作用和对应的协议如下： 我们都知道数据从应用层发下来，会在每一层都会加上头部信息，进行封装，然后再发送到数据接收端。接收端的操作就刚好与此相反，它是一层一层剥数据的过程。 每个数据都会经过数据的封装和解封装的过程。 TCP头部 主要字段的作用： Source Port和Destination Port:分别占用16位，表示源端口号和目的端口号；用于区别主机中的不同进程，而IP地址是用来区分不同的主机的，源端口号和目的端口号配合上IP首部中的源IP地址和目的IP地址就能唯一的确定一个TCP连接； Sequence Number:用来标识从TCP发端向TCP收端发送的数据字节流，它表示在这个报文段中的的第一个数据字节在数据流中的序号；主要用来解决网络报乱序的问题； Acknowledgment Number:32位确认序列号包含发送确认的一端所期望收到的下一个序号，比如收到了12345这5个字节的数据，序列号是1，那确认序列号就是6，因为下次需要从第6个字节发了。因此，确认序号应当是上次已成功收到数据字节序号加1。不过，只有当标志位中的ACK标志（下面介绍）为1时该确认序列号的字段才有效。主要用来解决不丢包的问题； Offset:首部长度，需要这个值是因为任选字段的长度是可变的。这个字段占4bit（最大能表示15，但这里1代表4个字节，即首部长度为4*15=60个字节），因此TCP最多有60字节的首部。然而，没有任选字段，正常的长度是20字节； TCP Flags:TCP首部中有6个标志比特，它们中的多个可同时被设置为1，主要是用于操控TCP的状态机的，依次为URG，ACK，PSH，RST，SYN，FIN。每个标志位的意思如下： URG：发送端的缓存窗口中的数据是顺序发送，如果想插队先发送这段数据，可设置该标志位1. ACK：此标志表示应答域有效，就是说前面所说的TCP应答号将会包含在TCP数据包中；有两个取值：0和1，为1的时候表示应答域有效，反之为0； PUSH：这个标志位表示Push操作。所谓Push操作就是指在数据包到达接收端以后，立即传送给应用程序，而不是在缓冲区中排队,意思就是在接收端进行插队，可以和URG类比记忆。 RST：发生了异常，需要重新建立链接。 SYN：表示同步序号，用来建立连接。SYN标志位和ACK标志位搭配使用，当连接请求的时候，SYN=1，ACK=0；连接被响应的时候，SYN=1，ACK=1；这个标志的数据包经常被用来进行端口扫描。扫描者发送一个只有SYN的数据包，如果对方主机响应了一个数据包回来 ，就表明这台主机存在这个端口；但是由于这种扫描方式只是进行TCP三次握手的第一次握手，因此这种扫描的成功表示被扫描的机器不很安全，一台安全的主机将会强制要求一个连接严格的进行TCP的三次握手； FIN： 表示发送端已经达到数据末尾，也就是说双方的数据传送完成，没有数据可以传送了，发送FIN标志位的TCP数据包后，连接将被断开。这个标志的数据包也经常被用于进行端口扫描。 窗口：表示我本地的接收缓存窗口还能接收多少数据。 超时重传 如图所示，接收方接收到数据后要给客户端响应，来告诉别人自己收到了数据。 如果压根就没发到服务端，肯定不会收到服务端的响应，客户端等，等到超时后重传上个丢失的数据。 确认丢失 客户端成功发M1到服务端，服务端响应确认，但是确认包丢失了，客户端一样还会等，重传M1，这时候服务端收到了重复的数据，就会舍弃第二次收到的包，并再发对M1的响应。 确认迟到 客户端在发送数据M1后，服务端发送响应，但是迟到了，客户端没有及时收到响应，超时后重发，服务端舍弃重复收到的数据并响应M1,此时客户端收到迟到的响应，并舍弃响应。 窗口 客户端每次发送后，服务端对其响应，没有收到响应时会等待并重发。这样效率是低下的，由此引入“窗口”来提高效率。 简单来说，在窗口范围内不用先等上次的响应，继续发下面的内容。服务端没有收到数据时会告诉客户端重发。客户端连续三次收到重发标示就会重发失败的数据。 窗口大小是服务端告诉客户端的。 那发送方连续发了好几个包，比如1、2、3、4、5,接收方成功接收到了1,2,3,4,5那么确认号就直接写6,表明我成功接收了5个，下次从6给我发。 发送端有发送缓存窗口，接收端有接收缓存窗口。 流量控制 如果发送方把数据发送得过快，接收方可能会来不及接收，这就会造成数据的丢失。所谓流量控制就是让发送方的发送速率不要太快，要让接收方来得及接收。 利用滑动窗口机制可以很方便地在TCP连接上实现对发送方的流量控制。 设A向B发送数据。在连接建立时，B告诉了A：“我的接收窗口是 rwnd = 400 ”(这里的 rwnd 表示 receiver window) 。因此，发送方的发送窗口不能超过接收方给出的接收窗口的数值。请注意，TCP的窗口单位是字节，不是报文段。 TCP为每一个连接设有一个持续计时器(persistence timer)。只要TCP连接的一方收到对方的零窗口通知，就启动持续计时器。若持续计时器设置的时间到期，就发送一个零窗口控测报文段（携1字节的数据），那么收到这个报文段的一方就重新设置持续计时器 拥塞控制 ","date":"2017-02-01","objectID":"/posts/network_tcp/:0:0","tags":["网络"],"title":"网络—TCP","uri":"/posts/network_tcp/"},{"categories":["笔记"],"content":"慢开始 上面讲到，窗口可以提高传输效率，但是刚开始窗口就比较大，就很可能造成堵塞。所以提出一个慢启动的概念,即刚开始还是先发送1个，慢慢增加，2个、4个、8个、这样指数型增长，指数增长速度极快，到达一个点后，开始慢速增长，这时候换成线性增长，一次增加一个。我们把这个转换的转折点叫做门限ssthresh。发送方维持一个叫做拥塞窗口 cwnd (congestion window)的状态变量。拥塞窗口的大小取决于网络的拥塞程度，并且动态地在变化。 我们图中可以看到，门限变为拥塞窗口的一半，再从1开始慢慢增加。 问题来了，如果网络一直不拥塞就可以一直增大吗？不会，发送的窗口大小还要受接收窗口rwnd大小的限制，所以 客户端发送窗口的上限 = min(rwnd,cwnd) ","date":"2017-02-01","objectID":"/posts/network_tcp/:1:0","tags":["网络"],"title":"网络—TCP","uri":"/posts/network_tcp/"},{"categories":["笔记"],"content":"快重传 又一个问题来了，怎么及时知道网络拥塞了？ 快重传算法首先要求接收方每收到一个失序的报文段后就立即发出重复确认。这样做可以让发送方及早知道有报文段没有到达接收方。 发送方只要一连收到三个重复确认就应当立即重传对方尚未收到的报文段。 不难看出，快重传并非取消重传计时器，而是在某些情况下可更早地重传丢失的报文段。 ","date":"2017-02-01","objectID":"/posts/network_tcp/:2:0","tags":["网络"],"title":"网络—TCP","uri":"/posts/network_tcp/"},{"categories":["笔记"],"content":"快恢复 当发送端收到连续三个重复的确认时，就执行“乘法减小”算法，把慢开始门限 ssthresh 减半。但接下去不执行慢开始算法。 换句话说，拥塞后并不是从1开始慢慢增加，而是从拥塞窗口的一半也就是ssthresh处开始慢慢增加。 三次握手 先举个形象的例子，两个人AB进行电话沟通，为了保证双向是通的，常进行以下对话： A：你能听到我说话吗？ B：可以，你能听到吗？ A：我也可以。 这样双方就知道无论是“去”还是“来”都是正常的，就可以聊天了。 Tcp三次握手于此类似： 客户端：发起请求（SYN） 服务端：响应（ACK）+ 请求（SYN） 客诉端：响应（ACK） 四次挥手 Tcp断开连接时需要4次挥手。 还是先举个例子，两个人进行电话沟通时突然有一个想挂断，一般会有如下对话： A：我不想说了（第一次挥手。A发送 FIN，表示不A不再说什么事情） B：好的 （第二次挥手。B发送ACK表示知道了，但是他可能有话没说话，会继续向A说） B：对了，还有一个事情我要说完…（B没说完话的话继续说） B：说完了，我也不想说了（第三次挥手。B发送FIN表示他也说完了） A：好的（第四次挥手。A发送ACK表示知道了，自此完全断开） 在释放连接时，由于TCP是全双工的，因此最后要由两端分别进行关闭，这个流程如下： 假设Client端发起中断连接请求，也就是发送FIN报文。Server端接到FIN报文后，意思是说\"我Client端没有数据要发给你了”，但是如果你还有数据没有发送完成，则不必急着关闭Socket，可以继续发送数据。所以你先发送ACK，“告诉Client端，你的请求我收到了，但是我还没准备好，请继续你等我的消息”。这个时候Client端就进入FIN_WAIT状态，继续等待Server端的FIN报文。当Server端确定数据已发送完成，则向Client端发送FIN报文，“告诉Client端，好了，我这边数据发完了，准备好关闭连接了”。Client端收到FIN报文后，“就知道可以关闭连接了，但是他还是不相信网络，怕Server端不知道要关闭，所以发送ACK后进入TIME_WAIT状态，如果Server端没有收到ACK则可以重传。“，Server端收到ACK后，“就知道可以断开连接了”。Client端等待了2MSL后依然没有收到回复，则证明Server端已正常关闭，那好，我Client端也可以关闭连接了。Ok，TCP连接就这样关闭了！ 关闭连接有主动关闭和被动关闭一说，这里为了简化理解，我们以客户端作为主动关闭方，服务器为被动关闭方。 三次握手，四次挥手完整流程： 整个过程Client端所经历的状态如下： 而Server端所经历的过程如下： 【注意】 在TIME_WAIT状态中，如果TCP client端最后一次发送的ACK丢失了，它将重新发送。TIME_WAIT状态中所需要的时间是依赖于实现方法的。典型的值为30秒、1分钟和2分钟。等待之后连接正式关闭，并且所有的资源(包括端口号)都被释放。 我们不仅疑问，为啥断开的时候会比握手的时候多一次？ 第二次握手时，SYN+ACK其实可以放一起，但是第二次挥手时ACK 和 ACK却不能放一起，因为B也许有话没说完 SYN Flood 攻击 原理：攻击者首先伪造地址对 服务器发起SYN请求，服务器回应(SYN+ACK)包，而真实的IP会认为，我没有发送请求，不作回应。服务 器没有收到回应，这样的话，服务器不知 道(SYN+ACK)是否发送成功，默认情况下会重试5次（tcp_syn_retries）。这样的话，对于服务器的内存，带宽都有很大的消耗。攻击者 如果处于公网，可以伪造IP的话，对于服务器就很难根据IP来判断攻击者，给防护带来很大的困难。 linux内核参数调优主要有下面三个： tcp_max_syn_backlog 从字面上就可以推断出是什么意思。在内核里有个队列用来存放还没有确认ACK的客户端请求，当等待的请求数大于tcp_max_syn_backlog时，后面的会被丢弃。 所以，适当增大这个值，可以在压力大的时候提高握手的成功率。手册里推荐大于1024。 tcp_synack_retries 这个是三次握手中，服务器回应ACK给客户端里，重试的次数。默认是5。显然攻击者是不会完成整个三次握手的，因此服务器在发出的ACK包在没有回应的情况下，会重试发送。当发送者是伪造IP时，服务器的ACK回应自然是无效的。 为了防止服务器做这种无用功，可以把tcp_synack_retries设置为0或者1。因为对于正常的客户端，如果它接收不到服务器回应的ACK包，它会再次发送SYN包，客户端还是能正常连接的，只是可能在某些情况下建立连接的速度变慢了一点。 tcp_syncookies Linux中SYN cookie是非常巧妙地利用了TCP规范来绕过了TCP连接建立过程的验证过程，从而让服务器的负载可以大大降低。 在三次握手中，当服务器回应（SYN + ACK）包后，客户端要回应一个n + 1的ACK到服务器。其中n是服务器自己指定的。当启用tcp_syncookies时，linux内核生成一个特定的n值，而不并把客户的连接放到半连接的队列里（即没有存储任何关于这个连接的信息）。当客户端提交第三次握手的ACK包时，linux内核取出n值，进行校验，如果通过，则认为这个是一个合法的连接。 面试问题 问题1. 为什么是四次挥手 发送FIN的一方就是主动关闭(客户端)，而另一方则为被动关闭(服务器)。当一方发送了FIN，则表示在这一方不再会有数据的发送。其中当被动关闭方受到对方的FIN时，此时往往可能还有数据需要发送过去，因此无法立即发送FIN(也就是无法将FIN与ACK合并发送)， 而是在等待自己的数据发送完毕后再单独发送FIN，因此整个过程需要四次交互。 问题2. 什么是半关闭 客户端在收到第一个FIN的ACK响应后，会进入FINWAIT2 状态时，此时服务器处于 CLOSEWAIT状态，这种状态就称之为半关闭。从半关闭到全关闭，需要等待第二次FIN的确认才算结束。此时，客户端要等到服务器的FIN才能进入TIMEWAIT， 如果对方迟迟不发送FIN呢，则会等待一段时间后超时，这个可以通过内核参数tcpfin_timeout控制，默认是60s。 问题3. RST 是什么，为什么会出现 RST 是一个特殊的标记，用来表示当前应该立即终止连接。以下这些情况都会产生RST： 向一个未被监听的端口发送数据 对方已经调用 close 关闭连接 存在一些数据未处理(接收缓冲区)，请求关闭连接时，会发送RST强制关闭 某些请求发生了超时 题4.为什么需要TIME_WAIT状态需要经过2MSL(最大报文段生存时间)才能返回到CLOSE状态？ 答：TIME_WAIT是主动提出结束的一方的状态，对方发送FIN说“我也说完了”，这边回应ACK，但是这个ACK可能会丢失，那么对方就会以为你没有收到，他会重发。如果不经历2MSL直接进入CLOSE状态，新的连接建立后，你会莫名收到一个FIN。 四次挥手中，A 发 FIN， B 响应 ACK，B 再发 FIN，A 响应 ACK 实现连接的关闭。而如果 A 响应的 ACK 包丢失，B 会以为 A 没有收到自己的关闭请求，然后会重试向 A 再发 FIN 包。 如果没有 TIME_WAIT 状态，A 不再保存这个连接的信息，收到一个不存在的连接的包，A 会响应 RST 包，导致 B 端异常响应。 此时， TIME_WAIT 是为了保证全双工的 TCP 连接正常终止。 题5.为什么TIME_WAIT状态需要经过2MSL才能返回到CLOSE状态？一个MSL行不行？ 答：一个不行。 最大分段寿命（MSL, Maximum Segment Lifetime），它表示一个 TCP 分段可以存在于互联网系统中的最大时间，由 TCP 的实现，超出这个寿命的分片都会被丢弃。 A主动提出结束。第四次挥手，A向B回复 ack。B如果没有收到会重发FIN，所以A要等待足够的时间来判断B会不会发FIN。 A并不知道B是否接到自己的ACK，A是这么想的：1）如果B没有收到自己的ACK，会超时重传FiN那么A再次接到重传的FIN，会再次发送ACK2）如果B收到自己的ACK，也不会再发任何消息，包括ACK无论是1还是2，A都需要等待，要取这两种情况等待时间的最大值，以应对最坏的情况发生，这个最坏情况是：去向ACK消息最大存活时间（MSL) + 来向FIN消息的最大存活时间(MSL)。这恰恰就是2MSL( Maximum Segment Life)。 参考文章： https://blog.csdn.net/Neo233/article/details/72866230 https://blog.csdn.net/hacker00011000/article/details/52319111 https://www.zhihu.com/question/67013338/answer/248375813 ","date":"2017-02-01","objectID":"/posts/network_tcp/:3:0","tags":["网络"],"title":"网络—TCP","uri":"/posts/network_tcp/"},{"categories":["笔记"],"content":"网络—UDP","date":"2017-02-01","objectID":"/posts/network_udp/","tags":["网络"],"title":"网络—UDP","uri":"/posts/network_udp/"},{"categories":["笔记"],"content":"UDP数据包的理论长度 udp数据包的理论长度是多少，合适的udp数据包应该是多少呢？从TCP-IP详解卷一第11章的udp数据包的包头可以看出，udp的最大包长度是2^16-1的个字节。由于udp包头占8个字节，而在ip层进行封装后的ip包头占去20字节，所以这个是udp数据包的最大理论长度是2^16-1-8-20=65507。 然而这个只是udp数据包的最大理论长度。UDP属于运输层，在传输过程中，udp包的整体是作为下层协议的数据字段进行传输的，它的长度大小受到下层ip层和数据链路层协议的制约。 ","date":"2017-02-01","objectID":"/posts/network_udp/:0:1","tags":["网络"],"title":"网络—UDP","uri":"/posts/network_udp/"},{"categories":["笔记"],"content":"网络—传输层","date":"2017-02-01","objectID":"/posts/network_transport_layer/","tags":["网络"],"title":"网络—传输层","uri":"/posts/network_transport_layer/"},{"categories":["笔记"],"content":"传输层 传输层为应用进程之间提供端到端的逻辑通信。（网络层是主机之间提供逻辑通信） tcp:传输控制协议。数据需要分段传输，建立会话（知道传了几个段），可靠传输，流量控制。 udp:用户数据报协议。数据不需要分段传输，一个数据包就够了，不需要会话，也不需要流量控制，不可靠，毕竟就一个数据包。dns域名解析就是使用udp。 我们可以根据传输数据的大小来粗略判断传输层使用哪种协议。比如发邮件、ftp文件可能比较大，一个包肯定不能完成，就要用tcp。 ","date":"2017-02-01","objectID":"/posts/network_transport_layer/:0:1","tags":["网络"],"title":"网络—传输层","uri":"/posts/network_transport_layer/"},{"categories":["笔记"],"content":"传输层端口 传输层协议+端口就能表示应用层协议 tpc + 80 = http tcp + 443 = https tcp + 445 = ftp udp + 53 = dns 端口范围0-65535，其中： 熟知端口0-1023 登记端口1024-49151 客户端端口49152-65535 ","date":"2017-02-01","objectID":"/posts/network_transport_layer/:0:2","tags":["网络"],"title":"网络—传输层","uri":"/posts/network_transport_layer/"},{"categories":["笔记"],"content":"网络—网络层","date":"2017-02-01","objectID":"/posts/network_network_layer/","tags":["网络"],"title":"网络—网络层","uri":"/posts/network_network_layer/"},{"categories":["笔记"],"content":"网络层 网络层负责在不同网络之间尽力转发数据包，基于数据包的ip地址转发。 不负责数据的丢失重传，丢就丢了，也不负责顺序，这些都是上层传输层该干的事。 ","date":"2017-02-01","objectID":"/posts/network_network_layer/:0:1","tags":["网络"],"title":"网络—网络层","uri":"/posts/network_network_layer/"},{"categories":["笔记"],"content":"ip协议 静态路由：路由器判断ip数据包要想到达目的地下一步需要去哪个路由器，可以有静态的路由表。 动态路由：网段过多，静态路由显然不合适，而且网络变化静态路由不会跟着变化。动态路由可以自己学习。 动态路由有RIP协议，可以判断最佳路由路径。 ","date":"2017-02-01","objectID":"/posts/network_network_layer/:0:2","tags":["网络"],"title":"网络—网络层","uri":"/posts/network_network_layer/"},{"categories":["笔记"],"content":"ARP协议 网络层中为ip协议服务，主要作用是在同一网段中通过ip找到对应的计算机的mac地址。实现方式是通过广播，当计算机中收到数据时和自己的ip比对，如果一致就返回自己的mac地址。 有恶意的计算机会假装满足所有的ip，返回自己的mac就能收到后面的数据，或者返回一个不存在的mac的地址，造成别人无法正常通讯。 ","date":"2017-02-01","objectID":"/posts/network_network_layer/:0:3","tags":["网络"],"title":"网络—网络层","uri":"/posts/network_network_layer/"},{"categories":["笔记"],"content":"ICMP协议 ICMP协议是网络层在ip上层的协议，主要用于检测网络层是否畅通。 ping（网络包嗅探器）命令就是其协议的应用。 ping命令结果中有ttl，其含义是每经过一个路由器ttl就减少1，当减少到0的时候就不往下传了，这样可以避免路由循环占用流量。ttl的开始数据根据操作系统不同而不同。 ","date":"2017-02-01","objectID":"/posts/network_network_layer/:0:4","tags":["网络"],"title":"网络—网络层","uri":"/posts/network_network_layer/"},{"categories":["笔记"],"content":"IGMP协议 多播组播 ","date":"2017-02-01","objectID":"/posts/network_network_layer/:0:5","tags":["网络"],"title":"网络—网络层","uri":"/posts/network_network_layer/"},{"categories":["笔记"],"content":"ip数据包 由首部和数据两部分组成.首部的前一部分是固定长度,共 20 字节,是所有IP数据报必须具有的.在首部的固定部分的后面是一些可选字段,其长度是可变的。ipv6已经将首部长度改为固定的，去掉可选部分。 版本:占4位,指IP协议的版本.通信双方使用的IP协议版本必须一致.日前广泛使用的 IP协议版本号为 4 (即 IPv4).IPv6 目前还处于起步阶段. 首部长度:占 4 位。 总长度:总长度指首都及数据之和的长度,单位为字节.因为总长度字段为 16位,所以数据报的最大长度为 216-1=65 535字节.在IP层下面的每一种数据链路层都有自己的帧格式,其中包括帧格式中的数据字段的最大长度,即最大传送单元 MTU (Maximum Transfer Unit).当一个数据报封装成链路层的帧时,此数据报的总长度 (即首部加上数据部分)一定不能超过下面的数据链路层的MTU值,否则要分片. 标识 (Identification):占 16位.IP软件在存储器中维持一个计数器,每产生一个数据报,计数器就加 1,并将此值赋给标识字段.但这个\"标识\"并不是序号,因为 IP是无连接的服务,数据报不存在按序接收的问题.当数据报由于长度超过网络的 MTU 而必须分片时,这个标识字段的值就被复制到所有分片后的数据报的标识字段中.相同的标识字段的值使分片后的各数据报片最后能正确地重装成为原来的数据报. 标志 (Flag):占3 位,但目前只有2位有意义. 标志字段中的最低位记为 MF(More Fragment).MF=1即表示后面\"还有分片\"的数据报.MF=0表示这已是若干数据报片中的最后一个.标志字段中间的一位记为DF(Don’t Fragment),意思是\"不能分片”,只有当 DF=0时才允许分片. 片偏移:占 13位.较长的分组在分片后,某片在原分组中的相对位置.也就是说,相对用户数据字段的起点,该片从何处开始.片偏移以 8个字节为偏移单位,这就是说,每个分片的长度一定是 8字节(64位)的整数倍. 生存时间:占 8位,生存时间字段常用的英文缩写是TTL(Time To Live),其表明数据报在网络中的寿命. 协议:占 8 位.协议字段指出此数据报携带的数据是使用何种协议,以便使目的主机的IP层知道应将数据部分上交给哪个处理过程。比如交给tcp还是udp。TCP的协议号为6，UDP的协议号为17。ICMP的协议号为1，IGMP的协议号为2. ","date":"2017-02-01","objectID":"/posts/network_network_layer/:0:6","tags":["网络"],"title":"网络—网络层","uri":"/posts/network_network_layer/"},{"categories":["笔记"],"content":"HTTP格式","date":"2017-01-02","objectID":"/posts/network_http/","tags":["网络"],"title":"HTTP格式","uri":"/posts/network_http/"},{"categories":["笔记"],"content":"http请求格式 示例： GET /mix/76.html?name=kelvin\u0026password=123456 HTTP/1.1 Host: www.fishbay.cn Upgrade-Insecure-Requests: 1 User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36 Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8 Accept-Encoding: gzip, deflate, sdch Accept-Language: zh-CN,zh;q=0.8,en;q=0.6 ","date":"2017-01-02","objectID":"/posts/network_http/:0:1","tags":["网络"],"title":"HTTP格式","uri":"/posts/network_http/"},{"categories":["笔记"],"content":"http响应格式 HTTP/1.1 200 OK Server: nginx Date: Mon, 20 Feb 2017 09:13:59 GMT Content-Type: text/plain;charset=UTF-8 Vary: Accept-Encoding Cache-Control: no-store Pragrma: no-cache Expires: Thu, 01 Jan 1970 00:00:00 GMT Cache-Control: no-cache Content-Encoding: gzip Transfer-Encoding: chunked Proxy-Connection: Keep-alive {\"name\":\"sss\",\"age\":1} ","date":"2017-01-02","objectID":"/posts/network_http/:0:2","tags":["网络"],"title":"HTTP格式","uri":"/posts/network_http/"},{"categories":["笔记"],"content":"响应状态码 1xx: 指示信息–表示请求已接收，继续处理 2xx: 成功–表示请求已被成功接收、理解、接受 3xx: 重定向–要完成请求必须进行更进一步的操作 4xx: 客户端错误–请求有语法错误或请求无法实现 5xx: 服务器端错误–服务器未能实现合法的请求 ","date":"2017-01-02","objectID":"/posts/network_http/:0:3","tags":["网络"],"title":"HTTP格式","uri":"/posts/network_http/"},{"categories":["笔记"],"content":"http一次流程 一次HTTP请求的过程： 在浏览器中输入URL，并按下回车键 浏览器向DNS服务器请求解析该URL中的域名对应的IP地址(如果是IP请求，则不需要该步骤) 解析出IP后，根据IP和端口号，和服务器建立TCP连接 浏览器向服务器发送请求，该请求报文作为TCP三次握手的第三个报文发送给服务器 服务器做出响应，把数据发送给浏览器 通信完成，断开TCP连接 浏览器解析收到的数据并显示 ","date":"2017-01-02","objectID":"/posts/network_http/:0:4","tags":["网络"],"title":"HTTP格式","uri":"/posts/network_http/"},{"categories":["笔记"],"content":"常用请求头 请求头 说明 示例 Accept 可接受的响应内容类型（Content-Types）。 Accept: text/plain Accept-Charset 可接受的字符集 Accept-Charset: utf-8 Accept-Encoding 可接受的响应内容的编码方式。 Accept-Encoding: gzip, deflate Accept-Language 可接受的响应内容语言列表。 Accept-Language: en-US Cache-Control 用来指定当前的请求/回复中的，是否使用缓存机制。 Cache-Control: no-cache Connection 客户端（浏览器）想要优先使用的连接类型 Connection: keep-alive Connection: Upgrade Cookie 由之前服务器通过Set-Cookie（见下文）设置的一个HTTP协议Cookie Cookie: $Version=1; Skin=new; Content-Length 以8进制表示的请求体的长度 Content-Length: 348 Content-Type 请求体的MIME类型 （用于POST和PUT请求中） Content-Type: application/x-www-form-urlencoded Range 表示请求某个实体的一部分，字节偏移以0开始。 Range: bytes=500-999 Referer 表示浏览器所访问的前一个页面，可以认为是之前访问页面的链接将浏览器带到了当前页面。Referer其实是Referrer这个单词，但RFC制作标准时给拼错了，后来也就将错就错使用Referer了。 Referer: http://itbilu.com/nodejs 我们可以看到这里规律就是多个单词首字母大写，中间中划线间隔。 ","date":"2017-01-02","objectID":"/posts/network_http/:0:5","tags":["网络"],"title":"HTTP格式","uri":"/posts/network_http/"},{"categories":["笔记"],"content":"响应头 请求头 说明 示例 Cache-Control 通知从服务器到客户端内的所有缓存机制，表示它们是否可以缓存这个对象及缓存有效时间。其单位为秒 Cache-Control: max-age=3600 Content-Encoding 响应资源所使用的编码类型。 Content-Encoding: gzip Content-Language 响就内容所使用的语言 Content-Language: zh-cn Content-Length 响应消息体的长度，用8进制字节表示 Content-Length: 348 Content-Type 当前内容的MIME类型 Content-Type: text/html; charset=utf-8 Location 用于在进行重定向，或在创建了某个新资源时使用。 Location: http://www.itbilu.com/nodejs Refresh 用于重定向，或者当一个新的资源被创建时。默认会在5秒后刷新重定向。 Refresh: 5; url=http://itbilu.com Set-Cookie 设置HTTP cookie Set-Cookie: UserID=itbilu; Max-Age=3600; Version=1 ","date":"2017-01-02","objectID":"/posts/network_http/:0:6","tags":["网络"],"title":"HTTP格式","uri":"/posts/network_http/"},{"categories":["笔记"],"content":"HTTPS","date":"2017-01-01","objectID":"/posts/network_https/","tags":["网络"],"title":"HTTPS","uri":"/posts/network_https/"},{"categories":["笔记"],"content":"为什么要有https http是明文，容易被监听拦截。 加密呗，用对称加密，秘钥传输的时候明文传输，会被监听拦截。 可以使用非对称加密，但是非对称加密过程慢，可以采用非对称+对称加密组合的方式，即用非对称加密方式来协商对称加密秘钥。 具体是这样子的： 服务器用明文的方式给客户端发送自己的公钥，客户端收到公钥之后，会生成一把密钥(对称加密用的)，然后用服务器的公钥对这把密钥进行加密，之后再把密钥传输给服务器，服务器收到之后进行解密，最后服务器就可以安全着得到这把密钥了，而客户端也有同样一把密钥，他们就可以进行对称加密了。 最后中间人再对这把密钥用刚才服务器的公钥进行加密，再发给服务器。如图： 毫无疑问，在这个过程中，中间人获取了对称加密中的密钥，在之后服务器和客户端的对称加密传输中，这些加密的数据对中间人来说，和明文没啥区别。 非对称加密之所以不安全，是因为客户端不确定这个公钥是来自正确的服务器，解决了这个问题就好办了。数字证书登场。 我们需要找到一个拥有公信力、大家都认可的认证中心(CA)。 服务器在给客户端传输公钥的过程中，会把公钥以及服务器的个人信息通过Hash算法生成信息摘要。如图 并且，最后还会把原来没Hash算法之前的个人信息以及公钥 和 数字签名合并在一起，形成数字证书。如图 当客户端拿到这份数字证书之后，就会用CA提供的公钥来对数字证书里面的数字签名进行解密来得到信息摘要，然后对数字证书里服务器的公钥以及个人信息进行Hash得到另外一份信息摘要。最后把两份信息摘要进行对比，如果一样，则证明这个人是服务器，否则就不是。如图： 什么是https https简单来说就是https = http + ssl(tls) 这样，http在到达tcp层之前就加密一次 简单一句话就是：https先利用非对称加密（RSA等）来协商对称秘钥，后续就用对称加密算法（AES等）来加密传输。 数字证书 服务端如果直接把公钥明文传输给客户端，容易被中间人接收。客户端必须知道接收的公钥是合法的。为了解决这个问题，把公钥告诉权威机构，来生成数字证书以便客户端验证。 验证证书的过程如下： 通过 HTTPS 建立了一个安全 Web 事务之后，现代的浏览器都会自动获取所连接服 务器的数字证书。如果服务器没有证书，安全连接就会失败。 浏览器收到证书时会对签名颁发机构进行检查。如果这个机构是个很有权威的公共签名机构，浏览器可能已经知道其公开密钥了(浏览器会预先安装很多签名颁发机构的证书)。 如果对签名颁发机构一无所知，浏览器就无法确定是否应该信任这个签名颁发机构， 它通常会向用户显示一个对话框，看看他是否相信这个签名发布者。签名发布者可 能是本地的 IT 部门或软件厂商。 数字证书校验一般是校验公钥是否正确，域名、有效期和是否被吊销等。 具体的证书生成及验证流程： CA 把有效日期，服务端公钥等基础信息hash取数据摘要，并对摘要进行CA私钥加密生成数字签名，签名和基础信息一同组成数字证书。 客户端一般有知名的CA公钥，先判断是否有对应的CA公钥，如果没有直接弹框不信任。如果有则对证书的签名进行CA公钥解密，然后按照和CA相同的hash方法对基础信息取得摘要，摘要和CA公钥解密的结果对比发现是一致的，则说明数据没有篡改过，顺利取得了服务端的公钥。这个方法很巧妙，即获得了公钥，而且还知道公钥是靠谱的。 数字签名 数字签名技术就是对“非对称密钥加解密”和“数字摘要“两项技术的应用，它将摘要信息用发送者的私钥加密，与原文一起传送给接收者。接收者只有用发送者的公钥才能解密被加密的摘要信息，然后用HASH函数对收到的原文产生一个摘要信息，与解密的摘要信息对比。如果相同，则说明收到的信息是完整的，在传输过程中没有被修改，否则说明信息被修改过，因此数字签名能够验证信息的完整性。 数字签名的过程如下： 明文 –\u003e hash运算 –\u003e 摘要 –\u003e 私钥加密 –\u003e 数字签名 数字签名有两种功效： 一、能确定消息确实是由发送方签名并发出来的，因为别人假冒不了发送方的签名。 二、数字签名能确定消息的完整性。 注意： 数字签名只能验证数据的完整性，数据本身是否加密不属于数字签名的控制范围 ","date":"2017-01-01","objectID":"/posts/network_https/:0:0","tags":["网络"],"title":"HTTPS","uri":"/posts/network_https/"},{"categories":["笔记"],"content":"ca数字证书 我们知道了服务端端的数字证书是来自ca的。验证服务端证书的时候需要用ca的公钥进行解密，然后校验。 怎么知道ca的公钥是安全的，不是伪造的？ 世界上的CA认证中心不止一家， CA认证中心之间是一个树状结构，根CA认证中心可以授权多个二级的CA认证中心，同理二级CA认证中心也可以授权多个3级的CA认证中心…如果你是数字证书申请人(比如说：交通银行)，你可以向根CA认证中心，或者二级，三级的CA认证中心申请数字证书，这是没有限制的，当你成功申请后，你就称为了数字证书所有人。值得注意的是，根CA认证中心是有多个的，也就是说会有多棵这样的结构树。 实际上每个CA认证中心/数字证书所有人，他们都有一个数字证书，和属于自己的RSA公钥和密钥，这些是他们的父CA认证中心给他们颁发的。 CA的数字证书和服务端的数字证书差不多，上一级Ca的秘钥对Hash(自己的ca信息)的结果进行加密。校验的时候需要上一级的公钥来解密，以此来验证该ca的证书是真的，那上一级的公钥怎么保证是安全的呢？就要再找上上一级的公钥，这样递归了，直到找到根证书，那根证书怎么保证呢，一般操作系统中都内置了根证书。 SSL握手流程 握手阶段分成五步。 第一步，爱丽丝给出协议版本号、一个客户端生成的随机数（Client random），以及客户端支持的加密方法。 第二步，鲍勃确认双方使用的加密方法，并给出数字证书、以及一个服务器生成的随机数（Server random）。 第三步，爱丽丝确认数字证书有效，然后生成一个新的随机数（Premaster secret），并使用数字证书中的公钥，加密这个随机数，发给鲍勃。 第四步，鲍勃使用自己的私钥，获取爱丽丝发来的随机数（即Premaster secret）。 第五步，爱丽丝和鲍勃根据约定的加密方法，使用前面的三个随机数，生成\"对话密钥”（session key），用来加密接下来的整个对话过程。 注意： 整个握手阶段都不加密（也没法加密），都是明文的。因此，如果有人窃听通信，他可以知道双方选择的加密方法，以及三个随机数中的两个。整个通话的安全，只取决于第三个随机数（Premaster secret）能不能被破解。 ","date":"2017-01-01","objectID":"/posts/network_https/:0:1","tags":["网络"],"title":"HTTPS","uri":"/posts/network_https/"},{"categories":["笔记"],"content":"服务端对客户端验证 对于非常重要的保密数据，服务端还需要对客户端进行验证，以保证数据传送给了安全的合法的客户端。服务端可以向客户端发出 Cerficate Request 消息，要求客户端发送证书对客户端的合法性进行验证。比如，金融机构往往只允许认证客户连入自己的网络，就会向正式客户提供USB密钥，里面就包含了一张客户端证书。 抓包原理 HTTPS即使安全，也是能够被抓包的，常见的抓包工具有：Charles、fildder等。 常用的HTTPS抓包方式是作为中间人，对客户端伪装成服务端，对服务端伪装成客户端。简单来说： 截获客户端的HTTPS请求，伪装成中间人客户端去向服务端发送HTTPS请求 接受服务端返回，用自己的证书伪装成中间人服务端向客户端发送数据内容。 具体过程如下图所示： 反抓包策略 为了防止中间人攻击，可以使用SSL-Pinning的技术来反抓包。 可以发现中间人攻击的要点的伪造了一个假的服务端证书给了客户端，客户端误以为真。解决思路就是，客户端也预置一份服务端的证书，比较一下就知道真假了。 SSL-pinning有两种方式： 证书锁定（Certificate Pinning） 和公钥锁定（ Public Key Pinning）。 证书锁定 需要在客户端代码内置仅接受指定域名的证书，而不接受操作系统或浏览器内置的CA根证书对应的任何证书，通过这种授权方式，保障了APP与服务端通信的唯一性和安全性，因此客户端与服务端（例如API网关）之间的通信是可以保证绝对安全。但是CA签发证书都存在有效期问题，缺点是在 证书续期后需要将证书重新内置到APP中。 公钥锁定 提取证书中的公钥并内置到客户端中，通过与服务器对比公钥值来验证连接的正确性。制作证书密钥时，公钥在证书的续期前后都可以保持不变（即密钥对不变），所以可以避免证书有效期问题，一般推荐这种做法。 没有绝对的安全，这不能挡住逆向反编译。 问题 HTTPS和HTTP的区别 https协议需要到CA申请证书。 http是超文本传输协议，信息是明文传输；https 则是具有安全性的ssl加密传输协议。 http和https使用的是完全不同的连接方式，用的端口也不一样，前者是80，后者是443。 http的连接很简单，是无状态的；HTTPS协议是由SSL+HTTP协议构建的可进行加密传输、身份认证的网络协议，比http协议安全。 抓包 Q: 使用 HTTPS 会被抓包吗？ A: 会被抓包，HTTPS 只防止用户在不知情的情况下通信被监听，如果用户主动授信，是可以构建“中间人”网络，代理软件可以对传输内容进行解密。 我们使用抓包工具的第一步就是在你自己设备中信任 Charles 的 CA 证书，在自己的设备中添加了一个 CA，请求的时候，Charles 通过自己的 CA 签名了一个自己的公钥，发送给客户端，客户端就误以为是服务器了，这样之后的流程都会先走到 Charles 然后才会走到目标服务器。 Charles 扮演了一个中间人的角色，而且这个中间人是我们自己设置的。 因此要想防止抓包，应用应该自己做处理。 为什么一定要用三个随机数来生成”会话密钥”呢？ 机器产生的随机数也许是一个范围的伪随机数，容易被猜到破解，如果随机数不随机，那么premaster secret就有可能被猜出来，那么仅适用premaster secret作为密钥就不合适了，因此必须引入新的随机因素，那么客户端和服务器加上premaster secret三个随机数一同生成的密钥就不容易被猜出了，一个伪随机数可能完全不随机，可是三个伪随机数就十分接近随机了。 参考资料： http://www.wxtlife.com/2016/03/27/%E8%AF%A6%E8%A7%A3https%E6%98%AF%E5%A6%82%E4%BD%95%E7%A1%AE%E4%BF%9D%E5%AE%89%E5%85%A8%E7%9A%84%EF%BC%9F/ https://mp.weixin.qq.com/s?__biz=Mzg2NzA4MTkxNQ==\u0026mid=2247485216\u0026idx=1\u0026sn=fd119ae8e9d184a81cc1dd2984e8d4b8\u0026scene=21#wechat_redirect ","date":"2017-01-01","objectID":"/posts/network_https/:0:2","tags":["网络"],"title":"HTTPS","uri":"/posts/network_https/"}]